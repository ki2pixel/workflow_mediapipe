```json
{
  "min": 100,    // Taille minimale de chunk en frames (null pour d√©faut)
  "max": 500     // Taille maximale de chunk en frames (null pour d√©faut)
}
```

**Param√®tres** :
- `min` : `int|null` ‚Äî Taille minimale de chunk. Doit √™tre un entier positif ou `null`.
- `max` : `int|null` ‚Äî Taille maximale de chunk. Doit √™tre un entier positif ou `null`.

**Response** (Success) :
```json
{
  "status": "success",
  "message": "Chunk bounds updated",
  "min": 100,
  "max": 500
}
```

**Response** (Error) :
```json
{
  "status": "error",
  "message": "Param√®tres invalides (min/max)"
}
```

**Status Codes** :
- `200` : Succ√®s
- `400` : Param√®tres invalides (non-entiers, n√©gatifs)
- `500` : Erreur interne

**Impl√©mentation** :

```python
# routes/api_routes.py
@api_bp.route('/step5/chunk_bounds', methods=['POST'])
@measure_api('/api/step5/chunk_bounds')
def set_step5_chunk_bounds():
    """Configure les limites de chunk pour Step5."""
    payload = request.get_json(silent=True) or {}
    min_val = payload.get('min', None)
    max_val = payload.get('max', None)
    
    def _norm(v):
        if v is None:
            return None
        if isinstance(v, int):
            return v
        try:
            return int(v)
        except:
            return 'invalid'
    
    min_norm = _norm(min_val)
    max_norm = _norm(max_val)
    
    if min_norm == 'invalid' or max_norm == 'invalid':
        return jsonify({"status": "error", "message": "Param√®tres invalides"}), 400
    
    result = WorkflowService.set_step5_chunk_bounds(min_norm, max_norm)
    if result.get('status') != 'success':
        return jsonify(result), 400
    
    return jsonify(result)
```

**Utilisation** :

```bash
curl -X POST http://localhost:5000/api/step5/chunk_bounds \
  -H "Content-Type: application/json" \
  -d '{"min": 100, "max": 500}'

curl -X POST http://localhost:5000/api/step5/chunk_bounds \
  -H "Content-Type: application/json" \
  -d '{"min": null, "max": null}'
```

**Notes** :
- Route instrument√©e via `@measure_api()` pour monitoring des performances
- D√©l√©gation compl√®te √† `WorkflowService.set_step5_chunk_bounds()`
- Validation stricte des param√®tres c√¥t√© route
- Support de `null` pour reset aux valeurs par d√©faut

## Sp√©cifications Techniques

### Environnement Virtuel
- **Environnement utilis√©** : `tracking_env/` (sp√©cialis√© MediaPipe)
- **Activation** : `source tracking_env/bin/activate`
- **Isolation** : Environnement d√©di√© pour MediaPipe et OpenCV

### Technologies et Biblioth√®ques Principales

#### MediaPipe et Computer Vision
```python
import mediapipe as mp                    # Framework de ML pour vision
from mediapipe.tasks import python       # API Python MediaPipe
from mediapipe.tasks.python import vision # T√¢ches de vision
import cv2                               # OpenCV pour traitement d'image
import numpy as np                       # Calculs num√©riques
```

#### Traitement Parall√®le et Optimisations
```python
from concurrent.futures import ThreadPoolExecutor  # Multi-threading
import multiprocessing                              # Multi-processing
import threading                                    # Synchronisation
import queue                                        # Communication inter-threads
```

#### Modules Personnalis√©s
```python
from utils.tracking_optimizations import apply_tracking_and_management
from utils.enhanced_speaking_detection import EnhancedSpeakingDetector
from utils.resource_manager import safe_video_processing
```

### Formats d'Entr√©e et de Sortie

#### Structure d'Entr√©e Attendue
```
projets_extraits/
‚îú‚îÄ‚îÄ projet_camille_001/
‚îÇ   ‚îî‚îÄ‚îÄ docs/
‚îÇ       ‚îú‚îÄ‚îÄ video1.mp4           # Vid√©o source
‚îÇ       ‚îú‚îÄ‚îÄ video1.csv           # Sc√®nes (STEP3)
‚îÇ       ‚îú‚îÄ‚îÄ video1_audio.json    # Analyse audio (STEP4)
‚îÇ       ‚îú‚îÄ‚îÄ video2.mov           # Vid√©o source
‚îÇ       ‚îú‚îÄ‚îÄ video2.csv           # Sc√®nes (STEP3)
‚îÇ       ‚îî‚îÄ‚îÄ video2_audio.json    # Analyse audio (STEP4)
```

#### Structure de Sortie G√©n√©r√©e
```
projets_extraits/
‚îú‚îÄ‚îÄ projet_camille_001/
‚îÇ   ‚îî‚îÄ‚îÄ docs/
‚îÇ       ‚îú‚îÄ‚îÄ video1.mp4
‚îÇ       ‚îú‚îÄ‚îÄ video1.csv
‚îÇ       ‚îú‚îÄ‚îÄ video1_audio.json
‚îÇ       ‚îú‚îÄ‚îÄ video1_tracking.json     # Donn√©es de tracking
‚îÇ       ‚îú‚îÄ‚îÄ video2.mov
‚îÇ       ‚îú‚îÄ‚îÄ video2.csv
‚îÇ       ‚îú‚îÄ‚îÄ video2_audio.json
‚îÇ       ‚îî‚îÄ‚îÄ video2_tracking.json     # Donn√©es de tracking
```

### Param√®tres de Configuration

#### Configuration d'ex√©cution

**Mode par d√©faut (v4.1)**: L'√âtape 5 utilise exclusivement le CPU avec 15 workers internes par d√©faut, offrant de meilleures performances globales sur la configuration actuelle.

**Mode GPU d√©sactiv√© (v4.2+)**: Depuis la d√©cision du 27/12/2025, **tous les moteurs fonctionnent en mode CPU**. L'utilisation du GPU est r√©serv√©e exclusivement √† InsightFace, et tous les autres moteurs (MediaPipe Face Landmarker, OpenSeeFace, OpenCV YuNet/PyFeat, EOS) sont forc√©s en mode CPU m√™me lorsque `STEP5_ENABLE_GPU=1`.

**Activation InsightFace GPU** :
1. D√©finir `STEP5_ENABLE_GPU=1` et s‚Äôassurer que `STEP5_GPU_ENGINES=insightface`.
2. `run_tracking_manager.py` invoque `Config.check_gpu_availability()` pour v√©rifier VRAM et providers CUDA (ONNXRuntime). En cas d‚Äô√©chec, les logs affichent `GPU requested but unavailable: ...`. Avec `STEP5_GPU_FALLBACK_AUTO=1` (d√©faut), l‚Äôex√©cution bascule automatiquement en CPU.
3. Lorsqu‚Äôun worker InsightFace GPU d√©marre, le gestionnaire injecte automatiquement le `LD_LIBRARY_PATH` contenant les biblioth√®ques CUDA empaquet√©es dans `insightface_env` et les chemins syst√®me d√©tect√©s.

‚ö†Ô∏è **Contraintes GPU** :
- **GPU r√©serv√© exclusivement √† InsightFace** ‚Äî tous les autres moteurs utilisent le CPU
- 1 worker GPU s√©quentiel uniquement (pas de parall√©lisation)
- N√©cessite NVIDIA GPU avec CUDA 12.0+ et ‚â•2 Go VRAM libres (4 Go recommand√©s pour coexister avec STEP2/NVENC)
- Installation de `onnxruntime-gpu` dans `insightface_env`
- CPU-only reste recommand√© pour batch processing de 10+ vid√©os

## Moteurs de D√©tection Faciale

### Moteurs Disponibles

1. **MediaPipe Face Landmarker** (par d√©faut)
   - Utilise `face_landmarker_v2_with_blendshapes.task`
   - Support natif des blendshapes ARKit
   - Optimis√© pour la d√©tection en temps r√©el
   - **Mode CPU-only** : l'acc√©l√©ration GPU n'est plus support√©e pour MediaPipe (r√©serv√©e √† InsightFace).

2. **OpenCV Haar Cascade**
   - Moteur de base pour la d√©tection de visages
   - Moins pr√©cis mais tr√®s rapide
   - Utile pour les cas simples ou le mat√©riel limit√©

3. **OpenCV YuNet**
   - D√©tecteur de visages bas√© sur CNN
   - Mod√®le : `face_detection_yunet_2023mar.onnx`
   - Configurable via `STEP5_YUNET_MODEL_PATH`

4. **OpenCV YuNet + PyFeat**
   - Combine YuNet pour la d√©tection et PyFeat pour les expressions
   - Extrait des blendshapes avanc√©s
   - Activation : `--face_engine opencv_yunet_pyfeat`
   - Mode hybride : d√©tection YuNet reste sur CPU (OpenCV), mais FaceMesh ONNX et py-feat peuvent tirer parti du GPU (`CUDAExecutionProvider` / PyTorch CUDA) lorsque `use_gpu=True`, offrant ~5-6√ó de gains sur l‚Äôextraction de blendshapes.

5. **OpenSeeFace**
   - Alternative open source compl√®te
   - N√©cessite des mod√®les sp√©cifiques dans `STEP5_OPENSEEFACE_MODELS_DIR`
   - Activation : `--face_engine openseeface`
   - Compatibilit√© GPU conditionnelle : si `onnxruntime-gpu` est install√©, les sessions ONNX utilisent `CUDAExecutionProvider` (log explicite). Sinon, le worker reste en CPU sans interrompre l‚Äôex√©cution.

6. **EOS (3D Morphable Model)**
   - Mod√®le 3D param√©trique pour l'ajustement pr√©cis des expressions
   - Utilise YuNet pour la d√©tection initiale puis ajuste 68 points 3D
   - S'ex√©cute dans un **environnement d√©di√© `eos_env`** (rout√© automatiquement par `run_tracking_manager.py`, override possible via `STEP5_EOS_ENV_PYTHON`)
   - Activation : `--face_engine eos` ou `STEP5_TRACKING_ENGINE=eos`
   - Variables d'environnement cl√©s :
     ```env
     STEP5_EOS_MODELS_DIR=workflow_scripts/step5/models/engines/eos/share   # peut pointer vers /home/kidpixel6/kidpixel_assets/eos/share
     STEP5_EOS_SFM_MODEL_PATH=${STEP5_EOS_MODELS_DIR}/sfm_model.bin
     STEP5_EOS_EXPRESSION_BLENDSHAPES_PATH=${STEP5_EOS_MODELS_DIR}/expression_blendshapes_57.bin
     STEP5_EOS_LANDMARK_MAPPER_PATH=${STEP5_EOS_MODELS_DIR}/ibug_to_eos_landmarks.json
     STEP5_EOS_EDGE_TOPOLOGY_PATH=${STEP5_EOS_MODELS_DIR}/sfm_3448_edge_topology.json
     STEP5_EOS_MODEL_CONTOUR_PATH=${STEP5_EOS_MODELS_DIR}/sfm_model_contours.json
     STEP5_EOS_CONTOUR_LANDMARKS_PATH=${STEP5_EOS_MODELS_DIR}/ibug_to_eos_contour_landmarks.json
     STEP5_EOS_FIT_EVERY_N=2                         # fallback auto sur STEP5_BLENDSHAPES_THROTTLE_N si absent
     STEP5_EOS_MAX_WIDTH=1280                        # downscale + rescale coordonn√©es/landmarks
     STEP5_EOS_MAX_FACES=1                           # optionnel
     STEP5_ENABLE_PROFILING=1                        # logs [PROFILING] toutes les 20 frames (YuNet, FaceMesh, fit eos)
     ```
   - Exporte `tracked_objects[].eos = {shape_coeffs, expression_coeffs}` et `landmarks` 68x3 (toujours rescal√©s).
   - Les assets peuvent √™tre install√©s hors repo (NAS/SSD). Il suffit d'ajuster `STEP5_EOS_MODELS_DIR`.
   - `workflow_scripts/step5/process_video_worker_multiprocessing.py` charge `.env` c√¥t√© worker pour propager l'ensemble de ces variables √† chaque sous-processus.

   > üí§ **Lazy import MediaPipe** : `process_video_worker.py` dispose de `_ensure_mediapipe_loaded(required=False)` afin d‚Äô√©viter l‚Äôimport du module tant que le moteur MediaPipe/objets n‚Äôest pas sollicit√©. Les moteurs OpenCV/EOS l‚Äôappellent en mode `required=False`, ce qui supprime les crashs TensorFlow lorsque seules les d√©pendances OpenCV sont install√©es. Quand MediaPipe est indispensable (`required=True`), l‚Äôerreur est logg√©e puis relanc√©e pour guider l‚Äôutilisateur.

7. **InsightFace (GPU-only, r√©introduit en v4.2)**
   - D√©tecteur + embeddings InsightFace `antelopev2` (SCRFD + glintr100) pilot√© par ONNX Runtime **CUDA**.
   - **GPU obligatoire** : si le provider CUDA √©choue, le manager abandonne l‚Äôex√©cution avec une erreur explicite (pas de fallback CPU).
   - Routage d√©di√© via `insightface_env` (cr√©√© sous `VENV_BASE_DIR/insightface_env`). Override possible avec `STEP5_INSIGHTFACE_ENV_PYTHON`.
   - Activation : `STEP5_TRACKING_ENGINE=insightface` (ou bouton dans l‚ÄôUI).  
     ‚ûú Le manager force automatiquement le mode GPU unique et d√©sactive le thread CPU.
   - Commandes InsightFace (tests rapides) :
     ```bash
     python - <<'PY'
     from insightface.app import FaceAnalysis
     app = FaceAnalysis(name='antelopev2')
     app.prepare(ctx_id=0, det_size=(640, 640))
     print(app.models.keys())
     PY
     ```

   ```env
   STEP5_TRACKING_ENGINE=insightface

   STEP5_ENABLE_GPU=1
   STEP5_GPU_ENGINES=mediapipe_landmarker,openseeface,opencv_yunet_pyfeat,insightface

   STEP5_GPU_MAX_VRAM_MB=3072        # GTX 1650 (4 Go) : laisse 1 Go pour le syst√®me
   STEP5_GPU_PROFILING=1             # Ajoute les logs [GPU] et [PROFILING]

   # STEP5_INSIGHTFACE_ENV_PYTHON=/mnt/venv_ext4/insightface_env/bin/python

   STEP5_INSIGHTFACE_MODEL_NAME=antelopev2
   STEP5_INSIGHTFACE_CTX_ID=0
   STEP5_INSIGHTFACE_DET_SIZE=640
   STEP5_INSIGHTFACE_MAX_WIDTH=1280
   STEP5_INSIGHTFACE_MAX_FACES=4
   STEP5_INSIGHTFACE_DETECT_EVERY_N=2
   STEP5_INSIGHTFACE_JAWOPEN_SCALE=1.0
   ```

   Bonnes pratiques :
   - **VRAM** : ajuster `STEP5_GPU_MAX_VRAM_MB` en fonction du GPU. Sur RTX 30xx/40xx, 4096‚Äì6144 Mo permettent d‚Äô√©viter les OOM lors de runs prolong√©s.
   - **Tests courts** : pour valider rapidement la pile InsightFace, vous pouvez g√©n√©rer un clip de 100 frames via `ffmpeg -frames:v 100` et pointer `--videos_json_path` vers ce clip. Les temps de d√©tection doivent se stabiliser autour de 80‚Äì90‚ÄØms/frame si CUDA est bien actif.
   - **Logs** : surveiller `logs/step5/manager_tracking_<ts>.log` pour v√©rifier la pr√©sence de `CUDAExecutionProvider`. En cas d‚Äôerreur `libcufft.so.11`, v√©rifier l‚Äôinjection `LD_LIBRARY_PATH` (le manager ajoute automatiquement les libs `nvidia/*/lib` + `/usr/local/cuda-12.x/targets/.../lib`).

### Gestionnaire STEP5 & Routage des Environnements

- `workflow_scripts/step5/run_tracking_manager.py` charge automatiquement `config.settings` pour r√©cup√©rer les chemins des virtualenvs via `config.get_venv_python(<venv>)`.  
  - ‚úÖ `tracking_env` est la valeur par d√©faut.  
  - ‚úÖ Lorsque `STEP5_TRACKING_ENGINE=eos`, le gestionnaire bascule sur `eos_env` (override possible via `STEP5_EOS_ENV_PYTHON`).  
  - ‚úÖ Lorsque `STEP5_TRACKING_ENGINE=insightface`, le gestionnaire bascule sur `insightface_env` (override possible via `STEP5_INSIGHTFACE_ENV_PYTHON`).
  - ‚úÖ Aucun chemin `env/bin/python` ne doit √™tre hardcod√© : d√©placez simplement vos venvs et mettez `VENV_BASE_DIR=/mnt/cache/venv/workflow_mediapipe` dans `.env` ou votre environnement syst√®me.
- D√®s le d√©marrage, le planificateur dynamique cr√©e **un thread GPU s√©quentiel** (si `STEP5_ENABLE_GPU=1` + moteur autoris√©) et **un thread CPU** qui alimente des workers multiprocessing. Le nombre de processus internes d√©pend directement de `TRACKING_CPU_WORKERS` (15 par d√©faut via `app_new.py`) et est propag√© jusqu‚Äôaux workers via l‚Äôargument `--mp_num_workers_internal`. M√™me en mode GPU, `TRACKING_CPU_WORKERS` reste utilis√© lorsque le fallback object detector est actif pour parall√©liser les d√©tections MediaPipe Tasks.
- En mode GPU, **seul InsightFace est autoris√©**. Tous les autres moteurs (`mediapipe_landmarker`, `openseeface`, `opencv_yunet_pyfeat`, `opencv_haar`, `eos`) sont automatiquement forc√©s en CPU avec le log `GPU mode is reserved for InsightFace only`. Les logs pour InsightFace indiquent `GPU mode requested...` puis `‚úì GPU mode ENABLED` ou fallback CPU si le GPU est indisponible.
- Les bornes de chunk (`TRACKING_CHUNK_MIN/MAX`) et le nombre de workers (`TRACKING_CPU_WORKERS`) sont propag√©s depuis `app_new.py` jusqu‚Äôaux workers via l‚Äôenvironnement et les arguments CLI g√©n√©r√©s (`--chunk_size 0` active le mode adaptatif d√©crit ci-dessous).
- Les logs normalis√©s permettent de suivre la progression‚ÄØ:
  - `[Progression-MultiLine] video_a.mp4: 43% || video_b.mp4: 7%`
  - `[Gestionnaire] Succ√®s pour video_a.mp4` / `[Gestionnaire] √âchec pour video_b.mp4`
  - `[Progression]|43|video_a.mp4|chunk=2/5` (consomm√© par `app_new.py` pour la progression fractionnaire)
- Pensez √† archiver ces logs (`logs/step5/manager_tracking_<timestamp>.log`) lorsqu‚Äôun run doit √™tre audit√©.

### Configuration des Moteurs

```python

STEP5_YUNET_MODEL_PATH=models/face_detectors/opencv/face_detection_yunet_2023mar.onnx
STEP5_OPENSEEFACE_MODELS_DIR=models/engines/openseeface/
STEP5_EOS_MODEL_DIR=models/eos/
STEP5_BLENDSHAPES_THROTTLE_N=20  # Ne calcule les blendshapes que toutes les N frames


STEP5_ENABLE_GPU=0
STEP5_GPU_ENGINES=insightface
STEP5_GPU_MAX_VRAM_MB=2048
STEP5_GPU_FALLBACK_AUTO=1
```

## Optimisation des Performances

### Mode CPU Unifi√© (v4.1+)

**Am√©liorations majeures** :
- Tous les moteurs utilisent maintenant le multiprocessing
- Gestion unifi√©e des workers via `--mp_num_workers_internal` (d√©faut: 15)
- Optimisation m√©moire pour le traitement par lots

**Configuration recommand√©e** :
```env
# Nombre de workers pour le traitement parall√®le
MP_NUM_WORKERS_INTERNAL=15

# D√©sactiver le GPU pour forcer le mode CPU (recommand√©)
TRACKING_DISABLE_GPU=1

# Limiter la r√©solution maximale pour YuNet/OpenSeeFace/EOS
STEP5_YUNET_MAX_WIDTH=640
STEP5_OPENSEEFACE_MAX_WIDTH=640
STEP5_EOS_MAX_WIDTH=640
```

### Profilage et M√©triques

Le profilage peut √™tre activ√© avec :
```env
STEP5_ENABLE_PROFILING=1
```

Les m√©triques sont enregistr√©es dans les logs avec le tag `[PROFILING]` :
- Temps moyen par frame
- Utilisation m√©moire
- Taux de succ√®s de d√©tection
- Utilisation des workers

### Registre des D√©tecteurs d'Objets

Le syst√®me inclut un registre de mod√®les de d√©tection d'objets utilis√©s en fallback :

```python
# Exemple d'utilisation du registre
from workflow_scripts.step5.object_detector_registry import ObjectDetectorRegistry

# Obtenir les sp√©cifications d'un mod√®le
spec = ObjectDetectorRegistry.get_model_spec('efficientdet_lite2')

# R√©soudre le chemin du mod√®le (v√©rifie les surcharges)
model_path = ObjectDetectorRegistry.resolve_model_path('efficientdet_lite2')
```

**Mod√®les disponibles** :
- `efficientdet_lite0` √† `efficientdet_lite4` (mod√®les l√©gers pour CPU)
- `ssd_mobilenet_v3`
- `yolo11n_tflite`, `yolo11n_onnx`
- `nanodet-plus-m`

**Configuration** :
```env
# Activer la d√©tection d'objets (MediaPipe uniquement)
STEP5_ENABLE_OBJECT_DETECTION=1

# S√©lectionner le mod√®le (doit √™tre dans le registre)
STEP5_OBJECT_DETECTOR_MODEL=efficientdet_lite2

# Optionnel : surcharger le chemin du mod√®le
# STEP5_OBJECT_DETECTOR_MODEL_PATH=/chemin/vers/modele.tflite
```

Lorsque les moteurs OpenCV ne d√©tectent aucun visage, `process_video_worker.py` tente automatiquement d‚Äôinitialiser un `ObjectDetector` MediaPipe via `_ensure_mediapipe_loaded(required=False)`. Si la cr√©ation r√©ussit, les d√©tections d‚Äôobjets sont converties (bbox, centroid, label, confidence) et fusionn√©es avec la sortie du moteur en amont avant l‚Äôappel √† `apply_tracking_and_management()`, √©vitant ainsi les frames ‚Äúvides‚Äù. En cas d‚Äô√©chec (MediaPipe non install√© ou mod√®le manquant), un avertissement est logg√© mais le traitement continue en conservant un JSON dense.
- **Mode GPU + fallback multi-thread** : quand `STEP5_ENABLE_OBJECT_DETECTION=1` et que le worker est lanc√© avec `--use_gpu`, le gestionnaire propage `mp_num_workers_internal=TRACKING_CPU_WORKERS` afin de cr√©er un pool de threads (`queue.Queue` + `threading.Thread`) qui consomment chacun leur propre instance `ObjectDetector` en `VisionRunningMode.IMAGE`. Ce mode supprime les warnings `Input timestamp must be monotonically increasing` rencontr√©s avec `RunningMode.VIDEO` et am√©liore la latence du fallback en exploitant tous les c≈ìurs CPU disponibles, tout en conservant un worker GPU s√©quentiel pour la d√©tection principale.

### Chunking adaptatif & bornes API

- **Activation** : lorsque `run_tracking_manager.py` lance un worker CPU multiprocess, il ajoute `--chunk_size 0`. C√¥t√© worker, un chunking adaptatif calcule automatiquement une taille de lot pour cr√©er ~5 chunks par worker (avec un minimum global de 20 chunks) et applique les bornes `chunk_min/chunk_max`.
- **Borniers** :
  - via `.env` : `TRACKING_CHUNK_MIN` et `TRACKING_CHUNK_MAX` (valeurs positives, ignor√©es si non d√©finies) ;
  - via API : `POST /api/step5/chunk_bounds` (voir section d√©di√©e) qui alimente `WorkflowService.set_step5_chunk_bounds()` et persiste les valeurs dans `WorkflowState`.
- **Flux complet** : UI ‚Üí `WorkflowService` ‚Üí `app_new.py` ‚Üí variables d‚Äôenvironnement ‚Üí `run_tracking_manager.py` ‚Üí arguments CLI (`--chunk_min`, `--chunk_max`) ‚Üí worker multiprocessing.
- **B√©n√©fices** : saturation homog√®ne des workers, r√©duction du temps perdu sur les vid√©os courtes, et absence de fragmentation extr√™me sur les vid√©os longues (>10k frames). Les logs contiennent toujours la ligne `Adaptive chunking enabled ‚Ä¶ selected_chunk_size=XXX`, utile pour v√©rifier l‚Äôapplication des bornes.

### Configuration Recommand√©e
- **CPU** : 8+ c≈ìurs (15 workers internes par d√©faut)
- **M√©moire** : 16+ Go de RAM pour le traitement parall√®le

### M√©canisme de Warmup pour cv2.VideoCapture

**Probl√®me** : 
- Sur certains MP4, `cv2.VideoCapture().set(CAP_PROP_POS_FRAMES, start_frame)` peut √©chouer silencieusement
- Probl√®me particuli√®rement fr√©quent avec les vid√©os encod√©es avec certains codecs

**Solution** :
```python
def process_frame_chunk(video_path, start_frame, end_frame):
    cap = cv2.VideoCapture(video_path)
    
    # Warmup: Lire quelques frames pour initialiser le d√©codeur
    for _ in range(3):
        ret = cap.grab()
        if not ret:
            break
    
    # Positionnement pr√©cis apr√®s le warmup
    cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
    
    # V√©rification de la position
    actual_frame = int(cap.get(cv2.CAP_PROP_POS_FRAMES))
    if actual_frame != start_frame:
        # Fallback: Lire les frames une par une si n√©cessaire
        cap.release()
        cap = cv2.VideoCapture(video_path)
        for _ in range(start_frame):
            ret = cap.grab()
            if not ret:
                break
    
    # Traitement des frames...
    for _ in range(start_frame, end_frame):
        ret, frame = cap.read()
        if not ret:
            break
        # Traitement de la frame...
    
    cap.release()
```

**Avantages** :
- Garantit un positionnement pr√©cis des frames
- √âvite les erreurs silencieuses de lecture vid√©o
- Am√©liore la fiabilit√© du traitement par lots (chunking)
- **GPU** : D√©sactiv√© par d√©faut pour une meilleure stabilit√©

#### Robustesse du worker multiprocessing

- `process_video_worker_multiprocessing.py` recharge le `.env` avant toute lecture de variables pour que chaque sous-processus voie les m√™mes valeurs (profiling, throttling, chemins mod√®les). Il injecte √©galement dynamiquement toutes les variables sp√©cifiques re√ßues via CLI (`STEP5_OPENSEEFACE_*`, `STEP5_EOS_*`, `STEP5_ONNX_*`, etc.) dans `os.environ` avant d‚Äôinstancier le moteur, garantissant que les workers multiprocessing utilisent exactement la m√™me configuration que le processus parent.  
- Lorsqu'une frame est illisible, le worker r√©-ouvre la vid√©o, tente `frame_idx-1`, puis recourt √† `CAP_PROP_POS_MSEC` avant d'ins√©rer un placeholder vide en fin de vid√©o afin de pr√©server un JSON dense (`tracked_objects` pr√©sent pour chaque index).  
- Les logs `[WORKER-XXXX]` tracent les retries et rappellent quelle portion (`chunk_start`, `chunk_end`) est trait√©e, ce qui permet de corr√©ler rapidement un √©ventuel `WARNING Failed to read frame` avec la vid√©o incrimin√©e.

### Variables d'Environnement
```bash
# D√©sactive l'utilisation du GPU
export TRACKING_DISABLE_GPU=1

# D√©finit le nombre de workers CPU (d√©faut: 15)
export TRACKING_CPU_WORKERS=15

# Optimisation pour TensorFlow (si utilis√©)
export TF_GPU_THREAD_MODE=gpu_private
```

#### Chargement automatique du `.env` dans les workers

Les scripts d'ex√©cution (`workflow_scripts/step5/face_engines.py` et `workflow_scripts/step5/process_video_worker_multiprocessing.py`)
chargent d√©sormais explicitement le fichier `.env` **avant** de lire la moindre variable d'environnement.
Cette mesure garantit que les processus enfants cr√©√©s par `ProcessPoolExecutor` re√ßoivent bien les flags
de profiling, de throttling ou de configuration OpenSeeFace, m√™me lorsque STEP5 est lanc√© depuis un autre
r√©pertoire de travail.

- Aucun param√©trage suppl√©mentaire n'est requis¬†: assurez-vous simplement que le `.env` du projet contient les cl√©s STEP5.
- Les logs `[WORKER-XXXX]` affichent la configuration effective au d√©marrage, ce qui permet de v√©rifier rapidement
  qu'un changement (ex. `STEP5_BLENDSHAPES_THROTTLE_N=2`) est pris en compte.

#### Param√®tres de profiling & export des blendshapes

| Variable | Description | Recommandation |
| --- | --- | --- |
| `STEP5_ENABLE_PROFILING` | Active les logs `[PROFILING]` toutes les **20** frames (compatible chunking). | `1` uniquement lors d'un diagnostic de performance. |
| `STEP5_ONNX_INTRA_OP_THREADS` / `STEP5_ONNX_INTER_OP_THREADS` | Contr√¥le du threading ONNX Runtime pour YuNet/py-feat/OpenSeeFace. | `2/1` sur CPU 8¬†c≈ìurs, augmenter `intra` pour 12+¬†c≈ìurs. |
| `STEP5_BLENDSHAPES_THROTTLE_N` | Calcule les blendshapes toutes les *N* frames (cache utilis√© entre temps). | `2` pour r√©duire la charge py-feat de ~50¬†%. |
| `STEP5_OPENSEEFACE_DETECT_EVERY_N` | Sp√©cifique OpenSeeFace¬†: fr√©quence d'ex√©cution du d√©tecteur. Si absent, retombe sur `STEP5_BLENDSHAPES_THROTTLE_N`. | Garder `2` pour benchmark CPU, `1` pour pr√©cision maximale. |
| `STEP5_BLENDSHAPES_PROFILE` / `STEP5_BLENDSHAPES_EXPORT_KEYS` | Filtre l'export JSON (`full`, `mouth`, `none`, `mediapipe`, `custom`). | Documenter le profil choisi c√¥t√© int√©gration front. |

> **Astuce**¬†: lorsque `STEP5_ENABLE_PROFILING=1`, surveillez les logs toutes les 20¬†frames pour comparer le co√ªt
> YuNet vs FaceMesh vs py-feat et ajuster `STEP5_BLENDSHAPES_THROTTLE_N` en cons√©quence.

Ces param√®tres s'appuient sur `utils/tracking_optimizations._filter_blendshapes_for_export()` (profil `full`, `mouth`, `mediapipe`, `custom`, etc.).  
Les tests `tests/unit/test_step5_export_verbose_fields.py` et `tests/unit/test_tracking_blendshape_profiles.py` assurent la couverture de ces combinaisons : pensez √† les mettre √† jour si vous ajoutez un nouveau profil ou modifiez le filtrage.

#### Downscale YuNet & OpenSeeFace

Les moteurs YuNet et OpenSeeFace partagent maintenant des limites de r√©solution configurables¬†:

| Variable | Impact |
| --- | --- |
| `STEP5_YUNET_MAX_WIDTH` | Si la vid√©o d√©passe cette largeur, YuNet travaille sur une frame r√©duite, puis les `bbox/centroid` sont **rescal√©s** vers la r√©solution originale. |
| `STEP5_OPENSEEFACE_MAX_WIDTH` | M√™me logique c√¥t√© OpenSeeFace (retombe sur `STEP5_YUNET_MAX_WIDTH` si absent) pour garantir un d√©bit constant sur CPU modestes. |
| `STEP5_EOS_MAX_WIDTH` | Applique le m√™me downscale lors de la d√©tection YuNet pr√©alable au fit EOS, avec rescale syst√©matique des coordonn√©es et des landmarks 3D. |

Des logs `DEBUG` (`[WORKER-XXXX] Rescale bbox ...`) confirment la remont√©e des coordonn√©es pour YuNet, OpenSeeFace **et** EOS. Coupl√© √† `STEP5_ENABLE_PROFILING=1`, cela permet de visualiser les timings toutes les 20 frames (d√©tection, landmarks ONNX, fit eos) et d'identifier les goulots d'√©tranglement.

#### Limitation du nombre de visages & scaling `jawOpen`

Ces garde-fous √©vitent de saturer le CPU quand plusieurs moteurs tournent en parall√®le. Ils sont tous charg√©s automatiquement dans les workers multiprocessing (voir logs `[WORKER-XXXX]`).

| Variable | Moteur | Description | Recommandation |
| --- | --- | --- | --- |
| `STEP5_OPENCV_MAX_FACES` | Haar, YuNet, YuNet+py-feat | Tronque la liste de visages retourn√©e par OpenCV avant post-traitement. | `1` √† `2` sur machines modestes, `None` pour tout capturer. |
| `STEP5_OPENCV_JAWOPEN_SCALE` | Haar, YuNet, YuNet+py-feat | Multiplie la valeur `jawOpen` calcul√©e √† partir des blendshapes FaceMesh/py-feat. | >1 pour lisser les faibles amplitudes (parole douce). |
| `STEP5_MEDIAPIPE_MAX_FACES` | MediaPipe Tasks | Nombre maximal de visages suivis simultan√©ment par MediaPipe. | `4` par d√©faut‚Äâ; descendre √† `1` pour du mono-speaker. |
| `STEP5_MEDIAPIPE_MAX_WIDTH` | MediaPipe Tasks | Limite facultative similaire √† YuNet pour r√©duire le co√ªt d‚Äôinf√©rence. | Laisser vide (= r√©solution native) sauf si CPU tr√®s limit√©. |
| `STEP5_MEDIAPIPE_JAWOPEN_SCALE` | MediaPipe Tasks | Ajuste `jawOpen` exactement comme pour OpenCV. | Harmoniser avec la valeur OpenCV pour comparabilit√©. |

> **Note** : `STEP5_OPENSEEFACE_MAX_FACES` et `STEP5_OPENSEEFACE_JAWOPEN_SCALE` restent document√©s dans la section suivante. Ce tableau couvre uniquement les moteurs OpenCV/Mediapipe r√©cemment harmonis√©s c√¥t√© code (`face_engines.py`, `config.settings`).

R√©duire la largeur √† `640` offre ~69¬†FPS pour YuNet sur une vid√©o 1080p¬†; remonter √† `1280` privil√©gie la pr√©cision.
Adapter la valeur au hardware et au type de contenus (talking heads vs plans larges).

#### Tuning OpenSeeFace (profil l√©ger)

Les variables suivantes sont expos√©es dans `.env` et logu√©es au d√©marrage de chaque worker¬†:

| Variable | Description |
| --- | --- |
| `STEP5_OPENSEEFACE_MODELS_DIR`, `_DETECTION_MODEL_PATH`, `_LANDMARK_MODEL_PATH` | R√©solution automatique des mod√®les ONNX (d√©tection + landmarks). |
| `STEP5_OPENSEEFACE_MODEL_ID` | S√©lection du mod√®le landmark (√©quilibre pr√©cision/vitesse). |
| `STEP5_OPENSEEFACE_DETECTION_THRESHOLD` | Confiance minimale pour conserver une d√©tection. |
| `STEP5_OPENSEEFACE_MAX_FACES` | Nombre maximum de visages suivis simultan√©ment. |
| `STEP5_OPENSEEFACE_JAWOPEN_SCALE` | Ajustement du seuil `jawOpen` pour les analyses vocales. |

> Les logs `[WORKER-XXXX] OpenSeeFace config: ...` facilitent le support¬†: capturez-les lors d'une demande d'aide.

#### Hi√©rarchie des mod√®les STEP5

L‚Äôensemble des mod√®les n√©cessaires √† STEP5 est structur√© par moteur et type pour faciliter les mises √† jour‚ÄØ:

```
workflow_scripts/step5/models/
‚îú‚îÄ‚îÄ face_detectors/
‚îÇ   ‚îú‚îÄ‚îÄ mediapipe/face_landmarker_v2_with_blendshapes.task
‚îÇ   ‚îî‚îÄ‚îÄ opencv/face_detection_yunet_2023mar.onnx
‚îú‚îÄ‚îÄ face_landmarks/
‚îÇ   ‚îî‚îÄ‚îÄ opencv/face_landmark.onnx
‚îú‚îÄ‚îÄ blendshapes/
‚îÇ   ‚îî‚îÄ‚îÄ mediapipe/face_blendshapes*.onnx
‚îú‚îÄ‚îÄ object_detectors/
‚îÇ   ‚îú‚îÄ‚îÄ tflite/EfficientDet-Lite*.tflite
‚îÇ   ‚îú‚îÄ‚îÄ onnx/yolo11n*.onnx
‚îÇ   ‚îî‚îÄ‚îÄ onnx/nanodet-plus-m_416.onnx
‚îî‚îÄ‚îÄ engines/openseeface/
    ‚îú‚îÄ‚îÄ mnv3_detection_opt.onnx
    ‚îî‚îÄ‚îÄ lm_model*.onnx
```

> **Astuce**‚ÄØ: si vous ajoutez un mod√®le custom, respectez cette hi√©rarchie et versionnez uniquement les m√©tadonn√©es (les poids volumineux peuvent √™tre mont√©s via volume externe).

#### Registry de d√©tection d'objets

Le fallback object detection (MediaPipe Tasks uniquement) est centralis√© dans `workflow_scripts/step5/object_detector_registry.py`. Il expose 6 mod√®les pr√©-analys√©s‚ÄØ:

| Mod√®le | Backend | Fichier | Hardware recommand√© | mAP COCO | Notes |
| --- | --- | --- | --- | --- | --- |
| `efficientdet_lite0` | TFLite | `EfficientDet-Lite0.tflite` | Edge‚ÄØTPU / CPU ARM | 25.69 | +50‚ÄØ% plus rapide que Lite2 |
| `efficientdet_lite1` | TFLite | `EfficientDet-Lite1.tflite` | Edge‚ÄØTPU √©quilibr√© | 30.55 | Latence ~49‚ÄØms Pixel‚ÄØ4 |
| `efficientdet_lite2` *(d√©faut)* | TFLite | `EfficientDet-Lite2-32.tflite` | CPU desktop / GPU | 33.97 | Baseline historique |
| `ssd_mobilenet_v3` | TFLite | `ssd_mobilenet_v3.tflite` | CPU ARM | 28.0 | √âcosyst√®me mature |
| `yolo11n_onnx` | ONNX | `yolo11n.onnx` | CPU desktop | 39.5 | Pr√©cision max, n√©cessite `onnxruntime` |
| `nanodet_plus` | ONNX | `nanodet-plus-m_416.onnx` | CPU ARM l√©ger | 34.1 | ~25‚ÄØms sur ARM |

**Variables `.env` √† d√©clarer**‚ÄØ:

```bash
STEP5_ENABLE_OBJECT_DETECTION=1                       # Active le fallback (MediaPipe uniquement)
STEP5_OBJECT_DETECTOR_MODEL=efficientdet_lite2        # Nom pr√©sent dans le registry
# STEP5_OBJECT_DETECTOR_MODEL_PATH=/chemin/custom     # Optionnel : surcharge absolue/relative
```

**Priorit√© de r√©solution**‚ÄØ: `override_path` CLI > `STEP5_OBJECT_DETECTOR_MODEL_PATH` > `workflow_scripts/step5/models/object_detectors/<backend>/...`. Chaque r√©solution est logu√©e pour audit (`[WORKER-XXXX] Using object detector model ‚Ä¶`).

**Bonnes pratiques**‚ÄØ:

1. Utiliser `efficientdet_lite0` sur Edge‚ÄØTPU/ARM pour limiter la latence.
2. Basculer vers `yolo11n_onnx` lorsqu‚Äô`onnxruntime` est disponible et que la pr√©cision prime.
3. Garder `STEP5_ENABLE_OBJECT_DETECTION=0` sur les moteurs OpenCV/OpenSeeFace‚ÄØ: seuls les workflows MediaPipe consomment cette d√©tection fallback.

> Pour une analyse d√©taill√©e des compromis, voir **[Alternatives GPU pour Tracking Facial Blendshapes](../optimization/Alternatives%20GPU%20pour%20Tracking%20Facial%20Blendshapes.md)**. Les tests de r√©gression couvrant la r√©solution et les chemins custom se trouvent dans `tests/unit/test_object_detector_registry.py`.

### Avantages du Mode CPU-Only
- **Stabilit√© accrue** : √âvite les probl√®mes de m√©moire GPU
- **Pr√©visibilit√©** : Performances plus constantes
- **Compatibilit√©** : Fonctionne sur n'importe quelle machine
- **√âvolutivit√©** : Mise √† l'√©chelle lin√©aire avec le nombre de c≈ìurs

### Gestion des Chunks

#### Configuration des Tailles de Chunks
```json
{
  "min": 100,    // Taille minimale de chunk en frames (par d√©faut: 100)
  "max": 500     // Taille maximale de chunk en frames (par d√©faut: 500)
}
```

#### API de Configuration
```python
# Exemple de configuration via l'API
response = requests.post(
    'http://localhost:5000/api/step5/chunk_bounds',
    json={"min": 100, "max": 500}
)
```

#### Propagation des bornes vers le sous-processus (ENV)

Lors de l'ex√©cution, les bornes configur√©es peuvent √™tre propag√©es au gestionnaire de tracking via des variables d'environnement si d√©finies c√¥t√© application‚ÄØ; le gestionnaire reprend ensuite ces valeurs et les transmet au worker multiprocessing.

```bash
# Propagation optionnelle si fix√©es dynamiquement par le service
export TRACKING_CHUNK_MIN=100
export TRACKING_CHUNK_MAX=500
```

Le service `WorkflowService.set_step5_chunk_bounds(min,max)` met √† jour ces bornes au niveau de l'application. Lors du lancement de STEP5, `app_new.py` transmet ces valeurs au sous-processus via l'environnement si elles sont pr√©sentes.

#### Strat√©gie de D√©coupage (Adaptive Chunking)

Le worker multiprocessing utilise un **adaptive chunking** qui calcule dynamiquement la taille des chunks :

1. **Calcul cible** : vise `max(internal_workers * 5, 20)` chunks
2. **Clampage** : taille finale limit√©e par `--chunk_min` et `--chunk_max`
3. **Priorit√© des param√®tres** :
   - `--chunk_size > 0` : force une taille fixe (d√©sactive l'adaptive)
   - Sinon : adaptive avec `--chunk_min/--chunk_max`
   - Valeurs par d√©faut si non sp√©cifi√©es : `min=20`, `max=400`

- **Cha√Æne de propagation** :
  1. API `/api/step5/chunk_bounds` ‚Üí `WorkflowService.set_step5_chunk_bounds()`
  2. Stockage en m√©moire via `WorkflowState` + variables `TRACKING_CHUNK_MIN/MAX`
  3. `app_new.py` exporte ces variables avant de lancer STEP5
  4. `run_tracking_manager.py` lit les bornes et les injecte dans la commande (`--chunk_min/--chunk_max`)
  5. `process_video_worker_multiprocessing.py` applique l‚Äôadaptive chunking avec ces limites

Les bornes effectives et la taille r√©elle par chunk sont trac√©es dans les logs `[WORKER-XXXX]` pour faciliter le support.

**Cas d'usage** :
- Petites vid√©os (< min frames) : trait√©es en un seul chunk
- Vid√©os moyennes : d√©coup√©es selon calcul adaptatif
- Grandes vid√©os : limit√©es par la borne maximale

### Surveillance des Performances
- M√©triques en temps r√©el via l'API `/api/system_monitor`
- Journalisation d√©taill√©e dans les logs d'application
- Suivi de la m√©moire et de l'utilisation CPU
- **OS** : Linux/Windows/macOS (test√© sur Ubuntu 20.04+)

### Activation du Mode CPU

#### Variables d'Environnement (d√©finies dans `app_new.py`)
- `TRACKING_DISABLE_GPU=1` ‚Äî D√©sactive compl√®tement l'utilisation du GPU
- `TRACKING_CPU_WORKERS=15` ‚Äî Nombre de workers CPU internes par vid√©o

#### Configuration via API
```bash
# D√©sactiver le GPU et configurer 15 workers CPU
curl -X POST http://localhost:5000/api/step5/configuration \
  -H "Content-Type: application/json" \
  -d '{"use_gpu": false, "cpu_workers": 15}'
```

### Avantages du Mode CPU Uniquement

#### 1. Stabilit√© Am√©lior√©e
- √âlimination des probl√®mes de m√©moire GPU partag√©e
- Pas de conflits entre les processus pour les ressources GPU
- Meilleure isolation entre les t√¢ches

#### 2. Performances Pr√©dictibles
- Pas de variation des performances due √† la charge du GPU
- Meilleure scalabilit√© sur les serveurs multi-c≈ìurs
- Facilit√© de parall√©lisation

#### 3. Utilisation des Ressources
- R√©partition uniforme de la charge sur les c≈ìurs CPU
- Possibilit√© d'ajuster dynamiquement le nombre de workers
- Meilleure gestion de la m√©moire partag√©e

### Configuration Recommand√©e

#### Pour les Stations de Travail
- **CPU** : 16+ c≈ìurs
- **RAM** : 32+ Go
- **Workers** : 15 (valeur par d√©faut)

#### Pour les Serveurs
- **CPU** : 32+ c≈ìurs
- **RAM** : 64+ Go
- **Workers** : Nombre de c≈ìurs - 1

### Monitoring des Performances
```bash
# Utilisation CPU
htop

# Utilisation m√©moire
top -o %MEM

# Suivi des processus
watch -n 1 "ps -eo pid,ppid,cmd,%mem,%cpu --sort=-%cpu | head -20"
```

### D√©pannage

#### Probl√®mes Courants
1. **Surcharge CPU**
   - R√©duire le nombre de workers
   - Activer le mode basse consommation

2. **M√©moire Insuffisante**
   - R√©duire la taille des chunks vid√©o
   - Diminuer le nombre de workers
   - Augmenter la m√©moire swap

3. **Performances Lentes**
   - V√©rifier la charge syst√®me
   - S'assurer que le mode GPU est bien d√©sactiv√©
   - Optimiser les param√®tres de d√©tection

### M√©triques de Performance
| M√©trique | Valeur Moyenne | Unit√© |
|----------|----------------|-------|
| FPS (CPU) | 12-18 | images/s |
| Utilisation CPU | 90-100% | % |
| Utilisation RAM | 8-12 | Go |
| Temps de Traitement (1min) | 3-5 | secondes |

### Bonnes Pratiques
- **√âviter la surcharge** : Ne pas d√©passer 80% d'utilisation CPU moyenne
- **Surveillance** : Mettre en place un syst√®me de monitoring
- **Mises √† jour** : Maintenir le syst√®me et les d√©pendances √† jour
- **Tests** : Valider les performances avec un sous-ensemble de donn√©es avant le traitement complet

**Flags CLI √©quivalents** (si expos√©s par le runner):
- `--disable_gpu` ‚Äî D√©sactivation explicite du GPU
- `--cpu_internal_workers 15` ‚Äî Configuration du nombre de workers CPU

**Raison de conception**:
- Performances observ√©es 2.1x sup√©rieures avec CPU (15 workers) vs GPU
- R√©duction significative de la consommation √©nerg√©tique
- Meilleure stabilit√© du syst√®me
- √âlimination des conflits de ressources GPU

**Configuration CPU optimis√©e**:
```python
CPU_OPTIMIZED_CONFIG = {
    "mp_landmarker_min_face_detection_confidence": 0.3,  # Seuils plus bas pour meilleur taux
    "mp_landmarker_min_face_presence_confidence": 0.2,   # de d√©tection CPU
    "mp_landmarker_min_tracking_confidence": 0.3,        # 
    "object_score_threshold": 0.4,                       # Seuil plus permissif
    "mp_max_distance_tracking": 80,                      # Distance l√©g√®rement plus permissive
    "mp_num_workers_internal": 15                        # 15 workers CPU par d√©faut
}
```

**Note importante**: Le batching GPU+CPU n'est plus utilis√© par d√©faut en √âtape 5. L'acc√©l√©ration GPU reste principalement exploit√©e en √âtape 2 (Conversion vid√©o).

#### Configuration MediaPipe Face Landmarker
```python
FACE_LANDMARKER_CONFIG = {
    "mp_landmarker_num_faces": 5,                           # Nombre max de visages
    "mp_landmarker_min_face_detection_confidence": 0.5,     # Seuil d√©tection
    "mp_landmarker_min_face_presence_confidence": 0.3,      # Seuil pr√©sence
    "mp_landmarker_min_tracking_confidence": 0.5,           # Seuil tracking
    "mp_landmarker_output_blendshapes": True                # Export blendshapes
}
```

#### Configuration Object Detection
```python
OBJECT_DETECTION_CONFIG = {
    "enable_object_detection": True,     # Activation d√©tection objets
    "object_score_threshold": 0.5,       # Seuil de confiance objets
    "object_max_results": 5               # Nombre max d'objets d√©tect√©s
}
```

#### Configuration Tracking et Gestion
```python
TRACKING_CONFIG = {
    "mp_max_distance_tracking": 70,                    # Distance max pour tracking
    "mp_frames_unseen_deregister": 7,                  # Frames avant d√©senregistrement
    "speaking_detection_jaw_open_threshold": 0.08      # Seuil ouverture m√¢choire
}
```

#### Configuration Performance
```python
# CPU Optimizations (multiprocessing)
CPU_OPTIMIZED_CONFIG = {
    "mp_landmarker_min_face_detection_confidence": 0.3,  # Seuils plus bas
    "mp_landmarker_min_face_presence_confidence": 0.2,   # pour meilleur taux
    "mp_landmarker_min_tracking_confidence": 0.3,        # de d√©tection CPU
    "mp_num_workers_internal": 15                        # 15 workers CPU
}

# GPU Configuration (sequential)
GPU_CONFIG = {
    "mp_num_workers_internal": 1  # Traitement s√©quentiel GPU
}
```

#### Syst√®me de Progression Avanc√© (v4.1)

L'√âtape 5 impl√©mente un syst√®me de progression sophistiqu√© pour g√©rer l'avancement global sur plusieurs vid√©os, √©vitant les sauts visuels d√©routants et fournissant une exp√©rience utilisateur fluide.

##### Composants du Syst√®me

**Backend (`app_new.py`)**:
- **Progression Fractionnaire**: Utilisation de `progress_current_fractional` pour repr√©senter la progression pr√©cise par vid√©o
- **Garde-fous anti-100% pr√©matur√©**: Chaque contribution de fichier est limit√©e √† 99% pendant le traitement
- **R√©initialisation apr√®s succ√®s**: La progression fractionnaire est effac√©e apr√®s chaque succ√®s pour √©viter les reports
- **Initialisation robuste**: Le compteur `files_completed` est initialis√© lors de la d√©tection du total

**Frontend (`static/uiUpdater.js`)**:
- **D√©sactivation fallback parsing**: Pour STEP5, le parsing des pourcentages dans le texte est d√©sactiv√© pendant l'ex√©cution
- **Garde-fous UI**: La progression est limit√©e √† 99% tant que `progress_current == progress_total` mais status ‚â† 'completed'
- **Gestion sp√©ciale STEP5**: Logique d√©di√©e pour pr√©venir les sauts √† 100% entre vid√©os

##### Logique de Calcul de Progression

```python
# Pour chaque fichier en cours de traitement
files_completed = int(info.get('files_completed', 0))
current_file_progress = max(0.0, min(0.99, percent / 100.0))  # Max 99% pendant traitement
overall_progress = (files_completed + current_file_progress)
info['progress_current_fractional'] = max(0.0, min(float(total_files), overall_progress))
```

**Avantages**:
- Progression fluide et cumulative sur plusieurs vid√©os
- √âlimination des sauts intempestifs √† 100%
- Meilleure exp√©rience utilisateur avec suivi pr√©cis
- Gestion robuste des √©tats interm√©diaires

## Architecture Interne

### Structure du Code

#### Gestionnaire Principal (`run_tracking_manager.py`)
```python
def main():
    """Gestionnaire principal avec strat√©gie par lots roulants GPU+CPU."""
    
def launch_worker_process(video_path, use_gpu, internal_workers):
    """Lance un processus worker avec configuration optimis√©e."""
    
def run_job_and_monitor(job_info, processes, progress_map, lock):
    """Ex√©cute et surveille un job de tracking."""
    
def monitor_progress(processes, progress_map, lock, total_jobs):
    """Monitoring en temps r√©el de la progression."""
```

#### Worker S√©quentiel (`process_video_worker.py`)
```python
def main():
    """Worker principal pour traitement s√©quentiel (GPU) ou multi-thread√© (CPU)."""
    
def process_video_multithreaded(args, video_capture, landmarker, object_detector, enhanced_speaking_detector, total_frames):
    """Traitement multi-thread√© pour CPU avec ThreadPoolExecutor."""
    
class FrameProcessor:
    """Processeur de frame thread-safe pour traitement parall√®le."""
```

#### Worker Multiprocessing (`process_video_worker_multiprocessing.py`)
```python
def init_worker_process(models_dir, args_dict):
    """Initialise les mod√®les MediaPipe pour le worker multiprocessing."""
    
def process_frame_chunk(chunk_data):
    """Traite un chunk de frames en multiprocessing avec warmup OpenCV."""
    
def process_video_multiprocessing(args, video_capture, total_frames):
    """Orchestrate le traitement multiprocessing avec export dense."""
    
def main():
    """Worker multiprocessing pour CPU avec 15 processus parall√®les."""
```

### Algorithmes et M√©thodes

#### Strat√©gie par Lots Roulants
```python
def rolling_batch_strategy():
    """
    Strat√©gie optimis√©e pour traitement parall√®le GPU+CPU.
    
    Workflow:
    1. Lot 1: GPU (vid√©o 1) + CPU (vid√©o 2) en parall√®le
    2. Lot 2: GPU (vid√©o 3) + CPU (vid√©o 4) en parall√®le
    3. Continue jusqu'√† √©puisement de la queue
    
    Avantages:
    - Utilisation maximale des ressources
    - CPU 2.1x plus rapide que GPU avec 15 workers
    - Pas de conflit de ressources
    """
    while videos_to_process:
        current_batch = []
        
        # GPU job
        if videos_to_process:
            gpu_job = {'path': videos_to_process.popleft(), 'use_gpu': True}
            current_batch.append(gpu_job)
        
        # CPU job (15 workers)
        if videos_to_process:
            cpu_job = {'path': videos_to_process.popleft(), 'use_gpu': False}
            current_batch.append(cpu_job)
        
        # Ex√©cution parall√®le du lot
        execute_batch_parallel(current_batch)
```

#### D√©tection et Tracking Int√©gr√©s
```python
def integrated_detection_workflow(frame, frame_idx, timestamp_ms):
    """
    Workflow int√©gr√© de d√©tection et tracking.
    
    1. D√©tection faciale primaire (MediaPipe Face Landmarker)
    2. Fallback d√©tection d'objets si taux de r√©ussite < 10%
    3. Extraction des blendshapes pour analyse de parole
    4. Application du tracking avec gestion des ID
    5. D√©tection de parole enrichie (audio + visuel)
    """
    
    # 1. D√©tection faciale
    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, 
                       data=cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
    face_result = landmarker.detect_for_video(mp_image, timestamp_ms)
    
    current_detections = []
    face_detected = False
    
    if face_result.face_landmarks:
        for i, landmarks in enumerate(face_result.face_landmarks):
            # Extraction bbox et centroid
            bbox, centroid = extract_face_bbox_and_centroid(landmarks, frame.shape)
            
            # Extraction blendshapes
            blendshapes = None
            if face_result.face_blendshapes and i < len(face_result.face_blendshapes):
                blendshapes = {bs.category_name: bs.score 
                             for bs in face_result.face_blendshapes[i]}
            
            detection = {
                "bbox": bbox,
                "centroid": centroid,
                "source_detector": "face_landmarker",
                "confidence": calculate_face_confidence(landmarks),
                "blendshapes": blendshapes,
                "landmarks": landmarks
            }
            current_detections.append(detection)
            face_detected = True
    
    # 2. Fallback d√©tection d'objets
    if not face_detected and use_object_detection_fallback:
        object_result = object_detector.detect_for_video(mp_image, timestamp_ms)
        for detection in object_result.detections:
            # Conversion en format unifi√©
            object_detection = convert_object_to_detection_format(detection)
            current_detections.append(object_detection)
    
    return current_detections, face_detected
```

#### D√©tection de Parole Enrichie
```python
def enhanced_speaking_detection(frame_num, blendshapes, enhanced_detector):
    """
    D√©tection de parole multi-source avec fusion audio/visuel.
    
    Sources:
    1. Audio (STEP4): is_speech_present, active_speakers
    2. Visuel: jaw_open (blendshapes), mouth_activity
    3. Contextuel: segment de sc√®ne (STEP3)
    
    Fusion:
    - Poids audio: 60%
    - Poids visuel: 40%
    - Seuil confiance: 30%
    """
    
    if not enhanced_detector:
        # Fallback simple bas√© sur jaw_open
        jaw_open = blendshapes.get('jawOpen', 0.0) if blendshapes else 0.0
        return jaw_open > jaw_threshold
    
    # D√©tection enrichie multi-source
    result = enhanced_detector.detect_speaking(
        frame_num=frame_num,
        blendshapes=blendshapes,
        source_detector="face_landmarker"
    )
    
    return result.is_speaking, result.confidence, result.method
```

#### Gestion du Tracking et des ID
```python
def apply_tracking_and_management(active_objects, current_detections, next_id_counter, 
                                distance_threshold, frames_unseen_to_deregister):
    """
    Gestion compl√®te du cycle de vie des objets track√©s.
    
    Algorithme:
    1. Incr√©mentation frames_unseen pour tous les objets actifs
    2. Association d√©tections ‚Üî objets existants (KDTree optimis√©)
    3. Mise √† jour des objets associ√©s
    4. Cr√©ation de nouveaux objets pour d√©tections non associ√©es
    5. D√©senregistrement des objets non vus depuis N frames
    6. G√©n√©ration de la sortie finale avec m√©tadonn√©es
    """
    
    # Phase 1: Incr√©mentation frames_unseen
    for obj_id in active_objects:
        active_objects[obj_id]["frames_unseen"] += 1
    
    # Phase 2: Association via KDTree (optimis√©)
    if active_objects and current_detections:
        associations = find_optimal_associations_kdtree(
            active_objects, current_detections, distance_threshold
        )
    
    # Phase 3: Mise √† jour objets existants
    for obj_id, detection_idx in associations:
        update_tracked_object(active_objects[obj_id], current_detections[detection_idx])
    
    # Phase 4: Cr√©ation nouveaux objets
    for unmatched_detection in unmatched_detections:
        create_new_tracked_object(active_objects, unmatched_detection, next_id_counter)
    
    # Phase 5: D√©senregistrement objets perdus
    remove_lost_objects(active_objects, frames_unseen_to_deregister)
    
    # Phase 6: G√©n√©ration sortie
    return generate_frame_output(active_objects)
```

### Warmup OpenCV et Retry (Multiprocessing)

#### Probl√®me
Sur certains MP4, `cv2.VideoCapture().set(CAP_PROP_POS_FRAMES, start_frame)` peut √©chouer silencieusement si le d√©codeur n'a pas √©t√© ¬´ r√©veill√© ¬ª par une lecture pr√©alable.

#### Solution impl√©ment√©e
```python
# Dans process_frame_chunk()
cap.read()  # Warmup obligatoire avant le seek
cap.set(cv2.CAP_PROP_POS_FRAMES, chunk_start)

# Retry si √©chec √† la premi√®re frame du chunk
if frame_idx == chunk_start and not ret:
    cap.release()
    cap = cv2.VideoCapture(video_path)
    if cap.isOpened():
        cap.read()  # Warmup
        cap.set(cv2.CAP_PROP_POS_FRAMES, chunk_start)  # Retry
        ret, frame = cap.read()
```

#### Validation
Test unitaire `tests/unit/test_step5_mp_seek_warmup.py` v√©rifie que `read()` est bien appel√© avant `set(CAP_PROP_POS_FRAMES)`.

### Export Dense des Frames

Pour garantir l'alignement avec STEP4, le worker multiprocessing s'assure que **toutes les frames sont export√©es**, m√™me si une frame n'a pas √©t√© trait√©e (chunk incomplet) :

```python
# Boucle dense sur toutes les frames
for frame_idx in range(total_frames):
    result = all_results.get(frame_idx)
    if result is None:
        # Frame manquante c√¥t√© multiprocessing ‚Üí export vide
        detections = []
        logging.warning(f"Missing detection results for frame {frame_idx + 1}. Output remains dense.")
    else:
        detections = result.get('detections', [])
    
    final_output["frames"].append({
        "frame": frame_idx + 1,
        "tracked_objects": tracked_for_frame if tracked_for_frame else []
    })
```

**Diff√©rence importante** :
- **Pas de d√©tection** : frame trait√©e mais aucune bbox ‚Üí `tracked_objects: []`
- **Frame manquante** : chunk incomplet ‚Üí `tracked_objects: []` avec warning

### Gestion des Erreurs et Logging

#### Niveaux de Logging
```python
logging.INFO     # Progression normale et statistiques
logging.WARNING  # Fallback d√©tection objets, probl√®mes de performance
logging.ERROR    # √âchecs de traitement MediaPipe ou OpenCV
logging.CRITICAL # Mod√®les MediaPipe non trouv√©s
```

#### Types d'Erreurs G√©r√©es
- **Mod√®les MediaPipe manquants** : Face landmarker ou object detector non trouv√©s
- **Erreurs de traitement vid√©o** : Corruption, codecs non support√©s
- **Erreurs de m√©moire** : GPU/CPU overload, gestion gracieuse
- **Erreurs de multiprocessing** : Synchronisation, communication inter-processus
- **Erreurs de tracking** : Associations impossibles, objets corrompus

#### Structure des Logs
```
logs/step5/tracking_20240120_143022.log
```

Exemple de sortie :
```
2024-01-20 14:30:22 - INFO - --- D√âMARRAGE DU GESTIONNAIRE DE TRACKING (Strat√©gie par Lots Roulants) ---
2024-01-20 14:30:23 - INFO - Vid√©os √† traiter: 6
2024-01-20 14:30:24 - INFO - --- D√©marrage du lot n¬∞1 ---
2024-01-20 14:30:25 - INFO - Pr√©paration du job GPU pour: video1.mp4
2024-01-20 14:30:26 - INFO - Pr√©paration du job CPU (x15) pour: video2.mp4
2024-01-20 14:30:27 - INFO - Using multiprocessing worker with 15 processes
2024-01-20 14:30:28 - INFO - Applied CPU optimizations: lower confidence thresholds for better detection rate
2024-01-20 14:30:30 - INFO - [Progression-MultiLine]video1.mp4: Processing frame 150/2500 (6%) || video2.mp4: Processing frame 300/3000 (10%)
```

### Optimisations de Performance

#### Optimisations CPU (Multiprocessing)
- **15 workers parall√®les** : Optimal pour CPU modernes (2.1x plus rapide que GPU)
- **Seuils de confiance r√©duits** : Meilleur taux de d√©tection sur CPU
- **Traitement par batches** : R√©duction overhead de communication
- **KDTree optimis√©** : Association rapide d√©tections ‚Üî objets track√©s

#### Optimisations GPU (S√©quentiel)
- **Traitement s√©quentiel** : √âvite les conflits de m√©moire VRAM
- **Delegate GPU** : Utilisation native des acc√©l√©rations MediaPipe
- **Gestion m√©moire** : Lib√©ration explicite des ressources

#### Optimisations G√©n√©rales
```python
# Fallback intelligent d√©tection objets
if face_success_rate < 0.1:  # < 10% de r√©ussite
    enable_object_detection_fallback = True

# Threading optimis√© pour CPU
if not use_gpu and internal_workers > 1:
    use_multithreading = True
    max_workers = min(internal_workers, os.cpu_count())

# Gestion m√©moire vid√©o
with safe_video_processing(video_path) as video_capture:
    # Traitement s√©curis√© avec nettoyage automatique

## Interface et Utilisation

### Param√®tres d'Ex√©cution

#### Arguments de Ligne de Commande (Gestionnaire)
```bash
python run_tracking_manager.py --videos_json_path VIDEOS_JSON

# --videos_json_path : Fichier JSON avec liste des vid√©os √† traiter
```

#### Arguments de Ligne de Commande (Worker)
```bash
python process_video_worker.py VIDEO_PATH --models_dir MODELS_DIR [OPTIONS]

# VIDEO_PATH : Chemin vers la vid√©o √† traiter
# --models_dir : R√©pertoire des mod√®les MediaPipe
# --use_gpu : Utilisation GPU (optionnel)
# --mp_landmarker_num_faces : Nombre max de visages (d√©faut: 5)
# --mp_landmarker_min_face_detection_confidence : Seuil d√©tection (d√©faut: 0.5)
# --enable_object_detection : Activation d√©tection objets
# --mp_num_workers_internal : Nombre de workers internes (CPU uniquement)
```

#### Ex√©cution Automatique via Workflow
```python
# Via WorkflowService
result = WorkflowService.run_step("STEP5")

# Via API REST
curl -X POST http://localhost:5000/run/STEP5
```

#### Pr√©paration c√¥t√© service (helpers)

Avant le lancement du gestionnaire STEP5, la pr√©paration est r√©alis√©e c√¥t√© service pour fiabiliser la s√©lection des vid√©os et la communication:

- `WorkflowService.prepare_tracking_step(base_path, keyword, subdir)` ‚Äî Recherche les vid√©os √† traiter (ignore les vid√©os d√©j√† pourvues d'un JSON sibling) et retourne la liste.
- `WorkflowService.create_tracking_temp_file(videos)` ‚Äî Cr√©e un fichier JSON temporaire listant les vid√©os, transmis au gestionnaire via `--videos_json_path`.

Ces helpers r√©duisent le code sp√©cifique dans `app_new.py` et standardisent le flux de pr√©paration.

#### Ex√©cution Manuelle (Debug)
```bash
# Activation de l'environnement sp√©cialis√©
source tracking_env/bin/activate

# G√©n√©ration du fichier JSON des vid√©os (automatique dans le workflow)
python -c "
import json
from pathlib import Path
videos = [str(p) for p in Path('projets_extraits').rglob('*.mp4')]
with open('videos_to_track.json', 'w') as f:
    json.dump(videos, f, indent=2)
"

# Ex√©cution du gestionnaire
python workflow_scripts/step5/run_tracking_manager.py --videos_json_path videos_to_track.json

# Avec logging d√©taill√©
python workflow_scripts/step5/run_tracking_manager.py --videos_json_path videos_to_track.json 2>&1 | tee tracking.log
```

### Exemples d'Utilisation

#### Test de Tracking sur Vid√©o Unique
```bash
# Pr√©paration d'un test
mkdir -p test_tracking/docs
cp sample_video.mp4 test_tracking/docs/
cp sample_video.csv test_tracking/docs/  # Sc√®nes (STEP3)
cp sample_video_audio.json test_tracking/docs/  # Audio (STEP4)

# Activation de l'environnement
source tracking_env/bin/activate

# Test worker direct
cd test_tracking
python ../workflow_scripts/step5/process_video_worker.py docs/sample_video.mp4 --models_dir ../workflow_scripts/step5/models

# V√©rification du r√©sultat
ls -la docs/sample_video_tracking.json
head -50 docs/sample_video_tracking.json
```

#### Comparaison Performance GPU vs CPU
```bash
# Test GPU (s√©quentiel)
time python process_video_worker.py video.mp4 --models_dir models --use_gpu

# Test CPU (15 workers multiprocessing)
time python process_video_worker_multiprocessing.py video.mp4 --models_dir models --mp_num_workers_internal 15

# Test CPU (multi-thread√©)
time python process_video_worker.py video.mp4 --models_dir models --mp_num_workers_internal 8
```

#### Int√©gration dans S√©quence
```javascript
pollingManager.startPolling('step5Status', async () => {
    const status = await apiService.getStepStatus('STEP5');
    if (status.status === 'running') {
        updateTrackingProgress(status.progress);
    }
}, 1000);
```

## Structure des Donn√©es de Sortie

### Optimisation de Taille des Exports JSON (v4.1.3+)

**Probl√©matique** : Certains moteurs (`opencv_yunet_pyfeat`, `openseeface`, `eos`) exportent des donn√©es volumineuses (`landmarks`, coefficients EOS) qui sont **syst√©matiquement supprim√©es par STEP6** (`json_reducer.py`).

**Solution** : Variable `STEP5_EXPORT_VERBOSE_FIELDS` pour contr√¥ler l'export de ces donn√©es.

| Moteur | Taille avec export complet | Taille optimis√©e | R√©duction |
|--------|---------------------------|------------------|-----------|
| `opencv_yunet_pyfeat` | ~95M | ~5M | **95%** |
| `openseeface` | ~19M | ~5M | **74%** |
| `eos` | ~24M | ~5M | **79%** |
| `mediapipe_landmarker` | ~5M | ~5M | Aucune (d√©j√† optimis√©) |

**Configuration** :
```bash
# Dans .env - D√©faut recommand√© (export l√©ger)
STEP5_EXPORT_VERBOSE_FIELDS=false

# Pour debugging ou analyse approfondie uniquement
STEP5_EXPORT_VERBOSE_FIELDS=true
```

**Champs contr√¥l√©s** :
- `landmarks` : Coordonn√©es 3D des points faciaux (66-478 points selon moteur)
- `eos.shape_coeffs` : Coefficients de forme du mod√®le 3DMM
- `eos.expression_coeffs` : Coefficients d'expression du mod√®le 3DMM

**Compatibilit√© STEP6** : Totalement pr√©serv√©e. Les champs n√©cessaires pour After Effects (`id`, `centroid_x`, `bbox_width/height`, `active_speakers`) sont toujours export√©s.

**Validation** : le comportement est couvert par `tests/unit/test_step5_export_verbose_fields.py`, qui v√©rifie toutes les variantes (`true/false/1/0/...`) et garantit la pr√©sence des champs requis par STEP6 m√™me lorsque les donn√©es volumineuses sont supprim√©es.

## Hi√©rarchie des Mod√®les STEP5

```
workflow_scripts/step5/models/
‚îú‚îÄ‚îÄ face_detectors/
‚îÇ   ‚îú‚îÄ‚îÄ mediapipe/face_landmarker_v2_with_blendshapes.task
‚îÇ   ‚îî‚îÄ‚îÄ opencv/face_detection_yunet_2023mar.onnx
‚îú‚îÄ‚îÄ face_landmarks/
‚îÇ   ‚îî‚îÄ‚îÄ opencv/face_landmark.onnx
‚îú‚îÄ‚îÄ blendshapes/
‚îÇ   ‚îú‚îÄ‚îÄ mediapipe/face_blendshapes*.onnx
‚îÇ   ‚îî‚îÄ‚îÄ opencv/pyfeat_models/...
‚îú‚îÄ‚îÄ object_detectors/ (tflite, onnx, tensorflow)
‚îú‚îÄ‚îÄ engines/
‚îÇ   ‚îú‚îÄ‚îÄ openseeface/
‚îÇ   ‚îú‚îÄ‚îÄ eos/
‚îî‚îÄ‚îÄ metadata/labelmap.txt
```

| Variables .env | R√©pertoire | Description |
| --- | --- | --- |
| `STEP5_YUNET_MODEL_PATH` | `face_detectors/opencv/` | ONNX YuNet (d√©tection CPU). |
| `STEP5_OPENSEEFACE_MODELS_DIR`, `STEP5_OPENSEEFACE_*_PATH` | `engines/openseeface/` | Mod√®les detection/landmarks OpenSeeFace. |
| `STEP5_EOS_MODELS_DIR`, `STEP5_EOS_*_PATH` | `engines/eos/` | Assets EOS 3DMM (peuvent pointer vers un dossier externe). |
| `STEP5_OBJECT_DETECTOR_MODEL(_PATH)` | `object_detectors/<backend>/` | R√©solution via `ObjectDetectorRegistry`. |

> **Bonne pratique** : garder la structure intacte dans le repo et utiliser uniquement des overrides `.env` (relatifs ou absolus) lorsque les mod√®les sont plac√©s sur un SSD externe. Les workers STEP5 r√©solvent d'abord les overrides, puis les chemins du repo.

### Format JSON de Sortie (v4.1+)

Le format de sortie a √©t√© mis √† jour pour inclure des m√©triques suppl√©mentaires et une meilleure organisation des donn√©es :

```json
{
  "metadata": {
    "version": "4.1",
    "engine": "eos",  // ou "mediapipe", "openseeface", etc.
    "fps": 30,
    "total_frames": 1000,
    "processing_time_sec": 45.2,
    "blendshapes_profile": "arkit",
    "detection_stats": {
      "face_detection_rate": 0.95,
      "avg_faces_per_frame": 1.2,
      "object_detection_fallback_used": false
    },
    "performance_metrics": {
      "avg_frame_processing_ms": 45.2,
      "max_memory_usage_mb": 1200,
      "cpu_utilization_percent": 85.5
    }
  },
  "frames": [
    {
      "frame_number": 1,
      "timestamp_ms": 33.33,
      "faces": [
        {
          "bounding_box": [x, y, width, height],
          "landmarks": [[x, y, z], ...],  // 468 points
          "blendshapes": {
            "eyeBlinkLeft": 0.8,
            "mouthSmile": 0.6,
            // ... autres blendshapes ARKit
          },
          "rotation": [pitch, yaw, roll],
          "translation": [x, y, z],
          "tracking_id": 1,
          "detection_confidence": 0.98
        }
      ],
      "objects": [
        {
          "class": "person",
          "score": 0.92,
          "bounding_box": [x, y, width, height],
          "tracking_id": 2
        }
      ]
    }
  ]
}
```

### M√©triques de Performance

Les m√©triques suivantes sont maintenant incluses dans le fichier de sortie :

1. **Taux de d√©tection** : Pourcentage de frames o√π au moins un visage a √©t√© d√©tect√©
2. **Utilisation CPU** : Moyenne et pics d'utilisation du processeur
3. **M√©moire** : Utilisation maximale de la RAM
4. **D√©bit** : Temps moyen de traitement par frame
5. **Statistiques de suivi** : Nombre moyen de visages par frame, utilisation du fallback de d√©tection d'objets

### Ancien Format (Obsol√®te)

> **Note** : Les versions pr√©c√©dentes pouvaient contenir des champs obsol√®tes comme `processing_info`, `frames_tracked`, `frames_unseen`, etc. Ces champs ne sont plus utilis√©s dans la version 4.1+.

```json
{
  "metadata": {
    "video_path": "/path/to/video1.mp4",
    "total_frames": 2500,
    "fps": 25.0,
    "tracking_engine": "mediapipe_landmarker"
  },
  "frames": [
    {
      "frame": 1,
      "tracked_objects": []
    },
    {
      "frame": 125,
      "tracked_objects": [
        {
          "id": "obj_1",
          "bbox_xmin": 100,
          "bbox_xmax": 300,
          "centroid_x": 200,
          "centroid_y": 275,
          "source_detector": "face_landmarker",
          "label": "face",
          "confidence": 0.92,
          "is_speaking": true,
          "speaking_confidence": 0.87,
          "speaking_method": "audio_primary",
          "speaking_sources": ["audio", "blendshapes"],
          "blendshapes": {
            "jawOpen": 0.12,
            "mouthSmileLeft": 0.05,
            "mouthSmileRight": 0.04,
            "eyeBlinkLeft": 0.02,
            "eyeBlinkRight": 0.01
          }
        }
      ]
    },
    {
      "frame": 200,
      "tracked_objects": [
        {
          "id": "obj_2",
          "bbox_xmin": 150,
          "bbox_xmax": 330,
          "centroid_x": 240,
          "centroid_y": 210,
          "source_detector": "object_detector",
          "label": "person",
          "confidence": 0.78
        }
      ]
    }
  ]
}
```

#### Description des Champs

##### M√©tadonn√©es
- **video_path** : Chemin complet vers la vid√©o trait√©e
- **total_frames** : Nombre total de frames dans la vid√©o
- **fps** : Framerate de la vid√©o
- **tracking_engine** : Moteur utilis√© (`mediapipe_landmarker`, `opencv_haar`, `opencv_yunet`, `opencv_yunet_pyfeat`, `openseeface`)

##### Donn√©es par Frame
- **frame** : Num√©ro de frame (base 1)
- **tracked_objects** : Array des objets track√©s dans cette frame (vide si aucune d√©tection)

##### Objets Track√©s
- **id** : Identifiant unique persistant (cha√Æne, ex: `"obj_1"`, `"obj_2"`)
- **bbox_xmin**, **bbox_xmax**, **bbox_width**, **bbox_height** : Coordonn√©es et dimensions de la bounding box
  - Seuls `bbox_xmin` et `bbox_xmax` sont garantis pour les visages MediaPipe (compatibilit√© historique)
- **centroid_x**, **centroid_y** : Centre de l'objet
- **source** : Source de d√©tection export√©e dans le JSON (`"face_landmarker"` ou `"object_detector"`)
  - **Note** : en interne, les d√©tections sont produites avec `source_detector`; lors de l‚Äôexport final (`apply_tracking_and_management`), le champ s√©rialis√© est `source`.
- **label** : √âtiquette (`"face"` pour les visages, nom de classe pour les objets)
- **confidence** : Confiance de la d√©tection (0.0-1.0)
- **is_speaking** : Bool√©en (uniquement pour les visages avec d√©tection de parole)
- **speaking_confidence** : Confiance de la d√©tection de parole (0.0-1.0)
- **speaking_method** : M√©thode utilis√©e (`"audio_primary"`, `"visual_primary"`, `"blendshapes"`, `"no_blendshapes"`, etc.)
- **speaking_sources** : Liste des sources utilis√©es pour la d√©cision (`["audio"]`, `["blendshapes"]`, `["audio","blendshapes"]`)
- **blendshapes** : Coefficients d'animation faciale (voir section **Filtrage Blendshapes** ci-dessous)
- **landmarks** : Points de rep√®re faciaux 3D (uniquement pour les moteurs qui les fournissent, ex: `opencv_yunet_pyfeat`, `openseeface`)

> **Note `openseeface`** :
> - Les landmarks sont une liste de 66 points, export√©s au format `[x, y, z]` avec `z=0.0`.
> - Les blendshapes sont export√©es au format ARKit 52 cl√©s (compatibilit√© JSON). Seule `jawOpen` est actuellement estim√©e (les autres cl√©s valent `0.0`).

> **Important** : Pour les objets non-faciaux (`source_detector: "object_detector"`), les champs `is_speaking`, `speaking_*`, et `blendshapes` sont **absents** (pas `null`).

#### Filtrage Blendshapes (export JSON)

Le champ `blendshapes` peut √™tre filtr√© √† l‚Äôexport via des variables d‚Äôenvironnement. Cela permet de r√©duire la taille des JSON ou de se concentrer sur des r√©gions d‚Äôint√©r√™t (ex: bouche pour la parole).

| Variable | Valeurs possibles | Comportement |
|----------|-------------------|--------------|
| `STEP5_BLENDSHAPES_PROFILE` | `full` (d√©faut) | Exporte toutes les cl√©s blendshapes d√©tect√©es |
| | `mouth` | Exporte uniquement les cl√©s commen√ßant par `mouth*` ou `jaw*` |
| | `none` | D√©sactive l‚Äôexport (`blendshapes: null`) |
| | `mediapipe` | Supprime `tongueOut`, ajoute `_neutral: 0.0` si absent |
| | `custom` | Exporte une whitelist d√©finie via `STEP5_BLENDSHAPES_EXPORT_KEYS` |
| `STEP5_BLENDSHAPES_INCLUDE_TONGUE` | `1`/`true`/`yes` (optionnel) | Avec profil `mouth`, inclut `tongueOut` si pr√©sent |
| `STEP5_BLENDSHAPES_EXPORT_KEYS` | `key1,key2,...` (s√©par√© par des virgules) | Whitelist pour profil `custom` |

**Exemples pratiques**

```bash
# .env : ne garder que la bouche (utile pour analyse parole)
STEP5_BLENDSHAPES_PROFILE=mouth
STEP5_BLENDSHAPES_INCLUDE_TONGUE=1

# .env : d√©sactiver les blendshapes (√©conomie de taille)
STEP5_BLENDSHAPES_PROFILE=none

# .env : export personnalis√© (m√¢choire + sourires)
STEP5_BLENDSHAPES_PROFILE=custom
STEP5_BLENDSHAPES_EXPORT_KEYS=jawOpen,mouthSmileLeft,mouthSmileRight
```

**R√©sultat JSON (profil `mouth` avec langue)**

```json
{
  "blendshapes": {
    "jawOpen": 0.12,
    "mouthSmileLeft": 0.05,
    "mouthSmileRight": 0.04,
    "tongueOut": 0.01
  }
}
```

> **Note technique** : Le filtrage est appliqu√© dans `utils/tracking_optimizations._filter_blendshapes_for_export()` **apr√®s** la d√©tection, donc il ne modifie pas les calculs internes (ex: EnhancedSpeakingDetector).

### M√©triques de Progression et Monitoring

#### Indicateurs de Progression Console (Gestionnaire)
```python
# Sortie standardis√©e pour l'interface utilisateur
print(f"[Progression-MultiLine]{' || '.join(progress_parts)}")
# Exemple: video1.mp4: Processing frame 150/2500 (6%) || video2.mp4: Processing frame 300/3000 (10%)
```

#### Indicateurs de Progression Console (Worker)
```python
# Progression d√©taill√©e par worker
print(f"Processing frame {frame_idx + 1}/{total_frames} ({progress_percent:.1f}%)")
print(f"Face detection success rate: {face_success_rate:.1%}")
print(f"Enhanced speaking detection initialized")
```

#### M√©triques de Performance
```python
# Statistiques de d√©tection
logging.info(f"Face detection success rate: {face_success_rate:.1%}")
logging.info(f"Object detection fallback enabled due to low face detection rate")
logging.info(f"Using multiprocessing worker with {internal_workers} processes")

# Temps de traitement
start_time = time.time()
# ... traitement ...
processing_time = time.time() - start_time
logging.info(f"Video processing completed in {processing_time:.2f} seconds")
```

#### Monitoring via Logs Structur√©s
```python
# Progression d√©taill√©e
logging.info(f"--- D√©marrage du lot n¬∞{lot_number} ---")
logging.info(f"Pr√©paration du job {worker_type_log} pour: {video_name}")
logging.info(f"Applied CPU optimizations: lower confidence thresholds for better detection rate")
logging.info(f"Enhanced speaking detection initialized")
```

## D√©pendances et Pr√©requis

### Logiciels Externes Requis

#### MediaPipe (Obligatoire)
```bash
# Installation via pip
pip install mediapipe

# V√©rification de l'installation
python -c "import mediapipe as mp; print(f'MediaPipe version: {mp.__version__}')"
```

#### OpenCV (Obligatoire)
```bash
# Ubuntu/Debian
sudo apt update
sudo apt install python3-opencv

# Via pip
pip install opencv-python

# V√©rification
python -c "import cv2; print(f'OpenCV version: {cv2.__version__}')"
```

#### Support GPU NVIDIA (Optionnel)
```bash
# V√©rification du support CUDA
nvidia-smi
python -c "import cv2; print(cv2.cuda.getCudaEnabledDeviceCount())"

# Installation des drivers NVIDIA
sudo apt install nvidia-driver-470
```

### Versions Sp√©cifiques des Biblioth√®ques

#### Requirements Python (tracking_env/)
```txt
# Computer Vision et ML
mediapipe>=0.10.0
opencv-python>=4.5.0
numpy>=1.21.0

# Traitement parall√®le
concurrent.futures  # Inclus dans Python 3.2+
multiprocessing     # Inclus dans Python standard

# Utilitaires
pathlib2>=2.3.7    # Pour compatibilit√©
scipy>=1.7.0        # Pour optimisations KDTree
```

#### Installation Recommand√©e
```bash
# Cr√©ation de l'environnement
python -m venv tracking_env
source tracking_env/bin/activate

# Installation des d√©pendances principales
pip install mediapipe opencv-python numpy scipy

# V√©rification des installations
python -c "import mediapipe, cv2, numpy, scipy; print('All dependencies OK')"
```

### Configuration Syst√®me Recommand√©e

#### Ressources Minimales
- **RAM** : 8 GB minimum, 16 GB recommand√©
- **CPU** : 8 c≈ìurs minimum pour multiprocessing optimal (15 workers)
- **GPU** : NVIDIA GTX 1060 ou sup√©rieure (optionnel)
- **Espace disque** : 2 GB pour mod√®les MediaPipe + espace de travail

#### Mod√®les MediaPipe Requis
```bash
# Structure attendue (mod√®les de base)
workflow_scripts/step5/models/
‚îú‚îÄ‚îÄ face_detectors/
‚îÇ   ‚îî‚îÄ‚îÄ mediapipe/
‚îÇ       ‚îî‚îÄ‚îÄ face_landmarker_v2_with_blendshapes.task  # ~3.6 MB (face tracking)
‚îî‚îÄ‚îÄ object_detectors/
    ‚îî‚îÄ‚îÄ tflite/
        ‚îî‚îÄ‚îÄ EfficientDet-Lite2-32.tflite              # ~23 MB (object detection fallback, default)

# Mod√®les alternatifs support√©s (optionnels)
‚îî‚îÄ‚îÄ object_detectors/
    ‚îú‚îÄ‚îÄ tflite/
    ‚îÇ   ‚îú‚îÄ‚îÄ EfficientDet-Lite0.tflite                 # ~4.4 MB (plus rapide, Edge TPU compatible)
    ‚îÇ   ‚îú‚îÄ‚îÄ EfficientDet-Lite1.tflite                 # ~5.8 MB (√©quilibr√©)
    ‚îÇ   ‚îî‚îÄ‚îÄ ssd_mobilenet_v3.tflite                   # ~5 MB (stable, CPU optimis√©)
    ‚îî‚îÄ‚îÄ onnx/
        ‚îî‚îÄ‚îÄ yolo11n.onnx                              # ~5 MB (exp√©rimental, ONNX Runtime requis)

# Configuration via variables d'environnement (.env)
STEP5_OBJECT_DETECTOR_MODEL=efficientdet_lite2  # Mod√®le par d√©faut (r√©trocompatible)
STEP5_OBJECT_DETECTOR_MODEL_PATH=               # Override chemin (optionnel)
STEP5_ENABLE_OBJECT_DETECTION=0                 # Activer fallback object detection

# Mod√®les disponibles : efficientdet_lite0, efficientdet_lite1, efficientdet_lite2,
#                       ssd_mobilenet_v3, yolo11n_onnx, nanodet_plus
```

#### Choix du Mod√®le de D√©tection d'Objets

Le mod√®le de d√©tection d'objets est utilis√© comme **fallback** lorsque la d√©tection de visages √©choue (MediaPipe uniquement).

**Recommandations par hardware** :
- **Edge TPU / Coral** : `efficientdet_lite0` (100% compatible ops, ~50% plus rapide)
- **CPU ARM faible puissance** : `efficientdet_lite0` ou `nanodet_plus` (ONNX)
- **CPU desktop** : `efficientdet_lite0` (balance vitesse/pr√©cision) ou `yolo11n_onnx` (meilleure pr√©cision)
- **GPU** : `efficientdet_lite2` (r√©solution sup√©rieure, baseline actuel)

**Exemple de configuration** :
```bash
# Dans .env pour Edge TPU
STEP5_OBJECT_DETECTOR_MODEL=efficientdet_lite0
STEP5_ENABLE_OBJECT_DETECTION=1

# Pour YOLO11 (ONNX Runtime requis)
STEP5_OBJECT_DETECTOR_MODEL=yolo11n_onnx
STEP5_OBJECT_DETECTOR_MODEL_PATH=/path/to/yolo11n.onnx
STEP5_ENABLE_OBJECT_DETECTION=1
```

#### Optimisations Syst√®me
```bash
# Augmentation des limites de processus
echo 'kernel.pid_max = 4194304' | sudo tee -a /etc/sysctl.conf

# Optimisation pour multiprocessing
echo 'kernel.shmmax = 68719476736' | sudo tee -a /etc/sysctl.conf
echo 'kernel.shmall = 4294967296' | sudo tee -a /etc/sysctl.conf

# Optimisation CPU
echo 'performance' | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor
```

#### Configuration GPU
```bash
# Variables d'environnement CUDA
export CUDA_VISIBLE_DEVICES=0

# Optimisation GPU MediaPipe
export TF_FORCE_GPU_ALLOW_GROWTH=true
export TF_GPU_THREAD_MODE=gpu_private
```

## Debugging et R√©solution de Probl√®mes

### Erreurs Courantes et Solutions

#### 1. Erreur : "Mod√®les MediaPipe non trouv√©s"
```python
# Erreur
FileNotFoundError: face_landmarker.task not found

# Diagnostic
ls -la workflow_scripts/step5/models/
python -c "import mediapipe as mp; print(mp.__file__)"

# Solutions
# T√©l√©charger les mod√®les manuellement
wget https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task
mv face_landmarker.task workflow_scripts/step5/models/face_detectors/mediapipe/

# V√©rifier les permissions
chmod 644 workflow_scripts/step5/models/face_detectors/mediapipe/*.task
```

#### 2. Erreur : "Multiprocessing spawn error"
```python
# Erreur
RuntimeError: context has already been set

# Diagnostic
python -c "import multiprocessing; print(multiprocessing.get_start_method())"

# Solutions
# Forcer la m√©thode de d√©marrage
export PYTHONPATH=$PWD
python -c "import multiprocessing; multiprocessing.set_start_method('spawn', force=True)"

# Ou utiliser le worker s√©quentiel
python process_video_worker.py video.mp4 --models_dir models --mp_num_workers_internal 1
```

#### 3. Erreur : "GPU memory allocation failed"
```python
# Erreur
RuntimeError: CUDA out of memory

# Diagnostic
nvidia-smi
python -c "import cv2; print(cv2.cuda.getCudaEnabledDeviceCount())"

# Solutions
# Forcer l'utilisation CPU
python process_video_worker.py video.mp4 --models_dir models  # Sans --use_gpu

# R√©duire la r√©solution de traitement
# Ou traiter les vid√©os une par une
```

#### 4. Erreur : "Low face detection rate"
```python
# Warning
Low face detection rate (8%). Enabling object detection fallback.

# Diagnostic
# C'est un comportement normal pour vid√©os avec peu de visages
grep "Face detection success rate" logs/step5/tracking_*.log

# Solutions
# Ajuster les seuils de confiance
--mp_landmarker_min_face_detection_confidence 0.3
--mp_landmarker_min_face_presence_confidence 0.2

# Ou accepter le fallback vers d√©tection d'objets
```

### Logs Sp√©cifiques √† Surveiller

#### Logs de Progression
```bash
# Progression du traitement
grep "Processing frame" logs/step5/tracking_*.log
grep "Progression-MultiLine" logs/step5/tracking_*.log
grep "Face detection success rate" logs/step5/tracking_*.log
```

#### Logs d'Optimisations
```bash
# Utilisation des optimisations
grep "Using multiprocessing worker" logs/step5/tracking_*.log
grep "Applied CPU optimizations" logs/step5/tracking_*.log
grep "Object detection fallback" logs/step5/tracking_*.log
```

#### Logs d'Erreurs
```bash
# Erreurs de traitement
grep "ERROR" logs/step5/tracking_*.log
grep "Failed to" logs/step5/tracking_*.log
grep "Exception" logs/step5/tracking_*.log
```

### Tests de Validation et V√©rification

#### Test de Fonctionnement Basique
```bash
# Cr√©er une vid√©o de test avec visage
ffmpeg -f lavfi -i "testsrc=duration=5:size=640x480:rate=25" -vf "drawtext=text='TEST':fontsize=30:x=10:y=10" test_face.mp4

# Placer dans la structure attendue
mkdir -p test_tracking/docs
mv test_face.mp4 test_tracking/docs/

# Cr√©er les fichiers pr√©requis (vides pour test)
echo "No,Timecode In,Timecode Out,Frame In,Frame Out" > test_tracking/docs/test_face.csv
echo '{"video_filename":"test_face.mp4","total_frames":125,"fps":25.0,"frames_analysis":[]}' > test_tracking/docs/test_face_audio.json

# Ex√©cuter le tracking
source tracking_env/bin/activate
cd test_tracking
python ../workflow_scripts/step5/process_video_worker.py docs/test_face.mp4 --models_dir ../workflow_scripts/step5/models

# V√©rifier le r√©sultat
cat docs/test_face_tracking.json | jq '.metadata'
```

#### Test de Performance Multiprocessing
```bash
# Mesurer les performances CPU vs GPU
echo "Testing CPU multiprocessing (15 workers):"
time python process_video_worker_multiprocessing.py video.mp4 --models_dir models --mp_num_workers_internal 15

echo "Testing GPU sequential:"
time python process_video_worker.py video.mp4 --models_dir models --use_gpu

echo "Testing CPU sequential:"
time python process_video_worker.py video.mp4 --models_dir models --mp_num_workers_internal 1
```

#### Validation de l'Int√©grit√© des Sorties
```python
#!/usr/bin/env python3
"""Script de validation pour l'√©tape 5."""

def validate_step5_output():
    """Valide les fichiers JSON de tracking."""
    import json
    from pathlib import Path

    base_dir = Path("projets_extraits")
    video_extensions = ['.mp4', '.mov', '.avi', '.mkv', '.webm']

    for video_file in base_dir.rglob("*"):
        if video_file.suffix.lower() in video_extensions:
            json_file = video_file.with_name(f"{video_file.stem}_tracking.json")

            if not json_file.exists():
                print(f"‚ùå Fichier JSON manquant pour {video_file}")
                return False

            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                # V√©rifier la structure des m√©tadonn√©es
                if 'metadata' not in data or 'frames' not in data:
                    print(f"‚ùå Structure JSON invalide dans {json_file}")
                    return False

                metadata = data['metadata']
                required_metadata = ['video_path', 'total_frames', 'fps']
                if not all(field in metadata for field in required_metadata):
                    print(f"‚ùå M√©tadonn√©es manquantes dans {json_file}")
                    return False

                # V√©rifier la coh√©rence des frames
                if len(data['frames']) != metadata['total_frames']:
                    print(f"‚ùå Incoh√©rence nombre de frames dans {json_file}")
                    return False

                # V√©rifier la structure des objets track√©s
                total_tracked_objects = 0
                for frame_data in data['frames'][:10]:  # V√©rifier les 10 premi√®res
                    if 'tracked_objects' not in frame_data:
                        print(f"‚ùå Champ tracked_objects manquant dans {json_file}")
                        return False

                    for obj in frame_data['tracked_objects']:
                        required_obj_fields = ['id', 'bbox', 'centroid', 'source_detector']
                        if not all(field in obj for field in required_obj_fields):
                            print(f"‚ùå Champs objet manquants dans {json_file}")
                            return False
                        total_tracked_objects += 1

                print(f"‚úÖ {json_file}: {metadata['total_frames']} frames, {total_tracked_objects} objets track√©s")

            except Exception as e:
                print(f"‚ùå Erreur lors de la lecture de {json_file}: {e}")
                return False

    print("‚úÖ Validation r√©ussie: tous les fichiers JSON sont valides")
    return True

if __name__ == "__main__":
    validate_step5_output()
```

### Monitoring et Alertes

#### Surveillance des Performances CPU
```bash
# Monitoring continu pendant traitement multiprocessing
watch -n 1 'ps aux | grep process_video_worker | wc -l; echo "CPU Usage:"; top -bn1 | grep "Cpu(s)" | head -1'

# Log de l'utilisation CPU
while true; do
    cpu_usage=$(top -bn1 | grep "Cpu(s)" | awk '{print $2}' | sed 's/%us,//')
    worker_count=$(ps aux | grep process_video_worker | grep -v grep | wc -l)
    echo "$(date): CPU: $cpu_usage%, Workers: $worker_count"
    sleep 5
done > cpu_usage_tracking.log
```

#### Surveillance de la M√©moire
```bash
# Monitoring de l'utilisation m√©moire
watch -n 2 'free -h; echo ""; ps aux | grep process_video_worker | head -5'

# Alerte m√©moire critique
while true; do
    mem_usage=$(free | grep Mem | awk '{printf "%.0f", $3/$2 * 100}')
    if [ $mem_usage -gt 90 ]; then
        echo "ALERTE: Utilisation m√©moire critique ($mem_usage%)"
        # Optionnel: tuer les workers les plus gourmands
    fi
    sleep 30
done
```

#### M√©triques de Qualit√© de Tracking
```bash
# Analyse des taux de d√©tection
for json_file in projets_extraits/*/docs/*_tracking.json; do
    if [ -f "$json_file" ]; then
        face_rate=$(jq -r '.metadata.processing_info.face_detection_success_rate // 0' "$json_file")
        fallback_used=$(jq -r '.metadata.processing_info.object_detection_fallback_used // false' "$json_file")
        total_objects=$(jq -r '[.frames[].tracked_objects | length] | add' "$json_file")
        echo "$(basename "$json_file"): Face rate: $face_rate, Fallback: $fallback_used, Objects: $total_objects"
    fi
done

# D√©tection d'anomalies (pas d'objets track√©s)
find projets_extraits/ -name "*_tracking.json" -exec sh -c 'objects=$(jq -r "[.frames[].tracked_objects | length] | add" "$1" 2>/dev/null); if [ "$objects" = "0" ] || [ "$objects" = "null" ]; then echo "ANOMALIE: $1 (aucun objet track√©)"; fi' _ {} \;
