# Documentation Technique - √âtape 5 : Suivi Vid√©o et Blendshapes

> **Code-Doc Context** ‚Äì Part of the 7‚Äëstep pipeline; see `../README.md` for the uniform template. Backend hotspots: critical complexity in STEP5 workers (radon F/E), especially `process_video_worker.py` and `run_tracking_manager.py`.

---

## Purpose & Pipeline Role

### Objectif
L'√âtape 5 effectue le suivi vid√©o en temps r√©el avec d√©tection de visages, extraction de landmarks faciaux et g√©n√©ration de blendshapes ARKit. Cette √©tape combine plusieurs moteurs de tracking (MediaPipe, OpenCV, OpenSeeFace, EOS) pour fournir une analyse faciale compl√®te frame par frame.

### R√¥le dans le Pipeline
- **Position** : Cinqui√®me √©tape du pipeline (STEP5)
- **Pr√©requis** : Vid√©os standardis√©es (STEP2) et analyses audio (STEP4)
- **Sortie** : JSON dense avec tracked_objects, landmarks, et blendshapes par frame
- **√âtape suivante** : R√©duction JSON (STEP6)

### Valeur Ajout√©e
- **Multi-moteurs** : Support MediaPipe, OpenCV YuNet, OpenSeeFace, EOS 3DMM
- **GPU optionnel** : Acc√©l√©ration s√©lective pour MediaPipe et InsightFace
- **Blendshapes ARKit** : 52 blendshapes standard pour animation 3D
- **Multiprocessing** : Traitement parall√®le avec workers configurables
- **Export dense** : Structure JSON optimis√©e pour les analyses suivantes

---

## Inputs & Outputs

### Inputs
- **Vid√©os standardis√©es** : Fichiers vid√©o √† 25 FPS de STEP2
- **Analyses audio** : JSON diarization de STEP4 pour d√©tection de parole
- **Configuration** : Moteur de tracking, param√®tres GPU/CPU

### Outputs
- **JSON tracking** : Structure frame par frame avec tracked_objects
- **Landmarks faciaux** : 468 points MediaPipe ou √©quivalents
- **Blendshapes** : 52 coefficients ARKit par visage d√©tect√©
- **Logs d√©taill√©s** : Journal de tracking dans `logs/step5/`

---

## Command & Environment

### Commande WorkflowCommandsConfig
```python
# Exemple de commande (voir WorkflowCommandsConfig pour la commande exacte)
python workflow_scripts/step5/run_tracking_manager.py --input-dir projets_extraits/ --engine mediapipe --workers 15
```

### Environnement Virtuel
- **Environnement utilis√©** : `tracking_env/` (sp√©cialis√© MediaPipe)
- **Activation** : `source tracking_env/bin/activate`
- **Isolation** : Environnement d√©di√© pour MediaPipe et OpenCV

---

## Dependencies

### Biblioth√®ques Principales
```python
import mediapipe as mp                    # Framework ML pour vision
import cv2                               # OpenCV pour traitement d'image
import numpy as np                       # Calculs num√©riques
import onnxruntime                       # ONNX pour mod√®les optimis√©s
import multiprocessing                    # Multi-processing
```

### D√©pendances Externes
- **MediaPipe** : Face Landmarker avec support GPU
- **OpenCV** : YuNet et autres d√©tecteurs
- **ONNX Runtime** : Mod√®les optimis√©s (FaceMesh, EOS)
- **CUDA** : Acc√©l√©ration GPU optionnelle

---

## Configuration

### Variables d'Environnement
- **STEP5_TRACKING_ENGINE** : `mediapipe`, `opencv_yunet_pyfeat`, `openseeface`, `eos`
- **STEP5_ENABLE_GPU** : Activation GPU (d√©faut: 0)
- **STEP5_GPU_ENGINES** : Moteurs autoris√©s en GPU
- **TRACKING_CPU_WORKERS** : Nombre de workers CPU (d√©faut: 15)
- **STEP5_BLENDSHAPES_THROTTLE_N** : Throttling blendshapes
- **STEP5_EXPORT_VERBOSE_FIELDS** : Contr√¥le verbosit√© export

### Configuration par Moteur
```json
{
  "mediapipe": {
    "max_faces": 5,
    "min_detection_confidence": 0.5,
    "model_complexity": 1
  },
  "opencv_yunet_pyfeat": {
    "max_faces": 5,
    "yunet_max_width": 640
  }
}
```

---

## Known Hotspots

### Complexit√© Backend (Critique)
- **`process_video_worker.py`** : Complexit√© critique (radon F) dans `main` et `process_frame_chunk`
- **`run_tracking_manager.py`** : Complexit√© critique (radon F) dans `main`
- **`face_engines.py`** : Complexit√© √©lev√©e (radon E) dans `detect` (InsightFace, EOS)
- **Points d'attention** : Gestion multiprocessing, lazy imports MediaPipe, profiling

---

## Multiprocessing Hotspots (Radon F)

### Architecture Workers
- **`process_video_worker_multiprocessing.py`** : Orchestrateur principal (Score F: 315 lignes)
- **`init_worker_process`** : Initialisation worker (Score F: 96 lignes) 
- **`process_frame_chunk`** : Traitement par chunk (Score F: 315 lignes)
- **`process_video_worker.py main`** : Worker principal (Score F: 399 lignes)

### Points chauds (Complexit√© F)
- **Gestion des chunks** : D√©coupage vid√©o en segments pour traitement parall√®le
- **Synchronisation IPC** : Communication inter-processus et partage d'√©tat
- **Gestion erreurs** : Recovery et fallback en cas d'√©chec worker
- **Optimisations** : Profiling toutes les 20 frames, throttling configurables

### Patterns de Communication
```python
# Structure IPC typique
worker_queue = multiprocessing.Queue()
result_queue = multiprocessing.Queue()

# Chunk processing
def process_frame_chunk(frames_chunk, config):
    # Traitement parall√®le avec logging int√©gr√©
    # Gestion OOM et recovery automatique
```

### Recommandations Refactoring
- **Documenter les patterns IPC** : √âchanges entre manager et workers
- **Simplifier `process_frame_chunk`** : Extraire helpers sp√©cialis√©s
- **Monitoring continu** : Logs `[PROFILING]` pour tuning performance

---

## Workers Multiprocessing

### Architecture
- **`process_video_worker_multiprocessing.py`** : Orchestrateur principal
- **`init_worker_process`** : Initialisation worker multiprocessing
- **`process_frame_chunk`** : Traitement par chunk
- **`process_video_worker.py main`** : Worker principal

### Complexit√© Radon
- **Tous les workers** : Score F (complexit√© critique)
- **Causes** : Gestion IPC, chunking, error recovery, profiling
- **Impact** : C≈ìur du pipeline de suivi vid√©o

### Patterns de Communication
```python
# Structure IPC typique
worker_queue = multiprocessing.Queue()
result_queue = multiprocessing.Queue()

# Chunk processing
def process_frame_chunk(frames_chunk, config):
    # Traitement parall√®le avec logging int√©gr√©
    # Gestion OOM et recovery automatique
```

### Recommandations
- **Documenter les patterns IPC** : √âchanges entre manager et workers
- **Simplifier `process_frame_chunk`** : Extraire helpers sp√©cialis√©s
- **Monitoring continu** : Logs `[PROFILING]` pour tuning performance

---

## Known Hotspots

### Backend Complexity (Radon Analysis)
- **`process_video_worker.py main`** (Score F) : 399 lignes, orchestration worker
- **`process_frame_chunk`** (Score F) : 315 lignes, traitement chunks parall√®les  
- **`init_worker_process`** (Score F) : 96 lignes, initialisation multiprocessing
- **`run_tracking_manager.py main`** (Score F) : 491 lignes, gestion STEP5

### Impact sur la Performance
- **Multiprocessing** : Parall√©lisme efficace mais complexit√© √©lev√©e
- **GPU Management** : Lazy imports et configuration CUDA
- **Memory Management** : Gestion OOM et nettoyage ressources

---

## Metrics & Monitoring

### Indicateurs de Performance
- **D√©bit de tracking** : FPS trait√©s par worker
- **Utilisation GPU** : % GPU et m√©moire VRAM
- **Pr√©cision d√©tection** : Nombre de visages d√©tect√©s
- **Taux de succ√®s** : % frames trait√©es avec succ√®s

### Patterns de Logging
```python
# Logs de progression
logger.info(f"Tracking {video_path} - {current}/{total}")

# Logs profiling (toutes les 20 frames)
if frame_count % 20 == 0:
    logger.info(f"[PROFILING] Engine: {engine}, FPS: {fps:.2f}")

# Logs GPU
logger.info(f"ONNX providers: {onnxruntime.get_providers()}")

# Logs d'erreur
logger.error(f"√âchec tracking {video_path}: {error}")
```

---

## Failure & Recovery

### Modes d'√âchec Communs
1. **GPU OOM** : Basculement automatique sur CPU
2. **Mod√®le non charg√©** : Retry avec t√©l√©chargement du mod√®le
3. **Worker crash** : Red√©marrage automatique du worker
4. **Timeout** : Augmentation du d√©lai ou r√©duction workers

### Proc√©dures de R√©cup√©ration
```bash
# R√©essayer avec CPU uniquement
STEP5_ENABLE_GPU=0 python workflow_scripts/step5/run_tracking_manager.py

# R√©duire les workers
TRACKING_CPU_WORKERS=4 python workflow_scripts/step5/run_tracking_manager.py

# Validation post-tracking
python scripts/validate_step5_output.py
```

---

## Related Documentation

- **Pipeline Overview** : `../README.md`
- **GPU Usage Guide** : `../pipeline/STEP5_GPU_USAGE.md`
- **OpenCV YuNet/PyFeat** : `../pipeline/STEP5_OPENCV_YUNET_PYFEAT.md`
- **Testing Strategy** : `../technical/TESTING_STRATEGY.md`
- **WorkflowState Integration** : `../core/ARCHITECTURE_COMPLETE_FR.md`

---

*Generated with Code-Doc protocol ‚Äì see `../cloc_stats.json` and `../complexity_report.txt`.*

### Formats d'Entr√©e et de Sortie

#### Structure d'Entr√©e Attendue
```
projets_extraits/
‚îú‚îÄ‚îÄ projet_camille_001/
‚îÇ   ‚îî‚îÄ‚îÄ docs/
‚îÇ       ‚îú‚îÄ‚îÄ video1.mp4           # Vid√©o source
‚îÇ       ‚îú‚îÄ‚îÄ video1.csv           # Sc√®nes (STEP3)
‚îÇ       ‚îú‚îÄ‚îÄ video1_audio.json    # Analyse audio (STEP4)
‚îÇ       ‚îú‚îÄ‚îÄ video2.mov           # Vid√©o source
‚îÇ       ‚îú‚îÄ‚îÄ video2.csv           # Sc√®nes (STEP3)
‚îÇ       ‚îî‚îÄ‚îÄ video2_audio.json    # Analyse audio (STEP4)
```

#### Structure de Sortie G√©n√©r√©e
```
projets_extraits/
‚îú‚îÄ‚îÄ projet_camille_001/
‚îÇ   ‚îî‚îÄ‚îÄ docs/
‚îÇ       ‚îú‚îÄ‚îÄ video1.mp4
‚îÇ       ‚îú‚îÄ‚îÄ video1.csv
‚îÇ       ‚îú‚îÄ‚îÄ video1_audio.json
‚îÇ       ‚îú‚îÄ‚îÄ video1_tracking.json     # Donn√©es de tracking
‚îÇ       ‚îú‚îÄ‚îÄ video2.mov
‚îÇ       ‚îú‚îÄ‚îÄ video2.csv
‚îÇ       ‚îú‚îÄ‚îÄ video2_audio.json
‚îÇ       ‚îî‚îÄ‚îÄ video2_tracking.json     # Donn√©es de tracking
```

### Param√®tres de Configuration

#### Configuration d'ex√©cution

**Mode par d√©faut (v4.1)**: L'√âtape 5 utilise exclusivement le CPU avec 15 workers internes par d√©faut, offrant de meilleures performances globales sur la configuration actuelle.

**Mode GPU (v4.2+)** : Le GPU est **strictement r√©serv√©** au moteur InsightFace. Tous les autres moteurs (MediaPipe, OpenSeeFace, OpenCV, EOS) s'ex√©cutent en mode CPU m√™me si `STEP5_ENABLE_GPU=1`. Le gestionnaire :
- force `args.disable_gpu=True` pour tout moteur non list√© dans `STEP5_GPU_ENGINES`.
- v√©rifie la disponibilit√© mat√©rielle via `Config.check_gpu_availability()` avant de lancer InsightFace.
- bascule automatiquement en mode CPU si `STEP5_GPU_FALLBACK_AUTO=1` et qu'une contrainte est d√©tect√©e (VRAM insuffisante, absence du provider CUDA, etc.).

#### Restrictions GPU (d√©cision 2025-12-27)
‚ö†Ô∏è **IMPORTANT** : Le support GPU est **r√©serv√© exclusivement √† InsightFace**
- **MediaPipe Face Landmarker** : CPU-only (15 workers)
- **OpenSeeFace** : CPU-only (multiprocessing) 
- **OpenCV YuNet/PyFeat** : CPU-only
- **EOS** : CPU-only (3DMM fitting)
- Le gestionnaire force `args.disable_gpu=True` pour tous les moteurs non-InsightFace

‚ö†Ô∏è **Contraintes GPU** :
- 1 worker GPU s√©quentiel uniquement (pas de parall√©lisation).
- N√©cessite un GPU NVIDIA (CUDA ‚â• 12.0) avec ‚â•‚ÄØ2‚ÄØGo de VRAM libres (4‚ÄØGo recommand√©s pour coexister avec STEP2/NVENC).
- `insightface_env` embarque ONNX Runtime GPU + d√©pendances InsightFace ; override possible via `STEP5_INSIGHTFACE_ENV_PYTHON`.
- CPU-only reste recommand√© pour les batchs massifs (10+ vid√©os) et demeure le mode par d√©faut (`TRACKING_DISABLE_GPU=1`).
- `STEP5_GPU_PROFILING=1` journalise l‚Äôusage VRAM et les timings toutes les 20 frames pour InsightFace.

#### Chunking adaptatif (depuis 2026-01-18)
- Le chunking adaptatif est d√©sormais enti√®rement g√©r√© c√¥t√© backend avec des **bornes internes par d√©faut** (‚âà20 chunks min, ‚âà400 chunks max) afin de saturer les workers CPU tout en √©vitant la fragmentation.
- **Plus aucune configuration dynamique** n‚Äôest expos√©e : l‚ÄôAPI `/api/step5/chunk_bounds`, les variables `TRACKING_CHUNK_MIN/MAX` et les contr√¥les UI associ√©s ont √©t√© retir√©s.
- Lorsqu‚Äôun worker multiprocessing se lance, il journalise toujours `Adaptive chunking enabled ... selected_chunk_size=XXX` pour v√©rifier l‚Äôapplication automatique de ces bornes.
- Pour les sc√©narios sp√©ciaux, la recommandation officielle est d‚Äôajuster le nombre de workers (`TRACKING_CPU_WORKERS`) ou de basculer en mode GPU InsightFace (s√©quentiel) plut√¥t que de modifier la taille des chunks.

## Moteurs de D√©tection Faciale

### Moteurs Disponibles

1. **MediaPipe Face Landmarker** (par d√©faut)
   - Utilise `face_landmarker_v2_with_blendshapes.task`
   - Support natif des blendshapes ARKit
   - Optimis√© pour la d√©tection en temps r√©el
   - **Mode CPU-only** : optimis√© pour 15 workers multiprocessing.

2. **OpenCV Haar Cascade**
   - Moteur de base pour la d√©tection de visages
   - Moins pr√©cis mais tr√®s rapide
   - Utile pour les cas simples ou le mat√©riel limit√©

3. **OpenCV YuNet**
   - D√©tecteur de visages bas√© sur CNN
   - Mod√®le : `face_detection_yunet_2023mar.onnx`
   - Configurable via `STEP5_YUNET_MODEL_PATH`

4. **OpenCV YuNet + PyFeat**
   - Combine YuNet pour la d√©tection et PyFeat pour les expressions
   - Extrait des blendshapes avanc√©s
   - Activation : `--face_engine opencv_yunet_pyfeat`
   - - **Mode CPU-only** : optimis√© pour 15 workers multiprocessing.

5. **OpenSeeFace**
   - Alternative open source compl√®te
   - N√©cessite des mod√®les sp√©cifiques dans `STEP5_OPENSEEFACE_MODELS_DIR`
   - Activation : `--face_engine openseeface`
   - - **Mode CPU-only** : optimis√© pour multiprocessing.

6. **EOS (3D Morphable Model)**
   - Mod√®le 3D param√©trique pour l'ajustement pr√©cis des expressions
   - Utilise YuNet pour la d√©tection initiale puis ajuste 68 points 3D
   - S'ex√©cute dans un **environnement d√©di√© `eos_env`** (rout√© automatiquement par `run_tracking_manager.py`, override possible via `STEP5_EOS_ENV_PYTHON`)
   - Activation : `--face_engine eos` ou `STEP5_TRACKING_ENGINE=eos`
   - Variables d'environnement cl√©s :
     ```env
     STEP5_EOS_MODELS_DIR=workflow_scripts/step5/models/engines/eos/share   # peut pointer vers /home/kidpixel6/kidpixel_assets/eos/share
     STEP5_EOS_SFM_MODEL_PATH=${STEP5_EOS_MODELS_DIR}/sfm_model.bin
     STEP5_EOS_EXPRESSION_BLENDSHAPES_PATH=${STEP5_EOS_MODELS_DIR}/expression_blendshapes_57.bin
     STEP5_EOS_LANDMARK_MAPPER_PATH=${STEP5_EOS_MODELS_DIR}/ibug_to_eos_landmarks.json
     STEP5_EOS_EDGE_TOPOLOGY_PATH=${STEP5_EOS_MODELS_DIR}/sfm_3448_edge_topology.json
     STEP5_EOS_MODEL_CONTOUR_PATH=${STEP5_EOS_MODELS_DIR}/sfm_model_contours.json
     STEP5_EOS_CONTOUR_LANDMARKS_PATH=${STEP5_EOS_MODELS_DIR}/ibug_to_eos_contour_landmarks.json
     STEP5_EOS_FIT_EVERY_N=2                         # fallback auto sur STEP5_BLENDSHAPES_THROTTLE_N si absent
     STEP5_EOS_MAX_WIDTH=1280                        # downscale + rescale coordonn√©es/landmarks
     STEP5_EOS_MAX_FACES=1                           # optionnel
     STEP5_ENABLE_PROFILING=1                        # logs [PROFILING] toutes les 20 frames (YuNet, FaceMesh, fit eos)
     ```
   - Exporte `tracked_objects[].eos = {shape_coeffs, expression_coeffs}` et `landmarks` 68x3 (toujours rescal√©s).
   - Les assets peuvent √™tre install√©s hors repo (NAS/SSD). Il suffit d'ajuster `STEP5_EOS_MODELS_DIR`.
   - `workflow_scripts/step5/process_video_worker_multiprocessing.py` charge `.env` c√¥t√© worker pour propager l'ensemble de ces variables √† chaque sous-processus.

   > üí§ **Lazy import MediaPipe** : `process_video_worker.py` dispose de `_ensure_mediapipe_loaded(required=False)` afin d‚Äô√©viter l‚Äôimport du module tant que le moteur MediaPipe/objets n‚Äôest pas sollicit√©. Les moteurs OpenCV/EOS l‚Äôappellent en mode `required=False`, ce qui supprime les crashs TensorFlow lorsque seules les d√©pendances OpenCV sont install√©es. Quand MediaPipe est indispensable (`required=True`), l‚Äôerreur est logg√©e puis relanc√©e pour guider l‚Äôutilisateur.

7. **InsightFace (GPU s√©quentiel)**
   - Moteur ONNX Runtime r√©serv√© au mode GPU (`STEP5_TRACKING_ENGINE=insightface`).
   - Requiert l‚Äôenvironnement `insightface_env` et un GPU NVIDIA compatible CUDA‚ÄØ‚â•‚ÄØ12 (‚â•‚ÄØ2‚ÄØGo VRAM libres, 4‚ÄØGo recommand√©s).
   - Les variables `STEP5_ENABLE_GPU`, `STEP5_GPU_ENGINES=insightface` et `STEP5_INSIGHTFACE_*` (chemins mod√®les, throttling, overrides Python) se valident via `config/settings.py` @docs/workflow/core/GUIDE_DEMARRAGE_RAPIDE.md#125-189.
   - Respecte la d√©cision du 27‚ÄØd√©cembre‚ÄØ2025 : **aucun autre moteur n‚Äôest autoris√© sur GPU** (@memory-bank/decisionLog.md).
   - Profil recommand√© : 1 worker GPU s√©quentiel, chunking automatique + fallback CPU (`STEP5_GPU_FALLBACK_AUTO=1`).

8. **Maxine (NVIDIA AR SDK)**
   - Moteur exp√©rimental accessible via `STEP5_TRACKING_ENGINE=maxine` lorsque les biblioth√®ques Maxine sont install√©es (non distribu√©es dans le d√©p√¥t).
   - Doit √™tre explicitement list√© dans `STEP5_GPU_ENGINES` pour activer les optimisations CUDA des filtres Maxine ; sinon il fonctionne en mode CPU.
   - `STEP5_MAXINE_ENV_PYTHON` (optionnel) permet de pointer vers un environnement sp√©cialis√© bas√© sur les exemples NVIDIA Maxine.
   - Destin√© aux installations avanc√©es (studios) : v√©rifier les licences Maxine et mettre √† jour `config/settings.py` pour renseigner les binaires.

### Optimisations r√©centes (D√©cembre 2025)

| Optimisation | Description | Variables cl√©s |
|--------------|-------------|----------------|
| Downscale YuNet / OpenSeeFace | YuNet et OpenSeeFace plafonnent la largeur d‚Äôanalyse (`STEP5_YUNET_MAX_WIDTH`, `STEP5_OPENSEEFACE_MAX_WIDTH`) et rescalaient automatiquement les coordonn√©es/landmarks (logs DEBUG pour tracer les facteurs). | `STEP5_YUNET_MAX_WIDTH`, `STEP5_OPENSEEFACE_MAX_WIDTH` |
| Profiling g√©n√©ralis√© | Les workers rechargent `.env` avant chaque chunk pour propager `STEP5_ENABLE_PROFILING`, `STEP5_BLENDSHAPES_THROTTLE_N`, `STEP5_EOS_*`. Logs `[PROFILING]` toutes les 20 frames, m√™me sur de petits chunks multiprocessing. | Variables `STEP5_*` |
| Filtrage des blendshapes | `STEP5_BLENDSHAPES_PROFILE` (`full`, `mouth`, `mediapipe`, `custom`, `none`) + `STEP5_BLENDSHAPES_EXPORT_KEYS` r√©duisent la taille JSON (jusqu‚Äô√† -95‚ÄØ%) tout en conservant la compatibilit√© STEP6. | `STEP5_BLENDSHAPES_PROFILE`, `STEP5_BLENDSHAPES_EXPORT_KEYS`, `STEP5_BLENDSHAPES_INCLUDE_TONGUE` |
| Registry Object Detector | `workflow_scripts/step5/object_detector_registry.py` centralise EfficientDet/SSD/YOLO/NanoDet et applique l‚Äôoverride `STEP5_OBJECT_DETECTOR_MODEL_PATH` si fourni. | `STEP5_ENABLE_OBJECT_DETECTION`, `STEP5_OBJECT_DETECTOR_MODEL`, `STEP5_OBJECT_DETECTOR_MODEL_PATH` |
| JSON all√©g√© | `STEP5_EXPORT_VERBOSE_FIELDS=0` (d√©faut) supprime l‚Äôexport des `landmarks`/`eos` pour les moteurs non MediaPipe afin d‚Äôacc√©l√©rer STEP6 et r√©duire les transferts. | `STEP5_EXPORT_VERBOSE_FIELDS` |
| Warmup & seek robustes | Les workers OpenCV lisent une frame avant `cap.set()` et ins√®rent un placeholder si la frame est illisible, garantissant un JSON dense (1..N). | Impl√©ment√© dans `process_video_worker_multiprocessing.py` |

### Registry de d√©tection d‚Äôobjets

```
workflow_scripts/step5/object_detector_registry.py
‚îú‚îÄ‚îÄ efficientdet_lite0/1/2 (tflite)
‚îú‚îÄ‚îÄ ssd_mobilenet_v3 (tflite/tensorflow)
‚îú‚îÄ‚îÄ yolo11n (onnx)
‚îî‚îÄ‚îÄ nanodet_plus (onnx)
```

- `STEP5_OBJECT_DETECTOR_MODEL=efficientdet_lite2` pointe par d√©faut sur `workflow_scripts/step5/models/object_detectors/tflite/EfficientDet-Lite2-32.tflite`.
- Override absolu/relatif via `STEP5_OBJECT_DETECTOR_MODEL_PATH`.
- Le fallback MediaPipe Tasks fonctionne en mode `RunningMode.IMAGE` multi-threads (dimensionn√©s par `TRACKING_CPU_WORKERS`) pour InsightFace GPU, et single-thread pour les moteurs CPU historiques afin d‚Äô√©viter la contention.

### JSON d‚Äôexport & r√©duction

- `tracked_objects` reste dense : m√™me sans d√©tection, un tableau vide est √©mis par frame pour pr√©server l‚Äôalignement avec STEP6/7.
- `STEP5_EXPORT_VERBOSE_FIELDS=0` √©vite l‚Äô√©criture des champs volumineux (`landmarks`, `eos`) pour la plupart des moteurs ; activer ce flag uniquement pour le debug ou lorsque STEP6 requiert un export complet.
- Les logs `[Progression-MultiLine]` signalent lorsqu‚Äôun chunk bascule en mode r√©duit, facilitant le suivi depuis `WorkflowState`.

### Gestionnaire STEP5 & Routage des Environnements

- `workflow_scripts/step5/run_tracking_manager.py` charge automatiquement `config.settings` pour r√©cup√©rer les chemins des virtualenvs via `config.get_venv_python(<venv>)`.  
  - ‚úÖ `tracking_env` est la valeur par d√©faut.  
  - ‚úÖ Lorsque `STEP5_TRACKING_ENGINE=eos`, le gestionnaire bascule sur `eos_env` (override possible via `STEP5_EOS_ENV_PYTHON`).  
- `_EnvConfig` centralise d√©sormais la lecture typ√©e des variables `STEP5_*` (GPU, engines, workers, overrides). Le manager s‚Äôappuie sur cette couche pour :
  - appliquer les restrictions InsightFace GPU-only (`STEP5_ENABLE_GPU=1`, `STEP5_GPU_ENGINES`), 
  - construire un environnement subprocess via `_build_subprocess_env()` qui injecte automatiquement `LD_LIBRARY_PATH` (CUDA libs d√©couvertes dans les venvs + chemins syst√®me `/usr/local/cuda*` lorsque n√©cessaire),
  - propager les limites CPU (`OMP_NUM_THREADS`, `OPENBLAS_NUM_THREADS`, etc.) afin d‚Äô√©viter la contention inter-process,
  - router chaque moteur vers son interpr√©teur d√©di√© (`tracking_env`, `eos_env`, `insightface_env`, ou `STEP5_TF_GPU_ENV_PYTHON` pour MediaPipe GPU) avec v√©rification d‚Äôexistence et messages d‚Äôerreur explicites.
- Les workers multiprocessing rechargent toujours `.env` pour r√©cup√©rer ces variables √† chaque fork, garantissant que les r√©glages (`STEP5_BLENDSHAPES_THROTTLE_N`, profil d‚Äôexport, profiling) restent synchronis√©s m√™me en mode chunk√©.
