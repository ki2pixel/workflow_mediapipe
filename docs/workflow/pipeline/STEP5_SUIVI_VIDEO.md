## Sp√©cifications Techniques

### Environnement Virtuel
- **Environnement utilis√©** : `tracking_env/` (sp√©cialis√© MediaPipe)
- **Activation** : `source tracking_env/bin/activate`
- **Isolation** : Environnement d√©di√© pour MediaPipe et OpenCV

### Technologies et Biblioth√®ques Principales

#### MediaPipe et Computer Vision
```python
import mediapipe as mp                    # Framework de ML pour vision
from mediapipe.tasks import python       # API Python MediaPipe
from mediapipe.tasks.python import vision # T√¢ches de vision
import cv2                               # OpenCV pour traitement d'image
import numpy as np                       # Calculs num√©riques
```

#### Traitement Parall√®le et Optimisations
```python
from concurrent.futures import ThreadPoolExecutor  # Multi-threading
import multiprocessing                              # Multi-processing
import threading                                    # Synchronisation
import queue                                        # Communication inter-threads
```

#### Modules Personnalis√©s
```python
from utils.tracking_optimizations import apply_tracking_and_management
from utils.enhanced_speaking_detection import EnhancedSpeakingDetector
from utils.resource_manager import safe_video_processing
```

### Formats d'Entr√©e et de Sortie

#### Structure d'Entr√©e Attendue
```
projets_extraits/
‚îú‚îÄ‚îÄ projet_camille_001/
‚îÇ   ‚îî‚îÄ‚îÄ docs/
‚îÇ       ‚îú‚îÄ‚îÄ video1.mp4           # Vid√©o source
‚îÇ       ‚îú‚îÄ‚îÄ video1.csv           # Sc√®nes (STEP3)
‚îÇ       ‚îú‚îÄ‚îÄ video1_audio.json    # Analyse audio (STEP4)
‚îÇ       ‚îú‚îÄ‚îÄ video2.mov           # Vid√©o source
‚îÇ       ‚îú‚îÄ‚îÄ video2.csv           # Sc√®nes (STEP3)
‚îÇ       ‚îî‚îÄ‚îÄ video2_audio.json    # Analyse audio (STEP4)
```

#### Structure de Sortie G√©n√©r√©e
```
projets_extraits/
‚îú‚îÄ‚îÄ projet_camille_001/
‚îÇ   ‚îî‚îÄ‚îÄ docs/
‚îÇ       ‚îú‚îÄ‚îÄ video1.mp4
‚îÇ       ‚îú‚îÄ‚îÄ video1.csv
‚îÇ       ‚îú‚îÄ‚îÄ video1_audio.json
‚îÇ       ‚îú‚îÄ‚îÄ video1_tracking.json     # Donn√©es de tracking
‚îÇ       ‚îú‚îÄ‚îÄ video2.mov
‚îÇ       ‚îú‚îÄ‚îÄ video2.csv
‚îÇ       ‚îú‚îÄ‚îÄ video2_audio.json
‚îÇ       ‚îî‚îÄ‚îÄ video2_tracking.json     # Donn√©es de tracking
```

### Param√®tres de Configuration

#### Configuration d'ex√©cution

**Mode par d√©faut (v4.1)**: L'√âtape 5 utilise exclusivement le CPU avec 15 workers internes par d√©faut, offrant de meilleures performances globales sur la configuration actuelle.

**Mode GPU (v4.2+)** : Le GPU est **strictement r√©serv√©** au moteur InsightFace. Tous les autres moteurs (MediaPipe, OpenSeeFace, OpenCV, EOS) s'ex√©cutent en mode CPU m√™me si `STEP5_ENABLE_GPU=1`. Le gestionnaire :
- force `args.disable_gpu=True` pour tout moteur non list√© dans `STEP5_GPU_ENGINES`.
- v√©rifie la disponibilit√© mat√©rielle via `Config.check_gpu_availability()` avant de lancer InsightFace.
- bascule automatiquement en mode CPU si `STEP5_GPU_FALLBACK_AUTO=1` et qu'une contrainte est d√©tect√©e (VRAM insuffisante, absence du provider CUDA, etc.).

#### Restrictions GPU (d√©cision 2025-12-27)
‚ö†Ô∏è **IMPORTANT** : Le support GPU est **r√©serv√© exclusivement √† InsightFace**
- **MediaPipe Face Landmarker** : CPU-only (15 workers)
- **OpenSeeFace** : CPU-only (multiprocessing) 
- **OpenCV YuNet/PyFeat** : CPU-only
- **EOS** : CPU-only (3DMM fitting)
- Le gestionnaire force `args.disable_gpu=True` pour tous les moteurs non-InsightFace

‚ö†Ô∏è **Contraintes GPU** :
- 1 worker GPU s√©quentiel uniquement (pas de parall√©lisation).
- N√©cessite un GPU NVIDIA (CUDA ‚â• 12.0) avec ‚â•‚ÄØ2‚ÄØGo de VRAM libres (4‚ÄØGo recommand√©s pour coexister avec STEP2/NVENC).
- `insightface_env` embarque ONNX Runtime GPU + d√©pendances InsightFace ; override possible via `STEP5_INSIGHTFACE_ENV_PYTHON`.
- CPU-only reste recommand√© pour les batchs massifs (10+ vid√©os) et demeure le mode par d√©faut (`TRACKING_DISABLE_GPU=1`).
- `STEP5_GPU_PROFILING=1` journalise l‚Äôusage VRAM et les timings toutes les 20 frames pour InsightFace.

## Moteurs de D√©tection Faciale

### Moteurs Disponibles

1. **MediaPipe Face Landmarker** (par d√©faut)
   - Utilise `face_landmarker_v2_with_blendshapes.task`
   - Support natif des blendshapes ARKit
   - Optimis√© pour la d√©tection en temps r√©el
   - **Mode CPU-only** : optimis√© pour 15 workers multiprocessing.

2. **OpenCV Haar Cascade**
   - Moteur de base pour la d√©tection de visages
   - Moins pr√©cis mais tr√®s rapide
   - Utile pour les cas simples ou le mat√©riel limit√©

3. **OpenCV YuNet**
   - D√©tecteur de visages bas√© sur CNN
   - Mod√®le : `face_detection_yunet_2023mar.onnx`
   - Configurable via `STEP5_YUNET_MODEL_PATH`

4. **OpenCV YuNet + PyFeat**
   - Combine YuNet pour la d√©tection et PyFeat pour les expressions
   - Extrait des blendshapes avanc√©s
   - Activation : `--face_engine opencv_yunet_pyfeat`
   - - **Mode CPU-only** : optimis√© pour 15 workers multiprocessing.

5. **OpenSeeFace**
   - Alternative open source compl√®te
   - N√©cessite des mod√®les sp√©cifiques dans `STEP5_OPENSEEFACE_MODELS_DIR`
   - Activation : `--face_engine openseeface`
   - - **Mode CPU-only** : optimis√© pour multiprocessing.

6. **EOS (3D Morphable Model)**
   - Mod√®le 3D param√©trique pour l'ajustement pr√©cis des expressions
   - Utilise YuNet pour la d√©tection initiale puis ajuste 68 points 3D
   - S'ex√©cute dans un **environnement d√©di√© `eos_env`** (rout√© automatiquement par `run_tracking_manager.py`, override possible via `STEP5_EOS_ENV_PYTHON`)
   - Activation : `--face_engine eos` ou `STEP5_TRACKING_ENGINE=eos`
   - Variables d'environnement cl√©s :
     ```env
     STEP5_EOS_MODELS_DIR=workflow_scripts/step5/models/engines/eos/share   # peut pointer vers /home/kidpixel6/kidpixel_assets/eos/share
     STEP5_EOS_SFM_MODEL_PATH=${STEP5_EOS_MODELS_DIR}/sfm_model.bin
     STEP5_EOS_EXPRESSION_BLENDSHAPES_PATH=${STEP5_EOS_MODELS_DIR}/expression_blendshapes_57.bin
     STEP5_EOS_LANDMARK_MAPPER_PATH=${STEP5_EOS_MODELS_DIR}/ibug_to_eos_landmarks.json
     STEP5_EOS_EDGE_TOPOLOGY_PATH=${STEP5_EOS_MODELS_DIR}/sfm_3448_edge_topology.json
     STEP5_EOS_MODEL_CONTOUR_PATH=${STEP5_EOS_MODELS_DIR}/sfm_model_contours.json
     STEP5_EOS_CONTOUR_LANDMARKS_PATH=${STEP5_EOS_MODELS_DIR}/ibug_to_eos_contour_landmarks.json
     STEP5_EOS_FIT_EVERY_N=2                         # fallback auto sur STEP5_BLENDSHAPES_THROTTLE_N si absent
     STEP5_EOS_MAX_WIDTH=1280                        # downscale + rescale coordonn√©es/landmarks
     STEP5_EOS_MAX_FACES=1                           # optionnel
     STEP5_ENABLE_PROFILING=1                        # logs [PROFILING] toutes les 20 frames (YuNet, FaceMesh, fit eos)
     ```
   - Exporte `tracked_objects[].eos = {shape_coeffs, expression_coeffs}` et `landmarks` 68x3 (toujours rescal√©s).
   - Les assets peuvent √™tre install√©s hors repo (NAS/SSD). Il suffit d'ajuster `STEP5_EOS_MODELS_DIR`.
   - `workflow_scripts/step5/process_video_worker_multiprocessing.py` charge `.env` c√¥t√© worker pour propager l'ensemble de ces variables √† chaque sous-processus.

   > üí§ **Lazy import MediaPipe** : `process_video_worker.py` dispose de `_ensure_mediapipe_loaded(required=False)` afin d‚Äô√©viter l‚Äôimport du module tant que le moteur MediaPipe/objets n‚Äôest pas sollicit√©. Les moteurs OpenCV/EOS l‚Äôappellent en mode `required=False`, ce qui supprime les crashs TensorFlow lorsque seules les d√©pendances OpenCV sont install√©es. Quand MediaPipe est indispensable (`required=True`), l‚Äôerreur est logg√©e puis relanc√©e pour guider l‚Äôutilisateur.

### Optimisations r√©centes (D√©cembre 2025)

| Optimisation | Description | Variables cl√©s |
|--------------|-------------|----------------|
| Downscale YuNet / OpenSeeFace | YuNet et OpenSeeFace plafonnent la largeur d‚Äôanalyse (`STEP5_YUNET_MAX_WIDTH`, `STEP5_OPENSEEFACE_MAX_WIDTH`) et rescalaient automatiquement les coordonn√©es/landmarks (logs DEBUG pour tracer les facteurs). | `STEP5_YUNET_MAX_WIDTH`, `STEP5_OPENSEEFACE_MAX_WIDTH` |
| Profiling g√©n√©ralis√© | Les workers rechargent `.env` avant chaque chunk pour propager `STEP5_ENABLE_PROFILING`, `STEP5_BLENDSHAPES_THROTTLE_N`, `STEP5_EOS_*`. Logs `[PROFILING]` toutes les 20 frames, m√™me sur de petits chunks multiprocessing. | Variables `STEP5_*` |
| Filtrage des blendshapes | `STEP5_BLENDSHAPES_PROFILE` (`full`, `mouth`, `mediapipe`, `custom`, `none`) + `STEP5_BLENDSHAPES_EXPORT_KEYS` r√©duisent la taille JSON (jusqu‚Äô√† -95‚ÄØ%) tout en conservant la compatibilit√© STEP6. | `STEP5_BLENDSHAPES_PROFILE`, `STEP5_BLENDSHAPES_EXPORT_KEYS`, `STEP5_BLENDSHAPES_INCLUDE_TONGUE` |
| Registry Object Detector | `workflow_scripts/step5/object_detector_registry.py` centralise EfficientDet/SSD/YOLO/NanoDet et applique l‚Äôoverride `STEP5_OBJECT_DETECTOR_MODEL_PATH` si fourni. | `STEP5_ENABLE_OBJECT_DETECTION`, `STEP5_OBJECT_DETECTOR_MODEL`, `STEP5_OBJECT_DETECTOR_MODEL_PATH` |
| JSON all√©g√© | `STEP5_EXPORT_VERBOSE_FIELDS=0` (d√©faut) supprime l‚Äôexport des `landmarks`/`eos` pour les moteurs non MediaPipe afin d‚Äôacc√©l√©rer STEP6 et r√©duire les transferts. | `STEP5_EXPORT_VERBOSE_FIELDS` |
| Warmup & seek robustes | Les workers OpenCV lisent une frame avant `cap.set()` et ins√®rent un placeholder si la frame est illisible, garantissant un JSON dense (1..N). | Impl√©ment√© dans `process_video_worker_multiprocessing.py` |

### Registry de d√©tection d‚Äôobjets

```
workflow_scripts/step5/object_detector_registry.py
‚îú‚îÄ‚îÄ efficientdet_lite0/1/2 (tflite)
‚îú‚îÄ‚îÄ ssd_mobilenet_v3 (tflite/tensorflow)
‚îú‚îÄ‚îÄ yolo11n (onnx)
‚îî‚îÄ‚îÄ nanodet_plus (onnx)
```

- `STEP5_OBJECT_DETECTOR_MODEL=efficientdet_lite2` pointe par d√©faut sur `workflow_scripts/step5/models/object_detectors/tflite/EfficientDet-Lite2-32.tflite`.
- Override absolu/relatif via `STEP5_OBJECT_DETECTOR_MODEL_PATH`.
- Le fallback MediaPipe Tasks fonctionne en mode `RunningMode.IMAGE` multi-threads (dimensionn√©s par `TRACKING_CPU_WORKERS`) pour InsightFace GPU, et single-thread pour les moteurs CPU historiques afin d‚Äô√©viter la contention.

### JSON d‚Äôexport & r√©duction

- `tracked_objects` reste dense : m√™me sans d√©tection, un tableau vide est √©mis par frame pour pr√©server l‚Äôalignement avec STEP6/7.
- `STEP5_EXPORT_VERBOSE_FIELDS=0` √©vite l‚Äô√©criture des champs volumineux (`landmarks`, `eos`) pour la plupart des moteurs ; activer ce flag uniquement pour le debug ou lorsque STEP6 requiert un export complet.
- Les logs `[Progression-MultiLine]` signalent lorsqu‚Äôun chunk bascule en mode r√©duit, facilitant le suivi depuis `WorkflowState`.

### Gestionnaire STEP5 & Routage des Environnements

- `workflow_scripts/step5/run_tracking_manager.py` charge automatiquement `config.settings` pour r√©cup√©rer les chemins des virtualenvs via `config.get_venv_python(<venv>)`.  
  - ‚úÖ `tracking_env` est la valeur par d√©faut.  
  - ‚úÖ Lorsque `STEP5_TRACKING_ENGINE=eos`, le gestionnaire bascule sur `eos_env` (override possible via `STEP5_EOS_ENV_PYTHON`).  
- `_EnvConfig` centralise d√©sormais la lecture typ√©e des variables `STEP5_*` (GPU, engines, workers, overrides). Le manager s‚Äôappuie sur cette couche pour :
  - appliquer les restrictions InsightFace GPU-only (`STEP5_ENABLE_GPU=1`, `STEP5_GPU_ENGINES`), 
  - construire un environnement subprocess via `_build_subprocess_env()` qui injecte automatiquement `LD_LIBRARY_PATH` (CUDA libs d√©couvertes dans les venvs + chemins syst√®me `/usr/local/cuda*` lorsque n√©cessaire),
  - propager les limites CPU (`OMP_NUM_THREADS`, `OPENBLAS_NUM_THREADS`, etc.) afin d‚Äô√©viter la contention inter-process,
  - router chaque moteur vers son interpr√©teur d√©di√© (`tracking_env`, `eos_env`, `insightface_env`, ou `STEP5_TF_GPU_ENV_PYTHON` pour MediaPipe GPU) avec v√©rification d‚Äôexistence et messages d‚Äôerreur explicites.
- Les workers multiprocessing rechargent toujours `.env` pour r√©cup√©rer ces variables √† chaque fork, garantissant que les r√©glages (`STEP5_BLENDSHAPES_THROTTLE_N`, profil d‚Äôexport, profiling) restent synchronis√©s m√™me en mode chunk√©.
