## SpÃ©cifications Techniques

### Environnement Virtuel
- **Environnement utilisÃ©** : `tracking_env/` (spÃ©cialisÃ© MediaPipe)
- **Activation** : `source tracking_env/bin/activate`
- **Isolation** : Environnement dÃ©diÃ© pour MediaPipe et OpenCV

### Technologies et BibliothÃ¨ques Principales

#### MediaPipe et Computer Vision
```python
import mediapipe as mp                    # Framework de ML pour vision
from mediapipe.tasks import python       # API Python MediaPipe
from mediapipe.tasks.python import vision # TÃ¢ches de vision
import cv2                               # OpenCV pour traitement d'image
import numpy as np                       # Calculs numÃ©riques
```

#### Traitement ParallÃ¨le et Optimisations
```python
from concurrent.futures import ThreadPoolExecutor  # Multi-threading
import multiprocessing                              # Multi-processing
import threading                                    # Synchronisation
import queue                                        # Communication inter-threads
```

#### Modules PersonnalisÃ©s
```python
from utils.tracking_optimizations import apply_tracking_and_management
from utils.enhanced_speaking_detection import EnhancedSpeakingDetector
from utils.resource_manager import safe_video_processing
```

### Formats d'EntrÃ©e et de Sortie

#### Structure d'EntrÃ©e Attendue
```
projets_extraits/
â”œâ”€â”€ projet_camille_001/
â”‚   â””â”€â”€ docs/
â”‚       â”œâ”€â”€ video1.mp4           # VidÃ©o source
â”‚       â”œâ”€â”€ video1.csv           # ScÃ¨nes (STEP3)
â”‚       â”œâ”€â”€ video1_audio.json    # Analyse audio (STEP4)
â”‚       â”œâ”€â”€ video2.mov           # VidÃ©o source
â”‚       â”œâ”€â”€ video2.csv           # ScÃ¨nes (STEP3)
â”‚       â””â”€â”€ video2_audio.json    # Analyse audio (STEP4)
```

#### Structure de Sortie GÃ©nÃ©rÃ©e
```
projets_extraits/
â”œâ”€â”€ projet_camille_001/
â”‚   â””â”€â”€ docs/
â”‚       â”œâ”€â”€ video1.mp4
â”‚       â”œâ”€â”€ video1.csv
â”‚       â”œâ”€â”€ video1_audio.json
â”‚       â”œâ”€â”€ video1_tracking.json     # DonnÃ©es de tracking
â”‚       â”œâ”€â”€ video2.mov
â”‚       â”œâ”€â”€ video2.csv
â”‚       â”œâ”€â”€ video2_audio.json
â”‚       â””â”€â”€ video2_tracking.json     # DonnÃ©es de tracking
```

### ParamÃ¨tres de Configuration

#### Configuration d'exÃ©cution

**Mode par dÃ©faut (v4.1)**: L'Ã‰tape 5 utilise exclusivement le CPU avec 15 workers internes par dÃ©faut, offrant de meilleures performances globales sur la configuration actuelle.

**Mode GPU (v4.2+)** : Le GPU est **strictement rÃ©servÃ©** au moteur InsightFace. Tous les autres moteurs (MediaPipe, OpenSeeFace, OpenCV, EOS) s'exÃ©cutent en mode CPU mÃªme si `STEP5_ENABLE_GPU=1`. Le gestionnaire :
- force `args.disable_gpu=True` pour tout moteur non listÃ© dans `STEP5_GPU_ENGINES`.
- vÃ©rifie la disponibilitÃ© matÃ©rielle via `Config.check_gpu_availability()` avant de lancer InsightFace.
- bascule automatiquement en mode CPU si `STEP5_GPU_FALLBACK_AUTO=1` et qu'une contrainte est dÃ©tectÃ©e (VRAM insuffisante, absence du provider CUDA, etc.).

#### Restrictions GPU (dÃ©cision 2025-12-27)
âš ï¸ **IMPORTANT** : Le support GPU est **rÃ©servÃ© exclusivement Ã  InsightFace**
- **MediaPipe Face Landmarker** : CPU-only (15 workers)
- **OpenSeeFace** : CPU-only (multiprocessing) 
- **OpenCV YuNet/PyFeat** : CPU-only
- **EOS** : CPU-only (3DMM fitting)
- Le gestionnaire force `args.disable_gpu=True` pour tous les moteurs non-InsightFace

âš ï¸ **Contraintes GPU** :
- 1 worker GPU sÃ©quentiel uniquement (pas de parallÃ©lisation).
- NÃ©cessite un GPU NVIDIA (CUDA â‰¥ 12.0) avec â‰¥â€¯2â€¯Go de VRAM libres (4â€¯Go recommandÃ©s pour coexister avec STEP2/NVENC).
- `insightface_env` embarque ONNX Runtime GPU + dÃ©pendances InsightFace ; override possible via `STEP5_INSIGHTFACE_ENV_PYTHON`.
- CPU-only reste recommandÃ© pour les batchs massifs (10+ vidÃ©os) et demeure le mode par dÃ©faut (`TRACKING_DISABLE_GPU=1`).
- `STEP5_GPU_PROFILING=1` journalise lâ€™usage VRAM et les timings toutes les 20 frames pour InsightFace.

## Moteurs de DÃ©tection Faciale

### Moteurs Disponibles

1. **MediaPipe Face Landmarker** (par dÃ©faut)
   - Utilise `face_landmarker_v2_with_blendshapes.task`
   - Support natif des blendshapes ARKit
   - OptimisÃ© pour la dÃ©tection en temps rÃ©el
   - **Mode CPU-only** : optimisÃ© pour 15 workers multiprocessing.

2. **OpenCV Haar Cascade**
   - Moteur de base pour la dÃ©tection de visages
   - Moins prÃ©cis mais trÃ¨s rapide
   - Utile pour les cas simples ou le matÃ©riel limitÃ©

3. **OpenCV YuNet**
   - DÃ©tecteur de visages basÃ© sur CNN
   - ModÃ¨le : `face_detection_yunet_2023mar.onnx`
   - Configurable via `STEP5_YUNET_MODEL_PATH`

4. **OpenCV YuNet + PyFeat**
   - Combine YuNet pour la dÃ©tection et PyFeat pour les expressions
   - Extrait des blendshapes avancÃ©s
   - Activation : `--face_engine opencv_yunet_pyfeat`
   - - **Mode CPU-only** : optimisÃ© pour 15 workers multiprocessing.

5. **OpenSeeFace**
   - Alternative open source complÃ¨te
   - NÃ©cessite des modÃ¨les spÃ©cifiques dans `STEP5_OPENSEEFACE_MODELS_DIR`
   - Activation : `--face_engine openseeface`
   - - **Mode CPU-only** : optimisÃ© pour multiprocessing.

6. **EOS (3D Morphable Model)**
   - ModÃ¨le 3D paramÃ©trique pour l'ajustement prÃ©cis des expressions
   - Utilise YuNet pour la dÃ©tection initiale puis ajuste 68 points 3D
   - S'exÃ©cute dans un **environnement dÃ©diÃ© `eos_env`** (routÃ© automatiquement par `run_tracking_manager.py`, override possible via `STEP5_EOS_ENV_PYTHON`)
   - Activation : `--face_engine eos` ou `STEP5_TRACKING_ENGINE=eos`
   - Variables d'environnement clÃ©s :
     ```env
     STEP5_EOS_MODELS_DIR=workflow_scripts/step5/models/engines/eos/share   # peut pointer vers /home/kidpixel6/kidpixel_assets/eos/share
     STEP5_EOS_SFM_MODEL_PATH=${STEP5_EOS_MODELS_DIR}/sfm_model.bin
     STEP5_EOS_EXPRESSION_BLENDSHAPES_PATH=${STEP5_EOS_MODELS_DIR}/expression_blendshapes_57.bin
     STEP5_EOS_LANDMARK_MAPPER_PATH=${STEP5_EOS_MODELS_DIR}/ibug_to_eos_landmarks.json
     STEP5_EOS_EDGE_TOPOLOGY_PATH=${STEP5_EOS_MODELS_DIR}/sfm_3448_edge_topology.json
     STEP5_EOS_MODEL_CONTOUR_PATH=${STEP5_EOS_MODELS_DIR}/sfm_model_contours.json
     STEP5_EOS_CONTOUR_LANDMARKS_PATH=${STEP5_EOS_MODELS_DIR}/ibug_to_eos_contour_landmarks.json
     STEP5_EOS_FIT_EVERY_N=2                         # fallback auto sur STEP5_BLENDSHAPES_THROTTLE_N si absent
     STEP5_EOS_MAX_WIDTH=1280                        # downscale + rescale coordonnÃ©es/landmarks
     STEP5_EOS_MAX_FACES=1                           # optionnel
     STEP5_ENABLE_PROFILING=1                        # logs [PROFILING] toutes les 20 frames (YuNet, FaceMesh, fit eos)
     ```
   - Exporte `tracked_objects[].eos = {shape_coeffs, expression_coeffs}` et `landmarks` 68x3 (toujours rescalÃ©s).
   - Les assets peuvent Ãªtre installÃ©s hors repo (NAS/SSD). Il suffit d'ajuster `STEP5_EOS_MODELS_DIR`.
   - `workflow_scripts/step5/process_video_worker_multiprocessing.py` charge `.env` cÃ´tÃ© worker pour propager l'ensemble de ces variables Ã  chaque sous-processus.

   > ðŸ’¤ **Lazy import MediaPipe** : `process_video_worker.py` dispose de `_ensure_mediapipe_loaded(required=False)` afin dâ€™Ã©viter lâ€™import du module tant que le moteur MediaPipe/objets nâ€™est pas sollicitÃ©. Les moteurs OpenCV/EOS lâ€™appellent en mode `required=False`, ce qui supprime les crashs TensorFlow lorsque seules les dÃ©pendances OpenCV sont installÃ©es. Quand MediaPipe est indispensable (`required=True`), lâ€™erreur est loggÃ©e puis relancÃ©e pour guider lâ€™utilisateur.

### Optimisations rÃ©centes (DÃ©cembre 2025)

| Optimisation | Description | Variables clÃ©s |
|--------------|-------------|----------------|
| Downscale YuNet / OpenSeeFace | YuNet et OpenSeeFace plafonnent la largeur dâ€™analyse (`STEP5_YUNET_MAX_WIDTH`, `STEP5_OPENSEEFACE_MAX_WIDTH`) et rescalaient automatiquement les coordonnÃ©es/landmarks (logs DEBUG pour tracer les facteurs). | `STEP5_YUNET_MAX_WIDTH`, `STEP5_OPENSEEFACE_MAX_WIDTH` |
| Profiling gÃ©nÃ©ralisÃ© | Les workers rechargent `.env` avant chaque chunk pour propager `STEP5_ENABLE_PROFILING`, `STEP5_BLENDSHAPES_THROTTLE_N`, `STEP5_EOS_*`. Logs `[PROFILING]` toutes les 20 frames, mÃªme sur de petits chunks multiprocessing. | Variables `STEP5_*` |
| Filtrage des blendshapes | `STEP5_BLENDSHAPES_PROFILE` (`full`, `mouth`, `mediapipe`, `custom`, `none`) + `STEP5_BLENDSHAPES_EXPORT_KEYS` rÃ©duisent la taille JSON (jusquâ€™Ã  -95â€¯%) tout en conservant la compatibilitÃ© STEP6. | `STEP5_BLENDSHAPES_PROFILE`, `STEP5_BLENDSHAPES_EXPORT_KEYS`, `STEP5_BLENDSHAPES_INCLUDE_TONGUE` |
| Registry Object Detector | `workflow_scripts/step5/object_detector_registry.py` centralise EfficientDet/SSD/YOLO/NanoDet et applique lâ€™override `STEP5_OBJECT_DETECTOR_MODEL_PATH` si fourni. | `STEP5_ENABLE_OBJECT_DETECTION`, `STEP5_OBJECT_DETECTOR_MODEL`, `STEP5_OBJECT_DETECTOR_MODEL_PATH` |
| JSON allÃ©gÃ© | `STEP5_EXPORT_VERBOSE_FIELDS=0` (dÃ©faut) supprime lâ€™export des `landmarks`/`eos` pour les moteurs non MediaPipe afin dâ€™accÃ©lÃ©rer STEP6 et rÃ©duire les transferts. | `STEP5_EXPORT_VERBOSE_FIELDS` |
| Warmup & seek robustes | Les workers OpenCV lisent une frame avant `cap.set()` et insÃ¨rent un placeholder si la frame est illisible, garantissant un JSON dense (1..N). | ImplÃ©mentÃ© dans `process_video_worker_multiprocessing.py` |

### Registry de dÃ©tection dâ€™objets

```
workflow_scripts/step5/object_detector_registry.py
â”œâ”€â”€ efficientdet_lite0/1/2 (tflite)
â”œâ”€â”€ ssd_mobilenet_v3 (tflite/tensorflow)
â”œâ”€â”€ yolo11n (onnx)
â””â”€â”€ nanodet_plus (onnx)
```

- `STEP5_OBJECT_DETECTOR_MODEL=efficientdet_lite2` pointe par dÃ©faut sur `workflow_scripts/step5/models/object_detectors/tflite/EfficientDet-Lite2-32.tflite`.
- Override absolu/relatif via `STEP5_OBJECT_DETECTOR_MODEL_PATH`.
- Le fallback MediaPipe Tasks fonctionne en mode `RunningMode.IMAGE` multi-threads (dimensionnÃ©s par `TRACKING_CPU_WORKERS`) pour InsightFace GPU, et single-thread pour les moteurs CPU historiques afin dâ€™Ã©viter la contention.

### JSON dâ€™export & rÃ©duction

- `tracked_objects` reste dense : mÃªme sans dÃ©tection, un tableau vide est Ã©mis par frame pour prÃ©server lâ€™alignement avec STEP6/7.
- `STEP5_EXPORT_VERBOSE_FIELDS=0` Ã©vite lâ€™Ã©criture des champs volumineux (`landmarks`, `eos`) pour la plupart des moteurs ; activer ce flag uniquement pour le debug ou lorsque STEP6 requiert un export complet.
- Les logs `[Progression-MultiLine]` signalent lorsquâ€™un chunk bascule en mode rÃ©duit, facilitant le suivi depuis `WorkflowState`.

### Gestionnaire STEP5 & Routage des Environnements

- `workflow_scripts/step5/run_tracking_manager.py` charge automatiquement `config.settings` pour rÃ©cupÃ©rer les chemins des virtualenvs via `config.get_venv_python(<venv>)`.  
  - âœ… `tracking_env` est la valeur par dÃ©faut.  
  - âœ… Lorsque `STEP5_TRACKING_ENGINE=eos`, le gestionnaire bascule sur `eos_env` (override possible via `STEP5_EOS_ENV_PYTHON`).  
