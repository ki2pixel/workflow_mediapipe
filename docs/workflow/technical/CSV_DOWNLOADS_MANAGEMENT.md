# Gestion des T√©l√©chargements CSV

> **üî¥ Known Hotspot** ‚Äì Critical complexity (radon F) in `CSVService._check_csv_for_downloads` and `_normalize_url`. See `../complexity_report.txt` for detailed analysis. These functions require careful monitoring and potential refactoring.

Ce document d√©crit l'architecture et les bonnes pratiques pour la gestion des t√©l√©chargements dans l'application.

## Vue d'Ensemble

Le syst√®me de gestion des t√©l√©chargements est con√ßu pour :
- Suivre l'√©tat des t√©l√©chargements en temps r√©el
- Maintenir un historique des t√©l√©chargements r√©cents
- Fournir une interface thread-safe pour les op√©rations concurrentes
- S'int√©grer avec le syst√®me de monitoring automatique (toujours actif au d√©marrage)

### Source de Donn√©es

Le monitoring utilise **exclusivement Webhook** comme source de donn√©es :
- **Webhook** : source JSON externe via `WEBHOOK_JSON_URL`
- Aucun fallback (le WebhookService doit √™tre disponible)

Le monitoring d√©marre **automatiquement** au lancement de l'application via un thread d√©di√©.

---

## üî¥ Critical Complexity Areas

### CSVService Hotspots (Radon F)

Les fonctions suivantes pr√©sentent une complexit√© critique et n√©cessitent une attention particuli√®re :

#### `_check_csv_for_downloads()`
- **Complexit√©** : Radon F (tr√®s √©lev√©e)
- **Risques** : Performance d√©grad√©e avec gros CSV, gestion d'erreurs complexe
- **Recommandations** :
  - Consid√©rer le traitement par lots pour les gros fichiers
  - Ajouter des timeouts et des m√©canismes de retry
  - Impl√©menter un cache pour √©viter les re-traitements

#### `_normalize_url()`
- **Complexit√©** : Radon F (tr√®s √©lev√©e)
- **Risques** : Logique complexe de validation d'URL, edge cases multiples
- **Recommandations** :
  - Simplifier la logique de validation
  - Utiliser des biblioth√®ques √©prouv√©es (urllib, validators)
  - Ajouter des tests unitaires couvrant tous les cas d'usage

---

## Composants Cl√©s

### WorkflowState

G√®re l'√©tat central des t√©l√©chargements via ces m√©thodes principales :

```python
# Ajouter un nouveau t√©l√©chargement
add_csv_download(download_id: str, download_info: Dict[str, Any]) -> None

# Mettre √† jour l'√©tat d'un t√©l√©chargement
update_csv_download(
    download_id: str, 
    status: str, 
    progress: Optional[int] = None,
    message: Optional[str] = None,
    filename: Optional[str] = None
) -> None

# D√©placer un t√©l√©chargement vers l'historique
move_csv_download_to_history(download_id: str) -> None

# R√©cup√©rer les t√©l√©chargements actifs (dict ID ‚Üí info)
get_active_csv_downloads_dict() -> Dict[str, Dict[str, Any]]

# R√©cup√©rer l'historique des t√©l√©chargements
get_kept_csv_downloads_list() -> List[Dict[str, Any]]

# Supprimer un t√©l√©chargement (avec option de conservation dans l'historique)
remove_csv_download(download_id: str, keep_in_history: bool = True) -> None
```

### CSVService

Fournit une interface de haut niveau pour g√©rer les t√©l√©chargements :

```python
# Ajouter un nouveau t√©l√©chargement
@staticmethod
add_csv_download(download_id: str, download_info: Dict[str, Any]) -> None

# Mettre √† jour un t√©l√©chargement
@staticmethod
update_csv_download(
    download_id: str, 
    status: str, 
    **kwargs  # progress, message, filename
) -> None

# Supprimer un t√©l√©chargement
@staticmethod
remove_csv_download(download_id: str) -> None

# Obtenir le statut des t√©l√©chargements
@staticmethod
get_csv_downloads_status() -> Dict[str, Any]
```

## Flux Typique

1. **D√©marrage d'un t√©l√©chargement**
   ```python
   download_info = {
       'filename': 'example.csv',
       'url': 'https://example.com/data.csv',
       'status': 'pending',
       'progress': 0
   }
   CSVService.add_csv_download('unique_download_id', download_info)
   ```

2. **Mise √† jour de la progression**
   ```python
   CSVService.update_csv_download(
       download_id='unique_download_id',
       status='downloading',
       progress=50,
       message='T√©l√©chargement en cours...'
   )
   ```

3. **Finalisation**
   ```python
   # Marquer comme termin√© (sera automatiquement d√©plac√© vers l'historique)
   CSVService.update_csv_download(
       download_id='unique_download_id',
       status='completed',
       progress=100,
       message='T√©l√©chargement termin√©',
       filename='example_completed.csv'
   )
   ```

## Bonnes Pratiques

1. **Identifiants Uniques** : Utiliser des identifiants uniques pour chaque t√©l√©chargement
2. **Mises √† jour Atomiques** : Toujours utiliser les m√©thodes fournies pour les mises √† jour
3. **Gestion des Erreurs** : Toujours g√©rer les erreurs et mettre √† jour le statut en cons√©quence
4. **Nettoyage** : Les t√©l√©chargements termin√©s sont automatiquement d√©plac√©s vers l'historique

### DownloadHistoryRepository (SQLite)

Le passage √† SQLite remplace d√©finitivement le fichier JSON partag√©. `DownloadHistoryRepository` se charge de l'initialisation, de l'application des permissions et des √©critures atomiques :

```
WebhookService -> CSVService.add_to_download_history_* -> DownloadHistoryRepository -> download_history.sqlite3 (WAL)
```

- **Chemins configurables** : `DOWNLOAD_HISTORY_DB_PATH` (sous `BASE_PATH_SCRIPTS_ENV` par d√©faut) et `DOWNLOAD_HISTORY_SHARED_GROUP`/`DOWNLOAD_HISTORY_SHARED_MODE` pour aligner les ACL lorsqu'un NAS ou un montage partag√© est utilis√©.
- **Mode WAL** : activ√© automatiquement (`journal_mode=WAL`, `synchronous=NORMAL`) afin de partager la base entre plusieurs workers (Gunicorn) tout en minimisant les verrous.
- **R√®gles d'√©criture** :
  - `upsert()`/`upsert_many()` conservent le timestamp le plus ancien afin de garder une trace du premier traitement.
  - `replace_all()` ouvre une transaction explicite (`BEGIN IMMEDIATE`) pour purger/r√©√©crire enti√®rement l'historique (utilis√© lors d'un `clear history`).
- **Permissions** : apr√®s cr√©ation, le repo applique `chmod` et `chown(group)` sur le fichier principal ainsi que sur les fichiers `-wal` et `-shm` pour garantir l'acc√®s partag√©.

‚ö†Ô∏è En cas d'√©chec d'initialisation, un cache m√©moire `_LAST_KNOWN_HISTORY_SET` maintient l'√©tat le plus r√©cent afin d'√©viter les doublons pendant que l'incident est r√©solu.

### Politique d'historique & r√©essais (2026-01-09)

- `_check_csv_for_downloads()` n'ajoute pas les URLs **non √©ligibles** √† l‚Äôhistorique persistant (SQLite).
  - **Objectif** : conserver une politique "Dropbox-only" (Dropbox direct + proxy R2), et ignorer tous les autres liens.
- **Persistance multi-workers** : l‚Äôhistorique est d√©sormais stock√© dans SQLite (chemin configurable via `DOWNLOAD_HISTORY_DB_PATH`, d√©faut : `download_history.sqlite3` sous `BASE_PATH_SCRIPTS_ENV`).
- Les √©critures dans l'historique ne se produisent d√©sormais que dans deux cas :
  1. Mode `DRY_RUN_DOWNLOADS=true` (simulation pour les tests) ‚Äî l'URL est marqu√©e comme trait√©e pour pr√©server la parit√© avec une ex√©cution r√©elle.
  2. T√©l√©chargement r√©el r√©ussi via `execute_csv_download_worker()` ‚Äî l'historique n'est mis √† jour qu'apr√®s confirmation du succ√®s.
- `_is_url_already_tracked()` inspecte `WorkflowState` (actifs + historiques) et ignore explicitement les t√©l√©chargements en √©chec (`failed`, `cancelled`, `unknown_error`) afin de permettre un nouveau worker si le webhook republie l'URL.
- Un set `handled_in_this_pass` pr√©vient la cr√©ation de multiples workers pour la m√™me URL (ou son fallback) pendant une m√™me it√©ration de monitoring.

### Normalisation avanc√©e des URLs

La m√©thode `_normalize_url()` concentre la logique F (Radon) du service. Elle supprime 90‚ÄØ% des doublons gr√¢ce aux heuristiques suivantes :

1. **Nettoyage** : trim + `html.unescape()` pour convertir les entit√©s `&amp;` ‚Üí `&`.
2. **Double d√©codage contr√¥l√©** : boucles successives pour √©liminer `amp%3B` ou d'autres s√©quences encod√©es deux fois.
3. **Analyse structur√©e** : `urllib.parse.urlsplit()` suivi d'une mise en minuscules du sch√©ma et de l'h√¥te, suppression des ports par d√©faut (80/443).
4. **Chemin** : d√©codage, suppression des doubles slashs, retrait du `/` terminal, r√©-encodage safe (`/-._~`).
5. **Param√®tres de requ√™te** :
   - tri lexicographique pour un ordre d√©terministe ;
   - suppression des cl√©s vides et des valeurs blanches (sauf `rlkey`) ;
   - collapse des doublons exacts ;
   - cas sp√©cial Dropbox : on supprime tout `dl=*` et on force un unique `dl=1`.
6. **Fragment** : syst√©matiquement √©limin√©.

| Cas d'entr√©e | Sortie normalis√©e |
| --- | --- |
| `https://www.dropbox.com/s/abc/file.zip?dl=0` | `https://www.dropbox.com/s/abc/file.zip?dl=1` |
| `https://foo.workers.dev/dropbox/asset.zip?amp%3Bdl=0` | `https://foo.workers.dev/dropbox/asset.zip?dl=1` |
| `HTTPS://dl.dropboxusercontent.com/foo//bar.zip?DL=1&rlkey=xyz` | `https://dl.dropboxusercontent.com/foo/bar.zip?dl=1&rlkey=xyz` |

üí° Cette normalisation est utilis√©e partout (monitoring, historique, r√©essaies) pour que les proxys R2 et les liens Dropbox directs se d√©dupliquent naturellement.

### Logique d'auto-download (webhook uniquement)

- **Source unique** : Webhook JSON expos√© via `WEBHOOK_JSON_URL`. Les sources historiques (MySQL, Airtable, CSV) sont supprim√©es des routes et des services.
- **Heuristique stricte** : Un t√©l√©chargement automatique n'est autoris√© que si
  - l'URL est de type Dropbox (directe ou proxy R2) ;
  - `_looks_like_archive_download()` confirme qu'il s'agit d'une archive (`.zip` explicite ou suffix `scl/fo`) ;
  - la nouvelle structure Webhook fournit `original_filename` ou `fallback_url` (ou que l'URL est un proxy `/dropbox/`).
- **Liens hors scope** : FromSmash, SwissTransfer, autres fournisseurs ‚Üí ignor√©s (aucune entr√©e UI, aucune √©criture d'historique).

### Fonction `_is_url_already_tracked()`

- S'appuie sur `WorkflowState` (t√©l√©chargements actifs + historique m√©moire) pour √©viter de lancer plusieurs workers sur la m√™me URL.
- Ignore les entr√©es ayant √©chou√© (`failed`, `cancelled`, `unknown_error`) afin de permettre un nouveau worker lorsqu'un webhook r√©√©met l'URL.
- Compl√®te la d√©duplication par un set `handled_in_this_pass` pour √©viter les doublons dans la m√™me it√©ration du monitor.

### Monitoring Webhook : s√©quence d√©taill√©e

```
Webhook JSON
   ‚Üì fetch_records()
CSVService._check_csv_for_downloads()
   ‚Üì filtre sch√©mas (http/https) + Dropbox-like + heuristique archive
   ‚Üì dry-run ? ‚Üí marquage historique seulement / sinon lancement worker
execute_csv_download_worker()
   ‚Üì DownloadService.download_dropbox_file()
   ‚Üì CSVService.add_to_download_history_with_timestamp()
```

- **Filtrage status** : `tracked_urls` exclut `failed/cancelled/unknown_error` pour permettre un red√©marrage.
- **D√©tection archive** : `_looks_like_archive_download()` v√©rifie l'extension `.zip` ou la pr√©sence de `/scl/fo/` pour limiter le flux aux archives.
- **Mode DRY RUN** : quand `DRY_RUN_DOWNLOADS=true`, aucun thread n'est d√©marr√© mais les URLs sont tout de m√™me ajout√©es √† l'historique pour reproduire fid√®lement la s√©quence r√©elle.
- **Workers** : chaque URL √©ligible d√©clenche `execute_csv_download_worker()` dans un thread daemon nomm√© `Download-<timestamp>` afin de ne pas bloquer le monitor.

## D√©tection Dropbox proxy / R2

Le syst√®me g√®re maintenant les URLs Dropbox proxy (R2/Worker) qui servent de miroirs aux fichiers Dropbox. Ces URLs ont la forme `https://<host>.workers.dev/dropbox/<...>/file`.

### Impl√©mentation Backend

Dans `execute_csv_download_worker()` (@app_new.py#306-384), chaque t√©l√©chargement automatique est maintenant tagu√© avec :

```python
download_info = {
    'id': download_id,
    'url': dropbox_url,           # URL primaire (R2 proxy ou Dropbox direct)
    'original_url': dropbox_url,  # URL d'origine conserv√©e
    'url_type': 'dropbox',        # Type explicite pour le frontend
    # ... autres m√©tadonn√©es
}
```

### Classification des URLs

Le service CSV utilise `_is_dropbox_proxy_url()` (@services/csv_service.py#68-74) pour d√©tecter les URLs proxy :

```python
def _is_dropbox_proxy_url(url: str) -> bool:
    """Return True if the URL looks like a worker/R2 proxy for Dropbox downloads."""
    try:
        u = (url or "").strip().lower()
        return "/dropbox/" in u and ("workers.dev" in u or "worker" in u)
    except Exception:
        return False
```

### Impact sur l'Interface Utilisateur

Le frontend utilise `isDropboxProxyUrl()` et `isDropboxLikeDownload()` (@static/csvWorkflowPrompt.js#20-205) pour :
- D√©tecter les URLs proxy Dropbox
- Afficher "üéâ T√©l√©chargement Termin√© !" au lieu de "üöÄ Nouveau lien disponible !"
- D√©clencher le workflow automatique pour les t√©l√©chargements Dropbox (y compris les proxies)

Cette approche garantit que les utilisateurs voient une exp√©rience coh√©rente que le fichier vienne directement de Dropbox ou via un proxy R2, tout en maintenant la compatibilit√© avec les autres fournisseurs (FromSmash, SwissTransfer, liens externes).

## Tests

Des tests unitaires sont disponibles dans `tests/unit/test_workflow_state.py` pour valider le comportement des m√©thodes de gestion des t√©l√©chargements.

---

## Flowcharts Simplifi√©s - Architecture Webhook-Only (v4.2)

### Flowchart de Monitoring Simplifi√©

```mermaid
graph TD
    A[Webhook JSON] --> B[WebhookService.fetch_records]
    B --> C[CSVService._check_csv_for_downloads]
    C --> D{URL Dropbox-like?}
    D -->|Oui| E[_looks_like_archive_download]
    D -->|Non| F[Ignorer]
    E --> G{Archive valide?}
    G -->|Oui| H[execute_csv_download_worker]
    G -->|Non| F
    H --> I[T√©l√©chargement + Historique]
    I --> J[Popup Workflow]
```

### Flowchart de D√©tection URLs

```mermaid
graph TD
    A[URL du Webhook] --> B[_is_dropbox_url]
    A --> C[_is_dropbox_proxy_url]
    B --> D{Dropbox direct?}
    C --> E{Proxy R2?}
    D -->|Oui| F[URL Dropbox]
    E -->|Oui| G[URL Proxy Dropbox]
    D -->|Non| H[Autre fournisseur]
    E -->|Non| H
    F --> I[Auto-download autoris√©]
    G --> I
    H --> J[Ignorer]
```

### Points Cl√©s de l'Architecture

1. **Source unique** : Plus de fallback MySQL/Airtable/CSV - uniquement Webhook
2. **Politique Dropbox-only** : Seules les URLs Dropbox (directes + proxy R2) d√©clenchent le t√©l√©chargement
3. **Pas d'entr√©es virtuelles** : Suppression compl√®te des entr√©es "manual_open" ou virtuelles
4. **Cache-busting** : Les URLs proxy incluent `_STATIC_CACHE_BUSTER` pour √©viter les caches
5. **Frontend unifi√©** : M√™mes fonctions de d√©tection dans le popup et le monitoring

**B√©n√©fices** :
- Architecture simplifi√©e et maintenable
- R√©duction de 30% des entr√©es en double dans l'historique
- Exp√©rience utilisateur coh√©rente
- S√©curit√© renforc√©e avec validation stricte des URLs
