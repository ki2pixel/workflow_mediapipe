This file is a merged representation of a subset of the codebase, containing specifically included files and files not matching ignore patterns, combined into a single document by Repomix.

# File Summary

## Purpose
This file contains a packed representation of a subset of the repository's contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

## File Format
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  a. A header with the file path (## File: path/to/file)
  b. The full contents of the file in a code block

## Usage Guidelines
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
- Pay special attention to the Repository Description. These contain important context and guidelines specific to this project.

## Notes
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: app_new.py, routes/**, services/**, workflow_scripts/**/*.py, utils/**, config/**, static/**, templates/**
- Files matching these patterns are excluded: archives/**, sound-design/**, assets/**, transnetv2-weights/**, workflow_scripts/**/models/**, workflow_scripts/**/assets/**, logs/**, debug/**, download_history*.json, download_history*.bak, download_history*.sqlite3, download_history*.sqlite3-wal, download_history*.sqlite3-shm, *.mp3, *.wav, *.tflite, *.task, *.onnx, *.pth, *.zip, *.tar, *.tar.gz, *.rar, *.7z
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Long base64 data strings (e.g., data:image/png;base64,...) have been truncated to reduce token count
- Files are sorted by Git change count (files with more changes are at the bottom)

# User Provided Header
Workflow MediaPipe — extrait optimisé pour IA.
Ce bundle exclut les artefacts volumineux (archives, modèles, logs) pour rester sous la limite de Repomix.
Consultez `docs/workflow/guides/REPOMIX_USAGE.md` pour l'utilisation et `.windsurf/rules/codingstandards.md` pour les règles obligatoires.

# Directory Structure
```
config/
  __init__.py
  optimal_tv_config.json
  security.py
  settings.py
  step3_transnet.json
  workflow_commands.py
routes/
  __init__.py
  api_routes.py
  workflow_routes.py
services/
  deprecated/
    airtable_service.py
    mysql_service.py
    README.md
  __init__.py
  cache_service.py
  csv_service.py
  download_history_repository.py
  download_service.py
  filesystem_service.py
  lemonfox_audio_service.py
  monitoring_service.py
  performance_service.py
  report_service.py
  results_archiver.py
  visualization_service.py
  webhook_service.py
  workflow_service.py
  workflow_state.py
static/
  css/
    components/
      controls.css
      csv-workflow-prompt.css
      downloads.css
      logs.css
      notifications.css
      popups.css
      steps.css
      widgets.css
      workflow-buttons.css
    features/
      reports.css
      responsive.css
      stats-dashboard.css
    utils/
      animations.css
    base.css
    layout.css
    themes.css
    variables.css
  state/
    AppState.js
  test-exports/
    fetchWithLoadingState.js
  utils/
    DOMBatcher.js
    ErrorHandler.js
    PerformanceMonitor.js
    PerformanceOptimizer.js
    PollingManager.js
  apiService.js
  constants.js
  csvDownloadMonitor.js
  csvWorkflowPrompt.js
  domElements.js
  eventHandlers.js
  main.js
  popupManager.js
  reportViewer.js
  scrollManager.js
  sequenceManager.js
  soundManager.js
  state.js
  stepDetailsPanel.js
  themeManager.js
  uiUpdater.js
  utils.js
templates/
  reports/
    analysis_report.html
    monthly_archive_report.html
    project_report.html
  index_new.html
utils/
  enhanced_speaking_detection.py
  filename_security.py
  mediapipe_asset_helper.py
  performance.py
  resource_manager.py
  simple_profiling.py
  tracking_optimizations.py
  transnetv2_library.py
  worker_wrapper.py
workflow_scripts/
  step1/
    extract_archives.py
  step2/
    convert_videos.py
  step3/
    run_transnet.py
    transnetv2_pytorch.py
  step4/
    run_audio_analysis_lemonfox.py
    run_audio_analysis.py
  step5/
    face_engines.py
    object_detector_registry.py
    onnx_facemesh_detector.py
    process_video_worker_multiprocessing.py
    process_video_worker.py
    pyfeat_blendshape_extractor.py
    run_tracking_manager.py
  step6/
    json_reducer.py
  step7/
    finalize_and_copy.py
app_new.py
```

# Files

## File: config/__init__.py
```python
"""Config package marker for pytest/package imports."""
```

## File: config/optimal_tv_config.json
```json
{
  "segmentation": {
    "min_duration_off": 0.15
  },
  "clustering": {
    "min_cluster_size": 3
  }
}
```

## File: config/security.py
```python
"""
Security configuration and authentication management for workflow_mediapipe.

This module provides secure token management and authentication decorators
following the project's security guidelines.
"""

import os
import logging
from functools import wraps
from typing import Optional
from flask import request, jsonify

logger = logging.getLogger(__name__)


class SecurityConfig:
    """
    Centralized security configuration management.
    
    Loads security tokens from environment variables and provides
    validation methods to ensure proper configuration.
    """
    
    def __init__(self):
        """Initialize security configuration from environment variables."""
        self.INTERNAL_WORKER_TOKEN = os.environ.get('INTERNAL_WORKER_COMMS_TOKEN')
        self.RENDER_REGISTER_TOKEN = os.environ.get('RENDER_REGISTER_TOKEN')
        
    def validate_tokens(self, strict: bool = True) -> bool:
        """
        Validate that all required security tokens are configured.

        Args:
            strict: If True, raise errors for missing tokens. If False, log warnings.

        Returns:
            bool: True if all tokens are valid, False otherwise

        Raises:
            ValueError: If required tokens are missing and strict=True
        """
        errors = []
        warnings = []

        if not self.INTERNAL_WORKER_TOKEN:
            msg = "INTERNAL_WORKER_COMMS_TOKEN environment variable is required"
            if strict:
                errors.append(msg)
            else:
                warnings.append(msg)
                # Set a development default
                self.INTERNAL_WORKER_TOKEN = "dev-internal-worker-token"
                logger.warning(f"{msg} - using development default")

        if not self.RENDER_REGISTER_TOKEN:
            msg = "RENDER_REGISTER_TOKEN environment variable is required"
            if strict:
                errors.append(msg)
            else:
                warnings.append(msg)
                # Set a development default
                self.RENDER_REGISTER_TOKEN = "dev-render-register-token"
                logger.warning(f"{msg} - using development default")

        if errors:
            error_msg = f"Security configuration errors: {'; '.join(errors)}"
            logger.error(error_msg)
            raise ValueError(error_msg)

        if warnings:
            logger.warning(f"Security configuration warnings: {'; '.join(warnings)}")
            logger.warning("Using development defaults - NOT SUITABLE FOR PRODUCTION")
            return False

        logger.info("Security tokens validated successfully")
        return True
    
    def get_token(self, token_name: str) -> Optional[str]:
        """
        Get a specific token by name.
        
        Args:
            token_name: Name of the token to retrieve
            
        Returns:
            Token value or None if not found
        """
        return getattr(self, token_name, None)


def require_internal_worker_token(func):
    """
    Decorator for endpoints requiring internal worker authentication.
    
    Validates the X-Worker-Token header against the configured
    INTERNAL_WORKER_COMMS_TOKEN.
    
    Args:
        func: Flask route function to protect
        
    Returns:
        Decorated function with token validation
    """
    @wraps(func)
    def wrapper(*args, **kwargs):
        security_config = SecurityConfig()
        
        if not security_config.INTERNAL_WORKER_TOKEN:
            logger.error("Internal worker token not configured")
            return jsonify({"error": "Authentication not configured"}), 500
        
        received_token = request.headers.get('X-Worker-Token')
        if not received_token:
            logger.warning("Missing X-Worker-Token header in request")
            return jsonify({"error": "Missing authentication token"}), 401
            
        if received_token != security_config.INTERNAL_WORKER_TOKEN:
            logger.warning(f"Invalid worker token received from {request.remote_addr}")
            return jsonify({"error": "Invalid authentication token"}), 401
        
        logger.debug("Internal worker token validated successfully")
        return func(*args, **kwargs)
    
    return wrapper


def require_render_register_token(func):
    """
    Decorator for endpoints requiring render registration authentication.
    
    Validates the X-Render-Token header against the configured
    RENDER_REGISTER_TOKEN.
    
    Args:
        func: Flask route function to protect
        
    Returns:
        Decorated function with token validation
    """
    @wraps(func)
    def wrapper(*args, **kwargs):
        security_config = SecurityConfig()
        
        if not security_config.RENDER_REGISTER_TOKEN:
            logger.error("Render register token not configured")
            return jsonify({"error": "Authentication not configured"}), 500
        
        received_token = request.headers.get('X-Render-Token')
        if not received_token:
            logger.warning("Missing X-Render-Token header in request")
            return jsonify({"error": "Missing authentication token"}), 401
            
        if received_token != security_config.RENDER_REGISTER_TOKEN:
            logger.warning(f"Invalid render token received from {request.remote_addr}")
            return jsonify({"error": "Invalid authentication token"}), 401
        
        logger.debug("Render register token validated successfully")
        return func(*args, **kwargs)
    
    return wrapper


def validate_file_path(file_path: str, allowed_base_paths: list) -> bool:
    """
    Validate file path to prevent directory traversal attacks.
    
    Args:
        file_path: Path to validate
        allowed_base_paths: List of allowed base directory paths
        
    Returns:
        bool: True if path is safe, False otherwise
    """
    try:
        from pathlib import Path
        
        # Resolve the path to handle any .. or . components
        resolved_path = Path(file_path).resolve()
        
        # Check if the resolved path starts with any allowed base path
        for base_path in allowed_base_paths:
            base_resolved = Path(base_path).resolve()
            try:
                resolved_path.relative_to(base_resolved)
                return True
            except ValueError:
                continue
                
        logger.warning(f"File path validation failed for: {file_path}")
        return False
        
    except Exception as e:
        logger.error(f"Error validating file path {file_path}: {e}")
        return False
```

## File: config/step3_transnet.json
```json
{
  "threshold": 0.5,
  "window": 100,
  "stride": 50,
  "padding": 25,
  "device": "auto",
  "ffmpeg_threads": 0,
  "mixed_precision": false,
  "amp_dtype": "float16",
  "num_workers": 1,
  "torchscript": false,
  "warmup": true,
  "warmup_batches": 2,
  "torchscript_auto_fallback": true
}
```

## File: config/workflow_commands.py
```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Workflow Commands Configuration

Centralized configuration for all workflow steps.
Defines commands, working directories, log paths, and progress patterns.
"""

import re
from pathlib import Path
from typing import Dict, Any, List, Optional
import logging

from config.settings import config

logger = logging.getLogger(__name__)


class WorkflowCommandsConfig:
    """Centralized workflow commands configuration.
    
    This class provides configuration for all 7 workflow steps, including:
    - Command line arguments
    - Working directories
    - Log file locations
    - Progress parsing patterns
    - Display names for UI
    """
    
    def __init__(self, base_path: Path = None, hf_token: str = None):
        """Initialize workflow commands configuration.
        
        Args:
            base_path: Base path for scripts (defaults to config.BASE_PATH_SCRIPTS)
            hf_token: HuggingFace authentication token for Step 4
        """
        self.base_path = base_path or config.BASE_PATH_SCRIPTS
        self.hf_token = hf_token
        
        # Setup log directories
        self.logs_base_dir = self.base_path / "logs"
        self._ensure_log_directories()
        
        # Build configuration
        self._config = self._build_configuration()
        
        logger.info(f"WorkflowCommandsConfig initialized with base_path: {self.base_path}")
    
    def _ensure_log_directories(self) -> None:
        """Ensure all log directories exist."""
        self.logs_base_dir.mkdir(exist_ok=True)
        for step in range(1, 8):
            (self.logs_base_dir / f"step{step}").mkdir(exist_ok=True)
    
    def _build_configuration(self) -> Dict[str, Dict[str, Any]]:
        """Build the complete workflow commands configuration.
        
        Returns:
            Dictionary mapping step keys to their configuration
        """
        return {
            "STEP1": self._get_step1_config(),
            "STEP2": self._get_step2_config(),
            "STEP3": self._get_step3_config(),
            "STEP4": self._get_step4_config(),
            "STEP5": self._get_step5_config(),
            "STEP6": self._get_step6_config(),
            "STEP7": self._get_step7_config(),
        }
    
    def _get_step1_config(self) -> Dict[str, Any]:
        """Get configuration for Step 1: Archive Extraction.
        
        Returns:
            Step 1 configuration dictionary
        """
        step1_log_dir = self.logs_base_dir / "step1"
        
        return {
            "display_name": "1. Extraction des archives",
            "cmd": [
                str(config.get_venv_python("env")),
                str(self.base_path / "workflow_scripts" / "step1" / "extract_archives.py"),
                "--source-dir", str(Path.home() / "Téléchargements")
            ],
            "cwd": str(self.base_path),
            "specific_logs": [
                {
                    "name": "Log Extraction",
                    "type": "directory_latest",
                    "path": step1_log_dir,
                    "pattern": "*.log",
                    "lines": 150
                },
                {
                    "name": "Liste Archives Traitées",
                    "type": "file",
                    "path": step1_log_dir / "processed_archives.txt",
                    "lines": 50
                }
            ],
            "progress_patterns": {
                "total": re.compile(r"Trouvé (\d+) archive\(s\) à traiter", re.IGNORECASE),
                "current_success_line_pattern": re.compile(
                    r"Extraction terminée pour (.*?)$", re.IGNORECASE
                ),
                "current_item_text_from_success_line": True
            }
        }
    
    def _get_step2_config(self) -> Dict[str, Any]:
        """Get configuration for Step 2: Video Conversion.
        
        Returns:
            Step 2 configuration dictionary
        """
        step2_log_dir = self.logs_base_dir / "step2"
        
        return {
            "display_name": "2. Conversion des vidéos",
            "cmd": [
                str(config.get_venv_python("env")),
                str(self.base_path / "workflow_scripts" / "step2" / "convert_videos.py")
            ],
            "cwd": str(self.base_path / "projets_extraits"),
            "specific_logs": [
                {
                    "name": "Log Conversion",
                    "type": "directory_latest",
                    "path": step2_log_dir,
                    "pattern": "*.log",
                    "lines": 200
                }
            ],
            "progress_patterns": {
                "total": re.compile(r"TOTAL_VIDEOS_TO_PROCESS:\s*(\d+)", re.IGNORECASE),
                "current": re.compile(
                    r"--- Traitement de la vidéo \((\d+)/(\d+)\): (.*?) ---", re.IGNORECASE
                )
            }
        }
    
    def _get_step3_config(self) -> Dict[str, Any]:
        """Get configuration for Step 3: Scene Detection.
        
        Returns:
            Step 3 configuration dictionary
        """
        step3_log_dir = self.logs_base_dir / "step3"
        
        return {
            "display_name": "3. Analyse des transitions",
            "cmd": [
                str(config.get_venv_python("transnet_env")),
                str(self.base_path / "workflow_scripts" / "step3" / "run_transnet.py"),
            ],
            "cwd": str(self.base_path / "projets_extraits"),
            "specific_logs": [
                {
                    "name": "Log Analyse Transitions",
                    "type": "directory_latest",
                    "path": step3_log_dir,
                    "pattern": "*.log",
                    "lines": 150
                }
            ],
            "progress_patterns": {
                # Accept both underscore and space variants
                "total": re.compile(r"TOTAL[_ ]VIDEOS[_ ]TO[_ ]PROCESS:\s*(\d+)", re.IGNORECASE),
                "current": re.compile(r"PROCESSING[_ ]VIDEO:\s*(.*)$", re.IGNORECASE),
                "internal_simple": re.compile(
                    r"INTERNAL[_ ]PROGRESS:\s*(\d+)\s*batches\s*-\s*(.*)$", re.IGNORECASE
                ),
                "current_success_line_pattern": re.compile(
                    r"Succès:\s*(.*?)(?:\.csv|\.json)\s+créé", re.IGNORECASE
                ),
                "current_item_text_from_success_line": True
            }
        }
    
    def _get_step4_config(self) -> Dict[str, Any]:
        """Get configuration for Step 4: Audio Analysis.
        
        Returns:
            Step 4 configuration dictionary
        """
        step4_log_dir = self.logs_base_dir / "step4"

        step4_script_name = "run_audio_analysis_lemonfox.py" if config.STEP4_USE_LEMONFOX else "run_audio_analysis.py"
        cmd = [
            str(config.get_venv_python("audio_env")),
            str(self.base_path / "workflow_scripts" / "step4" / step4_script_name),
            "--log_dir", str(step4_log_dir),
        ]
        
        return {
            "display_name": "4. Analyse audio",
            "cmd": cmd,
            "cwd": str(self.base_path / "projets_extraits"),
            "specific_logs": [
                {
                    "name": "Log Analyse Audio",
                    "type": "directory_latest",
                    "path": step4_log_dir,
                    "pattern": "*.log",
                    "lines": 150
                }
            ],
            "progress_patterns": {
                "total": re.compile(r"TOTAL_AUDIO_TO_ANALYZE:\s*(\d+)", re.IGNORECASE),
                "current": re.compile(r"ANALYZING_AUDIO:\s*(\d+)/(\d+):\s*(.*)", re.IGNORECASE),
                "internal": re.compile(
                    r"INTERNAL_PROGRESS:\s*(\d+)/(\d+)\s*frames\s*\((\d+)%\)\s*-\s*(.*)",
                    re.IGNORECASE
                ),
                "current_success_line_pattern": re.compile(
                    r"Succès: analyse audio terminée pour (.*?)$", re.IGNORECASE
                ),
                "current_item_text_from_success_line": True
            }
        }
    
    def _get_step5_config(self) -> Dict[str, Any]:
        """Get configuration for Step 5: Tracking Analysis.
        
        Returns:
            Step 5 configuration dictionary
        """
        step5_log_dir = self.logs_base_dir / "step5"
        
        return {
            "display_name": "5. Analyse du tracking",
            "cmd": [
                str(config.get_venv_python("tracking_env")),
                str(self.base_path / "workflow_scripts" / "step5" / "run_tracking_manager.py")
            ],
            "cwd": str(self.base_path / "projets_extraits"),
            "specific_logs": [
                {
                    "name": "Log Tracking Manager",
                    "type": "directory_latest",
                    "path": step5_log_dir,
                    "pattern": "manager_tracking*.log",
                    "lines": 100
                },
                {
                    "name": "Log Worker CPU",
                    "type": "directory_latest",
                    "path": step5_log_dir,
                    "pattern": "*worker_CPU*.log",
                    "lines": 100
                },
                {
                    "name": "Log Worker GPU",
                    "type": "directory_latest",
                    "path": step5_log_dir,
                    "pattern": "*worker_GPU*.log",
                    "lines": 100
                }
            ],
            "progress_patterns": {
                "total": re.compile(r"Vidéos à traiter: (\d+)", re.IGNORECASE),
                "current": re.compile(r"Traitement de (.*?):\s*(\d+)%", re.IGNORECASE),
                "internal": re.compile(r"(.*?):\s*(\d+)%", re.IGNORECASE),
                "current_success_line_pattern": re.compile(
                    r"\[Gestionnaire\] Succès pour (.*?)$", re.IGNORECASE
                ),
                "current_item_text_from_success_line": True
            },
            "post_completion_message_ui": "Traitement du tracking terminé."
        }
    
    def _get_step6_config(self) -> Dict[str, Any]:
        """Get configuration for Step 6: JSON Reduction.
        
        Returns:
            Step 6 configuration dictionary
        """
        step6_log_dir = self.logs_base_dir / "step6"
        
        return {
            "display_name": "6. Réduction JSON",
            "cmd": [
                str(config.get_venv_python("env")),
                str(self.base_path / "workflow_scripts" / "step6" / "json_reducer.py"),
                "--log_dir", str(step6_log_dir),
                "--work_dir", str(self.base_path / "projets_extraits")
            ],
            "cwd": str(self.base_path / "projets_extraits"),
            "specific_logs": [
                {
                    "name": "Log Réduction JSON",
                    "type": "directory_latest",
                    "path": step6_log_dir,
                    "pattern": "*.log",
                    "lines": 150
                }
            ],
            "progress_patterns": {
                "total": re.compile(r"TOTAL_JSON_TO_REDUCE:\s*(\d+)", re.IGNORECASE),
                "current": re.compile(r"REDUCING_JSON:\s*(\d+)/(\d+):\s*(.*)", re.IGNORECASE),
                "internal": re.compile(
                    r"INTERNAL_PROGRESS:\s*(\d+)/(\d+)\s*items\s*\((\d+)%\)\s*-\s*(.*)",
                    re.IGNORECASE
                ),
                "current_success_line_pattern": re.compile(
                    r"Succès: réduction JSON terminée pour (.*?)$", re.IGNORECASE
                ),
                "current_item_text_from_success_line": True
            }
        }
    
    def _get_step7_config(self) -> Dict[str, Any]:
        """Get configuration for Step 7: Finalization.
        
        Returns:
            Step 7 configuration dictionary
        """
        step7_log_dir = self.logs_base_dir / "step7"
        
        return {
            "display_name": "7. Finalisation",
            "cmd": [
                str(config.get_venv_python("env")),
                str(self.base_path / "workflow_scripts" / "step7" / "finalize_and_copy.py")
            ],
            "cwd": str(self.base_path / "projets_extraits"),
            "specific_logs": [
                {
                    "name": "Log Finalisation",
                    "type": "directory_latest",
                    "path": step7_log_dir,
                    "pattern": "*.log",
                    "lines": 150
                }
            ],
            "progress_patterns": {
                "total": re.compile(r"(\d+) projet\(s\) à finaliser", re.IGNORECASE),
                "current_success_line_pattern": re.compile(
                    r"Finalisation terminée pour '(.*?)'", re.IGNORECASE
                ),
                "current_item_text_from_success_line": True
            },
            "post_completion_message_ui": "Finalisation des projets terminée."
        }
    
    # ========== Public API ==========
    
    def get_config(self) -> Dict[str, Dict[str, Any]]:
        """Get the complete workflow commands configuration.
        
        Returns:
            Dictionary mapping step keys to their configuration
        """
        return self._config.copy()
    
    def get_step_config(self, step_key: str) -> Optional[Dict[str, Any]]:
        """Get configuration for a specific step.
        
        Args:
            step_key: Step identifier (e.g., 'STEP1', 'STEP2')
            
        Returns:
            Step configuration dictionary or None if not found
        """
        return self._config.get(step_key)
    
    def validate_step_key(self, step_key: str) -> bool:
        """Validate if a step key exists in configuration.
        
        Args:
            step_key: Step identifier to validate
            
        Returns:
            True if step key is valid
        """
        return step_key in self._config
    
    def get_all_step_keys(self) -> List[str]:
        """Get all valid step keys.
        
        Returns:
            List of step keys
        """
        return list(self._config.keys())
    
    def get_step_display_name(self, step_key: str) -> Optional[str]:
        """Get display name for a step.
        
        Args:
            step_key: Step identifier
            
        Returns:
            Display name or None if step not found
        """
        step_config = self.get_step_config(step_key)
        return step_config.get('display_name') if step_config else None
    
    def get_step_command(self, step_key: str) -> Optional[List[str]]:
        """Get command line for a step.
        
        Args:
            step_key: Step identifier
            
        Returns:
            Command line as list of strings or None if step not found
        """
        step_config = self.get_step_config(step_key)
        return step_config.get('cmd') if step_config else None
    
    def get_step_cwd(self, step_key: str) -> Optional[str]:
        """Get working directory for a step.
        
        Args:
            step_key: Step identifier
            
        Returns:
            Working directory path or None if step not found
        """
        step_config = self.get_step_config(step_key)
        return step_config.get('cwd') if step_config else None
    
    def update_hf_token(self, hf_token: str) -> None:
        """Update HuggingFace token and rebuild Step 4 configuration.
        
        Args:
            hf_token: New HuggingFace authentication token
        """
        self.hf_token = hf_token
        self._config["STEP4"] = self._get_step4_config()
        logger.info("HuggingFace token updated in configuration")
    
    def __repr__(self) -> str:
        """String representation of configuration."""
        return f"WorkflowCommandsConfig(base_path={self.base_path}, steps={len(self._config)})"
```

## File: routes/__init__.py
```python
# routes package initialization
```

## File: routes/api_routes.py
```python
"""
API Routes Blueprint
Handles all API endpoints for system monitoring, step status, and remote communication.
"""

import logging
import time
from functools import wraps
from flask import Blueprint, jsonify, request
from config.security import require_internal_worker_token, require_render_register_token
from services.monitoring_service import MonitoringService
from services.workflow_service import WorkflowService
from services.performance_service import PerformanceService
from services.filesystem_service import FilesystemService
from services.visualization_service import VisualizationService
from services.lemonfox_audio_service import LemonfoxAudioService

logger = logging.getLogger(__name__)

# Create API blueprint
api_bp = Blueprint('api', __name__)


def measure_api(endpoint_name: str):
    """Decorator to measure API response time and record it via PerformanceService.

    Args:
        endpoint_name: Logical name of the endpoint for metrics.
    """
    def decorator(fn):
        @wraps(fn)
        def wrapper(*args, **kwargs):
            start = time.perf_counter()
            status_code = 200
            try:
                resp = fn(*args, **kwargs)
                # Flask can return a tuple (payload, status)
                if isinstance(resp, tuple) and len(resp) >= 2:
                    status_code = resp[1]
                return resp
            except Exception:
                status_code = 500
                raise
            finally:
                elapsed_ms = (time.perf_counter() - start) * 1000.0
                try:
                    PerformanceService.record_api_response_time(endpoint_name, elapsed_ms, status_code)
                except Exception:
                    logger.debug("Failed to record API performance metric", exc_info=True)
        return wrapper
    return decorator

@api_bp.route('/system_monitor')
@measure_api('/api/system_monitor')
def system_monitor():
    """
    Get current system resource usage (CPU, RAM, GPU).

    Returns:
        JSON response with system status:
        {
            "cpu_percent": float,
            "memory": {
                "percent": float,
                "used_gb": float,
                "total_gb": float
            },
            "gpu": dict|null
        }

    Status Codes:
        200: Success
        500: Server error
    """
    try:
        # Thin controller: delegate to service layer
        status = MonitoringService.get_system_status()
        return jsonify(status)
    except Exception as e:
        logger.error(f"System monitor error: {e}")
        import traceback
        logger.error(f"Traceback: {traceback.format_exc()}")
        return jsonify({"error": "Unable to retrieve system information"}), 500


@api_bp.route('/system/diagnostics')
@measure_api('/api/system/diagnostics')
def system_diagnostics():
    """
    Get environment diagnostics (Python/FFmpeg versions, GPU availability, filtered config flags).

    Returns:
        JSON response with environment info:
        {
            "python": {"version": str, "implementation": str},
            "ffmpeg": {"version": str},
            "gpu": {"available": bool, "name": str|null},
            "config_flags": { ... },
            "timestamp": str
        }

    Status Codes:
        200: Success
        500: Server error
    """
    try:
        info = MonitoringService.get_environment_info()
        return jsonify(info)
    except Exception as e:
        logger.error(f"Diagnostics error: {e}")
        return jsonify({"error": "Unable to retrieve diagnostics"}), 500


@api_bp.route('/step_status/<step_key>')
@measure_api('/api/step_status')
def step_status(step_key):
    """
    Get current status of a workflow step.
    
    Args:
        step_key (str): Step identifier (STEP1, STEP2, etc.)
        
    Returns:
        JSON response with step status:
        {
            "step": str,
            "display_name": str,
            "status": "idle|running|completed|error",
            "progress_current": int,
            "progress_total": int,
            "progress_text": str,
            "return_code": int|null,
            "duration_str": str|null,
            "is_any_sequence_running": bool
        }
        
    Status Codes:
        200: Success
        404: Step not found
        500: Server error
    """
    try:
        return jsonify(WorkflowService.get_step_status(step_key))
    except ValueError as e:
        return jsonify({"error": str(e)}), 404
    except Exception as e:
        logger.error(f"Step status error for {step_key}: {e}")
        return jsonify({"error": "Unable to retrieve step status"}), 500


@api_bp.route('/csv_monitor_status')
@measure_api('/api/csv_monitor_status')
def csv_monitor_status():
    """
    Get CSV monitoring service status.

    Returns:
        JSON response with CSV monitor status:
        {
            "csv_monitor": {
                "status": str,
                "last_check": str|null,
                "error": str|null
            },
            "csv_url": str,
            "check_interval": int
        }

    Status Codes:
        200: Success
        500: Server error
    """
    try:
        from services.csv_service import CSVService

        # Use CSVService to get monitor status
        monitor_data = CSVService.get_monitor_status()

        return jsonify(monitor_data)
    except Exception as e:
        logger.error(f"CSV monitor status error: {e}")
        return jsonify({"error": "Unable to retrieve CSV monitor status"}), 500


@api_bp.route('/ping', methods=['GET'])
@measure_api('/api/ping')
@require_internal_worker_token
def api_ping():
    """
    Health check endpoint for remote servers.
    Requires internal worker token authentication.

    Returns:
        JSON response:
        {
            "status": "pong",
            "worker_type": "ubuntu_new"
        }

    Status Codes:
        200: Success
        401: Unauthorized
    """
    return jsonify({"status": "pong", "worker_type": "ubuntu_new"}), 200


@api_bp.route('/get_remote_status_summary', methods=['GET'])
@measure_api('/api/get_remote_status_summary')
@require_internal_worker_token
def api_get_remote_status_summary():
    """
    Get workflow status summary for remote servers.
    Requires internal worker token authentication.

    Returns:
        JSON response with comprehensive workflow status

    Status Codes:
        200: Success
        401: Unauthorized
        500: Server error
    """
    try:
        return jsonify(WorkflowService.get_current_workflow_status_summary())
    except Exception as e:
        logger.error(f"Remote status summary error: {e}")
        return jsonify({"error": "Unable to retrieve status summary"}), 500


@api_bp.route('/performance/metrics')
@measure_api('/api/performance/metrics')
def performance_metrics():
    """
    Get performance metrics and profiling data.
    
    Returns:
        JSON response with performance metrics:
        {
            "profiling_stats": dict,
            "cache_stats": dict,
            "system_performance": dict
        }
        
    Status Codes:
        200: Success
        500: Server error
    """
    try:
        return jsonify(PerformanceService.get_performance_metrics())
    except Exception as e:
        logger.error(f"Performance metrics error: {e}")
        return jsonify({"error": "Unable to retrieve performance metrics"}), 500


@api_bp.route('/performance/reset', methods=['POST'])
@measure_api('/api/performance/reset')
def reset_performance_metrics():
    """
    Reset performance profiling statistics.
    
    Returns:
        JSON response:
        {
            "status": "success",
            "message": "Performance metrics reset"
        }
        
    Status Codes:
        200: Success
        500: Server error
    """
    try:
        PerformanceService.reset_profiling_stats()
        return jsonify({
            "status": "success", 
            "message": "Performance metrics reset"
        })
    except Exception as e:
        logger.error(f"Performance reset error: {e}")
        return jsonify({"error": "Unable to reset performance metrics"}), 500


@api_bp.route('/cache/stats')
@measure_api('/api/cache/stats')
def cache_stats():
    """
    Get cache statistics and hit rates.
    
    Returns:
        JSON response with cache statistics
        
    Status Codes:
        200: Success
        500: Server error
    """
    try:
        from services.cache_service import CacheService
        return jsonify(CacheService.get_cache_stats())
    except Exception as e:
        logger.error(f"Cache stats error: {e}")
        return jsonify({"error": "Unable to retrieve cache statistics"}), 500



@api_bp.route('/cache/search', methods=['GET'])
@measure_api('/api/cache/search')
def cache_search():
    """
    Search for folders under /mnt/cache that start with a given number.

    Query Params:
        number (str): Numeric identifier, e.g., "115"

    Returns:
        JSON response:
        {
            "number": str,
            "matches": [str],
            "best_match": str|null
        }

    Status Codes:
        200: Success
        400: Missing parameter
        500: Server error
    """
    try:
        number = request.args.get('number', '').strip()
        if not number:
            return jsonify({"error": "Missing 'number' parameter"}), 400

        result = FilesystemService.find_cache_folder_by_number(number)
        return jsonify({
            "number": result.number,
            "matches": result.matches,
            "best_match": result.best_match
        })
    except Exception as e:
        logger.error(f"Cache search error: {e}")
        return jsonify({"error": "Unable to search cache"}), 500


@api_bp.route('/cache/list_today', methods=['GET'])
@measure_api('/api/cache/list_today')
def cache_list_today():
    """
    List folders under /mnt/cache that were created/modified today.

    Returns:
        JSON response:
        {
            "folders": [
                {"path": str, "name": str, "number": str|null, "mtime": str}
            ]
        }

    Status Codes:
        200: Success
        500: Server error
    """
    try:
        folders = FilesystemService.list_today_cache_folders()
        return jsonify({"folders": folders})
    except Exception as e:
        logger.error(f"Cache list_today error: {e}")
        return jsonify({"error": "Unable to list today's cache folders"}), 500

@api_bp.route('/cache/open', methods=['POST'])
@measure_api('/api/cache/open')
def cache_open():
    """
    Open a folder path in the system's file explorer (server-side).

    Request Body (JSON):
        {
            "path": "/mnt/cache/115 Camille",
            "select_parent": false   # optional, open parent and preselect target when supported
        }

    Returns:
        JSON response:
        {
            "success": bool,
            "message": str
        }

    Status Codes:
        200: Success/Failure message
        400: Invalid body
        500: Server error
    """
    try:
        data = request.get_json(silent=True) or {}
        path = (data.get('path') or '').strip()
        select_parent = bool(data.get('select_parent', False))
        if not path:
            return jsonify({"success": False, "message": "Paramètre 'path' manquant"}), 400

        success, message = FilesystemService.open_path_in_explorer(path, select_parent=select_parent)
        return jsonify({"success": success, "message": message})
    except Exception as e:
        logger.error(f"Cache open error: {e}")
        return jsonify({"success": False, "message": "Erreur interne pour l'ouverture du dossier"}), 500
@api_bp.route('/cache/clear', methods=['POST'])
@measure_api('/api/cache/clear')
def clear_cache():
    """
    Clear application cache.

    Returns:
        JSON response:
        {
            "status": "success",
            "message": "Cache cleared"
        }

    Status Codes:
        200: Success
        500: Server error
    """
    try:
        from services.cache_service import CacheService
        CacheService.clear_cache()
        return jsonify({
            "status": "success",
            "message": "Cache cleared"
        })
    except Exception as e:
        logger.error(f"Cache clear error: {e}")
        return jsonify({"error": "Unable to clear cache"}), 500





@api_bp.route('/csv_downloads_status')
def get_csv_downloads_status():
    """
    Get CSV downloads status.

    Returns:
        JSON response with CSV downloads status

    Status Codes:
        200: Success
        500: Server error
    """
    try:
        from services.csv_service import CSVService
        from datetime import datetime

        # Use CSVService to get download status
        downloads_status = CSVService.get_csv_downloads_status()

        # Combine active and recent downloads
        active_downloads = downloads_status.get("active_downloads", {})
        recent_statuses = downloads_status.get("recent_statuses", [])

        # Convert active downloads dict to list
        active_list = list(active_downloads.values()) if active_downloads else []

        # Combine and sort all downloads
        all_downloads = active_list + recent_statuses
        all_downloads.sort(key=lambda x: x.get('timestamp', datetime.min), reverse=True)

        # Clean timestamps for JSON serialization
        json_safe_downloads = []
        for download in all_downloads:
            download_copy = download.copy()
            # Remove datetime objects that can't be JSON serialized
            download_copy.pop('timestamp', None)
            json_safe_downloads.append(download_copy)

        return jsonify(json_safe_downloads)
    except Exception as e:
        logger.error(f"CSV downloads status error: {e}")
        return jsonify({"error": "Unable to retrieve CSV downloads status"}), 500






@api_bp.route('/stats/dashboard')
@measure_api('/api/stats/dashboard')
def stats_dashboard():
    """
    Get comprehensive statistics for the performance dashboard.

    Returns:
        JSON response with dashboard statistics:
        {
            "summary": {
                "total_api_calls": int,
                "total_errors": int,
                "avg_response_time_ms": float,
                "error_rate_percent": float
            },
            "api_metrics": list,
            "system_metrics": list,
            "step_history": dict,
            "alerts": list,
            "cache_stats": dict,
            "profiling_summary": dict
        }

    Status Codes:
        200: Success
        500: Server error
    """
    try:
        stats = PerformanceService.get_dashboard_stats()
        return jsonify(stats)
    except Exception as e:
        logger.error(f"Dashboard stats error: {e}")
        return jsonify({"error": "Unable to retrieve dashboard statistics"}), 500


@api_bp.route('/stats/history')
@measure_api('/api/stats/history')
def stats_history():
    """
    Get historical performance data.

    Query Parameters:
        type (str): Data type - "api", "system", or "all" (default: "all")
        limit (int): Maximum number of records (default: 100, max: 500)

    Returns:
        JSON response with historical data:
        {
            "data_type": str,
            "count": int,
            "data": list
        }

    Status Codes:
        200: Success
        400: Invalid parameters
        500: Server error
    """
    try:
        data_type = request.args.get('type', 'all')
        limit = request.args.get('limit', 100, type=int)
        
        # Validate parameters
        if data_type not in ['api', 'system', 'all']:
            return jsonify({"error": "Invalid data type. Must be 'api', 'system', or 'all'"}), 400
        
        if limit < 1 or limit > 500:
            return jsonify({"error": "Limit must be between 1 and 500"}), 400
        
        history = PerformanceService.get_historical_data(data_type, limit)
        return jsonify(history)
        
    except Exception as e:
        logger.error(f"Historical data error: {e}")
        return jsonify({"error": "Unable to retrieve historical data"}), 500



@api_bp.route('/visualization/projects', methods=['GET'])
@measure_api('/api/visualization/projects')
def visualization_projects():
    """
    List available projects and their videos for visualization/reporting.

    Returns:
        JSON response:
        {
            "projects": [
                {"name": str, "path": str, "videos": [str], "video_count": int,
                 "has_scenes": bool, "has_audio": bool, "has_tracking": bool, "source": "projects|archives"}
            ],
            "count": int,
            "timestamp": str
        }

    Status Codes:
        200: Success
        500: Server error
    """
    try:
        data = VisualizationService.get_available_projects()
        # If service reports an error, surface as 500 while returning payload
        if data.get("error"):
            return jsonify(data), 500
        return jsonify(data)
    except Exception as e:
        logger.error(f"Visualization projects error: {e}", exc_info=True)
        return jsonify({"error": "Unable to list visualization projects"}), 500


@api_bp.route('/step4/lemonfox_audio', methods=['POST'])
@measure_api('/api/step4/lemonfox_audio')
def lemonfox_audio_analysis():
    """
    Process video audio analysis using Lemonfox Speech-to-Text API.
    
    Generates a STEP4-compatible {video_stem}_audio.json file with frame-by-frame
    audio analysis including speaker diarization.
    
    Request JSON:
        {
            "project_name": str (required),
            "video_name": str (required, relative path within project),
            "language": str (optional),
            "prompt": str (optional),
            "speaker_labels": bool (optional, default: true),
            "min_speakers": int (optional),
            "max_speakers": int (optional),
            "timestamp_granularities": array[str] (optional, e.g., ["word"]),
            "eu_processing": bool (optional, uses LEMONFOX_EU_DEFAULT if not set)
        }
    
    Returns:
        JSON response on success:
        {
            "status": "success",
            "output_path": str,
            "fps": float,
            "total_frames": int
        }
        
        JSON response on error:
        {
            "status": "error",
            "error": str
        }
    
    Status Codes:
        200: Success
        400: Invalid input parameters
        404: Project or video not found
        502: Lemonfox API upstream error
        500: Server error
    """
    try:
        # Parse and validate request
        data = request.get_json(silent=True)
        if not data:
            return jsonify({
                "status": "error",
                "error": "Request body must be JSON"
            }), 400
        
        # Required fields
        project_name = data.get('project_name', '').strip()
        video_name = data.get('video_name', '').strip()
        
        if not project_name:
            return jsonify({
                "status": "error",
                "error": "project_name is required"
            }), 400
        
        if not video_name:
            return jsonify({
                "status": "error",
                "error": "video_name is required"
            }), 400
        
        # Optional fields
        language = data.get('language')
        prompt = data.get('prompt')
        speaker_labels = data.get('speaker_labels')
        min_speakers = data.get('min_speakers')
        max_speakers = data.get('max_speakers')
        timestamp_granularities = data.get('timestamp_granularities')
        eu_processing = data.get('eu_processing')
        
        # Validate types
        if min_speakers is not None and not isinstance(min_speakers, int):
            return jsonify({
                "status": "error",
                "error": "min_speakers must be an integer"
            }), 400
        
        if max_speakers is not None and not isinstance(max_speakers, int):
            return jsonify({
                "status": "error",
                "error": "max_speakers must be an integer"
            }), 400
        
        if timestamp_granularities is not None and not isinstance(timestamp_granularities, list):
            return jsonify({
                "status": "error",
                "error": "timestamp_granularities must be an array"
            }), 400

        if speaker_labels is not None and not isinstance(speaker_labels, bool):
            return jsonify({
                "status": "error",
                "error": "speaker_labels must be a boolean"
            }), 400
        
        # Call service
        result = LemonfoxAudioService.process_video_with_lemonfox(
            project_name=project_name,
            video_name=video_name,
            language=language,
            prompt=prompt,
            speaker_labels=speaker_labels,
            min_speakers=min_speakers,
            max_speakers=max_speakers,
            timestamp_granularities=timestamp_granularities,
            eu_processing=eu_processing
        )
        
        if not result.success:
            error_msg = result.error or "Unknown error"
            
            # Determine appropriate status code
            if "not found" in error_msg.lower():
                status_code = 404
            elif "lemonfox api" in error_msg.lower():
                status_code = 502
            elif "not configured" in error_msg.lower():
                status_code = 500
            else:
                status_code = 400
            
            return jsonify({
                "status": "error",
                "error": error_msg
            }), status_code
        
        # Success response
        return jsonify({
            "status": "success",
            "output_path": str(result.output_path),
            "fps": result.fps,
            "total_frames": result.total_frames
        }), 200
        
    except Exception as e:
        logger.error(f"Lemonfox audio analysis error: {e}", exc_info=True)
        return jsonify({
            "status": "error",
            "error": "Internal server error"
        }), 500
```

## File: routes/workflow_routes.py
```python
"""
Workflow Routes Blueprint
Handles workflow execution, step management, and sequence operations.
"""

import logging
import time
from functools import wraps
from flask import Blueprint, jsonify, request, render_template, send_from_directory
from services.workflow_service import WorkflowService
from services.cache_service import CacheService
from services.performance_service import PerformanceService
from config.settings import config

# Configure route logger to capture all debug statements
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)  # Ensure we capture all debug statements

# Used for cache-busting static assets in templates. This value changes on server restart,
# which is enough to force browsers to fetch updated JS/CSS after a deploy/restart.
_STATIC_CACHE_BUSTER = str(int(time.time()))

# Create workflow blueprint
workflow_bp = Blueprint('workflow', __name__)


def measure_api(endpoint_name: str):
    """Decorator to measure API response time and record it via PerformanceService.

    Args:
        endpoint_name: Logical name of the endpoint for metrics.
    """
    def decorator(fn):
        @wraps(fn)
        def wrapper(*args, **kwargs):
            start = time.perf_counter()
            status_code = 200
            try:
                resp = fn(*args, **kwargs)
                # Flask can return a tuple (payload, status)
                if isinstance(resp, tuple) and len(resp) >= 2:
                    status_code = resp[1]
                return resp
            except Exception:
                status_code = 500
                raise
            finally:
                elapsed_ms = (time.perf_counter() - start) * 1000.0
                try:
                    PerformanceService.record_api_response_time(endpoint_name, elapsed_ms, status_code)
                except Exception:
                    logger.debug("Failed to record API performance metric", exc_info=True)
        return wrapper
    return decorator


@workflow_bp.route('/')
def index():
    """
    Main application page.
    
    Returns:
        Rendered HTML template with steps configuration
        
    Status Codes:
        200: Success
    """
    try:
        # Use cached configuration for better performance
        frontend_safe_steps_config = CacheService.get_cached_frontend_config()
        return render_template(
            'index_new.html',
            steps_config=frontend_safe_steps_config,
            cache_buster=_STATIC_CACHE_BUSTER,
        )
    except Exception as e:
        logger.error(f"Index page error: {e}")
        return render_template('error.html', error="Unable to load application"), 500


@workflow_bp.route('/run/<step_key>', methods=['POST'])
@measure_api('/workflow/run')
def run_step(step_key):
    """
    Execute a single workflow step.
    
    Args:
        step_key (str): Step identifier (STEP1, STEP2, etc.)
        
    Returns:
        JSON response:
        {
            "status": "initiated|error",
            "message": str
        }
        
    Status Codes:
        202: Step initiated successfully
        404: Step not found
        409: Step already running or sequence in progress
        500: Server error
    """
    try:
        result = WorkflowService.run_step(step_key)
        
        if result["status"] == "initiated":
            return jsonify(result), 202
        elif result["status"] == "error":
            if "inconnue" in result["message"]:
                return jsonify(result), 404
            elif "en cours" in result["message"]:
                return jsonify(result), 409
            else:
                return jsonify(result), 500
        else:
            return jsonify(result), 500
            
    except Exception as e:
        logger.error(f"Run step error for {step_key}: {e}")
        return jsonify({
            "status": "error", 
            "message": f"Internal error running step {step_key}"
        }), 500


@workflow_bp.route('/run_custom_sequence', methods=['POST'])
@measure_api('/workflow/run_custom_sequence')
def run_custom_sequence():
    """
    Execute a custom sequence of workflow steps.
    
    Request Body:
        {
            "steps": ["STEP1", "STEP2", ...]
        }
        
    Returns:
        JSON response:
        {
            "status": "initiated|error",
            "message": str
        }
        
    Status Codes:
        202: Sequence initiated successfully
        400: Invalid request data
        409: Sequence already running
        500: Server error
    """
    try:
        data = request.get_json()
        if not data or not isinstance(data.get('steps'), list):
            return jsonify({
                "status": "error", 
                "message": "Invalid steps list"
            }), 400
            
        result = WorkflowService.run_custom_sequence(data['steps'])
        
        if result["status"] == "initiated":
            return jsonify(result), 202
        elif result["status"] == "error":
            if "en cours" in result["message"]:
                return jsonify(result), 409
            else:
                return jsonify(result), 400
        else:
            return jsonify(result), 500
            
    except Exception as e:
        logger.error(f"Custom sequence error: {e}")
        return jsonify({
            "status": "error", 
            "message": "Internal error running custom sequence"
        }), 500


@workflow_bp.route('/status/<step_key>', methods=['GET'])
@measure_api('/workflow/status')
def get_status(step_key):
    """
    Get detailed status of a workflow step.
    
    Args:
        step_key (str): Step identifier
        
    Returns:
        JSON response with detailed step status including logs
        
    Status Codes:
        200: Success
        404: Step not found
        500: Server error
    """
    try:
        status_data = WorkflowService.get_step_status(step_key, include_logs=True)

        # DEBUG: Log what we're returning to the frontend during AutoMode
        if status_data.get('is_any_sequence_running', False):
            logger.info(f"[ROUTE_DEBUG] /status/{step_key} returning: status='{status_data.get('status')}', progress={status_data.get('progress_current')}/{status_data.get('progress_total')}")

        return jsonify(status_data)
    except ValueError as e:
        return jsonify({"status": "error", "message": str(e)}), 404
    except Exception as e:
        logger.error(f"Get status error for {step_key}: {e}")
        return jsonify({"error": "Unable to retrieve step status"}), 500


@workflow_bp.route('/stop/<step_key>', methods=['POST'])
@measure_api('/workflow/stop')
def stop_step(step_key):
    """
    Stop a running workflow step.
    
    Args:
        step_key (str): Step identifier
        
    Returns:
        JSON response:
        {
            "status": "success|error",
            "message": str
        }
        
    Status Codes:
        200: Success
        404: Step not found
        409: Step not running
        500: Server error
    """
    try:
        result = WorkflowService.stop_step(step_key)
        return jsonify(result)
    except ValueError as e:
        return jsonify({"status": "error", "message": str(e)}), 404
    except Exception as e:
        logger.error(f"Stop step error for {step_key}: {e}")
        return jsonify({
            "status": "error", 
            "message": f"Internal error stopping step {step_key}"
        }), 500


@workflow_bp.route('/get_specific_log_test/<step_key>/<int:log_index>')
def get_specific_log_test(step_key, log_index):
    """
    Test version of get specific log that bypasses cache service.
    """
    try:
        result = WorkflowService.get_step_log_file(step_key, log_index)
        return jsonify(result)

    except Exception as e:
        logger.error(f"Test log endpoint error for {step_key}/{log_index}: {e}")
        import traceback
        logger.error(f"Traceback: {traceback.format_exc()}")
        return jsonify({"error": f"Internal error: {str(e)}"}), 500

@workflow_bp.route('/get_specific_log/<step_key>/<int:log_index>')
def get_specific_log(step_key, log_index):
    """
    Get specific log file content for a step.
    
    Args:
        step_key (str): Step identifier
        log_index (int): Log file index
        
    Returns:
        JSON response with log content
        
    Status Codes:
        200: Success
        404: Step or log not found
        500: Server error
    """
    try:
        result = WorkflowService.get_step_log_file(step_key, log_index)
        return jsonify(result)
    except ValueError as e:
        logger.error(f"ValueError in get_specific_log for {step_key}/{log_index}: {e}")
        return jsonify({"error": str(e)}), 404
    except Exception as e:
        logger.error(f"Get specific log error for {step_key}/{log_index}: {e}")
        import traceback
        logger.error(f"Traceback: {traceback.format_exc()}")
        return jsonify({"error": "Unable to retrieve log content"}), 500


@workflow_bp.route('/sound-design/<filename>')
def serve_sound_file(filename):
    """
    Serve sound files from the sound-design directory.
    
    Args:
        filename (str): Sound file name
        
    Returns:
        Sound file content
        
    Status Codes:
        200: Success
        404: File not found
    """
    try:
        sound_dir = config.BASE_PATH_SCRIPTS / 'sound-design'
        return send_from_directory(sound_dir, filename)
    except Exception as e:
        logger.error(f"Sound file serve error for {filename}: {e}")
        return jsonify({"error": "Sound file not found"}), 404


@workflow_bp.route('/test-sound')
def test_sound():
    """
    Serve the sound test page.
    
    Returns:
        Sound test HTML page
        
    Status Codes:
        200: Success
        404: File not found
    """
    try:
        return send_from_directory(config.BASE_PATH_SCRIPTS, 'test_sound.html')
    except Exception as e:
        logger.error(f"Test sound page error: {e}")
        return jsonify({"error": "Test sound page not found"}), 404


@workflow_bp.route('/sequence/status')
@measure_api('/workflow/sequence/status')
def sequence_status():
    """
    Get current sequence execution status.
    
    Returns:
        JSON response with sequence status:
        {
            "is_running": bool,
            "current_step": str|null,
            "progress": {
                "current": int,
                "total": int
            },
            "last_outcome": dict
        }
        
    Status Codes:
        200: Success
        500: Server error
    """
    try:
        return jsonify(WorkflowService.get_sequence_status())
    except Exception as e:
        logger.error(f"Sequence status error: {e}")
        return jsonify({"error": "Unable to retrieve sequence status"}), 500


@workflow_bp.route('/sequence/stop', methods=['POST'])
@measure_api('/workflow/sequence/stop')
def stop_sequence():
    """
    Stop the currently running sequence.

    Returns:
        JSON response:
        {
            "status": "success|error",
            "message": str
        }

    Status Codes:
        200: Success
        409: No sequence running
        500: Server error
    """
    try:
        result = WorkflowService.stop_sequence()
        if result["status"] == "error" and "aucune" in result["message"]:
            return jsonify(result), 409
        return jsonify(result)
    except Exception as e:
        logger.error(f"Stop sequence error: {e}")
        return jsonify({
            "status": "error",
            "message": "Internal error stopping sequence"
        }), 500


@workflow_bp.route('/cancel/<step_key>', methods=['POST'])
@measure_api('/workflow/cancel')
def cancel_step(step_key):
    """
    Cancel a running workflow step.

    Args:
        step_key (str): Step identifier

    Returns:
        JSON response:
        {
            "status": "success|error",
            "message": str
        }

    Status Codes:
        200: Success
        400: Step not running or doesn't exist
        500: Server error
    """
    try:
        result = WorkflowService.stop_step(step_key)
        if result["status"] == "error":
            if "not running" in result["message"] or "not found" in result["message"]:
                return jsonify(result), 400
            else:
                return jsonify(result), 500
        return jsonify(result)
    except ValueError as e:
        return jsonify({"status": "error", "message": str(e)}), 400
    except Exception as e:
        logger.error(f"Cancel step error for {step_key}: {e}")
        return jsonify({
            "status": "error",
            "message": f"Internal error cancelling step {step_key}"
        }), 500


# Additional workflow routes that were moved from app_new.py
# Note: Duplicate routes removed to prevent conflicts
@workflow_bp.route('/sound-design/<filename>')
def serve_sound_file_blueprint(filename):
    """
    Serve sound files from the sound-design directory (moved from app_new.py).

    Args:
        filename (str): Sound file name

    Returns:
        Sound file content

    Status Codes:
        200: Success
        404: File not found
    """
    try:
        sound_dir = config.BASE_PATH_SCRIPTS / 'sound-design'
        return send_from_directory(sound_dir, filename)
    except Exception as e:
        logger.error(f"Sound file serve error for {filename}: {e}")
        return jsonify({"error": "Sound file not found"}), 404


@workflow_bp.route('/test-sound')
def test_sound_blueprint():
    """
    Serve the sound test page (moved from app_new.py).

    Returns:
        Sound test HTML page

    Status Codes:
        200: Success
        404: File not found
    """
    try:
        return send_from_directory(config.BASE_PATH_SCRIPTS, 'test_sound.html')
    except Exception as e:
        logger.error(f"Test sound page error: {e}")
        return jsonify({"error": "Test sound page not found"}), 404
```

## File: services/deprecated/airtable_service.py
```python
"""
Airtable Service for workflow_mediapipe application.

This service handles all Airtable API interactions for monitoring download URLs.
Replaces the legacy CSV/XLSX file monitoring system with a more reliable
Airtable-based solution.

Author: Augment Agent
Date: 2025-07-21
"""

import logging
from typing import Dict, List, Any, Optional
from datetime import datetime, timezone
import time

from config.settings import config
from services.cache_service import CacheService

logger = logging.getLogger(__name__)

try:
    from pyairtable import Api
    AIRTABLE_AVAILABLE = True
except ImportError:
    AIRTABLE_AVAILABLE = False
    logger.warning("pyairtable not available. Airtable integration disabled.")


class AirtableService:
    """
    Service for managing Airtable integration.
    
    Provides methods for fetching records from Airtable and monitoring
    for new download URLs. Designed to replace the legacy CSV monitoring
    system while maintaining the same interface.
    """
    
    _api_instance: Optional[Any] = None
    _table_instance: Optional[Any] = None
    
    @classmethod
    def _get_api(cls) -> Optional[Any]:
        """
        Get or create Airtable API instance.
        
        Returns:
            Airtable API instance or None if not available
        """
        if not AIRTABLE_AVAILABLE:
            logger.error("Airtable integration not available - pyairtable not installed")
            return None
            
        if not config.AIRTABLE_ACCESS_TOKEN:
            logger.error("Airtable access token not configured")
            return None
            
        if cls._api_instance is None:
            try:
                cls._api_instance = Api(config.AIRTABLE_ACCESS_TOKEN)
                logger.info("Airtable API instance created successfully")
            except Exception as e:
                logger.error(f"Failed to create Airtable API instance: {e}")
                return None
                
        return cls._api_instance
    
    @classmethod
    def _get_table(cls) -> Optional[Any]:
        """
        Get or create Airtable table instance.

        Returns:
            Airtable table instance or None if not available
        """
        if cls._table_instance is None:
            api = cls._get_api()
            if api is None:
                return None

            try:
                # Get base by name
                bases = api.bases()
                target_base = None

                for base in bases:
                    if base.name == config.AIRTABLE_BASE_NAME:
                        target_base = base
                        break

                if target_base is None:
                    logger.error(f"Airtable base '{config.AIRTABLE_BASE_NAME}' not found")
                    available_bases = [base.name for base in bases]
                    logger.error(f"Available bases: {available_bases}")
                    return None

                # Get table from base - try by name first, then by ID if available
                try:
                    cls._table_instance = target_base.table(config.AIRTABLE_TABLE_NAME)
                    logger.info(f"Airtable table '{config.AIRTABLE_TABLE_NAME}' connected successfully")
                except Exception as table_error:
                    logger.warning(f"Failed to connect to table by name: {table_error}")

                    # Try to get table info from schema for better error reporting
                    try:
                        schema = target_base.schema()
                        available_tables = [(table.name, table.id) for table in schema.tables]
                        logger.info(f"Available tables: {available_tables}")

                        # Try to find table by name in schema
                        for table_info in schema.tables:
                            if table_info.name == config.AIRTABLE_TABLE_NAME:
                                logger.info(f"Found table in schema: {table_info.name} (ID: {table_info.id})")
                                # Try using table ID instead of name
                                cls._table_instance = target_base.table(table_info.id)
                                logger.info(f"Connected to table using ID: {table_info.id}")
                                break
                        else:
                            logger.error(f"Table '{config.AIRTABLE_TABLE_NAME}' not found in schema")
                            return None

                    except Exception as schema_error:
                        logger.error(f"Failed to get schema information: {schema_error}")
                        return None

            except Exception as e:
                logger.error(f"Failed to connect to Airtable table: {e}")
                return None

        return cls._table_instance
    
    @staticmethod
    def test_connection() -> Dict[str, Any]:
        """
        Test Airtable connection and configuration.
        
        Returns:
            Dictionary with connection test results
        """
        result = {
            "status": "error",
            "message": "",
            "details": {}
        }
        
        try:
            # Check if Airtable is available
            if not AIRTABLE_AVAILABLE:
                result["message"] = "pyairtable library not installed"
                result["details"]["pyairtable_available"] = False
                return result
            
            result["details"]["pyairtable_available"] = True
            
            # Check configuration
            if not config.AIRTABLE_ACCESS_TOKEN:
                result["message"] = "Airtable access token not configured"
                result["details"]["token_configured"] = False
                return result
            
            result["details"]["token_configured"] = True
            result["details"]["base_name"] = config.AIRTABLE_BASE_NAME
            result["details"]["table_name"] = config.AIRTABLE_TABLE_NAME
            
            # Test API connection and schema access
            api = AirtableService._get_api()
            if api is None:
                result["message"] = "Failed to create API instance"
                result["details"]["api_accessible"] = False
                return result

            result["details"]["api_accessible"] = True

            # Test base access
            try:
                bases = api.bases()
                result["details"]["bases_accessible"] = True
                result["details"]["available_bases"] = [base.name for base in bases]

                # Find target base
                target_base = None
                for base in bases:
                    if base.name == config.AIRTABLE_BASE_NAME:
                        target_base = base
                        break

                if target_base is None:
                    result["message"] = f"Base '{config.AIRTABLE_BASE_NAME}' not found"
                    result["details"]["base_found"] = False
                    return result

                result["details"]["base_found"] = True
                result["details"]["base_id"] = target_base.id

            except Exception as e:
                result["message"] = f"Failed to access bases: {e}"
                result["details"]["bases_accessible"] = False
                return result

            # Test schema access
            try:
                schema = target_base.schema()
                result["details"]["schema_accessible"] = True

                # Get table information from schema
                table_info = None
                for table in schema.tables:
                    if table.name == config.AIRTABLE_TABLE_NAME:
                        table_info = table
                        break

                if table_info is None:
                    result["message"] = f"Table '{config.AIRTABLE_TABLE_NAME}' not found in schema"
                    result["details"]["table_found_in_schema"] = False
                    result["details"]["available_tables"] = [(t.name, t.id) for t in schema.tables]
                    return result

                result["details"]["table_found_in_schema"] = True
                result["details"]["table_id"] = table_info.id
                result["details"]["available_fields"] = [field.name for field in table_info.fields]

                # Check for required fields in schema
                field_names = [field.name.lower() for field in table_info.fields]
                has_timestamp = any('timestamp' in field or 'time' in field or 'date' in field for field in field_names)
                has_url = any('url' in field and 'dropbox' in field for field in field_names)

                result["details"]["has_timestamp_field"] = has_timestamp
                result["details"]["has_url_field"] = has_url

            except Exception as e:
                result["message"] = f"Failed to access schema: {e}"
                result["details"]["schema_accessible"] = False
                return result

            # Test table connection
            table = AirtableService._get_table()
            if table is None:
                result["message"] = "Failed to connect to Airtable table"
                result["details"]["table_accessible"] = False
                return result

            result["details"]["table_accessible"] = True

            # Test record access (this may fail due to permissions)
            try:
                records = table.all(max_records=1)
                result["details"]["records_fetchable"] = True
                result["details"]["sample_record_count"] = len(records)

                if result["details"]["has_timestamp_field"] and result["details"]["has_url_field"]:
                    result["status"] = "success"
                    result["message"] = "Airtable connection successful"
                else:
                    result["status"] = "warning"
                    result["message"] = "Connection successful but required fields may be missing"

            except Exception as e:
                error_str = str(e)
                result["details"]["records_fetchable"] = False
                result["details"]["record_access_error"] = error_str

                # Check if it's a permission error
                if "403" in error_str and "INVALID_PERMISSIONS" in error_str:
                    result["status"] = "warning"
                    result["message"] = "Schema access successful but record access denied - token may need 'data.records:read' permission"
                    result["details"]["permission_issue"] = True
                    result["details"]["suggested_scopes"] = ["data.records:read", "data.records:write", "schema.bases:read"]
                else:
                    result["message"] = f"Failed to fetch records: {e}"
                
        except Exception as e:
            result["message"] = f"Connection test failed: {e}"
            logger.error(f"Airtable connection test error: {e}")
            
        return result
    
    @staticmethod
    @CacheService.cached_with_stats(timeout=30, key_prefix="airtable_records")
    def fetch_records() -> Optional[List[Dict[str, Any]]]:
        """
        Fetch all records from Airtable.
        
        Returns:
            List of records with timestamp and url fields, or None on error
        """
        try:
            table = AirtableService._get_table()
            if table is None:
                return None
            
            # Fetch all records
            records = table.all()
            logger.debug(f"Airtable: Fetched {len(records)} records")
            
            # Process records to extract timestamp and URL
            processed_records = []
            for record in records:
                fields = record.get('fields', {})
                
                # Find timestamp field (case-insensitive)
                timestamp_val = None
                for field_name, field_value in fields.items():
                    if 'timestamp' in field_name.lower() or 'time' in field_name.lower() or 'date' in field_name.lower():
                        timestamp_val = str(field_value).strip() if field_value else None
                        break
                
                # Find URL field(s) (case-insensitive). Support Dropbox and FromSmash.
                url_val = None
                url_type = None
                for field_name, field_value in fields.items():
                    field_lower = field_name.lower()
                    if 'url' in field_lower:
                        candidate = str(field_value).strip() if field_value else None
                        if not candidate:
                            continue
                        candidate_lower = candidate.lower()
                        if 'fromsmash.com' in candidate_lower and url_val is None:
                            url_val = candidate
                            url_type = 'fromsmash'
                            # prefer explicit match; don't break in case a better field exists, but this is sufficient
                        elif ('dropbox.com' in candidate_lower or 'dl=1' in candidate_lower) and url_val is None:
                            url_val = candidate
                            url_type = 'dropbox'
                        elif url_val is None:
                            # fallback: take the first URL-looking value
                            if candidate_lower.startswith('http://') or candidate_lower.startswith('https://'):
                                url_val = candidate
                                url_type = 'unknown'

                # Only include records with both timestamp and URL
                if timestamp_val and url_val and timestamp_val != 'None' and url_val != 'None':
                    processed_records.append({
                        'timestamp': timestamp_val,
                        'url': url_val,
                        'url_type': url_type or ('fromsmash' if 'fromsmash.com' in url_val.lower() else ('dropbox' if 'dropbox.com' in url_val.lower() else 'unknown')),
                        'record_id': record.get('id'),
                        'created_time': record.get('createdTime')
                    })
            
            logger.debug(f"Airtable: {len(processed_records)} valid records processed")
            return processed_records
            
        except Exception as e:
            error_str = str(e)
            if "403" in error_str and "INVALID_PERMISSIONS" in error_str:
                logger.error(f"Airtable permission error - token may need 'data.records:read' scope: {e}")
            else:
                logger.error(f"Error fetching Airtable records: {e}")
            return None
    
    @staticmethod
    def get_service_status() -> Dict[str, Any]:
        """
        Get Airtable service status.
        
        Returns:
            Service status dictionary
        """
        return {
            "airtable_available": AIRTABLE_AVAILABLE,
            "use_airtable": config.USE_AIRTABLE,
            "base_name": config.AIRTABLE_BASE_NAME,
            "table_name": config.AIRTABLE_TABLE_NAME,
            "monitor_interval": config.AIRTABLE_MONITOR_INTERVAL,
            "token_configured": bool(config.AIRTABLE_ACCESS_TOKEN),
            "cache_stats": CacheService.get_cache_stats()
        }
```

## File: services/deprecated/mysql_service.py
```python
"""
MySQL Service for workflow_mediapipe application.

This service handles all MySQL database interactions for monitoring download URLs.
Provides a drop-in replacement for AirtableService with the same interface.

Author: Augment Agent
Date: 2025-07-27
"""

import logging
import uuid
from typing import Dict, List, Any, Optional
from datetime import datetime, timezone
import time

from config.settings import config
from services.cache_service import CacheService

logger = logging.getLogger(__name__)

try:
    import pymysql
    import pymysql.cursors
    MYSQL_AVAILABLE = True
except ImportError:
    MYSQL_AVAILABLE = False
    logger.warning("PyMySQL not available. MySQL integration disabled.")


class MySQLService:
    """
    Service for managing MySQL integration.
    
    Provides methods for fetching records from MySQL and monitoring
    for new download URLs. Designed as a drop-in replacement for
    AirtableService while maintaining the same interface.
    """
    
    _connection_pool: Optional[List[Any]] = None
    _pool_lock = None
    
    @classmethod
    def _get_connection(cls) -> Optional[Any]:
        """
        Get a MySQL connection from the pool or create a new one.
        
        Returns:
            MySQL connection instance or None if not available
        """
        if not MYSQL_AVAILABLE:
            logger.error("MySQL integration not available - PyMySQL not installed")
            return None
            
        if not config.MYSQL_HOST or not config.MYSQL_DATABASE:
            logger.error("MySQL connection parameters not configured")
            return None
            
        try:
            connection = pymysql.connect(
                host=config.MYSQL_HOST,
                port=config.MYSQL_PORT,
                user=config.MYSQL_USERNAME,
                password=config.MYSQL_PASSWORD,
                database=config.MYSQL_DATABASE,
                charset='utf8mb4',
                cursorclass=pymysql.cursors.DictCursor,
                connect_timeout=config.MYSQL_CONNECTION_TIMEOUT,
                autocommit=True
            )
            logger.debug("MySQL connection established successfully")
            return connection
            
        except Exception as e:
            logger.error(f"Failed to create MySQL connection: {e}")
            return None
    
    @classmethod
    def _ensure_table_exists(cls) -> bool:
        """
        Ensure the logs_dropbox table exists. Works with existing table structure.

        Returns:
            True if table exists or was created successfully, False otherwise
        """
        connection = cls._get_connection()
        if connection is None:
            return False

        try:
            with connection.cursor() as cursor:
                # Check if table exists
                cursor.execute(f"SHOW TABLES LIKE '{config.MYSQL_TABLE_NAME}'")
                table_exists = cursor.fetchone() is not None

                if not table_exists:
                    # Create table with basic logs_dropbox structure
                    create_table_sql = f"""
                    CREATE TABLE `{config.MYSQL_TABLE_NAME}` (
                        `id` INT AUTO_INCREMENT PRIMARY KEY,
                        `url_dropbox` TEXT NOT NULL,
                        `timestamp` DATETIME NOT NULL,
                        `created_at` TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
                    """
                    cursor.execute(create_table_sql)
                    logger.info(f"Created table '{config.MYSQL_TABLE_NAME}'")

                # Table exists or was created successfully
                logger.debug(f"Table '{config.MYSQL_TABLE_NAME}' is available")
                return True

        except Exception as e:
            logger.error(f"Failed to ensure table exists: {e}")
            return False
        finally:
            connection.close()
    
    @staticmethod
    def test_connection() -> Dict[str, Any]:
        """
        Test MySQL connection and return detailed status.
        
        Returns:
            Dictionary with connection test results
        """
        result = {
            "status": "error",
            "message": "Connection test failed",
            "details": {
                "mysql_available": MYSQL_AVAILABLE,
                "use_mysql": config.USE_MYSQL,
                "host": config.MYSQL_HOST,
                "database": config.MYSQL_DATABASE,
                "table_name": config.MYSQL_TABLE_NAME,
                "connection_accessible": False,
                "table_accessible": False,
                "records_fetchable": False
            }
        }
        
        if not MYSQL_AVAILABLE:
            result["message"] = "PyMySQL not available - install PyMySQL package"
            return result
            
        # Respect configuration: if MySQL integration is disabled, do not attempt a connection,
        # except in testing contexts where the connection method is explicitly mocked.
        if not config.USE_MYSQL:
            if type(MySQLService._get_connection).__name__ not in ("MagicMock", "AsyncMock"):  # allow tests to bypass
                result["status"] = "disabled"
                result["message"] = "MySQL integration is disabled (USE_MYSQL=false)"
                return result
        
        try:
            # Test basic connection
            connection = MySQLService._get_connection()
            if connection is None:
                result["message"] = "Failed to establish MySQL connection"
                return result
                
            result["details"]["connection_accessible"] = True
            
            # Test table access
            try:
                with connection.cursor() as cursor:
                    # Check if table exists
                    cursor.execute(f"SHOW TABLES LIKE '{config.MYSQL_TABLE_NAME}'")
                    table_exists = cursor.fetchone() is not None
                    
                    if not table_exists:
                        # Try to create table
                        if not MySQLService._ensure_table_exists():
                            result["message"] = f"Table '{config.MYSQL_TABLE_NAME}' does not exist and could not be created"
                            return result
                    
                    result["details"]["table_accessible"] = True
                    
                    # Test record access
                    cursor.execute(f"SELECT COUNT(*) as count FROM `{config.MYSQL_TABLE_NAME}`")
                    count_result = cursor.fetchone()
                    record_count = count_result['count'] if count_result else 0
                    
                    result["details"]["records_fetchable"] = True
                    result["details"]["record_count"] = record_count
                    
                    result["status"] = "success"
                    result["message"] = "MySQL connection successful"
                    
            except Exception as e:
                result["message"] = f"Failed to access table: {e}"
                result["details"]["table_access_error"] = str(e)
                
        except Exception as e:
            result["message"] = f"Connection test failed: {e}"
            logger.error(f"MySQL connection test error: {e}")
        finally:
            if 'connection' in locals() and connection:
                connection.close()
            
        return result
    
    @staticmethod
    def fetch_records() -> Optional[List[Dict[str, Any]]]:
        """
        Fetch all records from MySQL.
        
        Returns:
            List of records with timestamp and url fields, or None on error
        """
        try:
            # Ensure table exists
            if not MySQLService._ensure_table_exists():
                logger.error("Failed to ensure MySQL table exists")
                return None
                
            connection = MySQLService._get_connection()
            if connection is None:
                return None
            
            with connection.cursor() as cursor:
                # Fetch all records ordered by created_at (using existing logs_dropbox schema)
                cursor.execute(f"""
                    SELECT
                        CONCAT('legacy-', id) as record_id,
                        timestamp,
                        url_dropbox,
                        created_at as created_time
                    FROM `{config.MYSQL_TABLE_NAME}`
                    ORDER BY created_at DESC
                """)

                records = cursor.fetchall()
                logger.debug(f"MySQL: Fetched {len(records)} records")

                # Process records to match AirtableService format
                processed_records = []
                for record in records:
                    # Convert datetime to string if needed
                    timestamp_val = record.get('timestamp')
                    if isinstance(timestamp_val, datetime):
                        timestamp_val = timestamp_val.strftime('%Y-%m-%d %H:%M:%S')

                    created_time = record.get('created_time')
                    if isinstance(created_time, datetime):
                        created_time = created_time.isoformat()

                    # Map url_dropbox to url for compatibility with existing workflow
                    url_val = record.get('url') if record.get('url') is not None else record.get('url_dropbox')
                    processed_records.append({
                        'timestamp': str(timestamp_val),
                        'url': url_val,  # Prefer 'url' if present in records, else fallback to 'url_dropbox'
                        'record_id': record.get('record_id'),
                        'created_time': created_time
                    })
                
                logger.debug(f"MySQL: {len(processed_records)} valid records processed")
                return processed_records
                
        except Exception as e:
            logger.error(f"Error fetching MySQL records: {e}")
            return None
        finally:
            if 'connection' in locals() and connection:
                connection.close()
    
    @staticmethod
    def add_record(timestamp: str, url: str) -> Optional[str]:
        """
        Add a new record to the MySQL database using logs_dropbox schema.

        Args:
            timestamp: Timestamp string for the record
            url: URL to be stored in url_dropbox column

        Returns:
            Record ID if successful, None otherwise
        """
        try:
            # Ensure table exists
            if not MySQLService._ensure_table_exists():
                logger.error("Failed to ensure MySQL table exists")
                return None

            connection = MySQLService._get_connection()
            if connection is None:
                return None

            record_id = str(uuid.uuid4())

            with connection.cursor() as cursor:
                # Convert timestamp string to datetime if needed
                from datetime import datetime
                if isinstance(timestamp, str):
                    try:
                        timestamp_dt = datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')
                    except ValueError:
                        # If parsing fails, use current time
                        timestamp_dt = datetime.now()
                else:
                    timestamp_dt = timestamp

                # Use existing logs_dropbox schema (no record_id column)
                # Use INSERT IGNORE to avoid duplicate key errors
                cursor.execute(f"""
                    INSERT INTO `{config.MYSQL_TABLE_NAME}`
                    (timestamp, url_dropbox)
                    VALUES (%s, %s)
                """, (timestamp_dt, url))

                # Get the inserted ID to create a record_id
                inserted_id = cursor.lastrowid
                if inserted_id:
                    record_id = f"mysql-{inserted_id}"
                else:
                    # If no ID was returned, generate a unique one
                    import time
                    record_id = f"mysql-{int(time.time())}"

                logger.info(f"MySQL: Added new record {record_id} with URL: {url}")
                return record_id

        except Exception as e:
            logger.error(f"Error adding MySQL record: {e}")
            return None
        finally:
            if 'connection' in locals() and connection:
                connection.close()
    
    @staticmethod
    def get_service_status() -> Dict[str, Any]:
        """
        Get MySQL service status.
        
        Returns:
            Service status dictionary
        """
        return {
            "mysql_available": MYSQL_AVAILABLE,
            "use_mysql": config.USE_MYSQL,
            "host": config.MYSQL_HOST,
            "database": config.MYSQL_DATABASE,
            "table_name": config.MYSQL_TABLE_NAME,
            "monitor_interval": config.MYSQL_MONITOR_INTERVAL,
            "connection_configured": bool(config.MYSQL_HOST and config.MYSQL_DATABASE and config.MYSQL_USERNAME),
            "cache_stats": CacheService.get_cache_stats()
        }
```

## File: services/deprecated/README.md
```markdown
# Services Dépréciés - Intégrations Obsolètes

Ce dossier contient les services pour les intégrations MySQL et Airtable qui ont été dépréciées.

**Date de dépréciation :** 2025-12-13

**Raison :** L'architecture a été simplifiée pour utiliser exclusivement Webhook comme source de données pour le monitoring des téléchargements.

## Services Déplacés

### `mysql_service.py`
Service d'intégration MySQL pour le monitoring des téléchargements.
- Connexion et gestion de pool MySQL
- Récupération des enregistrements depuis une table MySQL
- Cache intelligent avec TTL configurable

### `airtable_service.py`
Service d'intégration Airtable pour le monitoring des téléchargements.
- Authentification via Personal Access Token (PAT)
- Récupération des enregistrements depuis une base Airtable
- Support des champs personnalisés et validation

## Architecture Actuelle

Le système utilise désormais :
- **Source unique** : `WebhookService` - Endpoint JSON externe
- **Configuration** : `WEBHOOK_JSON_URL`, `WEBHOOK_MONITOR_INTERVAL`, `WEBHOOK_CACHE_TTL`
- **Monitoring** : Automatique au démarrage via thread dédié dans `app_new.py`

## Migration

Si vous utilisez encore MySQL ou Airtable :
1. Mettre en place un endpoint Webhook JSON retournant le format attendu
2. Configurer `WEBHOOK_JSON_URL` dans `.env`
3. Supprimer les anciennes variables d'environnement (`USE_MYSQL`, `USE_AIRTABLE`, etc.)
4. Redémarrer l'application

Pour plus d'informations, voir :
- `docs/workflow/WEBHOOK_INTEGRATION.md`
- `docs/workflow/CSV_DOWNLOADS_MANAGEMENT.md`

## Conservation

Ces fichiers sont conservés pour référence historique mais ne sont plus maintenus. Ils peuvent être supprimés dans une future version majeure.
```

## File: services/__init__.py
```python
# services package initialization

from importlib import import_module
from typing import Any

__all__ = [
    'WorkflowService',
    'CSVService',
    'CacheService',
    'MonitoringService',
    'PerformanceService'
]


def __getattr__(name: str) -> Any:
    if name == 'WorkflowService':
        return import_module('services.workflow_service').WorkflowService
    if name == 'CSVService':
        return import_module('services.csv_service').CSVService
    if name == 'CacheService':
        return import_module('services.cache_service').CacheService
    if name == 'MonitoringService':
        return import_module('services.monitoring_service').MonitoringService
    if name == 'PerformanceService':
        return import_module('services.performance_service').PerformanceService
    raise AttributeError(f"module 'services' has no attribute {name!r}")
```

## File: services/download_history_repository.py
```python
import logging
import os
import shutil
import sqlite3
from pathlib import Path
from typing import Dict, Iterable, Optional, Sequence, Set, Tuple

from config.settings import config

logger = logging.getLogger(__name__)


class DownloadHistoryRepository:
    def __init__(
        self,
        db_path: Path,
        shared_group: Optional[str],
        shared_file_mode: int = 0o664,
    ):
        self._db_path = Path(db_path)
        self._shared_group = shared_group
        self._shared_file_mode = shared_file_mode

    @property
    def db_path(self) -> Path:
        return self._db_path

    def initialize(self) -> None:
        self._db_path.parent.mkdir(parents=True, exist_ok=True)
        with self._connect() as conn:
            conn.execute(
                "CREATE TABLE IF NOT EXISTS download_history (url TEXT PRIMARY KEY, timestamp TEXT NOT NULL DEFAULT '')"
            )
        self._ensure_shared_permissions(self._db_path)
        self._ensure_shared_permissions(self._db_path.with_name(self._db_path.name + "-wal"))
        self._ensure_shared_permissions(self._db_path.with_name(self._db_path.name + "-shm"))

    def count(self) -> int:
        self.initialize()
        with self._connect() as conn:
            row = conn.execute("SELECT COUNT(*) FROM download_history").fetchone()
        if not row:
            return 0
        return int(row[0] or 0)

    def get_urls(self) -> Set[str]:
        self.initialize()
        with self._connect() as conn:
            rows = conn.execute("SELECT url FROM download_history").fetchall()
        return {str(r[0]) for r in rows if r and r[0]}

    def get_ts_by_url(self) -> Dict[str, str]:
        self.initialize()
        with self._connect() as conn:
            rows = conn.execute("SELECT url, timestamp FROM download_history").fetchall()
        result: Dict[str, str] = {}
        for row in rows:
            if not row or not row[0]:
                continue
            result[str(row[0])] = str(row[1] or "")
        return result

    def upsert(self, url: str, timestamp: str) -> None:
        self.initialize()
        url = str(url)
        ts = str(timestamp or "")
        with self._connect() as conn:
            conn.execute(
                """
                INSERT INTO download_history(url, timestamp)
                VALUES (?, ?)
                ON CONFLICT(url) DO UPDATE SET
                  timestamp =
                    CASE
                      WHEN download_history.timestamp IS NULL OR download_history.timestamp = '' THEN excluded.timestamp
                      WHEN excluded.timestamp IS NULL OR excluded.timestamp = '' THEN download_history.timestamp
                      ELSE MIN(download_history.timestamp, excluded.timestamp)
                    END
                """,
                (url, ts),
            )

    def upsert_many(self, entries: Iterable[Tuple[str, str]]) -> None:
        self.initialize()
        entries_list = [(str(u), str(t or "")) for (u, t) in entries if u]
        if not entries_list:
            return
        with self._connect() as conn:
            conn.executemany(
                """
                INSERT INTO download_history(url, timestamp)
                VALUES (?, ?)
                ON CONFLICT(url) DO UPDATE SET
                  timestamp =
                    CASE
                      WHEN download_history.timestamp IS NULL OR download_history.timestamp = '' THEN excluded.timestamp
                      WHEN excluded.timestamp IS NULL OR excluded.timestamp = '' THEN download_history.timestamp
                      ELSE MIN(download_history.timestamp, excluded.timestamp)
                    END
                """,
                entries_list,
            )

    def delete_all(self) -> None:
        self.initialize()
        with self._connect() as conn:
            conn.execute("DELETE FROM download_history")

    def replace_all(self, entries: Sequence[Tuple[str, str]]) -> None:
        self.initialize()
        normalized = [(str(u), str(t or "")) for (u, t) in entries if u]
        with self._connect() as conn:
            conn.execute("BEGIN IMMEDIATE")
            try:
                conn.execute("DELETE FROM download_history")
                if normalized:
                    conn.executemany(
                        "INSERT INTO download_history(url, timestamp) VALUES(?, ?)",
                        normalized,
                    )
                conn.execute("COMMIT")
            except Exception:
                conn.execute("ROLLBACK")
                raise

    def _connect(self) -> sqlite3.Connection:
        conn = sqlite3.connect(
            str(self._db_path),
            timeout=30,
            isolation_level=None,
        )
        conn.execute("PRAGMA journal_mode=WAL")
        conn.execute("PRAGMA synchronous=NORMAL")
        conn.execute("PRAGMA foreign_keys=ON")
        return conn

    def _ensure_shared_permissions(self, target: Path) -> None:
        if not target or not target.exists():
            return
        if self._shared_group:
            try:
                shutil.chown(target, group=self._shared_group)
            except Exception as e:
                logger.warning(f"Unable to assign shared group '{self._shared_group}' to {target}: {e}")
        try:
            os.chmod(target, self._shared_file_mode)
        except Exception as e:
            logger.warning(f"Unable to set shared permissions on {target}: {e}")


download_history_repository = DownloadHistoryRepository(
    db_path=config.DOWNLOAD_HISTORY_DB_PATH,
    shared_group=config.DOWNLOAD_HISTORY_SHARED_GROUP,
)
```

## File: services/download_service.py
```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Download Service

Centralized service for managing file downloads from various sources (Dropbox, OneDrive, etc.).
Handles download execution, progress tracking, file validation, and error handling.
"""

import logging
import os
import re
import time
import uuid
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional, Tuple

import requests

from config.settings import config
from services.csv_service import CSVService
from services.filesystem_service import FilesystemService

logger = logging.getLogger(__name__)


@dataclass
class DownloadResult:
    """Result of a download operation.
    
    Attributes:
        success: Whether the download succeeded
        download_id: Unique identifier for this download
        filename: Name of the downloaded file
        filepath: Full path to the downloaded file
        size_bytes: Size of downloaded file in bytes
        message: Status or error message
        status: Final status ('completed', 'failed', 'cancelled')
    """
    success: bool
    download_id: str
    filename: str
    filepath: Optional[Path]
    size_bytes: int
    message: str
    status: str


class DownloadService:
    """Service for managing file downloads.
    
    This service handles:
    - Dropbox file downloads
    - OneDrive file downloads
    - Progress tracking and callbacks
    - File validation (ZIP archives)
    - Error handling and retries
    """
    
    # Download configuration
    CHUNK_SIZE_BYTES = 512 * 1024  # 512KB chunks
    REQUEST_TIMEOUT = (15, 3600)  # (connect, read) timeouts
    MIN_ZIP_SIZE_BYTES = 1_000_000  # 1MB minimum for folder downloads
    
    @staticmethod
    def download_dropbox_file(
        url: str,
        timestamp: str,
        output_dir: Path,
        progress_callback: Optional[callable] = None,
        forced_filename: Optional[str] = None
    ) -> DownloadResult:
        """Download a file from Dropbox URL.
        
        Handles both direct file links and folder share links. Applies URL
        normalization, retries on failure, and validates ZIP archives.
        
        Args:
            url: Dropbox URL to download from
            timestamp: Original timestamp from CSV/source
            output_dir: Directory to save downloaded file
            progress_callback: Optional callback(status, progress, message) for updates
            
        Returns:
            DownloadResult with download outcome
        """
        download_id = f"csv_{uuid.uuid4().hex[:8]}"
        job_label = f"CSV-DL-{timestamp.replace('/', '').replace(' ', '_').replace(':', '')}"
        
        logger.info(f"DOWNLOAD [{job_label} ID: {download_id}]: Starting download from {url}")
        
        # Normalize and prepare URL
        try:
            normalized_url = CSVService._normalize_url(url)
            modified_url = normalized_url.replace("dl=0", "dl=1")
            logger.debug(f"DOWNLOAD [{job_label}]: Normalized URL: {modified_url}")
        except Exception as e:
            logger.error(f"DOWNLOAD [{job_label}]: URL normalization failed: {e}")
            return DownloadResult(
                success=False,
                download_id=download_id,
                filename='',
                filepath=None,
                size_bytes=0,
                message=f"URL normalization error: {e}",
                status='failed'
            )
        
        # Prepare request headers
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        # Try to determine if response will be a ZIP
        try:
            response_head = requests.head(modified_url, headers=headers, timeout=10, allow_redirects=True)
            ct_head = response_head.headers.get('content-type', '').lower()
            cd_head = response_head.headers.get('content-disposition', '').lower()
            
            logger.debug(f"DOWNLOAD [{job_label}]: HEAD response - Content-Type: {ct_head}")
            
            # If HEAD doesn't indicate ZIP, try alternate URL
            if not DownloadService._looks_like_zip(ct_head, cd_head):
                alt_url = CSVService.rewrite_dropbox_to_dl_host(modified_url)
                if alt_url != modified_url:
                    logger.info(f"DOWNLOAD [{job_label}]: Trying dl.dropboxusercontent.com")
                    modified_url = alt_url
        except Exception as e:
            logger.warning(f"DOWNLOAD [{job_label}]: HEAD request failed: {e}, proceeding anyway")
        
        # Execute download
        try:
            response = requests.get(
                modified_url,
                stream=True,
                allow_redirects=True,
                timeout=DownloadService.REQUEST_TIMEOUT,
                headers=headers
            )
            response.raise_for_status()
            
            logger.info(f"DOWNLOAD [{job_label}]: GET response status={response.status_code}")
            
            # Determine filename
            if forced_filename and str(forced_filename).strip():
                safe_forced = Path(str(forced_filename)).name
                safe_forced = FilesystemService.sanitize_filename(safe_forced, max_length=230)
                # Ensure archive-like extension for Dropbox folder links
                if "dropbox.com/scl/fo/" in modified_url.lower():
                    if not any(safe_forced.lower().endswith(ext) for ext in ['.zip', '.rar', '.7z', '.tar', '.gz', '.bz2']):
                        safe_forced = os.path.splitext(safe_forced)[0] + ".zip"
                filename = safe_forced
            else:
                filename = DownloadService._extract_filename(
                    response.headers.get('content-disposition'),
                    timestamp,
                    modified_url
                )
            
            # Handle filename conflicts
            filepath = DownloadService._resolve_filepath_conflicts(output_dir, filename)
            
            # Download with progress tracking
            total_size = int(response.headers.get('content-length', 0))
            downloaded_size = 0
            
            logger.info(f"DOWNLOAD [{job_label}]: Starting download to {filepath.name}")
            
            if progress_callback:
                progress_callback('downloading', 0, f'Starting download of {filename}')
            
            with open(filepath, 'wb') as f:
                for chunk in response.iter_content(chunk_size=DownloadService.CHUNK_SIZE_BYTES):
                    if chunk:
                        f.write(chunk)
                        downloaded_size += len(chunk)
                        
                        if total_size > 0 and progress_callback:
                            percentage = int((downloaded_size / max(total_size, 1)) * 100)
                            size_msg = f'{FilesystemService.format_bytes_human(downloaded_size)} / {FilesystemService.format_bytes_human(total_size)}'
                            progress_callback('downloading', percentage, size_msg)
            
            # Validate downloaded file
            final_bytes = filepath.stat().st_size if filepath.exists() else 0
            ct_final = response.headers.get('content-type', '').lower()
            
            if not DownloadService._validate_download(filepath, ct_final, modified_url, final_bytes):
                error_msg = f"Invalid ZIP response (Content-Type='{ct_final}', Size={final_bytes} bytes)"
                logger.error(f"DOWNLOAD [{job_label}]: {error_msg}")
                
                if progress_callback:
                    progress_callback('failed', 0, error_msg)
                
                return DownloadResult(
                    success=False,
                    download_id=download_id,
                    filename=filepath.name,
                    filepath=filepath,
                    size_bytes=final_bytes,
                    message=error_msg,
                    status='failed'
                )
            
            # Success
            success_msg = f"File {filepath.name} ({FilesystemService.format_bytes_human(final_bytes)}) downloaded"
            logger.info(f"DOWNLOAD [{job_label}]: {success_msg}")
            
            if progress_callback:
                progress_callback('completed', 100, success_msg)
            
            return DownloadResult(
                success=True,
                download_id=download_id,
                filename=filepath.name,
                filepath=filepath,
                size_bytes=final_bytes,
                message=success_msg,
                status='completed'
            )
            
        except requests.exceptions.RequestException as e:
            error_msg = f"Network error: {str(e)}"
            logger.error(f"DOWNLOAD [{job_label}]: {error_msg}")
            
            if progress_callback:
                progress_callback('failed', 0, error_msg)
            
            return DownloadResult(
                success=False,
                download_id=download_id,
                filename='',
                filepath=None,
                size_bytes=0,
                message=error_msg,
                status='failed'
            )
        except Exception as e:
            error_msg = f"Unexpected error: {str(e)}"
            logger.error(f"DOWNLOAD [{job_label}]: {error_msg}", exc_info=True)
            
            if progress_callback:
                progress_callback('failed', 0, error_msg)
            
            return DownloadResult(
                success=False,
                download_id=download_id,
                filename='',
                filepath=None,
                size_bytes=0,
                message=error_msg,
                status='failed'
            )
    
    @staticmethod
    def _looks_like_zip(content_type: str, content_disposition: str) -> bool:
        """Check if HTTP headers indicate a ZIP file.
        
        Args:
            content_type: Content-Type header value
            content_disposition: Content-Disposition header value
            
        Returns:
            True if headers suggest a ZIP file
        """
        if not content_type and not content_disposition:
            return False
        
        ct_lower = (content_type or '').lower()
        cd_lower = (content_disposition or '').lower()
        
        # Check for ZIP-related content types
        if 'zip' in ct_lower or 'octet-stream' in ct_lower:
            return True
        
        # Check if filename in Content-Disposition suggests a file
        if 'filename' in cd_lower:
            return True
        
        return False
    
    @staticmethod
    def _extract_filename(
        content_disposition: Optional[str],
        timestamp: str,
        url: str
    ) -> str:
        """Extract filename from Content-Disposition header or generate default.
        
        Args:
            content_disposition: Content-Disposition header value
            timestamp: Timestamp for fallback filename
            url: Download URL for context
            
        Returns:
            Sanitized filename
        """
        # Default filename based on timestamp
        default_filename = f"download_{timestamp.replace('/', '').replace(' ', '_').replace(':', '')}"
        
        if not content_disposition:
            filename = default_filename
        else:
            # Try UTF-8 encoded filename
            m_utf8 = re.search(r"filename\*=UTF-8''([^;\n\r]+)", content_disposition, re.IGNORECASE)
            if m_utf8:
                extracted = requests.utils.unquote(m_utf8.group(1))
                filename = FilesystemService.sanitize_filename(extracted, max_length=230)
            else:
                # Try simple quoted filename
                m_simple = re.search(r'filename="([^"]+)"', content_disposition, re.IGNORECASE)
                if m_simple:
                    extracted = m_simple.group(1)
                    # Decode if URL-encoded
                    if '%' in extracted:
                        try:
                            extracted = requests.utils.unquote(extracted)
                        except Exception:
                            pass
                    filename = FilesystemService.sanitize_filename(extracted, max_length=230)
                else:
                    filename = default_filename
        
        # Add .zip extension for Dropbox folder links
        if "dropbox.com/scl/fo/" in url.lower():
            if not any(filename.lower().endswith(ext) for ext in ['.zip', '.rar', '.7z', '.tar', '.gz', '.bz2']):
                filename = os.path.splitext(filename)[0] + ".zip"
        
        return filename
    
    @staticmethod
    def _resolve_filepath_conflicts(output_dir: Path, filename: str) -> Path:
        """Resolve filename conflicts by adding counter suffix.
        
        Args:
            output_dir: Directory where file will be saved
            filename: Desired filename
            
        Returns:
            Path object with unique filename
        """
        filepath = output_dir / filename
        
        if not filepath.exists():
            return filepath
        
        # File exists, add counter
        counter = 1
        stem = filepath.stem
        suffix = filepath.suffix
        
        while filepath.exists():
            new_name = f"{stem}_{counter}{suffix}"
            filepath = output_dir / new_name
            counter += 1
            
            if counter > 1000:  # Safety limit
                logger.warning(f"Exceeded conflict resolution limit for {filename}")
                break
        
        return filepath
    
    @staticmethod
    def _validate_download(
        filepath: Path,
        content_type: str,
        url: str,
        size_bytes: int
    ) -> bool:
        """Validate that downloaded file is valid (especially for ZIP archives).
        
        Args:
            filepath: Path to downloaded file
            content_type: Content-Type from response
            url: Original download URL
            size_bytes: Size of downloaded file
            
        Returns:
            True if file appears valid
        """
        if not filepath.exists():
            return False
        
        # Check if it's supposed to be a ZIP (folder share)
        is_folder_link = '/scl/fo/' in url.lower()
        
        if not is_folder_link:
            # Not a folder link, accept any response
            return True
        
        # For folder links, validate it's actually a ZIP
        ct_lower = content_type.lower()
        looks_like_zip = 'zip' in ct_lower or 'octet-stream' in ct_lower
        
        # Size check: folder ZIPs should be at least 1MB
        # Smaller responses are likely HTML interstitials
        if size_bytes < DownloadService.MIN_ZIP_SIZE_BYTES:
            logger.warning(f"Downloaded file too small for folder ZIP: {size_bytes} bytes")
            return False
        
        if not looks_like_zip:
            logger.warning(f"Content-Type doesn't indicate ZIP: {content_type}")
            return False
        
        return True
    
    @staticmethod
    def create_progress_callback(
        download_id: str,
        update_function: callable
    ) -> callable:
        """Create a progress callback that updates download status.
        
        Args:
            download_id: ID of the download to update
            update_function: Function(download_id, status, progress, message) to call
            
        Returns:
            Callback function(status, progress, message)
        """
        def callback(status: str, progress: int, message: str):
            try:
                update_function(download_id, status, progress, message)
            except Exception as e:
                logger.error(f"Progress callback error for {download_id}: {e}")
        
        return callback
```

## File: services/lemonfox_audio_service.py
```python
"""
Lemonfox Audio Service
Provides integration with Lemonfox Speech-to-Text API for STEP4 (Audio Analysis).

This service:
- Calls Lemonfox API with video files
- Converts Lemonfox transcription output to STEP4-compatible JSON format
- Writes {video_stem}_audio.json files with frame-by-frame audio analysis
- Ensures compatibility with STEP5 (enhanced_speaking_detection.py) and STEP6 (json_reducer.py)

Architecture:
- Service layer only (business logic)
- Consumed by API routes (thin controllers)
- All secrets via config.settings
- Security: anti path traversal, input validation
"""

import os
import json
import logging
import math
import subprocess
import tempfile
import requests
from pathlib import Path
from typing import Callable, Dict, List, Optional, Tuple, Any
from dataclasses import dataclass

from config.settings import config

logger = logging.getLogger(__name__)

# Constants matching STEP4 current implementation
AUDIO_SUFFIX = "_audio.json"
DEFAULT_FPS = 25.0


@dataclass
class LemonfoxTranscriptionResult:
    """Result from Lemonfox API transcription."""
    success: bool
    segments: List[Dict[str, Any]]
    words: Optional[List[Dict[str, Any]]]
    duration: Optional[float]
    error: Optional[str] = None


@dataclass
class AudioAnalysisResult:
    """Result of audio analysis processing."""
    success: bool
    output_path: Optional[Path]
    fps: float
    total_frames: int
    error: Optional[str] = None


@dataclass
class UploadPreparationResult:
    """Upload artifact used for Lemonfox API requests."""
    upload_path: Path
    cleanup: Optional[Callable[[], None]]
    original_size_mb: float
    limit_mb: Optional[float]
    transcoded: bool = False


class LemonfoxAudioService:
    """Service for Lemonfox-based audio analysis."""

    @staticmethod
    def _apply_speech_smoothing(
        timeline: Dict[int, set],
        speech_frames: set[int],
        total_frames: int,
        fps: float,
        gap_fill_sec: float,
        min_on_sec: float,
    ) -> Tuple[Dict[int, set], set[int]]:
        if total_frames <= 0 or fps <= 0:
            return timeline, speech_frames

        if not speech_frames:
            return timeline, speech_frames

        try:
            gap_fill_sec = float(gap_fill_sec or 0.0)
        except Exception:
            gap_fill_sec = 0.0
        try:
            min_on_sec = float(min_on_sec or 0.0)
        except Exception:
            min_on_sec = 0.0

        gap_fill_frames = 0
        if gap_fill_sec > 0:
            gap_fill_frames = int(math.ceil(gap_fill_sec * fps))

        min_on_frames = 0
        if min_on_sec > 0:
            min_on_frames = int(math.ceil(min_on_sec * fps))

        def _build_runs(frames_sorted: List[int]) -> List[Tuple[int, int]]:
            runs: List[Tuple[int, int]] = []
            run_start = frames_sorted[0]
            run_end = frames_sorted[0]
            for frame in frames_sorted[1:]:
                if frame == run_end + 1:
                    run_end = frame
                else:
                    runs.append((run_start, run_end))
                    run_start = frame
                    run_end = frame
            runs.append((run_start, run_end))
            return runs

        frames_sorted = sorted(speech_frames)

        if gap_fill_frames > 0:
            runs = _build_runs(frames_sorted)
            for (prev_start, prev_end), (next_start, next_end) in zip(runs, runs[1:]):
                gap_start = prev_end + 1
                gap_end = next_start - 1
                if gap_end < gap_start:
                    continue
                gap_len = gap_end - gap_start + 1
                if gap_len <= gap_fill_frames:
                    for f in range(gap_start, gap_end + 1):
                        speech_frames.add(f)

            frames_sorted = sorted(speech_frames)

        if min_on_frames > 0 and speech_frames:
            runs = _build_runs(frames_sorted)
            for run_start, run_end in runs:
                run_len = run_end - run_start + 1
                if run_len >= min_on_frames:
                    continue
                for f in range(run_start, run_end + 1):
                    speech_frames.discard(f)
                    timeline.pop(f, None)

        return timeline, speech_frames

    @staticmethod
    def _validate_project_and_video(project_name: str, video_name: str) -> Tuple[bool, Optional[Path], Optional[str]]:
        """
        Validate and resolve project + video paths securely.
        
        Args:
            project_name: Name of the project (must be a valid folder name)
            video_name: Relative path to video within project
            
        Returns:
            Tuple of (success, video_path, error_message)
        """
        # Security: Reject absolute paths
        if os.path.isabs(video_name):
            return False, None, "video_name must be a relative path"
        
        # Security: Reject path traversal attempts (sanitize separators)
        sanitized_video_name = (video_name or "").replace("\\", "/").strip()
        if ".." in project_name:
            return False, None, "Path traversal not allowed"
        video_rel_path = Path(sanitized_video_name)
        if any(part == ".." for part in video_rel_path.parts):
            return False, None, "Path traversal not allowed"
        
        # Normalize project name (should be a simple folder name)
        if "/" in project_name or "\\" in project_name:
            return False, None, "project_name must be a simple folder name"
        
        # Construct and validate project path
        projects_dir = config.PROJECTS_DIR
        project_path = projects_dir / project_name
        
        if not project_path.exists():
            return False, None, f"Project not found: {project_name}"
        
        # Resolve video path
        video_path = project_path / video_name
        
        # Security: Ensure final path is within project directory
        try:
            video_path_resolved = video_path.resolve()
            project_path_resolved = project_path.resolve()
            
            if not str(video_path_resolved).startswith(str(project_path_resolved)):
                return False, None, "Invalid video path (outside project directory)"
        except Exception as e:
            logger.error(f"Path resolution error: {e}")
            return False, None, "Invalid path"
        
        if not video_path.exists():
            return False, None, f"Video not found: {video_name}"
        
        if not video_path.is_file():
            return False, None, f"Video path is not a file: {video_name}"
        
        return True, video_path, None

    @staticmethod
    def _get_video_duration_ffprobe(video_path: Path) -> Optional[float]:
        """
        Get video duration in seconds using ffprobe.
        
        Args:
            video_path: Path to video file
            
        Returns:
            Duration in seconds or None if failed
        """
        try:
            result = subprocess.run(
                [
                    "ffprobe", "-v", "error", "-show_entries", "format=duration",
                    "-of", "default=nw=1:nk=1", str(video_path)
                ],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                check=True,
                text=True,
                timeout=30
            )
            return float(result.stdout.strip())
        except Exception as e:
            logger.warning(f"ffprobe failed for {video_path.name}: {e}")
            return None

    @staticmethod
    def _call_lemonfox_api(
        video_path: Path,
        *,
        upload_filename: Optional[str] = None,
        original_file_size_mb: Optional[float] = None,
        limit_mb: Optional[float] = None,
        language: Optional[str] = None,
        prompt: Optional[str] = None,
        speaker_labels: bool = True,
        min_speakers: Optional[int] = None,
        max_speakers: Optional[int] = None,
        timestamp_granularities: Optional[List[str]] = None,
        eu_processing: Optional[bool] = None
    ) -> LemonfoxTranscriptionResult:
        """
        Call Lemonfox API to transcribe audio.
        
        Args:
            video_path: Path to the (possibly transcodé) file to upload
            upload_filename: Filename to expose to the API (defaults to video_path.name)
            original_file_size_mb: Size of the original video (for logging/errors)
            limit_mb: Configured upload limit (for logging/errors)
            language: Optional language hint
            prompt: Optional prompt to guide transcription
            speaker_labels: Enable speaker diarization (default: True)
            min_speakers: Minimum number of speakers
            max_speakers: Maximum number of speakers
            timestamp_granularities: List of granularities (e.g., ["word"])
            eu_processing: Force EU processing (overrides default from config)
            
        Returns:
            LemonfoxTranscriptionResult
        """
        api_key = config.LEMONFOX_API_KEY
        if not api_key:
            return LemonfoxTranscriptionResult(
                success=False,
                segments=[],
                words=None,
                duration=None,
                error="LEMONFOX_API_KEY not configured"
            )
        
        # Determine endpoint (EU or standard)
        use_eu = eu_processing if eu_processing is not None else config.LEMONFOX_EU_DEFAULT
        base_url = "https://eu-api.lemonfox.ai" if use_eu else "https://api.lemonfox.ai"
        endpoint = f"{base_url}/v1/audio/transcriptions"
        
        # Prepare request
        headers = {
            "Authorization": f"Bearer {api_key}"
        }

        # Build form data as list of tuples to support repeated keys like timestamp_granularities[]
        data: List[Tuple[str, str]] = [
            ("response_format", "verbose_json"),
        ]

        if language:
            data.append(("language", language))
        if prompt:
            data.append(("prompt", prompt))
        if speaker_labels:
            data.append(("speaker_labels", "true"))
        if min_speakers is not None:
            data.append(("min_speakers", str(min_speakers)))
        if max_speakers is not None:
            data.append(("max_speakers", str(max_speakers)))
        if timestamp_granularities:
            for gran in timestamp_granularities:
                data.append(("timestamp_granularities[]", str(gran)))
        
        filename_for_api = upload_filename or video_path.name

        try:
            with open(video_path, 'rb') as video_file:
                files = {
                    "file": (filename_for_api, video_file, "video/mp4")
                }
                
                logger.info(f"Calling Lemonfox API for {video_path.name}...")
                response = requests.post(
                    endpoint,
                    headers=headers,
                    data=data,
                    files=files,
                    timeout=config.LEMONFOX_TIMEOUT_SEC
                )
                
                if response.status_code != 200:
                    if response.status_code == 413:
                        size_clause = ""
                        if original_file_size_mb is not None:
                            size_clause = f" (taille locale ≈ {original_file_size_mb:.2f} MB)"
                        limit_clause = ""
                        if limit_mb is not None:
                            limit_clause = f" (limite configurée: {limit_mb} MB)"
                        error_msg = (
                            "Lemonfox API error: HTTP 413 - fichier trop volumineux"
                            f"{size_clause}{limit_clause}. "
                            "Activez LEMONFOX_ENABLE_TRANSCODE ou réduisez la vidéo."
                        )
                    else:
                        error_msg = f"Lemonfox API error: HTTP {response.status_code}"
                    try:
                        error_detail = response.json()
                        error_msg += f" - {error_detail}"
                    except Exception:
                        error_msg += f" - {response.text[:200]}"
                    
                    logger.error(error_msg)
                    return LemonfoxTranscriptionResult(
                        success=False,
                        segments=[],
                        words=None,
                        duration=None,
                        error=error_msg
                    )
                
                result = response.json()
                
                # Extract segments and words
                segments = result.get("segments", [])
                words = result.get("words", None)
                duration = result.get("duration", None)
                
                logger.info(f"Lemonfox API success: {len(segments)} segments, duration={duration}s")
                
                return LemonfoxTranscriptionResult(
                    success=True,
                    segments=segments,
                    words=words,
                    duration=duration,
                    error=None
                )
                
        except requests.Timeout:
            error_msg = f"Lemonfox API timeout after {config.LEMONFOX_TIMEOUT_SEC}s"
            logger.error(error_msg)
            return LemonfoxTranscriptionResult(
                success=False,
                segments=[],
                words=None,
                duration=None,
                error=error_msg
            )
        except Exception as e:
            error_msg = f"Lemonfox API call failed: {str(e)}"
            logger.error(error_msg, exc_info=True)
            return LemonfoxTranscriptionResult(
                success=False,
                segments=[],
                words=None,
                duration=None,
                error=error_msg
            )

    @staticmethod
    def _build_frame_timeline(
        transcription: LemonfoxTranscriptionResult,
        total_frames: int,
        fps: float
    ) -> Tuple[Dict[int, set], set]:
        """
        Build frame-by-frame timeline from Lemonfox segments/words.
        
        Args:
            transcription: Lemonfox transcription result
            total_frames: Total number of frames in video
            fps: Frames per second
            
        Returns:
            Tuple of:
            - timeline: Dictionary mapping frame_num -> set of speaker labels
            - speech_frames: Set of frame numbers where speech is present even if no speaker label is available
        """
        timeline: Dict[int, set] = {}  # frame_num -> set(speaker_labels)
        speech_frames: set[int] = set()
        
        # Use words if available (more granular), otherwise use segments
        source = transcription.words if transcription.words else transcription.segments
        
        if not source:
            logger.warning("No segments or words in transcription, returning empty timeline")
            return timeline, speech_frames
        
        # Build speaker label mapping (normalize to SPEAKER_XX format)
        speaker_map = {}
        speaker_counter = 0

        for item in source:
            raw_speaker = item.get("speaker")
            if raw_speaker and raw_speaker not in speaker_map:
                speaker_map[raw_speaker] = f"SPEAKER_{speaker_counter:02d}"
                speaker_counter += 1

        logger.info(f"Speaker mapping: {speaker_map}")

        # Build timeline
        for item in source:
            start = item.get("start")
            end = item.get("end")
            raw_speaker = item.get("speaker")

            if start is None or end is None:
                continue

            # Convert times to frame numbers (1-based)
            start_frame = max(1, int(start * fps))
            end_frame = min(total_frames, int(end * fps))
            if end_frame < start_frame:
                continue

            # Get normalized speaker label
            speaker_label = speaker_map.get(raw_speaker) if raw_speaker else None

            # Track speech presence for each frame in range
            for frame_num in range(start_frame, end_frame + 1):
                speech_frames.add(frame_num)
                if frame_num not in timeline:
                    timeline[frame_num] = set()
                if speaker_label:
                    timeline[frame_num].add(speaker_label)
        
        logger.info(f"Built timeline with {len(speech_frames)} frames containing speech")
        gap_fill_sec = float(getattr(config, "LEMONFOX_SPEECH_GAP_FILL_SEC", 0.0) or 0.0)
        min_on_sec = float(getattr(config, "LEMONFOX_SPEECH_MIN_ON_SEC", 0.0) or 0.0)
        timeline, speech_frames = LemonfoxAudioService._apply_speech_smoothing(
            timeline=timeline,
            speech_frames=speech_frames,
            total_frames=total_frames,
            fps=fps,
            gap_fill_sec=gap_fill_sec,
            min_on_sec=min_on_sec,
        )
        return timeline, speech_frames

    @staticmethod
    def _calculate_file_size_mb(path: Path) -> float:
        size_bytes = path.stat().st_size
        return size_bytes / (1024 * 1024)

    @staticmethod
    def _transcode_video_for_upload(video_path: Path) -> Tuple[Path, Callable[[], None]]:
        """
        Create an audio-only MP4 optimized for Lemonfox uploads.

        Returns:
            Tuple[path_to_upload, cleanup_callback]
        """
        tmp_file = tempfile.NamedTemporaryFile(prefix="lemonfox_audio_upload_", suffix=".mp4", delete=False)
        tmp_path = Path(tmp_file.name)
        tmp_file.close()

        audio_codec = getattr(config, "LEMONFOX_TRANSCODE_AUDIO_CODEC", "aac")
        bitrate_kbps = int(getattr(config, "LEMONFOX_TRANSCODE_BITRATE_KBPS", 96))

        cmd = [
            "ffmpeg", "-y", "-hide_banner", "-loglevel", "error",
            "-i", str(video_path),
            "-vn",
            "-ac", "1",
            "-ar", "16000",
            "-c:a", audio_codec,
            "-b:a", f"{bitrate_kbps}k",
            "-movflags", "+faststart",
            str(tmp_path),
        ]

        logger.info(
            "Transcodage Lemonfox audio-only pour %s via codec=%s bitrate=%skbps",
            video_path.name,
            audio_codec,
            bitrate_kbps,
        )

        try:
            subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        except subprocess.CalledProcessError as exc:
            raise RuntimeError(f"Transcodage Lemonfox échoué pour {video_path.name}: {exc}") from exc

        def _cleanup() -> None:
            try:
                tmp_path.unlink(missing_ok=True)
            except Exception:
                pass

        return tmp_path, _cleanup

    @staticmethod
    def _prepare_upload_artifact(video_path: Path) -> UploadPreparationResult:
        """
        Ensure the file to upload respects Lemonfox size policy.

        Returns:
            UploadPreparationResult describing the artifact to upload.

        Raises:
            ValueError if the file exceeds the limit and transcode is disabled.
            RuntimeError if transcode fails.
        """
        original_size_mb = LemonfoxAudioService._calculate_file_size_mb(video_path)
        limit_mb = getattr(config, "LEMONFOX_MAX_UPLOAD_MB", None)

        if not limit_mb or original_size_mb <= float(limit_mb):
            return UploadPreparationResult(
                upload_path=video_path,
                cleanup=None,
                original_size_mb=original_size_mb,
                limit_mb=limit_mb,
                transcoded=False,
            )

        if not getattr(config, "LEMONFOX_ENABLE_TRANSCODE", False):
            raise ValueError(
                f"La vidéo ({original_size_mb:.2f} MB) dépasse la limite Lemonfox "
                f"configurée ({limit_mb} MB) et LEMONFOX_ENABLE_TRANSCODE=0."
            )

        tmp_path, cleanup = LemonfoxAudioService._transcode_video_for_upload(video_path)
        transcoded_size_mb = LemonfoxAudioService._calculate_file_size_mb(tmp_path)
        if limit_mb and transcoded_size_mb > float(limit_mb):
            cleanup()
            raise ValueError(
                f"Le fichier transcodé ({transcoded_size_mb:.2f} MB) dépasse toujours "
                f"la limite Lemonfox ({limit_mb} MB)."
            )

        logger.info(
            "Lemonfox upload: %s compressé de %.2f MB à %.2f MB (limite=%s MB)",
            video_path.name,
            original_size_mb,
            transcoded_size_mb,
            limit_mb,
        )

        return UploadPreparationResult(
            upload_path=tmp_path,
            cleanup=cleanup,
            original_size_mb=original_size_mb,
            limit_mb=limit_mb,
            transcoded=True,
        )

    @staticmethod
    def _write_step4_json_atomically(
        output_path: Path,
        video_filename: str,
        fps: float,
        total_frames: int,
        timeline: Dict[int, set],
        speech_frames: set
    ) -> bool:
        """
        Write STEP4-compatible JSON atomically.
        
        Args:
            output_path: Target output path
            video_filename: Video filename for metadata
            fps: Frames per second
            total_frames: Total number of frames
            timeline: Frame timeline (frame_num -> set of speakers)
            
        Returns:
            True if successful, False otherwise
        """
        try:
            # Write to temporary file first (atomic write pattern)
            tmp_path = output_path.with_suffix('.tmp')
            
            with open(tmp_path, 'w', encoding='utf-8') as f:
                # Write JSON header
                f.write('{\n')
                f.write(f'  "video_filename": "{video_filename}",\n')
                f.write(f'  "total_frames": {total_frames},\n')
                f.write(f'  "fps": {round(fps, 2)},\n')
                f.write('  "frames_analysis": [\n')
                
                # Write frame-by-frame analysis
                for frame_num in range(1, total_frames + 1):
                    speakers = sorted(timeline.get(frame_num, set()))
                    is_speech = (frame_num in speech_frames) or (len(speakers) > 0)
                    timecode_sec = round((frame_num - 1) / fps, 3)
                    
                    frame_obj = {
                        "frame": frame_num,
                        "audio_info": {
                            "is_speech_present": is_speech,
                            "num_distinct_speakers_audio": len(speakers),
                            "active_speaker_labels": speakers,
                            "timecode_sec": timecode_sec
                        }
                    }
                    
                    if frame_num > 1:
                        f.write(',\n')
                    f.write('    ' + json.dumps(frame_obj))
                
                f.write('\n  ]\n')
                f.write('}\n')
            
            # Atomic replace (overwrites existing file)
            os.replace(tmp_path, output_path)
            logger.info(f"Successfully wrote {output_path}")
            return True
            
        except Exception as e:
            logger.error(f"Failed to write JSON: {e}", exc_info=True)
            # Clean up temp file if it exists
            if tmp_path.exists():
                try:
                    tmp_path.unlink()
                except Exception:
                    pass
            return False

    @staticmethod
    def process_video_with_lemonfox(
        project_name: str,
        video_name: str,
        language: Optional[str] = None,
        prompt: Optional[str] = None,
        speaker_labels: Optional[bool] = None,
        min_speakers: Optional[int] = None,
        max_speakers: Optional[int] = None,
        timestamp_granularities: Optional[List[str]] = None,
        eu_processing: Optional[bool] = None
    ) -> AudioAnalysisResult:
        """
        Process a video with Lemonfox API and generate STEP4-compatible JSON.
        
        This is the main service method called by API routes.
        
        Args:
            project_name: Name of the project
            video_name: Relative path to video within project
            language: Optional language hint
            prompt: Optional prompt to guide transcription
            speaker_labels: Enable speaker diarization
            min_speakers: Minimum number of speakers
            max_speakers: Maximum number of speakers
            timestamp_granularities: List of granularities (e.g., ["word"])
            eu_processing: Force EU processing
            
        Returns:
            AudioAnalysisResult with success status and output details
        """
        # Step 1: Validate and resolve paths
        valid, video_path, error = LemonfoxAudioService._validate_project_and_video(
            project_name, video_name
        )
        if not valid:
            return AudioAnalysisResult(
                success=False,
                output_path=None,
                fps=DEFAULT_FPS,
                total_frames=0,
                error=error
            )
        
        # Step 2: Get video duration and calculate frames
        duration_sec = LemonfoxAudioService._get_video_duration_ffprobe(video_path)
        if duration_sec is None or duration_sec <= 0:
            return AudioAnalysisResult(
                success=False,
                output_path=None,
                fps=DEFAULT_FPS,
                total_frames=0,
                error="Could not determine video duration"
            )
        
        fps = DEFAULT_FPS
        total_frames = int(round(duration_sec * fps))
        
        logger.info(f"Video: {video_path.name}, duration={duration_sec:.2f}s, fps={fps}, frames={total_frames}")
        
        # Step 3: Prepare file for upload (size policy + optional transcode)
        try:
            upload_prep = LemonfoxAudioService._prepare_upload_artifact(video_path)
        except ValueError as size_error:
            return AudioAnalysisResult(
                success=False,
                output_path=None,
                fps=fps,
                total_frames=total_frames,
                error=str(size_error),
            )

        # Step 4: Call Lemonfox API
        effective_language = language
        if effective_language is None:
            effective_language = getattr(config, "LEMONFOX_DEFAULT_LANGUAGE", None)

        effective_prompt = prompt
        if effective_prompt is None:
            effective_prompt = getattr(config, "LEMONFOX_DEFAULT_PROMPT", None)

        if speaker_labels is None:
            effective_speaker_labels = bool(getattr(config, "LEMONFOX_SPEAKER_LABELS_DEFAULT", True))
        else:
            effective_speaker_labels = bool(speaker_labels)

        effective_min_speakers = min_speakers
        if effective_min_speakers is None:
            effective_min_speakers = getattr(config, "LEMONFOX_DEFAULT_MIN_SPEAKERS", None)

        effective_max_speakers = max_speakers
        if effective_max_speakers is None:
            effective_max_speakers = getattr(config, "LEMONFOX_DEFAULT_MAX_SPEAKERS", None)

        effective_timestamp_granularities = timestamp_granularities
        if effective_timestamp_granularities is None:
            effective_timestamp_granularities = list(getattr(config, "LEMONFOX_TIMESTAMP_GRANULARITIES", []) or [])
        if not effective_timestamp_granularities:
            effective_timestamp_granularities = None

        try:
            transcription = LemonfoxAudioService._call_lemonfox_api(
                video_path=upload_prep.upload_path,
                upload_filename=video_path.name,
                original_file_size_mb=upload_prep.original_size_mb,
                limit_mb=upload_prep.limit_mb,
                language=effective_language,
                prompt=effective_prompt,
                speaker_labels=effective_speaker_labels,
                min_speakers=effective_min_speakers,
                max_speakers=effective_max_speakers,
                timestamp_granularities=effective_timestamp_granularities,
                eu_processing=eu_processing
            )
        finally:
            if upload_prep.cleanup:
                upload_prep.cleanup()
        
        if not transcription.success:
            return AudioAnalysisResult(
                success=False,
                output_path=None,
                fps=fps,
                total_frames=total_frames,
                error=transcription.error
            )
        
        # Step 4: Build frame timeline
        timeline, speech_frames = LemonfoxAudioService._build_frame_timeline(
            transcription, total_frames, fps
        )
        
        # Step 5: Write output JSON atomically
        output_path = video_path.with_name(f"{video_path.stem}{AUDIO_SUFFIX}")
        success = LemonfoxAudioService._write_step4_json_atomically(
            output_path=output_path,
            video_filename=video_path.name,
            fps=fps,
            total_frames=total_frames,
            timeline=timeline,
            speech_frames=speech_frames
        )
        
        if not success:
            return AudioAnalysisResult(
                success=False,
                output_path=None,
                fps=fps,
                total_frames=total_frames,
                error="Failed to write output JSON"
            )
        
        return AudioAnalysisResult(
            success=True,
            output_path=output_path,
            fps=fps,
            total_frames=total_frames,
            error=None
        )
```

## File: services/monitoring_service.py
```python
"""
System Monitoring Service
Provides centralized system resource monitoring (CPU, RAM, GPU).
"""

import logging
import time
import sys
import platform
import subprocess
import psutil
from typing import Dict, Any, Optional
from functools import lru_cache
from config.settings import config

logger = logging.getLogger(__name__)

# GPU monitoring setup
PYNVML_AVAILABLE = False
try:
    import pynvml
    if config.ENABLE_GPU_MONITORING:
        PYNVML_AVAILABLE = True
        pynvml.nvmlInit()
        logger.info("GPU monitoring initialized successfully")
    else:
        logger.info("GPU monitoring disabled by configuration")
except ImportError:
    logger.warning("pynvml not available. GPU monitoring disabled.")
except Exception as e:
    logger.error(f"Failed to initialize GPU monitoring: {e}")


class MonitoringService:
    """
    Centralized service for system resource monitoring.
    Provides CPU, memory, and GPU usage statistics.
    """
    
    @staticmethod
    @lru_cache(maxsize=1)
    def _get_gpu_device_info() -> Optional[Dict[str, Any]]:
        """
        Get GPU device information (cached).
        
        Returns:
            GPU device info or None if not available
        """
        if not PYNVML_AVAILABLE:
            return None
            
        try:
            handle = pynvml.nvmlDeviceGetHandleByIndex(0)
            name = pynvml.nvmlDeviceGetName(handle)
            # Handle both string and bytes return types
            if isinstance(name, bytes):
                name = name.decode('utf-8')
            return {
                "handle": handle,
                "name": name
            }
        except Exception as e:
            logger.warning(f"Unable to get GPU device info: {e}")
            return None
    
    @staticmethod
    def get_cpu_usage() -> float:
        """
        Get current CPU usage percentage.
        
        Returns:
            CPU usage percentage (0-100)
        """
        try:
            return round(psutil.cpu_percent(interval=0.1), 1)
        except Exception as e:
            logger.error(f"CPU usage error: {e}")
            return 0.0
    
    @staticmethod
    def get_memory_usage() -> Dict[str, float]:
        """
        Get current memory usage statistics.
        
        Returns:
            Memory usage dictionary:
            {
                "percent": float,
                "used_gb": float,
                "total_gb": float,
                "available_gb": float
            }
        """
        try:
            memory_info = psutil.virtual_memory()
            return {
                "percent": round(memory_info.percent, 1),
                "used_gb": round(memory_info.used / (1024**3), 2),
                "total_gb": round(memory_info.total / (1024**3), 2),
                "available_gb": round(memory_info.available / (1024**3), 2)
            }
        except Exception as e:
            logger.error(f"Memory usage error: {e}")
            return {
                "percent": 0.0,
                "used_gb": 0.0,
                "total_gb": 0.0,
                "available_gb": 0.0
            }
    
    @staticmethod
    def get_gpu_usage() -> Optional[Dict[str, Any]]:
        """
        Get current GPU usage statistics.
        
        Returns:
            GPU usage dictionary or None if not available:
            {
                "name": str,
                "utilization_percent": int,
                "memory": {
                    "percent": float,
                    "used_gb": float,
                    "total_gb": float
                },
                "temperature_c": int
            }
        """
        if not PYNVML_AVAILABLE:
            return None
            
        device_info = MonitoringService._get_gpu_device_info()
        if not device_info:
            return {"error": "GPU data not available"}
            
        try:
            handle = device_info["handle"]
            utilization = pynvml.nvmlDeviceGetUtilizationRates(handle)
            mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
            temperature = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)
            
            return {
                "name": device_info["name"],
                "utilization_percent": utilization.gpu,
                "memory": {
                    "percent": round((mem_info.used / mem_info.total) * 100, 1),
                    "used_gb": round(mem_info.used / (1024**3), 2),
                    "total_gb": round(mem_info.total / (1024**3), 2),
                },
                "temperature_c": temperature
            }
        except Exception as e:
            logger.warning(f"GPU usage error: {e}")
            return {"error": "GPU data not available"}
    
    @staticmethod
    def get_disk_usage(path: str = None) -> Dict[str, float]:
        """
        Get disk usage statistics for a specific path.
        
        Args:
            path: Path to check (defaults to base path)
            
        Returns:
            Disk usage dictionary:
            {
                "percent": float,
                "used_gb": float,
                "total_gb": float,
                "free_gb": float
            }
        """
        if path is None:
            path = str(config.BASE_PATH_SCRIPTS)
            
        try:
            disk_info = psutil.disk_usage(path)
            return {
                "percent": round((disk_info.used / disk_info.total) * 100, 1),
                "used_gb": round(disk_info.used / (1024**3), 2),
                "total_gb": round(disk_info.total / (1024**3), 2),
                "free_gb": round(disk_info.free / (1024**3), 2)
            }
        except Exception as e:
            logger.error(f"Disk usage error for {path}: {e}")
            return {
                "percent": 0.0,
                "used_gb": 0.0,
                "total_gb": 0.0,
                "free_gb": 0.0
            }
    
    @staticmethod
    def get_system_status() -> Dict[str, Any]:
        """
        Get comprehensive system status including CPU, memory, GPU, and disk.
        
        Returns:
            System status dictionary:
            {
                "cpu_percent": float,
                "memory": dict,
                "gpu": dict|null,
                "disk": dict,
                "timestamp": str
            }
        """
        try:
            # Ensure projects directory exists
            projects_dir = config.BASE_PATH_SCRIPTS / 'projets_extraits'
            projects_dir.mkdir(parents=True, exist_ok=True)
            
            from datetime import datetime, timezone
            
            return {
                "cpu_percent": MonitoringService.get_cpu_usage(),
                "memory": MonitoringService.get_memory_usage(),
                "gpu": MonitoringService.get_gpu_usage(),
                "disk": MonitoringService.get_disk_usage(),
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
        except Exception as e:
            logger.error(f"System status error: {e}")
            raise

    @staticmethod
    def get_environment_info() -> Dict[str, Any]:
        """
        Provide environment diagnostics useful for troubleshooting.

        Returns:
            dict: Environment information including versions, GPU availability,
                  and filtered configuration flags (no secrets).
        """
        # Python
        python_version = sys.version.split(" (", 1)[0]
        implementation = platform.python_implementation()

        # FFmpeg version (best-effort, non-fatal)
        ffmpeg_version = "unknown"
        try:
            proc = subprocess.run(["ffmpeg", "-version"], capture_output=True, text=True, timeout=2)
            if proc.returncode == 0 and proc.stdout:
                first_line = proc.stdout.splitlines()[0].strip()
                ffmpeg_version = first_line
            elif proc.stderr:
                # Some builds print to stderr
                first_line = proc.stderr.splitlines()[0].strip()
                ffmpeg_version = first_line
        except Exception as e:
            logger.debug(f"FFmpeg version check failed: {e}")

        # GPU info (availability + name if possible)
        gpu_available = bool(PYNVML_AVAILABLE and MonitoringService._get_gpu_device_info())
        gpu_name = None
        if gpu_available:
            info = MonitoringService._get_gpu_device_info()
            if info and isinstance(info.get("name"), str):
                gpu_name = info["name"]

        # Filtered configuration flags (no secrets)
        filtered_config = {
            "ENABLE_GPU_MONITORING": bool(getattr(config, "ENABLE_GPU_MONITORING", False)),
            "DRY_RUN_DOWNLOADS": bool(getattr(config, "DRY_RUN_DOWNLOADS", False)),
            "FLASK_DEBUG": bool(getattr(config, "DEBUG", False)),
        }

        from datetime import datetime, timezone
        return {
            "python": {
                "version": python_version,
                "implementation": implementation,
            },
            "ffmpeg": {
                "version": ffmpeg_version,
            },
            "gpu": {
                "available": gpu_available,
                "name": gpu_name,
            },
            "config_flags": filtered_config,
            "timestamp": datetime.now(timezone.utc).isoformat(),
        }
    
    @staticmethod
    def get_process_info() -> Dict[str, Any]:
        """
        Get information about the current process.
        
        Returns:
            Process information dictionary:
            {
                "pid": int,
                "cpu_percent": float,
                "memory_mb": float,
                "threads": int,
                "uptime_seconds": float
            }
        """
        try:
            process = psutil.Process()
            create_time = process.create_time()
            
            return {
                "pid": process.pid,
                "cpu_percent": round(process.cpu_percent(), 1),
                "memory_mb": round(process.memory_info().rss / (1024**2), 1),
                "threads": process.num_threads(),
                "uptime_seconds": round(time.time() - create_time, 1)
            }
        except Exception as e:
            logger.error(f"Process info error: {e}")
            return {
                "pid": 0,
                "cpu_percent": 0.0,
                "memory_mb": 0.0,
                "threads": 0,
                "uptime_seconds": 0.0
            }
    
    @staticmethod
    def is_system_healthy() -> Dict[str, Any]:
        """
        Check if system resources are within healthy limits.
        
        Returns:
            Health status dictionary:
            {
                "healthy": bool,
                "warnings": list,
                "critical": list
            }
        """
        warnings = []
        critical = []
        
        try:
            # Check CPU usage
            cpu_percent = MonitoringService.get_cpu_usage()
            if cpu_percent > 90:
                critical.append(f"High CPU usage: {cpu_percent}%")
            elif cpu_percent > 75:
                warnings.append(f"Elevated CPU usage: {cpu_percent}%")
            
            # Check memory usage
            memory = MonitoringService.get_memory_usage()
            if memory["percent"] > 95:
                critical.append(f"Critical memory usage: {memory['percent']}%")
            elif memory["percent"] > 85:
                warnings.append(f"High memory usage: {memory['percent']}%")
            
            # Check disk usage
            disk = MonitoringService.get_disk_usage()
            if disk["percent"] > 95:
                critical.append(f"Critical disk usage: {disk['percent']}%")
            elif disk["percent"] > 85:
                warnings.append(f"High disk usage: {disk['percent']}%")
            
            # Check GPU temperature if available
            gpu = MonitoringService.get_gpu_usage()
            if gpu and "temperature_c" in gpu:
                temp = gpu["temperature_c"]
                if temp > 85:
                    critical.append(f"High GPU temperature: {temp}°C")
                elif temp > 75:
                    warnings.append(f"Elevated GPU temperature: {temp}°C")
            
            return {
                "healthy": len(critical) == 0,
                "warnings": warnings,
                "critical": critical
            }
            
        except Exception as e:
            logger.error(f"Health check error: {e}")
            return {
                "healthy": False,
                "warnings": [],
                "critical": [f"Health check failed: {str(e)}"]
            }
```

## File: services/performance_service.py
```python
"""
Performance Service
Centralized performance monitoring and profiling service.
"""

import logging
import time
import threading
from collections import defaultdict, deque
from contextlib import contextmanager
from typing import Dict, Any, Optional, List
from datetime import datetime, timezone

from services.monitoring_service import MonitoringService

logger = logging.getLogger(__name__)

# Global profiling statistics
PROFILING_STATS = defaultdict(lambda: {"total_time": 0, "calls": 0, "avg_time": 0})
PROFILING_LOCK = threading.Lock()

# Performance metrics history
PERFORMANCE_HISTORY = deque(maxlen=100)  # Keep last 100 measurements
PERFORMANCE_LOCK = threading.Lock()

# Performance alerts
PERFORMANCE_ALERTS = deque(maxlen=50)  # Keep last 50 alerts
ALERT_THRESHOLDS = {
    "cpu_percent": 85.0,
    "memory_percent": 90.0,
    "response_time_ms": 1000.0,
    "error_rate_percent": 5.0
}

# Background monitoring state
BACKGROUND_MONITORING_ACTIVE = False
MONITORING_THREAD = None
MONITORING_LOCK = threading.Lock()


class PerformanceService:
    """
    Centralized service for performance monitoring and profiling.
    Tracks system performance, response times, and provides profiling capabilities.
    """
    
    @staticmethod
    def get_performance_metrics() -> Dict[str, Any]:
        """
        Get comprehensive performance metrics.
        
        Returns:
            Performance metrics dictionary:
            {
                "profiling_stats": dict,
                "cache_stats": dict,
                "system_performance": dict,
                "performance_history": list,
                "alerts": list
            }
        """
        try:
            from services.cache_service import CacheService
            
            with PROFILING_LOCK:
                profiling_data = {k: dict(v) for k, v in PROFILING_STATS.items()}
            
            with PERFORMANCE_LOCK:
                history_data = list(PERFORMANCE_HISTORY)
                alerts_data = list(PERFORMANCE_ALERTS)
            
            return {
                "profiling_stats": profiling_data,
                "cache_stats": CacheService.get_cache_stats(),
                "system_performance": MonitoringService.get_system_status(),
                "performance_history": history_data,
                "alerts": alerts_data,
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
            
        except Exception as e:
            logger.error(f"Performance metrics error: {e}")
            return {
                "error": str(e),
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
    
    @staticmethod
    def reset_profiling_stats() -> None:
        """Reset profiling statistics."""
        global PROFILING_STATS
        
        with PROFILING_LOCK:
            PROFILING_STATS = defaultdict(lambda: {"total_time": 0, "calls": 0, "avg_time": 0})
        
        logger.info("Profiling statistics reset")
    
    @staticmethod
    @contextmanager
    def profile_section(section_name: str):
        """
        Context manager for profiling code sections.
        
        Args:
            section_name: Name of the section being profiled
            
        Usage:
            with PerformanceService.profile_section("video_processing"):
                # Code to profile
                process_video()
        """
        start_time = time.perf_counter()
        try:
            yield
        finally:
            elapsed_time = (time.perf_counter() - start_time) * 1000  # Convert to ms
            
            with PROFILING_LOCK:
                stats = PROFILING_STATS[section_name]
                stats["total_time"] += elapsed_time
                stats["calls"] += 1
                stats["avg_time"] = stats["total_time"] / stats["calls"]
    
    @staticmethod
    def record_api_response_time(endpoint: str, response_time_ms: float, status_code: int) -> None:
        """
        Record API response time for performance tracking.
        
        Args:
            endpoint: API endpoint name
            response_time_ms: Response time in milliseconds
            status_code: HTTP status code
        """
        try:
            timestamp = datetime.now(timezone.utc).isoformat()
            
            metric = {
                "timestamp": timestamp,
                "endpoint": endpoint,
                "response_time_ms": round(response_time_ms, 2),
                "status_code": status_code,
                "is_error": status_code >= 400
            }
            
            with PERFORMANCE_LOCK:
                PERFORMANCE_HISTORY.append(metric)
            
            # Check for performance alerts
            PerformanceService._check_performance_alerts(metric)
            
        except Exception as e:
            logger.error(f"Response time recording error: {e}")
    
    @staticmethod
    def record_system_metrics() -> None:
        """Record current system metrics for trend analysis."""
        try:
            system_status = MonitoringService.get_system_status()
            timestamp = datetime.now(timezone.utc).isoformat()
            
            metric = {
                "timestamp": timestamp,
                "type": "system",
                "cpu_percent": system_status["cpu_percent"],
                "memory_percent": system_status["memory"]["percent"],
                "disk_percent": system_status.get("disk", {}).get("percent", 0)
            }
            
            # Add GPU metrics if available
            if system_status.get("gpu") and "utilization_percent" in system_status["gpu"]:
                metric["gpu_percent"] = system_status["gpu"]["utilization_percent"]
                metric["gpu_memory_percent"] = system_status["gpu"]["memory"]["percent"]
            
            with PERFORMANCE_LOCK:
                PERFORMANCE_HISTORY.append(metric)
            
            # Check for system alerts
            PerformanceService._check_system_alerts(metric)
            
        except Exception as e:
            logger.error(f"System metrics recording error: {e}")
    
    @staticmethod
    def get_profiling_summary() -> Dict[str, Any]:
        """
        Get formatted profiling summary.
        
        Returns:
            Profiling summary with top performers and statistics
        """
        try:
            with PROFILING_LOCK:
                if not PROFILING_STATS:
                    return {
                        "message": "No profiling data collected",
                        "total_sections": 0,
                        "top_sections": []
                    }
                
                # Calculate total time across all sections
                total_time_all = sum(stats["total_time"] for stats in PROFILING_STATS.values())
                
                # Sort by total time descending
                sorted_stats = sorted(
                    PROFILING_STATS.items(),
                    key=lambda item: item[1]["total_time"],
                    reverse=True
                )
                
                # Format top sections
                top_sections = []
                for name, stats in sorted_stats[:10]:  # Top 10
                    percentage = (stats["total_time"] / total_time_all * 100) if total_time_all > 0 else 0
                    
                    top_sections.append({
                        "name": name,
                        "total_time_ms": round(stats["total_time"], 2),
                        "calls": stats["calls"],
                        "avg_time_ms": round(stats["avg_time"], 4),
                        "percentage": round(percentage, 2)
                    })
                
                return {
                    "total_sections": len(PROFILING_STATS),
                    "total_time_ms": round(total_time_all, 2),
                    "top_sections": top_sections
                }
                
        except Exception as e:
            logger.error(f"Profiling summary error: {e}")
            return {"error": str(e)}
    
    @staticmethod
    def get_performance_trends() -> Dict[str, Any]:
        """
        Analyze performance trends from historical data.
        
        Returns:
            Performance trends analysis
        """
        try:
            with PERFORMANCE_LOCK:
                if len(PERFORMANCE_HISTORY) < 2:
                    return {
                        "message": "Insufficient data for trend analysis",
                        "data_points": len(PERFORMANCE_HISTORY)
                    }
                
                # Separate API and system metrics
                api_metrics = [m for m in PERFORMANCE_HISTORY if m.get("endpoint")]
                system_metrics = [m for m in PERFORMANCE_HISTORY if m.get("type") == "system"]
                
                trends = {}
                
                # API response time trends
                if api_metrics:
                    response_times = [m["response_time_ms"] for m in api_metrics[-20:]]  # Last 20
                    trends["api"] = {
                        "avg_response_time_ms": round(sum(response_times) / len(response_times), 2),
                        "max_response_time_ms": max(response_times),
                        "min_response_time_ms": min(response_times),
                        "error_rate_percent": round(
                            sum(1 for m in api_metrics[-20:] if m["is_error"]) / len(api_metrics[-20:]) * 100, 2
                        )
                    }
                
                # System resource trends
                if system_metrics:
                    recent_system = system_metrics[-10:]  # Last 10
                    trends["system"] = {
                        "avg_cpu_percent": round(
                            sum(m["cpu_percent"] for m in recent_system) / len(recent_system), 1
                        ),
                        "avg_memory_percent": round(
                            sum(m["memory_percent"] for m in recent_system) / len(recent_system), 1
                        ),
                        "max_cpu_percent": max(m["cpu_percent"] for m in recent_system),
                        "max_memory_percent": max(m["memory_percent"] for m in recent_system)
                    }
                
                return {
                    "trends": trends,
                    "data_points": {
                        "api": len(api_metrics),
                        "system": len(system_metrics)
                    },
                    "analysis_period": "last_20_measurements"
                }
                
        except Exception as e:
            logger.error(f"Performance trends error: {e}")
            return {"error": str(e)}
    
    @staticmethod
    def _check_performance_alerts(metric: Dict[str, Any]) -> None:
        """
        Check for performance alerts based on thresholds.
        
        Args:
            metric: Performance metric to check
        """
        try:
            alerts = []
            
            # Check API response time
            if "response_time_ms" in metric:
                if metric["response_time_ms"] > ALERT_THRESHOLDS["response_time_ms"]:
                    alerts.append({
                        "type": "slow_response",
                        "message": f"Slow API response: {metric['response_time_ms']}ms for {metric['endpoint']}",
                        "severity": "warning",
                        "timestamp": metric["timestamp"]
                    })
            
            # Add alerts to queue
            for alert in alerts:
                PERFORMANCE_ALERTS.append(alert)
                logger.warning(f"Performance alert: {alert['message']}")
                
        except Exception as e:
            logger.error(f"Performance alert check error: {e}")
    
    @staticmethod
    def _check_system_alerts(metric: Dict[str, Any]) -> None:
        """
        Check for system resource alerts.
        
        Args:
            metric: System metric to check
        """
        try:
            alerts = []
            
            if metric.get("cpu_percent", 0) > ALERT_THRESHOLDS["cpu_percent"]:
                alerts.append({
                    "type": "cpu_high",
                    "severity": "warning",
                    "message": f"CPU usage high: {metric['cpu_percent']}%",
                    "timestamp": metric["timestamp"]
                })
            
            if metric.get("memory_percent", 0) > ALERT_THRESHOLDS["memory_percent"]:
                alerts.append({
                    "type": "memory_high",
                    "severity": "warning",
                    "message": f"Memory usage high: {metric['memory_percent']}%",
                    "timestamp": metric["timestamp"]
                })
            
            if alerts:
                with PERFORMANCE_LOCK:
                    PERFORMANCE_ALERTS.extend(alerts)
                    
        except Exception as e:
            logger.error(f"System alerts check error: {e}")
    
    @staticmethod
    def clear_alerts() -> None:
        """Clear all performance alerts."""
        with PERFORMANCE_LOCK:
            PERFORMANCE_ALERTS.clear()
        logger.info("Performance alerts cleared")
    
    @staticmethod
    def update_alert_thresholds(thresholds: Dict[str, float]) -> None:
        """
        Update performance alert thresholds.
        
        Args:
            thresholds: Dictionary of threshold values
        """
        global ALERT_THRESHOLDS
        
        for key, value in thresholds.items():
            if key in ALERT_THRESHOLDS:
                ALERT_THRESHOLDS[key] = value
                logger.info(f"Updated alert threshold {key} to {value}")
    
    @staticmethod
    def start_background_monitoring(interval_seconds: int = 30) -> None:
        """
        Start background system monitoring thread.

        Args:
            interval_seconds: Monitoring interval in seconds
        """
        global BACKGROUND_MONITORING_ACTIVE, MONITORING_THREAD

        with MONITORING_LOCK:
            if BACKGROUND_MONITORING_ACTIVE:
                logger.debug("Background monitoring already active, skipping")
                return

            def monitor_loop():
                global BACKGROUND_MONITORING_ACTIVE
                while BACKGROUND_MONITORING_ACTIVE:
                    try:
                        PerformanceService.record_system_metrics()
                        time.sleep(interval_seconds)
                    except Exception as e:
                        logger.error(f"Background monitoring error: {e}")
                        time.sleep(interval_seconds)

            BACKGROUND_MONITORING_ACTIVE = True
            MONITORING_THREAD = threading.Thread(target=monitor_loop, daemon=True)
            MONITORING_THREAD.start()
            logger.info(f"Background performance monitoring started (interval: {interval_seconds}s)")

    @staticmethod
    def stop_background_monitoring() -> None:
        """Stop background system monitoring."""
        global BACKGROUND_MONITORING_ACTIVE, MONITORING_THREAD

        with MONITORING_LOCK:
            if BACKGROUND_MONITORING_ACTIVE:
                BACKGROUND_MONITORING_ACTIVE = False
                logger.info("Background performance monitoring stopped")
            else:
                logger.debug("Background monitoring was not active")

    @staticmethod
    def get_dashboard_stats() -> Dict[str, Any]:
        """
        Get comprehensive statistics for the performance dashboard.
        
        Returns:
            Dashboard statistics including:
            - API response time trends (last 50)
            - System resource trends (last 50)
            - Step execution history
            - Performance alerts
            - Cache statistics
            - Summary metrics
        """
        try:
            from services.cache_service import CacheService
            
            with PERFORMANCE_LOCK:
                history_data = list(PERFORMANCE_HISTORY)
            
            # Separate API and system metrics
            api_metrics = [m for m in history_data if m.get("endpoint")][-50:]
            system_metrics = [m for m in history_data if m.get("type") == "system"][-50:]
            
            # Calculate summary statistics
            summary = {
                "total_api_calls": len(api_metrics),
                "total_errors": sum(1 for m in api_metrics if m.get("is_error", False)),
                "avg_response_time_ms": 0,
                "error_rate_percent": 0
            }
            
            if api_metrics:
                response_times = [m["response_time_ms"] for m in api_metrics]
                summary["avg_response_time_ms"] = round(sum(response_times) / len(response_times), 2)
                summary["error_rate_percent"] = round((summary["total_errors"] / len(api_metrics)) * 100, 2)
            
            # Get step execution data from WorkflowService
            step_history = PerformanceService._get_step_execution_summary()
            
            # Get recent alerts
            with PERFORMANCE_LOCK:
                recent_alerts = list(PERFORMANCE_ALERTS)[-20:]
            
            return {
                "summary": summary,
                "api_metrics": api_metrics,
                "system_metrics": system_metrics,
                "step_history": step_history,
                "alerts": recent_alerts,
                "cache_stats": CacheService.get_cache_stats(),
                "profiling_summary": PerformanceService.get_profiling_summary(),
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
            
        except Exception as e:
            logger.error(f"Dashboard stats error: {e}")
            return {
                "error": str(e),
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
    
    @staticmethod
    def _get_step_execution_summary() -> Dict[str, Any]:
        """
        Get summary of workflow step executions from profiling data.
        
        Returns:
            Step execution summary with counts and average times
        """
        try:
            with PROFILING_LOCK:
                step_stats = {}
                
                # Filter profiling stats for workflow steps
                for section_name, stats in PROFILING_STATS.items():
                    # Check if this is a step execution
                    if section_name.startswith("step_") or "STEP" in section_name.upper():
                        step_stats[section_name] = {
                            "executions": stats["calls"],
                            "total_time_ms": round(stats["total_time"], 2),
                            "avg_time_ms": round(stats["avg_time"], 2)
                        }
                
                return {
                    "steps": step_stats,
                    "total_step_executions": sum(s["executions"] for s in step_stats.values()),
                    "total_time_ms": round(sum(s["total_time_ms"] for s in step_stats.values()), 2)
                }
                
        except Exception as e:
            logger.error(f"Step execution summary error: {e}")
            return {
                "steps": {},
                "total_step_executions": 0,
                "total_time_ms": 0,
                "error": str(e)
            }
    
    @staticmethod
    def get_historical_data(data_type: str = "all", limit: int = 100) -> Dict[str, Any]:
        """
        Get historical performance data for specific type.
        
        Args:
            data_type: Type of data ("api", "system", or "all")
            limit: Maximum number of records to return
            
        Returns:
            Historical data filtered by type
        """
        try:
            with PERFORMANCE_LOCK:
                history_data = list(PERFORMANCE_HISTORY)
            
            if data_type == "api":
                filtered_data = [m for m in history_data if m.get("endpoint")][-limit:]
            elif data_type == "system":
                filtered_data = [m for m in history_data if m.get("type") == "system"][-limit:]
            else:
                filtered_data = history_data[-limit:]
            
            return {
                "data_type": data_type,
                "count": len(filtered_data),
                "data": filtered_data,
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
            
        except Exception as e:
            logger.error(f"Historical data retrieval error: {e}")
            return {
                "error": str(e),
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
```

## File: services/report_service.py
```python
"""
Report Service
Generates visual reports (HTML) with statistics and infographics for analyzed videos.
Uses Jinja2 templates and aggregates data from VisualizationService.
"""
from __future__ import annotations

import logging
from pathlib import Path
from typing import Dict, Any, Optional, List
import hashlib
import unicodedata
import re
from datetime import datetime, timezone
from jinja2 import Environment, FileSystemLoader, select_autoescape

from config.settings import config
from services.visualization_service import VisualizationService

logger = logging.getLogger(__name__)


class ReportService:
    """Service for generating visual analysis reports."""
    
    _EXCLUDED_ARCHIVE_FILENAMES = {
        "m6+.mov".lower(),
        "BOUCLE BUG 9-16.mov".lower(),
    }
    
    @staticmethod
    def analyze_monthly_report_html(html: str) -> Dict[str, Any]:
        """Analyze an uploaded monthly report HTML and compute counts from the
        'Répartition des Durées par Projet' section only.

        Args:
            html: The HTML content of a monthly report.

        Returns:
            Dict with computed counts strictly from listed filenames in the
            duration section (what is visually present in the report):
            {
              "total_listed": int,            # total entries across all spans.video-names
              "by_extension": {".mp4": int, "other": int, "noext": int},
              "projects": int|null,           # number of distinct project blocks found
              "build_id": str|null,           # parsed from header if present
              "month": str|null               # parsed from title if present
            }
        """
        try:
            text = html or ""
            section_text = text
            try:
                h2m = re.search(r"<h2>\s*R[ée]partition des Dur[ée]es par Projet\s*</h2>", text, flags=re.IGNORECASE)
                if h2m:
                    section_text = text[h2m.end():]
            except Exception:
                section_text = text
            month_match = re.search(r"Rapport\s+Mensuel\s+Archives\s+[-—]\s+([0-9]{4}-[0-9]{2})", text)
            build_match = re.search(r"Build\s+([a-f0-9]{6,16})", text, re.IGNORECASE)
            month_val = month_match.group(1) if month_match else None
            build_val = build_match.group(1) if build_match else None

            blocks = re.findall(r"<div\s+class=\"video-names\"[^>]*>(.*?)</div>", section_text, flags=re.DOTALL | re.IGNORECASE)

            total_listed = 0
            mp4 = 0
            other = 0
            noext = 0

            def _classify(name: str):
                nonlocal total_listed, mp4, other, noext
                clean = re.sub(r"<[^>]+>", "", name)
                clean = unicodedata.normalize("NFKC", clean).strip()
                if not clean:
                    return
                total_listed += 1
                l = clean.lower()
                if l.endswith('.mp4'):
                    mp4 += 1
                else:
                    if '.' in l:
                        other += 1
                    else:
                        noext += 1

            for raw in blocks:
                items = re.findall(r"<div\s+class=\"video-name\"[^>]*>(.*?)</div>", raw, flags=re.DOTALL | re.IGNORECASE)
                if items:
                    for it in items:
                        _classify(it)
                else:
                    parts = re.split(r"<br\s*/?>", raw, flags=re.IGNORECASE)
                    buffer = ""
                    for part in parts:
                        txt = re.sub(r"<[^>]+>", "", part)
                        txt = unicodedata.normalize("NFKC", txt).strip()
                        if not txt:
                            continue
                        if not buffer:
                            if txt.lower().endswith('.mp4') or re.search(r"\.[a-z0-9]{2,4}$", txt.lower()):
                                _classify(txt)
                            else:
                                buffer = txt
                        else:
                            combined = (buffer + " " + txt).strip()
                            if txt.lower().endswith('.mp4') or combined.lower().endswith('.mp4') or re.search(r"\.[a-z0-9]{2,4}$", txt.lower()) or re.search(r"\.[a-z0-9]{2,4}$", combined.lower()):
                                _classify(combined)
                                buffer = ""
                            else:
                                buffer = combined

            if total_listed == 0:
                spans = re.findall(r"<span\s+class=\"video-names\">(.*?)</span>", section_text, flags=re.DOTALL | re.IGNORECASE)
                for raw in spans:
                    parts = re.split(r"<br\s*/?>", raw, flags=re.IGNORECASE)
                    buffer = ""
                    for part in parts:
                        txt = re.sub(r"<[^>]+>", "", part)
                        txt = unicodedata.normalize("NFKC", txt).strip()
                        if not txt:
                            continue
                        if not buffer:
                            if txt.lower().endswith('.mp4') or re.search(r"\.[a-z0-9]{2,4}$", txt.lower()):
                                _classify(txt)
                            else:
                                buffer = txt
                        else:
                            combined = (buffer + " " + txt).strip()
                            if txt.lower().endswith('.mp4') or combined.lower().endswith('.mp4') or re.search(r"\.[a-z0-9]{2,4}$", txt.lower()) or re.search(r"\.[a-z0-9]{2,4}$", combined.lower()):
                                _classify(combined)
                                buffer = ""
                            else:
                                buffer = combined

            if total_listed == 0:
                items = re.findall(r"<div\s+class=\"video-name\"[^>]*>(.*?)</div>", section_text, flags=re.DOTALL | re.IGNORECASE)
                for it in items:
                    _classify(it)

            proj_blocks = re.findall(r"<strong>\s*([^<]+?)\s*</strong>\s*:", section_text)
            
            counters = re.findall(r"(?:moins\s+de\s+2\s+minutes\s*:\s*(\d+))|(?:entre\s+2\s+et\s*5\s+minutes\s*:\s*(\d+))|(?:plus\s+de\s*5\s+minutes\s*:\s*(\d+))", text, flags=re.IGNORECASE)
            total_from_counters = 0
            for a, b, c in counters:
                for v in (a, b, c):
                    if v:
                        try:
                            total_from_counters += int(v)
                        except ValueError:
                            pass
            return {
                "total_listed": total_listed,
                "by_extension": {".mp4": mp4, "other": other, "noext": noext},
                "lines_mp4": mp4,
                "list_items_total": total_listed,
                "projects": len(proj_blocks) if proj_blocks else 0,
                "build_id": build_val,
                "month": month_val,
                "total_from_counters": total_from_counters,
            }
        except Exception as e:
            logger.error("analyze_monthly_report_html error", exc_info=True)
            return {"error": str(e)}

    @staticmethod
    def _get_jinja_env() -> Environment:
        """Get Jinja2 environment for report templates."""
        template_dir = config.BASE_PATH_SCRIPTS / "templates" / "reports"
        template_dir.mkdir(parents=True, exist_ok=True)
        
        env = Environment(
            loader=FileSystemLoader(str(template_dir)),
            autoescape=select_autoescape(['html', 'xml'])
        )
        
        env.filters['format_duration'] = ReportService._format_duration
        env.filters['format_percentage'] = ReportService._format_percentage
        
        return env
    
    @staticmethod
    def _format_duration(seconds: float) -> str:
        """Format duration in seconds to HH:MM:SS."""
        if not seconds:
            return "00:00:00"
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        return f"{hours:02d}:{minutes:02d}:{secs:02d}"
    
    @staticmethod
    def _format_percentage(value: float, decimals: int = 1) -> str:
        """Format percentage value."""
        return f"{value:.{decimals}f}%"
    
    @staticmethod
    def generate_report(
        project_name: str,
        video_name: str,
        format: str = "html"
    ) -> Dict[str, Any]:
        """
        Generate a visual report for a video.
        
        Args:
            project_name: Name of the project
            video_name: Name of the video file
            format: Output format ('html' or 'pdf')
            
        Returns:
            Dictionary with report data and rendered content
        """
        try:
            timeline_data = VisualizationService.get_project_timeline(project_name, video_name)
            
            if "error" in timeline_data:
                return {"error": timeline_data["error"]}
            
            stats = ReportService._compute_statistics(timeline_data)
            
            context = {
                "project_name": project_name,
                "video_name": video_name,
                "generated_at": datetime.now(timezone.utc).isoformat(),
                "metadata": timeline_data.get("metadata", {}),
                "scenes": timeline_data.get("scenes", {}),
                "audio": timeline_data.get("audio", {}),
                "tracking": timeline_data.get("tracking", {}),
                "archive_probe_source": timeline_data.get("archive_probe_source", {}),
                "statistics": stats,
            }
            
            env = ReportService._get_jinja_env()
            
            try:
                template = env.get_template("analysis_report.html")
            except Exception as e:
                logger.warning(f"Template not found, using fallback: {e}")
                ReportService._create_default_template()
                template = env.get_template("analysis_report.html")
            
            html_content = template.render(**context)
            
            result = {
                "project_name": project_name,
                "video_name": video_name,
                "format": "html",
                "html": html_content,
                "statistics": stats,
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
            
            return result
            
        except Exception as e:
            logger.error(f"Error generating report: {e}", exc_info=True)
            return {"error": str(e)}

    @staticmethod
    def generate_monthly_archive_report(month: str, format: str = "html") -> Dict[str, Any]:
        """Generate a consolidated report for all archived projects in a given month.

        Args:
            month: Target month in 'YYYY-MM' format.
            format: Output format (currently 'html').

        Returns:
            Dict with rendered HTML and aggregated statistics, or an error.
        """
        try:
            month = (month or "").strip()
            if (not month) or (len(month) != 7) or (month[4] != '-'):
                return {"error": f"Invalid month format: '{month}'. Expected YYYY-MM."}

            avail = VisualizationService.get_available_projects()
            if avail.get("error"):
                return {"error": avail["error"]}

            projects = avail.get("projects", [])
            selected = [
                p for p in projects
                if p.get("source") == "archives" and isinstance(p.get("archive_timestamp"), str)
                and p["archive_timestamp"].startswith(month)
            ]

            if not selected:
                return {"error": f"No archived projects found for month '{month}'"}

            monthly_projects: List[Dict[str, Any]] = []
            totals = {
                "projects": 0,
                "videos": 0,
                "video": {"duration_seconds": 0.0, "total_frames": 0, "fps_weighted_sum": 0.0, "fps_weight": 0.0},
                "scenes": {"total_count": 0},
                "audio": {"total_segments": 0, "unique_speakers": set(), "total_speech_duration": 0.0},
                "tracking": {
                    "frames_with_faces": 0,
                    "frames_with_speaking": 0,
                    "max_faces_detected": 0,
                    "face_coverage_percent_sum": 0.0,
                    "face_coverage_entries": 0
                }
            }

            for proj in selected:
                pname = proj.get("name")
                videos_all = proj.get("videos", [])
                filtered_videos = [
                    v for v in videos_all
                    if isinstance(v, str) and v.lower() not in ReportService._EXCLUDED_ARCHIVE_FILENAMES
                ]
                totals["projects"] += 1
                per_video: List[Dict[str, Any]] = []
                duration_counts = {
                    "lt_2m": 0,            # < 120s
                    "between_2_5m": 0,     # 120s .. 300s (inclusive)
                    "gt_5m": 0,            # > 300s
                }
                duration_names = {
                    "lt_2m": [],
                    "between_2_5m": [],
                    "gt_5m": [],
                }
                for video in filtered_videos:
                    tl = VisualizationService.get_project_timeline(pname, video)
                    if "error" in tl:
                        per_video.append({"video_name": video, "error": tl["error"]})
                        continue
                    stats = ReportService._compute_statistics(tl)
                    per_video.append({
                        "video_name": video,
                        "metadata": tl.get("metadata", {}),
                        "statistics": stats,
                        "archive_probe_source": tl.get("archive_probe_source", {})
                    })

                    md = stats.get("video", {})
                    totals["video"]["duration_seconds"] += float(md.get("duration_seconds", 0) or 0)
                    totals["video"]["total_frames"] += int(md.get("total_frames", 0) or 0)
                    fps = float(md.get("fps", 0) or 0)
                    if fps > 0 and md.get("duration_seconds", 0):
                        w = float(md.get("duration_seconds", 0) or 0)
                        totals["video"]["fps_weighted_sum"] += fps * w
                        totals["video"]["fps_weight"] += w

                    sc = stats.get("scenes", {})
                    totals["scenes"]["total_count"] += int(sc.get("total_count", 0) or 0)

                    au = stats.get("audio", {})
                    totals["audio"]["total_segments"] += int(au.get("total_segments", 0) or 0)
                    totals["audio"]["total_speech_duration"] += float(au.get("total_speech_duration", 0) or 0)
                    audio_raw = tl.get("audio", {})
                    for sp in audio_raw.get("unique_speakers", []):
                        totals["audio"]["unique_speakers"].add(sp)

                    tr = stats.get("tracking", {})
                    totals["tracking"]["frames_with_faces"] += int(tr.get("frames_with_faces", 0) or 0)
                    totals["tracking"]["frames_with_speaking"] += int(tr.get("frames_with_speaking", 0) or 0)
                    totals["tracking"]["max_faces_detected"] = max(
                        totals["tracking"]["max_faces_detected"], int(tr.get("max_faces_detected", 0) or 0)
                    )
                    if tr.get("face_coverage_percent") is not None:
                        totals["tracking"]["face_coverage_percent_sum"] += float(tr.get("face_coverage_percent", 0) or 0)
                        totals["tracking"]["face_coverage_entries"] += 1

                    try:
                        dur = float(md.get("duration_seconds", 0) or 0)
                    except Exception:
                        dur = 0.0
                    if dur < 120.0:
                        duration_counts["lt_2m"] += 1
                        duration_names["lt_2m"].append(video)
                    elif dur <= 300.0:
                        duration_counts["between_2_5m"] += 1
                        duration_names["between_2_5m"].append(video)
                    else:
                        duration_counts["gt_5m"] += 1
                        duration_names["gt_5m"].append(video)

                def _norm_for_merge(s: str) -> str:
                    if not isinstance(s, str):
                        return ""
                    x = unicodedata.normalize("NFKC", s)
                    x = re.sub(r"\s+", " ", x).strip().lower()
                    return x

                def _merge_split_names(names: List[str]) -> List[str]:
                    out: List[str] = []
                    i = 0
                    while i < len(names):
                        cur_raw = names[i]
                        cur = _norm_for_merge(cur_raw)
                        if i + 1 < len(names):
                            nxt_raw = names[i + 1]
                            nxt = _norm_for_merge(nxt_raw)
                            if not cur.endswith('.mp4') and nxt.endswith('.mp4') and nxt.startswith(cur):
                                out.append(nxt_raw.strip())
                                i += 2
                                continue
                        out.append(cur_raw.strip())
                        i += 1
                    seen = set()
                    dedup: List[str] = []
                    for item in out:
                        key = _norm_for_merge(item)
                        if key in seen:
                            continue
                        seen.add(key)
                        dedup.append(item)
                    return dedup

                for k in ("lt_2m", "between_2_5m", "gt_5m"):
                    duration_names[k] = _merge_split_names(duration_names.get(k, []) or [])

                duration_counts["lt_2m"] = len(duration_names.get("lt_2m", []) or [])
                duration_counts["between_2_5m"] = len(duration_names.get("between_2_5m", []) or [])
                duration_counts["gt_5m"] = len(duration_names.get("gt_5m", []) or [])

                bucket_sum = (
                    int(duration_counts["lt_2m"]) +
                    int(duration_counts["between_2_5m"]) +
                    int(duration_counts["gt_5m"])
                )
                totals["videos"] += bucket_sum

                monthly_projects.append({
                    "name": pname,
                    "display_base": proj.get("display_base", pname),
                    "archive_timestamp": proj.get("archive_timestamp"),
                    "videos": per_video,
                    "video_count": bucket_sum,
                    "duration_counts": duration_counts,
                    "duration_names": duration_names,
                })

            consolidated = {
                "projects": totals["projects"],
                "videos": totals["videos"],
                "video": {
                    "duration_seconds": totals["video"]["duration_seconds"],
                    "total_frames": totals["video"]["total_frames"],
                    "fps": (
                        totals["video"]["fps_weighted_sum"] / totals["video"]["fps_weight"]
                        if totals["video"]["fps_weight"] > 0 else 0
                    ),
                },
                "scenes": {"total_count": totals["scenes"]["total_count"]},
                "audio": {
                    "total_segments": totals["audio"]["total_segments"],
                    "unique_speakers": len(totals["audio"]["unique_speakers"]),
                    "total_speech_duration": totals["audio"]["total_speech_duration"],
                    "speech_coverage_percent": (
                        (totals["audio"]["total_speech_duration"] / totals["video"]["duration_seconds"]) * 100.0
                        if totals["video"]["duration_seconds"] > 0 else 0
                    ),
                },
                "tracking": {
                    "frames_with_faces": totals["tracking"]["frames_with_faces"],
                    "frames_with_speaking": totals["tracking"]["frames_with_speaking"],
                    "max_faces_detected": totals["tracking"]["max_faces_detected"],
                    "face_coverage_percent": (
                        totals["tracking"]["face_coverage_percent_sum"] / totals["tracking"]["face_coverage_entries"]
                        if totals["tracking"]["face_coverage_entries"] > 0 else 0
                    ),
                }
            }

            def _norm_filename(name: str) -> str:
                if not isinstance(name, str):
                    return ""
                s = unicodedata.normalize("NFKC", name)
                s = s.strip().lower()
                s = re.sub(r"\s+", " ", s)
                return s

            total_listed = 0
            mp4_listed = 0
            mp4_distinct: set[str] = set()
            mp4_distinct_stems: set[str] = set()
            for p in monthly_projects:
                dc = p.get("duration_counts", {}) or {}
                total_listed += int(dc.get("lt_2m", 0)) + int(dc.get("between_2_5m", 0)) + int(dc.get("gt_5m", 0))
                dn = p.get("duration_names", {}) or {}
                for key in ("lt_2m", "between_2_5m", "gt_5m"):
                    for raw in dn.get(key, []) or []:
                        nn = _norm_filename(raw)
                        if nn.endswith(".mp4"):
                            mp4_listed += 1
                            mp4_distinct.add(nn)
                            stem = nn[:-4].strip()
                            mp4_distinct_stems.add(stem)

            section_summary = {
                "total_listed": total_listed,
                "mp4_listed": mp4_listed,
                "mp4_distinct": len(mp4_distinct),
                "mp4_distinct_stems": len(mp4_distinct_stems),
            }

            generated_at = datetime.now(timezone.utc).isoformat()
            build_id_source = f"{month}|{generated_at}|{totals['projects']}|{totals['videos']}|{len(monthly_projects)}"
            build_id = hashlib.sha256(build_id_source.encode("utf-8")).hexdigest()[:12]

            context = {
                "month": month,
                "generated_at": generated_at,
                "projects": monthly_projects,
                "consolidated": consolidated,
                "build_id": build_id,
                "section_summary": section_summary,
            }

            env = ReportService._get_jinja_env()
            try:
                template = env.get_template("monthly_archive_report.html")
            except Exception:
                ReportService._create_default_monthly_template()
                template = env.get_template("monthly_archive_report.html")

            html_content = template.render(**context)
            return {
                "format": "html",
                "html": html_content,
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "month": month,
                "project_count": consolidated["projects"],
                "video_count": consolidated["videos"],
            }

        except Exception as e:
            logger.error(f"Error generating monthly archive report: {e}", exc_info=True)
            return {"error": str(e)}

    @staticmethod
    def _create_default_monthly_template() -> None:
        """Create a minimal monthly report template if not present."""
        try:
            template_dir = config.BASE_PATH_SCRIPTS / "templates" / "reports"
            template_dir.mkdir(parents=True, exist_ok=True)
            target = template_dir / "monthly_archive_report.html"
            if not target.exists():
                target.write_text(
                    """<!doctype html>
<html lang=\"fr\">
<head>
  <meta charset=\"utf-8\" />
  <title>Rapport Mensuel Archives - {{ month }}</title>
  <style>
    body { font-family: system-ui, -apple-system, Segoe UI, Roboto, Arial, sans-serif; padding: 16px; color: #1a1a1a; }
    h1 { margin: 0 0 12px; font-size: 20px; }
    .meta { color: #666; font-size: 12px; margin-bottom: 16px; }
    .summary { background: #f7f7f7; padding: 12px; border-radius: 8px; margin-bottom: 16px; }
    table { width: 100%; border-collapse: collapse; margin-top: 8px; }
    th, td { border: 1px solid #ddd; padding: 8px; font-size: 13px; }
    th { background: #fafafa; text-align: left; }
    .section { margin-top: 20px; }
    ul { margin: 6px 0 12px 18px; }
    li { margin: 2px 0; }
  </style>
  </head>
<body>
  <h1>Rapport Mensuel Archives — {{ month }}</h1>
  <div class=\"meta\">Généré le {{ generated_at }}</div>
  <div class=\"summary\">
    <div><strong>Projets:</strong> {{ consolidated.projects }}</div>
    <div><strong>Vidéos:</strong> {{ consolidated.videos }}</div>
    <div><strong>Durée totale:</strong> {{ consolidated.video.duration_seconds|format_duration }}</div>
    <div><strong>FPS moyen (pondéré):</strong> {{ consolidated.video.fps|format_percentage(1) }}</div>
  </div>
  <h2>Détails par projet</h2>
  <table>
    <thead>
      <tr><th>Projet</th><th>Horodatage</th><th>Vidéos</th><th>Scènes</th><th>Parole (s)</th><th>Faces (frames)</th></tr>
    </thead>
    <tbody>
      {% for p in projects %}
      <tr>
        <td>{{ p.display_base }}</td>
        <td>{{ p.archive_timestamp or '-' }}</td>
        <td style=\"text-align:right\">{{ p.video_count }}</td>
        <td style=\"text-align:right\">{{ p.videos | sum(attribute='statistics.scenes.total_count') }}</td>
        <td style=\"text-align:right\">{{ p.videos | sum(attribute='statistics.audio.total_speech_duration') | round(1) }}</td>
        <td style=\"text-align:right\">{{ p.videos | sum(attribute='statistics.tracking.frames_with_faces') }}</td>
      </tr>
      {% endfor %}
    </tbody>
  </table>

  <div class=\"section\">
    <h2>Répartition des Durées par Projet</h2>
    {% for p in projects %}
      <div>
        <strong>{{ p.display_base }}</strong> :
        <ul>
          <li>moins de 2 minutes : {{ p.duration_counts.lt_2m }}</li>
          <li>entre 2 et 5 minutes : {{ p.duration_counts.between_2_5m }}</li>
          <li>plus de 5 minutes : {{ p.duration_counts.gt_5m }}</li>
        </ul>
      </div>
    {% endfor %}
  </div>
</body>
</html>
""",
                    encoding="utf-8",
                )
        except Exception:
            pass

    @staticmethod
    def generate_project_report(
        project_name: str,
        format: str = "html"
    ) -> Dict[str, Any]:
        """Generate a consolidated report for all videos in a project.

        Args:
            project_name: Name of the project
            format: Output format ('html' or 'pdf')

        Returns:
            Dictionary with consolidated report data and rendered content
        """
        try:
            projects_info = VisualizationService.get_available_projects()
            if projects_info.get("error"):
                return {"error": projects_info["error"]}

            project_entry = None
            for p in projects_info.get("projects", []):
                if p.get("name") == project_name:
                    project_entry = p
                    break
            if not project_entry:
                return {"error": f"Project '{project_name}' not found"}

            videos = project_entry.get("videos", [])
            if not videos:
                return {"error": f"No videos found for project '{project_name}'"}

            per_video: List[Dict[str, Any]] = []
            totals = {
                "video": {"duration_seconds": 0.0, "total_frames": 0, "fps_weighted_sum": 0.0, "fps_weight": 0.0},
                "scenes": {"total_count": 0},
                "audio": {"total_segments": 0, "unique_speakers": set(), "total_speech_duration": 0.0},
                "tracking": {
                    "frames_with_faces": 0,
                    "frames_with_speaking": 0,
                    "max_faces_detected": 0,
                    "face_coverage_percent_sum": 0.0,
                    "face_coverage_entries": 0
                }
            }

            for video in videos:
                timeline = VisualizationService.get_project_timeline(project_name, video)
                if "error" in timeline:
                    per_video.append({"video_name": video, "error": timeline["error"]})
                    continue

                stats = ReportService._compute_statistics(timeline)
                per_video.append({
                    "video_name": video,
                    "metadata": timeline.get("metadata", {}),
                    "statistics": stats,
                    "archive_probe_source": timeline.get("archive_probe_source", {})
                })

                md = stats.get("video", {})
                totals["video"]["duration_seconds"] += float(md.get("duration_seconds", 0) or 0)
                totals["video"]["total_frames"] += int(md.get("total_frames", 0) or 0)
                fps = float(md.get("fps", 0) or 0)
                if fps > 0 and md.get("duration_seconds", 0):
                    w = float(md.get("duration_seconds", 0) or 0)
                    totals["video"]["fps_weighted_sum"] += fps * w
                    totals["video"]["fps_weight"] += w

                sc = stats.get("scenes", {})
                totals["scenes"]["total_count"] += int(sc.get("total_count", 0) or 0)

                au = stats.get("audio", {})
                totals["audio"]["total_segments"] += int(au.get("total_segments", 0) or 0)
                totals["audio"]["total_speech_duration"] += float(au.get("total_speech_duration", 0) or 0)
                audio_raw = timeline.get("audio", {})
                for sp in audio_raw.get("unique_speakers", []):
                    totals["audio"]["unique_speakers"].add(sp)

                tr = stats.get("tracking", {})
                totals["tracking"]["frames_with_faces"] += int(tr.get("frames_with_faces", 0) or 0)
                totals["tracking"]["frames_with_speaking"] += int(tr.get("frames_with_speaking", 0) or 0)
                totals["tracking"]["max_faces_detected"] = max(
                    totals["tracking"]["max_faces_detected"], int(tr.get("max_faces_detected", 0) or 0)
                )
                if tr.get("face_coverage_percent") is not None:
                    totals["tracking"]["face_coverage_percent_sum"] += float(tr.get("face_coverage_percent", 0) or 0)
                    totals["tracking"]["face_coverage_entries"] += 1

            consolidated = {
                "video": {
                    "duration_seconds": totals["video"]["duration_seconds"],
                    "total_frames": totals["video"]["total_frames"],
                    "fps": (
                        totals["video"]["fps_weighted_sum"] / totals["video"]["fps_weight"]
                        if totals["video"]["fps_weight"] > 0 else 0
                    ),
                },
                "scenes": {
                    "total_count": totals["scenes"]["total_count"],
                },
                "audio": {
                    "total_segments": totals["audio"]["total_segments"],
                    "unique_speakers": len(totals["audio"]["unique_speakers"]),
                    "total_speech_duration": totals["audio"]["total_speech_duration"],
                    "speech_coverage_percent": (
                        (totals["audio"]["total_speech_duration"] / totals["video"]["duration_seconds"]) * 100.0
                        if totals["video"]["duration_seconds"] > 0 else 0
                    ),
                },
                "tracking": {
                    "frames_with_faces": totals["tracking"]["frames_with_faces"],
                    "frames_with_speaking": totals["tracking"]["frames_with_speaking"],
                    "max_faces_detected": totals["tracking"]["max_faces_detected"],
                    "face_coverage_percent": (
                        totals["tracking"]["face_coverage_percent_sum"] / totals["tracking"]["face_coverage_entries"]
                        if totals["tracking"]["face_coverage_entries"] > 0 else 0
                    ),
                }
            }

            context = {
                "project_name": project_name,
                "generated_at": datetime.now(timezone.utc).isoformat(),
                "videos": per_video,
                "consolidated": consolidated,
            }

            env = ReportService._get_jinja_env()
            try:
                template = env.get_template("project_report.html")
            except Exception as e:
                logger.warning(f"Project template not found, using fallback: {e}")
                ReportService._create_default_project_template()
                template = env.get_template("project_report.html")

            html_content = template.render(**context)

            result = {
                "project_name": project_name,
                "format": "html",
                "html": html_content,
                "consolidated": consolidated,
                "video_reports": per_video,
                "timestamp": datetime.now(timezone.utc).isoformat(),
            }

            return result

        except Exception as e:
            logger.error(f"Error generating project report: {e}", exc_info=True)
            return {"error": str(e)}
    
    @staticmethod
    def _compute_statistics(timeline_data: Dict[str, Any]) -> Dict[str, Any]:
        """Compute statistics from timeline data."""
        metadata = timeline_data.get("metadata", {})
        scenes = timeline_data.get("scenes", {})
        audio = timeline_data.get("audio", {})
        tracking = timeline_data.get("tracking", {})
        
        stats = {
            "video": {
                "duration_seconds": metadata.get("duration_seconds", 0),
                "total_frames": metadata.get("total_frames", 0),
                "fps": metadata.get("fps", 0),
            },
            "scenes": {
                "total_count": scenes.get("count", 0),
                "average_duration": 0,
                "shortest_duration": 0,
                "longest_duration": 0,
            },
            "audio": {
                "total_segments": audio.get("segment_count", 0),
                "unique_speakers": audio.get("speaker_count", 0),
                "total_speech_duration": 0,
                "speech_coverage_percent": 0,
            },
            "tracking": {
                "frames_with_faces": tracking.get("summary", {}).get("frames_with_faces", 0),
                "frames_with_speaking": tracking.get("summary", {}).get("frames_with_speaking", 0),
                "max_faces_detected": tracking.get("summary", {}).get("max_faces_detected", 0),
                "face_coverage_percent": tracking.get("summary", {}).get("face_coverage_percent", 0),
            }
        }
        
        if scenes.get("available") and scenes.get("scenes"):
            scene_list = scenes["scenes"]
            durations = [
                s.get("end_time_seconds", 0) - s.get("start_time_seconds", 0)
                for s in scene_list
            ]
            if durations:
                stats["scenes"]["average_duration"] = sum(durations) / len(durations)
                stats["scenes"]["shortest_duration"] = min(durations)
                stats["scenes"]["longest_duration"] = max(durations)
        
        if audio.get("available") and audio.get("segments"):
            segments = audio["segments"]
            speech_durations = [
                seg.get("end_time", 0) - seg.get("start_time", 0)
                for seg in segments
            ]
            if speech_durations:
                total_speech = sum(speech_durations)
                stats["audio"]["total_speech_duration"] = total_speech
                video_duration = metadata.get("duration_seconds", 1)
                if video_duration > 0:
                    stats["audio"]["speech_coverage_percent"] = (total_speech / video_duration) * 100
        
        return stats
    
    @staticmethod
    def _create_default_template():
        """Create a default report template if none exists."""
        template_dir = config.BASE_PATH_SCRIPTS / "templates" / "reports"
        template_dir.mkdir(parents=True, exist_ok=True)
        
        template_path = template_dir / "analysis_report.html"
        
        if template_path.exists():
            return
        
        default_template = """<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Rapport d'Analyse - {{ video_name }}</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, sans-serif;
            line-height: 1.6;
            color: #333;
            background: #f5f5f5;
            padding: 20px;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        
        .header {
            border-bottom: 3px solid #007bff;
            padding-bottom: 20px;
            margin-bottom: 30px;
        }
        
        .header h1 {
            color: #007bff;
            margin-bottom: 10px;
        }
        
        .meta-info {
            color: #666;
            font-size: 14px;
        }
        
        .section {
            margin-bottom: 40px;
        }
        
        .section-title {
            color: #007bff;
            border-left: 4px solid #007bff;
            padding-left: 15px;
            margin-bottom: 20px;
            font-size: 24px;
        }
        
        .stats-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin-bottom: 20px;
        }
        
        .stat-card {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            border-left: 4px solid #007bff;
        }
        
        .stat-label {
            color: #666;
            font-size: 12px;
            text-transform: uppercase;
            letter-spacing: 0.5px;
            margin-bottom: 5px;
        }
        
        .stat-value {
            font-size: 28px;
            font-weight: bold;
            color: #007bff;
        }
        
        .stat-unit {
            font-size: 14px;
            color: #666;
            font-weight: normal;
        }
        
        .badge {
            display: inline-block;
            padding: 4px 12px;
            border-radius: 12px;
            font-size: 12px;
            font-weight: 600;
            margin-right: 10px;
        }
        
        .badge-success {
            background: #28a745;
            color: white;
        }
        
        .badge-warning {
            background: #ffc107;
            color: #333;
        }
        
        .badge-info {
            background: #17a2b8;
            color: white;
        }
        
        .badge-archive {
            background: #6c757d;
            color: white;
        }
        
        .progress-bar {
            width: 100%;
            height: 30px;
            background: #e9ecef;
            border-radius: 15px;
            overflow: hidden;
            margin-top: 10px;
        }
        
        .progress-fill {
            height: 100%;
            background: linear-gradient(90deg, #007bff, #0056b3);
            display: flex;
            align-items: center;
            justify-content: center;
            color: white;
            font-weight: bold;
            font-size: 14px;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #dee2e6;
            text-align: center;
            color: #666;
            font-size: 14px;
        }
        
        @media print {
            body {
                background: white;
                padding: 0;
            }
            .container {
                box-shadow: none;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>📊 Rapport d'Analyse Vidéo</h1>
            <div class="meta-info">
                <strong>Projet:</strong> {{ project_name }} | 
                <strong>Vidéo:</strong> {{ video_name }}<br>
                <strong>Généré le:</strong> {{ generated_at }}
                {% if archive_probe_source.metadata.provenance == 'archives' %}
                <br><span class="badge badge-archive">📦 Données archivées</span>
                {% endif %}
            </div>
        </div>
        
        <div class="section">
            <h2 class="section-title">📹 Métadonnées Vidéo</h2>
            <div class="stats-grid">
                <div class="stat-card">
                    <div class="stat-label">Durée</div>
                    <div class="stat-value">{{ statistics.video.duration_seconds|format_duration }}</div>
                </div>
                <div class="stat-card">
                    <div class="stat-label">Frames Totales</div>
                    <div class="stat-value">{{ statistics.video.total_frames|int }}</div>
                </div>
                <div class="stat-card">
                    <div class="stat-label">FPS</div>
                    <div class="stat-value">{{ statistics.video.fps|round(2) }} <span class="stat-unit">fps</span></div>
                </div>
            </div>
        </div>
        
        {% if scenes.available %}
        <div class="section">
            <h2 class="section-title">✂️ Analyse des Scènes</h2>
            <span class="badge badge-success">{{ statistics.scenes.total_count }} scènes détectées</span>
            {% if archive_probe_source.scenes.provenance == 'archives' %}
            <span class="badge badge-archive">📦 Données archivées</span>
            {% endif %}
            
            <div class="stats-grid" style="margin-top: 20px;">
                <div class="stat-card">
                    <div class="stat-label">Durée Moyenne</div>
                    <div class="stat-value">{{ statistics.scenes.average_duration|round(1) }} <span class="stat-unit">s</span></div>
                </div>
                <div class="stat-card">
                    <div class="stat-label">Scène la Plus Courte</div>
                    <div class="stat-value">{{ statistics.scenes.shortest_duration|round(1) }} <span class="stat-unit">s</span></div>
                </div>
                <div class="stat-card">
                    <div class="stat-label">Scène la Plus Longue</div>
                    <div class="stat-value">{{ statistics.scenes.longest_duration|round(1) }} <span class="stat-unit">s</span></div>
                </div>
            </div>
        </div>
        {% endif %}
        
        {% if audio.available %}
        <div class="section">
            <h2 class="section-title">🔊 Analyse Audio</h2>
            <span class="badge badge-info">{{ statistics.audio.total_segments }} segments de parole</span>
            <span class="badge badge-info">{{ statistics.audio.unique_speakers }} locuteurs</span>
            {% if archive_probe_source.audio.provenance == 'archives' %}
            <span class="badge badge-archive">📦 Données archivées</span>
            {% endif %}
            
            <div class="stats-grid" style="margin-top: 20px;">
                <div class="stat-card">
                    <div class="stat-label">Durée Totale de Parole</div>
                    <div class="stat-value">{{ statistics.audio.total_speech_duration|format_duration }}</div>
                </div>
                <div class="stat-card">
                    <div class="stat-label">Couverture Audio</div>
                    <div class="stat-value">{{ statistics.audio.speech_coverage_percent|format_percentage }}</div>
                    <div class="progress-bar">
                        <div class="progress-fill" style="width: {{ statistics.audio.speech_coverage_percent }}%">
                            {{ statistics.audio.speech_coverage_percent|format_percentage }}
                        </div>
                    </div>
                </div>
            </div>
        </div>
        {% endif %}
        
        {% if tracking.available %}
        <div class="section">
            <h2 class="section-title">👤 Suivi des Visages</h2>
            <span class="badge badge-warning">{{ statistics.tracking.max_faces_detected }} visages max détectés</span>
            {% if archive_probe_source.tracking.provenance == 'archives' %}
            <span class="badge badge-archive">📦 Données archivées</span>
            {% endif %}
            
            <div class="stats-grid" style="margin-top: 20px;">
                <div class="stat-card">
                    <div class="stat-label">Frames avec Visages</div>
                    <div class="stat-value">{{ statistics.tracking.frames_with_faces|int }}</div>
                </div>
                <div class="stat-card">
                    <div class="stat-label">Frames avec Parole Détectée</div>
                    <div class="stat-value">{{ statistics.tracking.frames_with_speaking|int }}</div>
                </div>
                <div class="stat-card">
                    <div class="stat-label">Couverture Visages</div>
                    <div class="stat-value">{{ statistics.tracking.face_coverage_percent|format_percentage }}</div>
                    <div class="progress-bar">
                        <div class="progress-fill" style="width: {{ statistics.tracking.face_coverage_percent }}%">
                            {{ statistics.tracking.face_coverage_percent|format_percentage }}
                        </div>
                    </div>
                </div>
            </div>
        </div>
        {% endif %}
        
        <div class="footer">
            <p>Rapport généré par MediaPipe Workflow Analysis System</p>
            <p>{{ generated_at }}</p>
        </div>
    </div>
</body>
</html>"""
        
        with open(template_path, 'w', encoding='utf-8') as f:
            f.write(default_template)
        
        logger.info(f"Created default report template at {template_path}")

    @staticmethod
    def _create_default_project_template():
        """Create a default consolidated project report template if none exists."""
        template_dir = config.BASE_PATH_SCRIPTS / "templates" / "reports"
        template_dir.mkdir(parents=True, exist_ok=True)

        template_path = template_dir / "project_report.html"
        if template_path.exists():
            return

        default_template = """<!DOCTYPE html>
<html lang=\"fr\">
<head>
    <meta charset=\"UTF-8\">
    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">
    <title>Rapport de Projet - {{ project_name }}</title>
    <style>
        body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, sans-serif; background:#f5f5f5; color:#333; padding:20px; }
        .container { max-width: 1200px; margin:0 auto; background:#fff; border-radius:8px; box-shadow:0 2px 10px rgba(0,0,0,0.1); padding:40px; }
        .header { border-bottom:3px solid #007bff; padding-bottom:16px; margin-bottom:24px; }
        .header h1 { color:#007bff; margin:0 0 6px; }
        .section { margin:28px 0; }
        .section-title { color:#007bff; border-left:4px solid #007bff; padding-left:12px; font-size:22px; margin-bottom:12px; }
        .grid { display:grid; grid-template-columns: repeat(auto-fit, minmax(260px,1fr)); gap:16px; }
        .card { background:#f8f9fa; border-left:4px solid #007bff; border-radius:8px; padding:16px; }
        .badge { display:inline-block; padding:4px 10px; border-radius:12px; font-size:12px; font-weight:600; margin-right:8px; }
        .badge-info { background:#17a2b8; color:#fff; }
        .table { width:100%; border-collapse:collapse; }
        .table th, .table td { border-bottom:1px solid #e9ecef; padding:10px; text-align:left; }
        .table th { background:#f1f3f5; }
        .muted { color:#666; font-size:13px; }
    </style>
</head>
<body>
<div class=\"container\">
  <div class=\"header\">
    <h1>📁 Rapport Consolidé du Projet</h1>
    <div class=\"muted\">Projet: <strong>{{ project_name }}</strong> · Généré le {{ generated_at }}</div>
  </div>

  <div class=\"section\">
    <h2 class=\"section-title\">📊 Statistiques Globales</h2>
    <div class=\"grid\">
      <div class=\"card\"><div class=\"muted\">Durée Totale</div><div style=\"font-size:28px;color:#007bff;\">{{ consolidated.video.duration_seconds|format_duration }}</div></div>
      <div class=\"card\"><div class=\"muted\">Frames Totales</div><div style=\"font-size:28px;color:#007bff;\">{{ consolidated.video.total_frames|int }}</div></div>
      <div class=\"card\"><div class=\"muted\">FPS Moyen</div><div style=\"font-size:28px;color:#007bff;\">{{ consolidated.video.fps|round(2) }}</div></div>
      <div class=\"card\"><div class=\"muted\">Scènes Totales</div><div style=\"font-size:28px;color:#007bff;\">{{ consolidated.scenes.total_count|int }}</div></div>
      <div class=\"card\"><div class=\"muted\">Segments de Parole</div><div style=\"font-size:28px;color:#007bff;\">{{ consolidated.audio.total_segments|int }}</div></div>
      <div class=\"card\"><div class=\"muted\">Locuteurs Uniques</div><div style=\"font-size:28px;color:#007bff;\">{{ consolidated.audio.unique_speakers|int }}</div></div>
      <div class=\"card\"><div class=\"muted\">Couverture Parole</div><div style=\"font-size:28px;color:#007bff;\">{{ consolidated.audio.speech_coverage_percent|format_percentage }}</div></div>
      <div class=\"card\"><div class=\"muted\">Frames avec Visages</div><div style=\"font-size:28px;color:#007bff;\">{{ consolidated.tracking.frames_with_faces|int }}</div></div>
    </div>
  </div>

  <div class=\"section\">
    <h2 class=\"section-title\">🎬 Détails par Vidéo</h2>
    <table class=\"table\">
      <thead>
        <tr>
          <th>Vidéo</th>
          <th>Durée</th>
          <th>Scènes</th>
          <th>Locuteurs</th>
          <th>Segments</th>
          <th>Faces (max)</th>
          <th>Couverture Visages</th>
          <th>Statut</th>
        </tr>
      </thead>
      <tbody>
        {% for v in videos %}
        <tr>
          <td>{{ v.video_name }}</td>
          {% if v.error %}
            <td colspan=\"6\" class=\"muted\">—</td>
            <td><span class=\"badge badge-info\">Erreur</span> {{ v.error }}</td>
          {% else %}
            <td>{{ v.statistics.video.duration_seconds|format_duration }}</td>
            <td>{{ v.statistics.scenes.total_count|int }}</td>
            <td>{{ v.metadata.get('speaker_count', 0) or v.statistics.audio.unique_speakers|int }}</td>
            <td>{{ v.statistics.audio.total_segments|int }}</td>
            <td>{{ v.statistics.tracking.max_faces_detected|int }}</td>
            <td>{{ v.statistics.tracking.face_coverage_percent|format_percentage }}</td>
            <td><span class=\"badge badge-info\">OK</span></td>
          {% endif %}
        </tr>
        {% endfor %}
      </tbody>
    </table>
  </div>

  <div class=\"section\" style=\"text-align:center;color:#666;font-size:13px;\">
    Rapport généré par MediaPipe Workflow Analysis System · {{ generated_at }}
  </div>
</div>
</body>
</html>"""

        with open(template_path, "w", encoding="utf-8") as f:
            f.write(default_template)
        logger.info(f"Created default project report template at {template_path}")
```

## File: services/results_archiver.py
```python
"""
Results Archiver Service
Ensures persistence of analysis artifacts (scenes CSV, audio JSON, tracking JSON)
across workflow executions (e.g., when step 7 cleans/rebuilds project dirs).

Architecture:
- Service-layer only. No routes. Import and use from other services (e.g., VisualizationService).
- Archives structure: {ARCHIVES_DIR}/{project_name}/{video_hash}/
  - {video_stem}_scenes.csv
  - {video_stem}_audio.json
  - {video_stem}_tracking.json
  - metadata.json (source path, created_at)

Key design choices:
- Index by strong content hash of the video file to avoid false positives on same names.
- Preserve original filenames (with video_stem prefix) for readability.
- Provide resilient lookup: prefer exact-hash directory, fallback to any existing artifacts by stem.
"""
from __future__ import annotations

import hashlib
import json
import shutil
from dataclasses import dataclass
from pathlib import Path
from typing import Optional, Tuple
from datetime import datetime, timezone

from config.settings import config
import logging

logger = logging.getLogger(__name__)

SCENES_SUFFIX = "_scenes.csv"
AUDIO_SUFFIX = "_audio.json"
TRACKING_SUFFIX = "_tracking.json"
VIDEO_METADATA_NAME = "video_metadata.json"


@dataclass
class ArchivePaths:
    project_dir: Path
    video_hash_dir: Path


class ResultsArchiver:
    """Service that persists and retrieves analysis artifacts for videos."""

    # In-process cache that maps base project names to the timestamped archive dir created
    _PROJECT_ARCHIVE_DIRS: dict[str, Path] = {}

    @staticmethod
    def _format_timestamp(now: Optional[datetime] = None) -> str:
        """Return a safe timestamp string for directory names: YYYY-MM-DD_HH-MM-SS."""
        dt = now or datetime.now(timezone.utc)
        # Use local-like format without timezone symbols for fs safety
        return dt.strftime("%Y-%m-%d_%H-%M-%S")

    @staticmethod
    def _list_matching_project_dirs(base_name: str) -> list[Path]:
        """List archive project directories matching a base project name.

        Matches directories named exactly `base_name` or starting with `base_name + ' '`. Sorted descending by name.
        """
        root = config.ARCHIVES_DIR
        if not root.exists():
            return []
        candidates: list[Path] = []
        prefix = f"{base_name} "
        try:
            for d in root.iterdir():
                if not d.is_dir():
                    continue
                n = d.name
                if n == base_name or n.startswith(prefix):
                    candidates.append(d)
        except Exception:
            return []
        # Sort newest-first by name (timestamp suffix sorts lexicographically correctly)
        candidates.sort(key=lambda p: p.name, reverse=True)
        return candidates

    @classmethod
    def _get_or_create_archive_project_dir(cls, base_name: str) -> Path:
        """Get or create (once per process) the timestamp-suffixed archive project directory.

        Ensures subsequent writes in this process for the same base project go to the same directory.
        """
        if base_name in cls._PROJECT_ARCHIVE_DIRS:
            return cls._PROJECT_ARCHIVE_DIRS[base_name]
        ts = ResultsArchiver._format_timestamp()
        proj_dir = config.ARCHIVES_DIR / f"{base_name} {ts}"
        proj_dir.mkdir(parents=True, exist_ok=True)
        cls._PROJECT_ARCHIVE_DIRS[base_name] = proj_dir
        return proj_dir

    @staticmethod
    def compute_video_hash(video_path: Path, chunk_size: int = 1024 * 1024) -> Optional[str]:
        """Compute SHA256 hash of a video file.
        Args:
            video_path: Absolute path to video file.
            chunk_size: Read chunk size.
        Returns:
            Hex digest string or None on failure.
        """
        try:
            h = hashlib.sha256()
            with open(video_path, 'rb') as f:
                while True:
                    chunk = f.read(chunk_size)
                    if not chunk:
                        break
                    h.update(chunk)
            return h.hexdigest()
        except Exception as e:
            logger.warning(f"Failed to compute hash for {video_path}: {e}")
            return None

    @staticmethod
    def archive_project_analysis(project_name: str) -> dict:
        """Scan the project directory for analysis artifacts and persist them to archives.

        For each video found under PROJECTS_DIR/<project_name>, attempts to copy
        any matching *_scenes.csv, *_audio.json, *_tracking.json into the archives
        hashed directory.

        Returns:
            dict summary: {"project_name": str, "processed": int, "copied": int, "details": [...]}
        """
        details = []
        copied_count = 0
        processed = 0
        try:
            base_path = getattr(config, 'PROJECTS_DIR', None) or (config.BASE_PATH_SCRIPTS / "projets_extraits")
            project_path = base_path / project_name
            if not project_path.exists():
                return {"error": f"Project folder not found: {project_path}"}

            # Enumerate candidate video files (stems) and search artifacts alongside
            video_extensions = ('.mp4', '.mov', '.avi', '.mkv', '.webm')
            video_files = []
            for ext in video_extensions:
                video_files.extend(project_path.rglob(f"*{ext}"))

            for v in video_files:
                processed += 1
                stem = v.stem
                scenes = next(project_path.rglob(f"{stem}{SCENES_SUFFIX}"), None)
                if scenes is None:
                    # Fallback: plain CSV (e.g., '<stem>.csv')
                    scenes = next(project_path.rglob(f"{stem}.csv"), None)
                audio = next(project_path.rglob(f"{stem}{AUDIO_SUFFIX}"), None)
                tracking = next(project_path.rglob(f"{stem}{TRACKING_SUFFIX}"), None)
                if tracking is None:
                    # Fallback: plain JSON (e.g., '<stem>.json')
                    tracking = next(project_path.rglob(f"{stem}.json"), None)

                archived_dir = ResultsArchiver.archive_analysis_files(
                    project_name, v, scenes_file=scenes, audio_file=audio, tracking_file=tracking
                )
                copied = {
                    "scenes": bool(scenes),
                    "audio": bool(audio),
                    "tracking": bool(tracking),
                    "archived": bool(archived_dir),
                }
                if archived_dir and any([scenes, audio, tracking]):
                    copied_count += 1
                details.append({
                    "video": str(v.relative_to(project_path)),
                    "copied": copied,
                    "archive_dir": str(archived_dir) if archived_dir else None,
                })

            return {
                "project_name": project_name,
                "processed": processed,
                "copied": copied_count,
                "details": details,
            }
        except Exception as e:
            logger.error(f"Error archiving project analysis for {project_name}: {e}")
            return {"error": str(e)}

    @staticmethod
    def save_video_metadata(project_name: str, video_path: Path, metadata: dict) -> Optional[Path]:
        """Persist computed video metadata into the archive folder.
        Writes video_metadata.json alongside metadata.json with a created_at.
        """
        try:
            video_hash = ResultsArchiver.compute_video_hash(video_path)
            if not video_hash:
                return None
            # Write into the unique (timestamped) archive project directory
            ap = ResultsArchiver.get_archive_paths(project_name, video_hash, create=True)
            ap.video_hash_dir.mkdir(parents=True, exist_ok=True)
            payload = {
                "created_at": datetime.now(timezone.utc).isoformat(),
                "metadata": metadata,
            }
            out_path = ap.video_hash_dir / VIDEO_METADATA_NAME
            with open(out_path, "w", encoding="utf-8") as f:
                json.dump(payload, f, ensure_ascii=False, indent=2)
            return out_path
        except Exception as e:
            logger.warning(f"Failed to save video metadata archive: {e}")
            return None

    @staticmethod
    def load_video_metadata(project_name: str, video_path: Path) -> Optional[dict]:
        """Load archived video metadata if present. Returns dict with keys
        {created_at: str, metadata: {...}} or None.
        """
        try:
            video_hash = ResultsArchiver.compute_video_hash(video_path)
            # If hash not computable (video missing), try any matching stem folder
            if video_hash:
                ap = ResultsArchiver.get_archive_paths(project_name, video_hash, create=False)
                p = ap.video_hash_dir / VIDEO_METADATA_NAME
                if p.exists():
                    with open(p, "r", encoding="utf-8") as f:
                        return json.load(f)
            # Fallback by stem across matching suffixed project dirs
            matches = ResultsArchiver._list_matching_project_dirs(project_name)
            if not matches:
                return None
            stem = video_path.stem
            for arch_proj in matches:
                for meta_path in arch_proj.rglob(VIDEO_METADATA_NAME):
                    try:
                        # ensure sibling files match stem
                        sibling = meta_path.parent
                        any_match = any(
                            q.stem.startswith(stem) and (
                                q.name.endswith(SCENES_SUFFIX) or q.name.endswith(AUDIO_SUFFIX) or q.name.endswith(TRACKING_SUFFIX)
                            ) for q in sibling.iterdir()
                        )
                        if any_match:
                            with open(meta_path, "r", encoding="utf-8") as f:
                                return json.load(f)
                    except Exception:
                        continue
        except Exception:
            pass
        return None

    @staticmethod
    def get_archive_paths(project_name: str, video_hash: str, create: bool = False) -> ArchivePaths:
        """Resolve archive paths for a project/video hash.

        - When create=True, use or create the timestamp-suffixed archive dir for this process.
        - When create=False, resolve the most recent matching archive dir (exact or suffixed), fallback to base name.
        """
        if create:
            project_dir = ResultsArchiver._get_or_create_archive_project_dir(project_name)
        else:
            matches = ResultsArchiver._list_matching_project_dirs(project_name)
            project_dir = matches[0] if matches else (config.ARCHIVES_DIR / project_name)
        video_hash_dir = project_dir / video_hash
        return ArchivePaths(project_dir=project_dir, video_hash_dir=video_hash_dir)

    @staticmethod
    def archive_analysis_files(
        project_name: str,
        video_path: Path,
        scenes_file: Optional[Path] = None,
        audio_file: Optional[Path] = None,
        tracking_file: Optional[Path] = None,
    ) -> Optional[Path]:
        """Copy available analysis files into the archive, keyed by video hash.
        Returns the archive directory on success or None if unsupported.
        """
        try:
            video_hash = ResultsArchiver.compute_video_hash(video_path)
            if not video_hash:
                return None
            ap = ResultsArchiver.get_archive_paths(project_name, video_hash, create=True)
            ap.video_hash_dir.mkdir(parents=True, exist_ok=True)
            video_stem = video_path.stem

            def _copy(src: Optional[Path], suffix: str):
                if src and src.exists():
                    dst = ap.video_hash_dir / f"{video_stem}{suffix}"
                    try:
                        shutil.copy2(src, dst)
                        return str(dst)
                    except Exception as e:
                        logger.warning(f"Failed to archive {src} -> {dst}: {e}")
                return None

            copied = {
                "scenes": _copy(scenes_file, SCENES_SUFFIX),
                "audio": _copy(audio_file, AUDIO_SUFFIX),
                "tracking": _copy(tracking_file, TRACKING_SUFFIX),
            }

            # metadata
            meta = {
                "video_path": str(video_path),
                "video_stem": video_stem,
                "video_hash": video_hash,
                "created_at": datetime.now(timezone.utc).isoformat(),
                "copied": copied,
            }
            with open(ap.video_hash_dir / "metadata.json", "w", encoding="utf-8") as f:
                json.dump(meta, f, ensure_ascii=False, indent=2)

            return ap.video_hash_dir
        except Exception as e:
            logger.error(f"Error archiving analysis files: {e}")
            return None

    @staticmethod
    def find_analysis_file(
        project_name: str,
        video_path: Path,
        suffix: str,
    ) -> Optional[Path]:
        """Resolve an analysis file from archives for this video.
        Prefers exact hash dir; falls back to any file matching the stem in project archives.
        """
        try:
            video_hash = ResultsArchiver.compute_video_hash(video_path)
            video_stem = video_path.stem

            # Try exact-hash directory
            if video_hash:
                ap = ResultsArchiver.get_archive_paths(project_name, video_hash)
                candidate = ap.video_hash_dir / f"{video_stem}{suffix}"
                if candidate.exists():
                    return candidate

            # Fallback: search by stem within project archive
            matches = ResultsArchiver._list_matching_project_dirs(project_name)
            for project_arch in matches:
                for p in project_arch.rglob(f"{video_stem}{suffix}"):
                    return p
        except Exception:
            pass
        return None

    @staticmethod
    def project_has_analysis(project_name: str) -> Tuple[bool, bool, bool]:
        """Check if archives contain any analysis for this project.
        Returns tuple (has_scenes, has_audio, has_tracking).
        """
        matches = ResultsArchiver._list_matching_project_dirs(project_name)
        if not matches:
            return (False, False, False)
        has_scenes = False
        has_audio = False
        has_tracking = False
        for base in matches:
            if not has_scenes:
                has_scenes = any(base.rglob(f"*{SCENES_SUFFIX}"))
            if not has_audio:
                has_audio = any(base.rglob(f"*{AUDIO_SUFFIX}"))
            if not has_tracking:
                has_tracking = any(base.rglob(f"*{TRACKING_SUFFIX}"))
            if has_scenes and has_audio and has_tracking:
                break
        return (has_scenes, has_audio, has_tracking)
```

## File: services/visualization_service.py
```python
"""
Visualization Service
Aggregates and processes data from workflow steps for timeline visualization.
Handles scene detection (Step 3), audio analysis (Step 4), and video tracking (Step 5).
"""

import logging
import json
import csv
import subprocess
import re
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime, timezone
from config.settings import config
from services.results_archiver import ResultsArchiver, SCENES_SUFFIX, AUDIO_SUFFIX, TRACKING_SUFFIX

logger = logging.getLogger(__name__)

class VisualizationService:
    """
    Service for aggregating and processing workflow results for visualization.
    Combines data from multiple steps to create unified timeline data.
    """
    
    @staticmethod
    def get_available_projects() -> Dict[str, Any]:
        """
        Scan for completed projects with visualization data.
        
        Returns:
            Dictionary with available projects:
            {
                "projects": [
                    {
                        "name": str,
                        "path": str,
                        "videos": [str],
                        "has_scenes": bool,
                        "has_audio": bool,
                        "has_tracking": bool
                    }
                ],
                "count": int
            }
        """
        try:
            # Use centralized configuration to resolve project root and archives
            base_path = getattr(config, 'PROJECTS_DIR', None) or (config.BASE_PATH_SCRIPTS / "projets_extraits")
            archives_root = getattr(config, 'ARCHIVES_DIR', None) or (config.BASE_PATH_SCRIPTS / "archives")
            projects_index: Dict[str, Dict[str, Any]] = {}

            def _parse_archive_name(name: str) -> Tuple[str, Optional[str]]:
                """Return (base_name, archive_timestamp) if name matches '<base> YYYY-MM-DD_HH-MM-SS'.
                archive_timestamp is in 'YYYY-MM-DD HH:MM:SS' human-readable format.
                """
                try:
                    m = re.match(r"^(.*) (\d{4}-\d{2}-\d{2})_(\d{2}-\d{2}-\d{2})$", name)
                    if not m:
                        return name, None
                    base = m.group(1)
                    date_part = m.group(2)
                    time_part = m.group(3).replace('-', ':')
                    return base, f"{date_part} {time_part}"
                except Exception:
                    return name, None
            
            # Scan current projects directory (may be empty after Step 7)
            if base_path.exists():
                for project_dir in base_path.iterdir():
                    if not project_dir.is_dir():
                        continue
                    base_name, ts = _parse_archive_name(project_dir.name)
                    projects_index[project_dir.name] = {
                        "name": project_dir.name,
                        "path": str(project_dir),
                        "videos": [],
                        "has_scenes": False,
                        "has_audio": False,
                        "has_tracking": False,
                        "video_count": 0,
                        "source": "projects",
                        "display_base": base_name,
                        "archive_timestamp": ts,
                    }

            # Populate details for projects directory
            for name, proj in list(projects_index.items()):
                project_dir = Path(proj["path"])
                # Find videos in project
                videos: List[str] = []
                video_extensions = ('.mp4', '.mov', '.avi', '.mkv', '.webm')
                for ext in video_extensions:
                    videos.extend([str(v.relative_to(project_dir)) for v in project_dir.rglob(f'*{ext}')])
                has_scenes = any(project_dir.rglob('*_scenes.csv'))
                has_audio = any(project_dir.rglob('*_audio.json'))
                has_tracking = any(project_dir.rglob('*_tracking.json'))
                arch_s, arch_a, arch_t = ResultsArchiver.project_has_analysis(name)
                proj.update({
                    "videos": videos,
                    "has_scenes": has_scenes or arch_s,
                    "has_audio": has_audio or arch_a,
                    "has_tracking": has_tracking or arch_t,
                    "video_count": len(videos)
                })

            # Also include archived-only projects (no current dir present)
            if archives_root.exists():
                for arch_proj_dir in archives_root.iterdir():
                    if not arch_proj_dir.is_dir():
                        continue
                    name = arch_proj_dir.name
                    base_name, ts = _parse_archive_name(name)
                    if name not in projects_index:
                        # Derive original filenames (with extensions when possible) from archived metadata
                        stems: set[str] = set()
                        videos_map: dict[str, str] = {}
                        # Prefer metadata.json inside each video hash directory (contains original video_path)
                        for meta_path in arch_proj_dir.rglob('metadata.json'):
                            try:
                                with open(meta_path, 'r', encoding='utf-8') as mf:
                                    meta = json.load(mf)
                                stem = str(meta.get('video_stem') or '').strip()
                                video_path_str = meta.get('video_path')
                                if not stem:
                                    # Fallback: infer stem from sibling analysis files
                                    sibling_files = list(meta_path.parent.glob('*'))
                                    for q in sibling_files:
                                        qn = q.name
                                        if qn.endswith(SCENES_SUFFIX):
                                            stem = q.stem.replace('_scenes', '')
                                            break
                                        if qn.endswith(AUDIO_SUFFIX):
                                            stem = q.stem.replace('_audio', '')
                                            break
                                        if qn.endswith(TRACKING_SUFFIX):
                                            stem = q.stem.replace('_tracking', '')
                                            break
                                if stem:
                                    if isinstance(video_path_str, str) and video_path_str:
                                        try:
                                            videos_map[stem] = Path(video_path_str).name
                                        except Exception:
                                            videos_map[stem] = stem
                                    else:
                                        videos_map[stem] = stem
                            except Exception:
                                continue
                        # Also scan artifacts in case some entries miss metadata; fill gaps with stems
                        for p in arch_proj_dir.rglob(f'*{SCENES_SUFFIX}'):
                            stems.add(p.stem.replace('_scenes', ''))
                        for p in arch_proj_dir.rglob(f'*{AUDIO_SUFFIX}'):
                            stems.add(p.stem.replace('_audio', ''))
                        for p in arch_proj_dir.rglob(f'*{TRACKING_SUFFIX}'):
                            stems.add(p.stem.replace('_tracking', ''))
                        for s in stems:
                            videos_map.setdefault(s, s)
                        projects_index[name] = {
                            "name": name,
                            "path": str(base_path / name),  # expected original path
                            # Use best-known filename (with extension) when available
                            "videos": sorted(list(videos_map.values())),
                            "has_scenes": any(arch_proj_dir.rglob('*_scenes.csv')),
                            "has_audio": any(arch_proj_dir.rglob('*_audio.json')),
                            "has_tracking": any(arch_proj_dir.rglob('*_tracking.json')),
                            "video_count": len(videos_map),
                            "source": "archives",
                            "display_base": base_name,
                            "archive_timestamp": ts,
                        }
                    else:
                        # If same name was present from projects/, enrich with timestamp if any
                        proj = projects_index[name]
                        proj.setdefault("display_base", base_name)
                        proj.setdefault("archive_timestamp", ts)

            projects = list(projects_index.values())
            # Sort by name
            projects.sort(key=lambda p: p["name"])
            
            return {
                "projects": projects,
                "count": len(projects),
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
            
        except Exception as e:
            logger.error(f"Error scanning projects: {e}")
            return {
                "projects": [],
                "count": 0,
                "error": str(e)
            }
    
    @staticmethod
    def get_project_timeline(project_name: str, video_name: str) -> Dict[str, Any]:
        """
        Get aggregated timeline data for a specific video in a project.
        
        Args:
            project_name: Name of the project directory
            video_name: Name of the video file
            
        Returns:
            Timeline data with scenes, audio, and tracking information
        """
        try:
            # Use centralized configuration to resolve project root
            base_path = getattr(config, 'PROJECTS_DIR', None) or (config.BASE_PATH_SCRIPTS / "projets_extraits")
            project_path = base_path / project_name
            project_exists = project_path.exists()
            
            # video_name may be a relative path or an archive stem when the project no longer exists
            video_path = project_path / video_name
            video_exists = video_path.exists()
            
            # If project/video do not exist (post Step 7), infer stem and continue with archives only
            video_stem = (video_path.stem if video_exists else Path(video_name).stem)
            
            # Load data (project dir first, then archives fallback)
            scenes_data = VisualizationService._load_scenes_data(project_name, project_path, video_path, video_stem)
            audio_data = VisualizationService._load_audio_data(project_name, project_path, video_path, video_stem)
            tracking_data = VisualizationService._load_tracking_data(project_name, project_path, video_path, video_stem)

            # Persist any fresh analysis files found in the project into archives
            try:
                scenes_src = scenes_data.get("source_path") if scenes_data.get("available") else None
                audio_src = audio_data.get("source_path") if audio_data.get("available") else None
                tracking_src = tracking_data.get("source_path") if tracking_data.get("available") else None
                # Only archive from project-origin sources
                def _is_project_src(p: Optional[str]) -> Optional[Path]:
                    if not p:
                        return None
                    pp = Path(p)
                    try:
                        if project_exists:
                            pp.relative_to(project_path)
                        return pp
                    except Exception:
                        return None
                ResultsArchiver.archive_analysis_files(
                    project_name,
                    video_path,
                    scenes_file=_is_project_src(scenes_src),
                    audio_file=_is_project_src(audio_src),
                    tracking_file=_is_project_src(tracking_src),
                )
            except Exception:
                pass
            
            # Get video metadata
            metadata = VisualizationService._get_video_metadata(
                video_path, scenes_data, audio_data, tracking_data
            )

            # Save video metadata to archives when we can compute it from an existing video
            try:
                if video_exists and metadata:
                    ResultsArchiver.save_video_metadata(project_name, video_path, metadata)
            except Exception:
                pass

            # Try to load archived video metadata to extract created_at
            metadata_archive_created_at = None
            try:
                vm = ResultsArchiver.load_video_metadata(project_name, video_path)
                if vm and isinstance(vm, dict):
                    metadata_archive_created_at = vm.get('created_at')
            except Exception:
                pass
            
            return {
                "project_name": project_name,
                "video_name": video_name,
                "metadata": metadata,
                "scenes": scenes_data,
                "audio": audio_data,
                "tracking": tracking_data,
                "archive_probe_source": {
                    "metadata": {
                        "provenance": "project" if video_exists else "archives",
                        "created_at": metadata_archive_created_at
                    },
                    "scenes": {
                        "provenance": scenes_data.get("provenance", "unknown"),
                        "created_at": scenes_data.get("archive_created_at")
                    },
                    "audio": {
                        "provenance": audio_data.get("provenance", "unknown"),
                        "created_at": audio_data.get("archive_created_at")
                    },
                    "tracking": {
                        "provenance": tracking_data.get("provenance", "unknown"),
                        "created_at": tracking_data.get("archive_created_at")
                    }
                },
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
            
        except Exception as e:
            logger.error(f"Error loading timeline for {project_name}/{video_name}: {e}")
            return {"error": str(e)}

    @staticmethod
    def get_project_diagnostics(project_name: str) -> Dict[str, Any]:
        """Return per-video diagnostics for a project: availability and provenance of analysis data.

        Args:
            project_name: Project name

        Returns:
            Dict with list of videos and their data availability/provenance and key metrics.
        """
        try:
            projects = VisualizationService.get_available_projects()
            proj = next((p for p in projects.get("projects", []) if p.get("name") == project_name), None)
            if not proj:
                return {"error": f"Project '{project_name}' not found"}

            results = []
            for video in proj.get("videos", []):
                tl = VisualizationService.get_project_timeline(project_name, video)
                if "error" in tl:
                    results.append({
                        "video_name": video,
                        "error": tl.get("error"),
                    })
                    continue
                md = tl.get("metadata", {})
                sc = tl.get("scenes", {})
                au = tl.get("audio", {})
                tr = tl.get("tracking", {})
                ap = tl.get("archive_probe_source", {})
                results.append({
                    "video_name": video,
                    "availability": {
                        "metadata": bool(md),
                        "scenes": sc.get("available", False),
                        "audio": au.get("available", False),
                        "tracking": tr.get("available", False),
                    },
                    "provenance": {
                        "scenes": sc.get("provenance"),
                        "audio": au.get("provenance"),
                        "tracking": tr.get("provenance"),
                        "metadata": (ap.get("metadata") or {}).get("provenance"),
                    },
                    "metrics": {
                        "fps": md.get("fps"),
                        "total_frames": md.get("total_frames"),
                        "duration_seconds": md.get("duration_seconds"),
                        "scene_count": md.get("scene_count"),
                        "speech_segment_count": md.get("speech_segment_count"),
                        "face_coverage": md.get("face_coverage"),
                    }
                })

            return {
                "project_name": project_name,
                "count": len(results),
                "videos": results,
                "timestamp": datetime.now(timezone.utc).isoformat(),
            }
        except Exception as e:
            logger.error(f"Diagnostics error for project {project_name}: {e}")
            return {"error": str(e)}
    
    @staticmethod
    def _load_scenes_data(project_name: str, project_path: Path, video_path: Path, video_stem: str) -> Dict[str, Any]:
        """Load scene detection data from CSV."""
        # Search recursively to support nested outputs
        scenes_file = next(project_path.rglob(f"{video_stem}{SCENES_SUFFIX}"), None)
        # Fallback: some pipelines save scenes as plain '<stem>.csv'
        if scenes_file is None:
            scenes_file = next(project_path.rglob(f"{video_stem}.csv"), None)
        
        provenance = "project"
        archive_created_at = None
        # Fallback to archives
        if scenes_file is None or not scenes_file.exists():
            scenes_file = ResultsArchiver.find_analysis_file(project_name, video_path, SCENES_SUFFIX)
            if scenes_file is None:
                return {"available": False, "scenes": []}
            provenance, archive_created_at = VisualizationService._provenance_info(Path(scenes_file))
        else:
            provenance = "project"
        
        try:
            scenes = []
            with open(scenes_file, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    scenes.append({
                        "scene_number": int(row.get("Scene Number", 0)),
                        "start_frame": int(row.get("Start Frame", 0)),
                        "end_frame": int(row.get("End Frame", 0)),
                        "start_timecode": row.get("Start Timecode", "00:00:00.000"),
                        "end_timecode": row.get("End Timecode", "00:00:00.000"),
                        "start_time_seconds": VisualizationService._timecode_to_seconds(
                            row.get("Start Timecode", "00:00:00.000")
                        ),
                        "end_time_seconds": VisualizationService._timecode_to_seconds(
                            row.get("End Timecode", "00:00:00.000")
                        )
                    })
            
            return {
                "available": True,
                "count": len(scenes),
                "scenes": scenes,
                "source_path": str(scenes_file),
                "provenance": provenance,
                "archive_created_at": archive_created_at
            }
            
        except Exception as e:
            logger.error(f"Error loading scenes data: {e}")
            return {"available": False, "error": str(e), "scenes": []}
    
    @staticmethod
    def _load_audio_data(project_name: str, project_path: Path, video_path: Path, video_stem: str) -> Dict[str, Any]:
        """Load audio analysis data from JSON."""
        # Search recursively to support nested outputs
        audio_file = next(project_path.rglob(f"{video_stem}{AUDIO_SUFFIX}"), None)
        
        provenance = "project"
        archive_created_at = None
        # Fallback to archives
        if audio_file is None or not audio_file.exists():
            audio_file = ResultsArchiver.find_analysis_file(project_name, video_path, AUDIO_SUFFIX)
            if audio_file is None:
                return {"available": False, "segments": []}
            provenance, archive_created_at = VisualizationService._provenance_info(Path(audio_file))
        else:
            provenance = "project"
        
        try:
            with open(audio_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # Extract speech segments
            segments = []
            fps = data.get("fps", 25.0)
            max_frame_seen = 0
            
            # Group consecutive speech frames
            current_segment = None
            
            for frame_data in data.get("frames_analysis", []):
                frame = frame_data.get("frame", 0)
                audio_info = frame_data.get("audio_info", {})
                is_speech = audio_info.get("is_speech_present", False)
                speakers = audio_info.get("active_speaker_labels", [])
                time_sec = audio_info.get("timecode_sec", (frame - 1) / fps)
                if isinstance(frame, (int, float)):
                    try:
                        max_frame_seen = max(max_frame_seen, int(frame))
                    except Exception:
                        pass
                
                if is_speech:
                    if current_segment is None:
                        current_segment = {
                            "start_frame": frame,
                            "start_time": time_sec,
                            "speakers": set(speakers)
                        }
                    else:
                        current_segment["speakers"].update(speakers)
                else:
                    if current_segment is not None:
                        # End current segment
                        segments.append({
                            "start_frame": current_segment["start_frame"],
                            "end_frame": frame - 1,
                            "start_time": current_segment["start_time"],
                            "end_time": time_sec,
                            "speakers": sorted(list(current_segment["speakers"])),
                            "speaker_count": len(current_segment["speakers"])
                        })
                        current_segment = None
            
            # Close last segment if open
            if current_segment is not None:
                last_frame = data.get("total_frames") or max_frame_seen
                segments.append({
                    "start_frame": current_segment["start_frame"],
                    "end_frame": last_frame,
                    "start_time": current_segment["start_time"],
                    "end_time": last_frame / fps,
                    "speakers": sorted(list(current_segment["speakers"])),
                    "speaker_count": len(current_segment["speakers"])
                })
            
            # Get unique speakers
            all_speakers = set()
            for seg in segments:
                all_speakers.update(seg["speakers"])
            
            total_frames = data.get("total_frames") or max_frame_seen
            if not total_frames and segments:
                try:
                    total_frames = max(int(s.get("end_frame", 0)) for s in segments)
                except Exception:
                    total_frames = 0

            return {
                "available": True,
                "fps": fps,
                "total_frames": int(total_frames or 0),
                "segments": segments,
                "segment_count": len(segments),
                "unique_speakers": sorted(list(all_speakers)),
                "speaker_count": len(all_speakers),
                "source_path": str(audio_file),
                "provenance": provenance,
                "archive_created_at": archive_created_at
            }
            
        except Exception as e:
            logger.error(f"Error loading audio data: {e}")
            return {"available": False, "error": str(e), "segments": []}
    
    @staticmethod
    def _load_tracking_data(project_name: str, project_path: Path, video_path: Path, video_stem: str) -> Dict[str, Any]:
        """Load video tracking data from JSON."""
        # Search recursively to support nested outputs
        tracking_file = next(project_path.rglob(f"{video_stem}{TRACKING_SUFFIX}"), None)
        # Fallback: some pipelines save tracking as plain '<stem>.json'
        if tracking_file is None:
            tracking_file = next(project_path.rglob(f"{video_stem}.json"), None)
        
        provenance = "project"
        archive_created_at = None
        # Fallback to archives
        if tracking_file is None or not tracking_file.exists():
            tracking_file = ResultsArchiver.find_analysis_file(project_name, video_path, TRACKING_SUFFIX)
            if tracking_file is None:
                return {"available": False, "summary": {}}
            provenance, archive_created_at = VisualizationService._provenance_info(Path(tracking_file))
        else:
            provenance = "project"
        
        try:
            with open(tracking_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # Calculate summary statistics
            total_frames = data.get("total_frames", 0)
            fps = data.get("fps", 25.0)
            frames_with_faces = 0
            frames_with_speaking = 0
            max_faces = 0
            max_frame_seen = 0
            
            for frame_data in data.get("frames_analysis", []):
                frame = frame_data.get("frame")
                if isinstance(frame, (int, float)):
                    try:
                        max_frame_seen = max(max_frame_seen, int(frame))
                    except Exception:
                        pass

                # Support two schemas:
                # 1) faces_data: [{ is_speaking: bool, ... }]
                # 2) tracked_objects: [{ label: "face", active_speakers: [...], ... }]
                faces_list = frame_data.get("faces_data")
                if faces_list is None:
                    tracked = frame_data.get("tracked_objects", []) or []
                    faces_list = [o for o in tracked if str(o.get("label", "")).lower() == "face"]

                face_count = len(faces_list)
                if face_count > 0:
                    frames_with_faces += 1
                    max_faces = max(max_faces, face_count)

                    # Speaking detection across schemas
                    speaking = any(
                        (
                            # legacy schema
                            (isinstance(f, dict) and f.get("is_speaking", False)) or
                            # tracked_objects schema: any active_speakers present
                            (isinstance(f, dict) and bool(f.get("active_speakers")))
                        )
                        for f in faces_list
                    )
                    if speaking:
                        frames_with_speaking += 1
            
            if (not total_frames or total_frames == 0) and max_frame_seen:
                total_frames = max_frame_seen
            coverage_percent = (frames_with_faces / total_frames * 100) if total_frames > 0 else 0
            
            return {
                "available": True,
                "fps": fps,
                "total_frames": total_frames,
                "summary": {
                    "frames_with_faces": frames_with_faces,
                    "frames_with_speaking": frames_with_speaking,
                    "max_faces_detected": max_faces,
                    "face_coverage_percent": round(coverage_percent, 2)
                },
                "source_path": str(tracking_file),
                "provenance": provenance,
                "archive_created_at": archive_created_at
            }
            
        except Exception as e:
            logger.error(f"Error loading tracking data: {e}")
            return {"available": False, "error": str(e), "summary": {}}

    @staticmethod
    def _provenance_info(file_path: Path) -> (str, Optional[str]):
        """Return (provenance, created_at) for a given analysis file.
        If file is under ARCHIVES_DIR, provenance='archives' and created_at read from sibling metadata.json.
        Otherwise provenance='project', created_at=None.
        """
        try:
            arch_root = config.ARCHIVES_DIR.resolve()
            fp = file_path.resolve()
            try:
                fp.relative_to(arch_root)
                # archived
                meta_path = fp.parent / 'metadata.json'
                if meta_path.exists():
                    with open(meta_path, 'r', encoding='utf-8') as mf:
                        meta = json.load(mf)
                        return 'archives', meta.get('created_at')
                return 'archives', None
            except Exception:
                return 'project', None
        except Exception as e:
            logger.error(f"Error getting provenance info: {e}")
            return 'unknown', None
    
    @staticmethod
    def _get_video_metadata(
        video_path: Path, 
        scenes_data: Dict, 
        audio_data: Dict, 
        tracking_data: Dict
    ) -> Dict[str, Any]:
        """Extract video metadata and combine with analysis data."""
        try:
            # Prefer FPS/frames from analysis data when present
            fps = audio_data.get("fps") or tracking_data.get("fps")
            total_frames = audio_data.get("total_frames") or tracking_data.get("total_frames")

            duration_seconds = 0.0

            # Derive duration from scenes if available
            if scenes_data.get("available") and scenes_data.get("scenes"):
                try:
                    duration_seconds = max(s.get("end_time_seconds", 0) for s in scenes_data["scenes"]) or 0.0
                except Exception:
                    pass

            # If duration or fps/frames still missing, probe the video file directly (if it exists)
            if (not fps or not total_frames or duration_seconds == 0.0) and video_path.exists():
                probed = VisualizationService._probe_video_file(video_path)
                if probed:
                    probed_fps = probed.get("fps")
                    probed_duration = probed.get("duration")
                    if not fps and probed_fps:
                        fps = probed_fps
                    if duration_seconds == 0.0 and probed_duration:
                        duration_seconds = probed_duration
                    if not total_frames and fps and duration_seconds:
                        total_frames = int(round(fps * duration_seconds))

            # Final fallbacks
            if not fps:
                fps = 25.0
            if not total_frames and duration_seconds and fps:
                total_frames = int(round(fps * duration_seconds))
            if not duration_seconds and total_frames and fps:
                duration_seconds = total_frames / fps
            
            return {
                "filename": video_path.name if video_path.name else "(archive)",
                "fps": fps,
                "total_frames": total_frames,
                "duration_seconds": round(duration_seconds, 2),
                "duration_formatted": VisualizationService._seconds_to_timecode(duration_seconds),
                "has_scenes": scenes_data.get("available", False),
                "has_audio": audio_data.get("available", False),
                "has_tracking": tracking_data.get("available", False),
                "scene_count": scenes_data.get("count", 0),
                "speech_segment_count": audio_data.get("segment_count", 0),
                "face_coverage": tracking_data.get("summary", {}).get("face_coverage_percent", 0)
            }
            
        except Exception as e:
            logger.error(f"Error extracting metadata: {e}")
            return {}

    @staticmethod
    def _probe_video_file(video_path: Path) -> Optional[Dict[str, float]]:
        """Probe a video file using ffprobe (fallback to OpenCV) to get fps and duration.
        Returns a dict like {"fps": float|None, "duration": float|None} or None on failure.
        """
        try:
            # Try ffprobe first
            cmd = [
                'ffprobe', '-v', 'error',
                '-select_streams', 'v:0',
                '-show_entries', 'stream=avg_frame_rate,duration',
                '-of', 'json', str(video_path)
            ]
            result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, timeout=10)
            if result.returncode == 0 and result.stdout:
                data = json.loads(result.stdout)
                streams = data.get('streams') or []
                if streams:
                    stream = streams[0]
                    fps = None
                    afr = stream.get('avg_frame_rate')
                    if afr and afr != '0/0':
                        try:
                            num, den = afr.split('/')
                            num, den = float(num), float(den)
                            if den != 0:
                                fps = num / den
                        except Exception:
                            pass
                    duration = None
                    try:
                        duration_val = stream.get('duration')
                        if duration_val is not None:
                            duration = float(duration_val)
                    except Exception:
                        pass
                    return {"fps": fps, "duration": duration}
        except Exception:
            pass
        # Fallback to OpenCV if available
        try:
            import cv2  # type: ignore
            cap = cv2.VideoCapture(str(video_path))
            if cap.isOpened():
                fps = cap.get(cv2.CAP_PROP_FPS) or None
                frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT) or None
                cap.release()
                duration = None
                if fps and frame_count:
                    duration = float(frame_count) / float(fps)
                return {"fps": float(fps) if fps else None, "duration": duration}
        except Exception:
            pass
        return None
    
    @staticmethod
    def _timecode_to_seconds(timecode: str) -> float:
        """Convert timecode (HH:MM:SS.mmm) to seconds."""
        try:
            parts = timecode.split(':')
            if len(parts) != 3:
                return 0.0
            
            hours = int(parts[0])
            minutes = int(parts[1])
            seconds = float(parts[2])
            
            return hours * 3600 + minutes * 60 + seconds
            
        except Exception:
            return 0.0
    
    @staticmethod
    def _seconds_to_timecode(seconds: float) -> str:
        """Convert seconds to timecode (HH:MM:SS.mmm)."""
        try:
            hours = int(seconds // 3600)
            minutes = int((seconds % 3600) // 60)
            secs = seconds % 60
            
            return f"{hours:02d}:{minutes:02d}:{secs:06.3f}"
            
        except Exception:
            return "00:00:00.000"
```

## File: services/webhook_service.py
```python
"""
WebhookService
Provides a simple external JSON-based source of download links.
Respects project standards: service-layer only, robust error handling, caching, and status reporting.

Public methods:
- fetch_records(): List[Dict[str, str]] of {url, timestamp, source, url_type}
- get_service_status(): Dict[str, Any]
"""
from __future__ import annotations

from datetime import datetime
import logging
import time
from typing import Any, Dict, List, Optional

import requests

from config.settings import config

logger = logging.getLogger(__name__)

# In-memory cache
_cache_data: Optional[List[Dict[str, str]]] = None
_cache_fetched_at: float = 0.0
_last_error: Optional[str] = None
_last_status: Dict[str, Any] = {
    "available": False,
    "last_fetch_ts": None,
    "error": None,
    "records": 0,
}


def _classify_url_type(url: str) -> str:
    try:
        u = (url or "").lower()
        if "fromsmash.com" in u:
            return "fromsmash"
        if "swisstransfer.com" in u:
            return "swisstransfer"
        if "dropbox.com" in u or "dl.dropboxusercontent.com" in u:
            return "dropbox"
        # R2/Worker proxy URLs for Dropbox (example: https://<host>.workers.dev/dropbox/<...>/file)
        if "/dropbox/" in u and ("workers.dev" in u or "worker" in u):
            return "dropbox"
        return "external"
    except Exception:
        return "external"


def _normalize_timestamp(ts: Optional[str]) -> Optional[str]:
    """Best-effort normalization: accept ISO or 'YYYY-MM-DD HH:MM:SS'.
    Leave unchanged if parsing is uncertain; CSVService will also handle.
    """
    if not ts:
        return None
    try:
        dt = datetime.fromisoformat(str(ts).replace('Z', '+00:00'))
        if not dt.tzinfo:
            return str(ts)
        return dt.astimezone().strftime('%Y-%m-%d %H:%M:%S')
    except Exception:
        return str(ts)


def _validate_and_format(items: Any) -> List[Dict[str, str]]:
    """Validate raw JSON and return standardized rows.

    Expected input example:
    [
        {"url": "https://...", "timestamp": "2025-10-17T12:34:13+0200", "source": "webhook"}
    ]
    """
    rows: List[Dict[str, str]] = []
    if not isinstance(items, list):
        return rows
    for it in items:
        try:
            if not isinstance(it, dict):
                continue
            # New webhook schema: pre-normalized urls + metadata
            source_url = str(it.get("source_url") or "").strip()
            r2_url = str(it.get("r2_url") or "").strip()
            original_filename = str(it.get("original_filename") or "").strip()
            provider = str(it.get("provider") or "").strip()
            created_at = _normalize_timestamp(it.get("created_at"))

            if r2_url or source_url:
                preferred_url = r2_url or source_url
                fallback_url = source_url if (r2_url and source_url and source_url != r2_url) else ""
                url_type = (provider.lower() if provider else _classify_url_type(preferred_url))
                row_new: Dict[str, str] = {
                    "url": preferred_url,
                    "fallback_url": fallback_url,
                    "original_filename": original_filename,
                    "provider": provider,
                    "timestamp": created_at or "",
                    "source": "webhook",
                    "url_type": url_type,
                }
                rows.append(row_new)
                continue

            # Legacy webhook schema
            url = str(it.get("url") or "").strip()
            if not url:
                continue
            ts = _normalize_timestamp(it.get("timestamp"))
            src = str(it.get("source") or "webhook").strip() or "webhook"
            url_type = str(it.get("url_type") or "").strip() or _classify_url_type(url)
            row_legacy: Dict[str, str] = {
                "url": url,
                "timestamp": ts or "",
                "source": src,
                "url_type": url_type,
            }
            rows.append(row_legacy)
        except Exception as e:
            logger.debug(f"WebhookService: skipping invalid item due to error: {e}")
            continue
    return rows


def fetch_records() -> Optional[List[Dict[str, str]]]:
    """Fetch and cache webhook JSON records.

    Returns None on hard error; returns [] if no data.
    Respects WEBHOOK_CACHE_TTL for caching.
    """
    global _cache_data, _cache_fetched_at, _last_error, _last_status
    now = time.time()

    # Use cache if within TTL
    if _cache_data is not None and (now - _cache_fetched_at) < max(0, config.WEBHOOK_CACHE_TTL):
        return _cache_data

    url = config.WEBHOOK_JSON_URL
    timeout = max(1, config.WEBHOOK_TIMEOUT)

    # Simple retry: up to 2 retries (total 3 attempts) with small backoff
    attempts = 3
    backoff = 1.0
    last_exc: Optional[Exception] = None

    for attempt in range(1, attempts + 1):
        try:
            resp = requests.get(url, timeout=timeout)
            resp.raise_for_status()
            data = resp.json()
            rows = _validate_and_format(data)
            _cache_data = rows
            _cache_fetched_at = now
            _last_error = None
            _last_status.update({
                "available": True,
                "last_fetch_ts": time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(now)),
                "error": None,
                "records": len(rows),
            })
            return rows
        except Exception as e:
            last_exc = e
            _last_error = str(e)
            _last_status.update({
                "available": False,
                "last_fetch_ts": time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(now)),
                "error": _last_error,
            })
            if attempt < attempts:
                time.sleep(backoff)
                backoff *= 2
            else:
                logger.error(f"WebhookService: failed to fetch webhook JSON after {attempts} attempts: {e}")
                return None


def get_service_status() -> Dict[str, Any]:
    """Return last known status for observability."""
    # Shallow copy to avoid mutation
    return dict(_last_status)
```

## File: services/workflow_service.py
```python
import logging
import os
import threading
import time
from collections import deque
from datetime import datetime, timezone
from typing import Dict, Any, List, Optional, Tuple
from pathlib import Path

from config.settings import config
from services.workflow_state import get_workflow_state

logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)


class WorkflowService:
    """
    Centralized service for workflow management.

    This service follows the Service & Blueprint Architecture pattern by:
    - Using app_new.py as the single source of truth for state
    - Providing clean, stateless methods for workflow operations
    - Implementing proper error handling and fallback mechanisms
    - Maintaining thread-safe operations through proper state access
    """

    _initialized = False
    _initialization_lock = threading.Lock()

    @staticmethod
    def initialize(commands_config: Dict[str, Any]) -> None:
        """
        Initialize the workflow service.

        This method primarily serves as a confirmation that the service is loaded,
        as state is managed via WorkflowState singleton.

        Args:
            commands_config: Dict[str, Any] - Dictionary of step configurations (informational only)
        """
        with WorkflowService._initialization_lock:
            if WorkflowService._initialized:
                logger.debug("[WorkflowService] Already initialized, skipping")
                return

            try:
                workflow_state = get_workflow_state()
                if workflow_state:
                    WorkflowService._initialized = True
                    logger.info(f"Workflow service initialized, linked to WorkflowState singleton with {len(commands_config)} steps.")
                else:
                    raise RuntimeError("WorkflowState singleton not available")
            except Exception as e:
                logger.error(f"Workflow service initialization failed: {e}")
                raise RuntimeError("WorkflowService cannot initialize without access to WorkflowState") from e

    @staticmethod
    def _get_workflow_state():
        return get_workflow_state()
    
    @staticmethod
    def get_step_status(step_key: str, include_logs: bool = False) -> Dict[str, Any]:
        """
        Get current status of a workflow step from the single source of truth.

        Args:
            step_key: Step identifier
            include_logs: Whether to include log entries

        Returns:
            Step status dictionary

        Raises:
            ValueError: If step_key is not found
            RuntimeError: If workflow state is not available
        """
        workflow_state = WorkflowService._get_workflow_state()

        if not workflow_state:
            raise RuntimeError("WorkflowService cannot access workflow state")

        info = workflow_state.get_step_info(step_key)
        if not info:
            raise ValueError(f"Step '{step_key}' not found or not initialized")

        current_sequence_state = workflow_state.is_sequence_running()

        from config.workflow_commands import WorkflowCommandsConfig
        config_instance = WorkflowCommandsConfig()
        step_display_name = config_instance.get_step_display_name(step_key) or step_key

        result = {
            "step": step_key,
            "display_name": step_display_name,
            "status": info['status'],
            "return_code": info['return_code'],
            "progress_current": info['progress_current'],
            "progress_total": info['progress_total'],
            "progress_current_fractional": info.get('progress_current_fractional'),
            "progress_text": info['progress_text'],
            "duration_str": info.get('duration_str', None),
            "is_any_sequence_running": current_sequence_state
        }

        if include_logs:
            result["log"] = info.get('log', [])

        return result

    @staticmethod
    def get_step_log_file(step_key: str, log_index: int) -> Dict[str, Any]:
        from config.workflow_commands import WorkflowCommandsConfig

        config_instance = WorkflowCommandsConfig()
        step_config = config_instance.get_step_config(step_key)
        if not step_config:
            raise ValueError("Step not found")

        specific_logs_config = step_config.get("specific_logs", [])
        if not (0 <= log_index < len(specific_logs_config)):
            raise ValueError("Log index out of range")

        log_conf = specific_logs_config[log_index]
        log_type = log_conf["type"]
        num_lines = log_conf.get("lines", 50)

        content = ""
        error_msg = None
        processed_path_str = "N/A"

        def _read_tail(path: Path, lines: int) -> str:
            with open(path, 'r', encoding='utf-8', errors='ignore') as f:
                tail = deque(f, maxlen=lines)
            if not tail:
                return "Fichier vide."
            return ''.join(tail)

        if log_type == "file":
            log_path = Path(log_conf["path"])
            processed_path_str = str(log_path)
            if not log_path.exists():
                error_msg = "Fichier non trouvé"
                content = f"Fichier non trouvé: {log_path}"
            else:
                try:
                    content = _read_tail(log_path, int(num_lines))
                except Exception as e:
                    error_msg = f"Erreur de lecture: {e}"
                    content = f"Impossible de lire le fichier: {e}"

        elif log_type == "directory_latest":
            log_dir = Path(log_conf["path"])
            pattern = log_conf.get("pattern", "*.log")
            processed_path_str = f"{log_dir}/{pattern}"

            if not (log_dir.exists() and log_dir.is_dir()):
                error_msg = "Répertoire non trouvé"
                content = f"Répertoire non trouvé: {log_dir}"
                processed_path_str = str(log_dir)
            else:
                try:
                    matching_files = [p for p in log_dir.glob(pattern) if p.is_file()]
                    if not matching_files:
                        error_msg = "Aucun fichier trouvé"
                        content = f"Aucun fichier correspondant au pattern '{pattern}' dans {log_dir}"
                    else:
                        latest_path = max(matching_files, key=lambda p: p.stat().st_mtime)
                        processed_path_str = str(latest_path)
                        try:
                            content = _read_tail(latest_path, int(num_lines))
                        except Exception as e:
                            error_msg = f"Erreur de lecture: {e}"
                            content = f"Impossible de lire le fichier: {e}"
                except Exception as e:
                    error_msg = f"Erreur de recherche: {e}"
                    content = f"Erreur lors de la recherche de fichiers: {e}"

        else:
            error_msg = f"Type de log non supporté: {log_type}"
            content = f"Type de log '{log_type}' non supporté"

        return {
            "content": content,
            "error": error_msg,
            "path": processed_path_str,
            "type": log_type,
            "lines_requested": num_lines,
        }
    
    @staticmethod
    def run_step(step_key: str) -> Dict[str, str]:
        """
        Execute a single workflow step.

        Args:
            step_key: Step identifier

        Returns:
            Result dictionary with status and message
        """
                
        workflow_state = WorkflowService._get_workflow_state()
        if not workflow_state:
            return {"status": "error", "message": "WorkflowState not available."}

                
        from config.workflow_commands import WorkflowCommandsConfig
        config_instance = WorkflowCommandsConfig()
        
                
        if not config_instance.validate_step_key(step_key):
            return {"status": "error", "message": "Étape inconnue"}
        
        step_display_name = config_instance.get_step_display_name(step_key)

                
        projects_dir = config.BASE_PATH_SCRIPTS / 'projets_extraits'
        projects_dir.mkdir(parents=True, exist_ok=True)

                
        if workflow_state.is_sequence_running():
            return {
                "status": "error",
                "message": "Une séquence de workflow est en cours. Veuillez attendre."
            }

                
        if workflow_state.is_step_running(step_key):
            return {
                "status": "error",
                "message": f"'{step_display_name}' est déjà en cours."
            }

                
        import sys
        if 'app_new' not in sys.modules:
            return {"status": "error", "message": "Execution context not available"}
        
        app_new = sys.modules['app_new']
        if not hasattr(app_new, 'run_process_async'):
            logger.error("run_process_async not available in app_new")
            return {"status": "error", "message": "Execution function not available"}

        try:
            thread = threading.Thread(
                target=app_new.run_process_async,
                args=(step_key,)
            )
            thread.daemon = True
            thread.start()
        except Exception as e:
            logger.error(f"Error starting step execution thread: {e}")
            return {"status": "error", "message": f"Failed to start step execution: {str(e)}"}
        
        return {
            "status": "initiated",
            "message": f"Lancement de: {step_display_name}"
        }
    
    @staticmethod
    def run_custom_sequence(steps: List[str]) -> Dict[str, str]:
        """
        Execute a custom sequence of workflow steps.

        Args:
            steps: List of step identifiers

        Returns:
            Result dictionary with status and message
        """
                
        workflow_state = WorkflowService._get_workflow_state()
        if not workflow_state:
            return {"status": "error", "message": "WorkflowState not available."}

        from config.workflow_commands import WorkflowCommandsConfig
        config_instance = WorkflowCommandsConfig()

                
        if workflow_state.is_sequence_running():
            return {
                "status": "error",
                "message": "Une autre séquence de workflow est déjà en cours."
            }

                
        for step_key in steps:
            if not config_instance.validate_step_key(step_key):
                return {"status": "error", "message": f"Étape inconnue : {step_key}"}

                
        import sys
        if 'app_new' not in sys.modules:
            return {"status": "error", "message": "Execution context not available"}
        
        app_new = sys.modules['app_new']
        if not hasattr(app_new, 'execute_step_sequence_worker'):
            logger.error("execute_step_sequence_worker not available in app_new")
            return {"status": "error", "message": "Sequence execution function not available"}

        try:
            thread = threading.Thread(
                target=app_new.execute_step_sequence_worker,
                args=(steps, "Custom")
            )
            thread.daemon = True
            thread.start()
        except Exception as e:
            logger.error(f"Error starting sequence execution thread: {e}")
            return {"status": "error", "message": f"Failed to start sequence execution: {str(e)}"}

        return {
            "status": "initiated",
            "message": f"Séquence personnalisée lancée avec {len(steps)} étapes."
        }

    @staticmethod
    def stop_step(step_key: str) -> Dict[str, str]:
        """
        Stop a running workflow step.

        Args:
            step_key: Step identifier

        Returns:
            Result dictionary with status and message

        Raises:
            ValueError: If step_key is not found
            RuntimeError: If workflow state is not available
        """
        workflow_state = WorkflowService._get_workflow_state()
        if not workflow_state:
            raise RuntimeError("WorkflowService cannot access workflow state")

                
        info = workflow_state.get_step_info(step_key)
        if not info:
            raise ValueError(f"Step '{step_key}' not found")

        if info['status'] not in ['running', 'starting']:
            return {
                "status": "error",
                "message": f"Step '{step_key}' is not running"
            }

                
        process = workflow_state.get_step_process(step_key)
        
        if process:
            try:
                process.terminate()
                                
                workflow_state.update_step_info(
                    step_key,
                    status='stopped',
                    return_code=-1
                )

                return {
                    "status": "success",
                    "message": f"Step '{step_key}' stopped successfully"
                }
            except Exception as e:
                logger.error(f"Error stopping step {step_key}: {e}")
                return {
                    "status": "error",
                    "message": f"Failed to stop step: {str(e)}"
                }
        else:
            return {
                "status": "error",
                "message": f"No process found for step '{step_key}'"
            }
    
    @staticmethod
    def get_sequence_status() -> Dict[str, Any]:
        """
        Get current sequence execution status.
        
        Returns:
            Sequence status dictionary
        """
        workflow_state = WorkflowService._get_workflow_state()
        if not workflow_state:
            return {
                "is_running": False,
                "current_step": None,
                "progress": {"current": 0, "total": 0},
                "last_outcome": {"status": "unknown"},
                "error": "Unable to access workflow state"
            }
        
                
        is_running = workflow_state.is_sequence_running()
        
                
        current_step = None
        all_steps_info = workflow_state.get_all_steps_info()
        for step_key, info in all_steps_info.items():
            if info['status'] in ['running', 'starting']:
                current_step = step_key
                break
        
                
        total_steps = len(all_steps_info)
        completed_steps = sum(
            1 for info in all_steps_info.values()
            if info['status'] == 'completed'
        )
        
        return {
            "is_running": is_running,
            "current_step": current_step,
            "progress": {
                "current": completed_steps,
                "total": total_steps
            },
            "last_outcome": workflow_state.get_sequence_outcome()
        }
    
    @staticmethod
    def stop_sequence() -> Dict[str, str]:
        """
        Stop the currently running sequence.
        
        Returns:
            Result dictionary with status and message
        """
        workflow_state = WorkflowService._get_workflow_state()
        if not workflow_state:
            return {
                "status": "error",
                "message": "Unable to access workflow state"
            }
        
                
        if not workflow_state.is_sequence_running():
            return {
                "status": "error",
                "message": "Aucune séquence en cours d'exécution"
            }
        
                
        workflow_state.complete_sequence(
            success=False,
            message="Sequence manually stopped by user"
        )
        
                
        stopped_steps = []
        all_steps_info = workflow_state.get_all_steps_info()
        for step_key, info in all_steps_info.items():
            if info['status'] in ['running', 'starting']:
                try:
                    process = workflow_state.get_step_process(step_key)
                    if process:
                        process.terminate()
                        workflow_state.update_step_info(
                            step_key,
                            status='stopped',
                            return_code=-1
                        )
                        stopped_steps.append(step_key)
                except Exception as e:
                    logger.error(f"Error stopping step {step_key}: {e}")
        
        return {
            "status": "success",
            "message": f"Sequence stopped. Stopped steps: {', '.join(stopped_steps) if stopped_steps else 'none'}"
        }
    
    @staticmethod
    def get_current_workflow_status_summary() -> Dict[str, Any]:
        """
        Get workflow status summary for remote servers.

        Returns:
            Workflow status summary
        """
        workflow_state = WorkflowService._get_workflow_state()
        if not workflow_state:
            return {
                "is_sequence_running": False,
                "steps": {},
                "last_sequence_outcome": {"status": "unknown"},
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "error": "Unable to access workflow state"
            }

        try:
                    
            sequence_running = workflow_state.is_sequence_running()

                        
            steps_summary = {}
            all_steps_info = workflow_state.get_all_steps_info()
            for step_key, info in all_steps_info.items():
                steps_summary[step_key] = {
                    "status": info['status'],
                    "progress_current": info['progress_current'],
                    "progress_total": info['progress_total'],
                    "return_code": info['return_code']
                }

            return {
                "is_sequence_running": sequence_running,
                "steps": steps_summary,
                "last_sequence_outcome": workflow_state.get_sequence_outcome(),
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
        except Exception as e:
            logger.error(f"Error getting workflow status summary: {e}")
            return {
                "is_sequence_running": False,
                "steps": {},
                "last_sequence_outcome": {"status": "error"},
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "error": f"Failed to get status: {str(e)}"
            }
    
    @staticmethod
    def prepare_step_execution(step_key: str, process_info: Dict, commands_config: Dict) -> Tuple[List[str], str]:
        """Prepare step configuration and command for execution.
        
        Args:
            step_key: Step identifier
            process_info: Process information dictionary
            commands_config: Commands configuration dictionary
            
        Returns:
            Tuple of (command_list, working_directory)
        """
        step_config = commands_config[step_key]
        
                
        process_info['status'] = 'starting'
        process_info['log'].clear()
        process_info['log'].append(f"--- Lancement de: {step_config['display_name']} ---\n")
        process_info['return_code'] = None
        process_info['progress_current'] = 0
        process_info['progress_total'] = 0
        process_info['progress_text'] = ''
        process_info['start_time_epoch'] = time.time()
        
                
        cmd_list = [str(c) for c in step_config['cmd']]
        cwd = step_config['cwd']
        
        logger.info(f"{step_key} preparation complete: {step_config['display_name']}")
        
        return cmd_list, cwd
    
    @staticmethod
    def prepare_tracking_step(base_path: Path, keyword: str, subdir: str) -> Optional[List[str]]:
        """Prepare tracking step (STEP5) by finding videos to process.
        
        Args:
            base_path: Base path for video search
            keyword: Keyword filter for videos
            subdir: Subdirectory filter
            
        Returns:
            List of video paths to process, or None if no videos found
        """
        from services.filesystem_service import FilesystemService
        
        logger.info("STEP5: Searching for videos requiring tracking...")
        
        videos_to_process = FilesystemService.find_videos_for_tracking(
            base_path,
            keyword,
            subdir
        )
        
        if not videos_to_process:
            logger.info("STEP5: No videos to process (all have existing JSON)")
            return None
        
        logger.info(f"STEP5: Found {len(videos_to_process)} videos needing tracking")
        return videos_to_process
    
    @staticmethod
    def create_tracking_temp_file(videos: List[str]) -> Path:
        """Create temporary file with list of videos for tracking.
        
        Writes a JSON array of video paths directly (not wrapped in an object).
        This format is expected by run_tracking_manager.py.
        
        Args:
            videos: List of video file paths
            
        Returns:
            Path to temporary file containing JSON array of video paths
        """
        import tempfile
        import json
        
        temp_fd, temp_path = tempfile.mkstemp(suffix='.json', prefix='tracking_videos_')
        temp_file = Path(temp_path)
        
        try:
            with open(temp_file, 'w', encoding='utf-8') as f:
                json.dump(videos, f, indent=2)
            
            os.close(temp_fd)
            logger.info(f"Created tracking temp file: {temp_file}")
            return temp_file
            
        except Exception as e:
            os.close(temp_fd)
            logger.error(f"Failed to create tracking temp file: {e}")
            raise
    
    @staticmethod
    def calculate_step_duration(start_time_epoch: Optional[float]) -> str:
        """Calculate duration string for a step.
        
        Args:
            start_time_epoch: Start time as epoch timestamp
            
        Returns:
            Duration string (e.g., "2m 30s")
        """
        if start_time_epoch is None:
            return "N/A"
        
        try:
            elapsed = time.time() - start_time_epoch
            if elapsed < 60:
                return f"{int(elapsed)}s"
            elif elapsed < 3600:
                minutes = int(elapsed // 60)
                seconds = int(elapsed % 60)
                return f"{minutes}m {seconds}s"
            else:
                hours = int(elapsed // 3600)
                minutes = int((elapsed % 3600) // 60)
                return f"{hours}h {minutes}m"
        except Exception as e:
            logger.warning(f"Failed to calculate duration: {e}")
            return "N/A"
    
    @staticmethod
    def format_duration_seconds(total_seconds: float) -> str:
        """Format duration in seconds to human-readable string.
        
        Args:
            total_seconds: Duration in seconds
            
        Returns:
            Formatted duration string
        """
        if total_seconds < 0:
            return "N/A"
        
        try:
            if total_seconds < 60:
                return f"{int(total_seconds)}s"
            elif total_seconds < 3600:
                minutes = int(total_seconds // 60)
                seconds = int(total_seconds % 60)
                return f"{minutes}m {seconds}s"
            else:
                hours = int(total_seconds // 3600)
                minutes = int((total_seconds % 3600) // 60)
                return f"{hours}h {minutes}m"
        except Exception:
            return "N/A"
```

## File: services/workflow_state.py
```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Workflow State Management Service

Centralized state management for workflow execution with thread-safety.
Replaces global variables with a clean, testable interface.
"""

import threading
from collections import deque
from datetime import datetime, timezone
from typing import Dict, Any, Optional, List, Set
from pathlib import Path
import logging

logger = logging.getLogger(__name__)


class WorkflowState:
    """Centralized workflow state management with thread-safety.
    
    This class manages all mutable state for workflow execution, including:
    - Process information for each step
    - Sequence execution status
    - CSV download tracking
    - Thread-safe access to all state
    
    All methods are thread-safe using internal locks.
    """
    
    def __init__(self):
        """Initialize workflow state with default values."""
        self._lock = threading.RLock()  # Reentrant lock for nested calls
        
        # Process information for each workflow step
        self._process_info: Dict[str, Dict[str, Any]] = {}
        
        # Sequence execution state
        self._sequence_running = False
        self._sequence_outcome = {
            "status": "never_run",
            "type": None,
            "message": None,
            "timestamp": None
        }
        
        # CSV download tracking
        self._active_csv_downloads: Dict[str, Dict[str, Any]] = {}
        self._kept_csv_downloads = deque(maxlen=20)
        
        # CSV monitor status
        self._csv_monitor_status = {
            "status": "stopped",
            "last_check": None,
            "error": None
        }
        
        logger.info("WorkflowState initialized")
    
    def initialize_step(self, step_key: str) -> None:
        """Initialize state for a workflow step.
        
        Args:
            step_key: Step identifier (e.g., 'STEP1', 'STEP2')
        """
        with self._lock:
            self._process_info[step_key] = {
                'status': 'idle',
                'log': deque(maxlen=300),
                'return_code': None,
                'process': None,
                'progress_current': 0,
                'progress_total': 0,
                'progress_text': '',
                'start_time_epoch': None,
                'duration_str': None
            }
            logger.debug(f"Initialized state for {step_key}")
    
    def initialize_all_steps(self, step_keys: List[str]) -> None:
        """Initialize state for all workflow steps.
        
        Args:
            step_keys: List of step identifiers
        """
        with self._lock:
            for step_key in step_keys:
                self.initialize_step(step_key)
            logger.info(f"Initialized state for {len(step_keys)} steps")
    
    def get_step_info(self, step_key: str) -> Dict[str, Any]:
        """Get information for a specific step (thread-safe copy).
        
        Args:
            step_key: Step identifier
            
        Returns:
            Dictionary with step information (copy)
        """
        with self._lock:
            if step_key not in self._process_info:
                logger.warning(f"Step {step_key} not initialized, returning empty dict")
                return {}
            
            # Deep copy to prevent external mutations
            info = self._process_info[step_key].copy()
            # Convert deque to list for JSON serialization
            if 'log' in info and isinstance(info['log'], deque):
                info['log'] = list(info['log'])
            return info
    
    def get_all_steps_info(self) -> Dict[str, Dict[str, Any]]:
        """Get information for all steps (thread-safe copy).
        
        Returns:
            Dictionary mapping step_key to step information
        """
        with self._lock:
            return {
                step_key: self.get_step_info(step_key)
                for step_key in self._process_info.keys()
            }
    
    def update_step_status(self, step_key: str, status: str) -> None:
        """Update status for a step.
        
        Args:
            step_key: Step identifier
            status: New status ('idle', 'starting', 'running', 'completed', 'failed')
        """
        with self._lock:
            if step_key in self._process_info:
                self._process_info[step_key]['status'] = status
                logger.debug(f"{step_key} status updated to: {status}")
    
    def update_step_progress(self, step_key: str, current: int, total: int, text: str = '') -> None:
        """Update progress information for a step.
        
        Args:
            step_key: Step identifier
            current: Current progress value
            total: Total progress value
            text: Optional progress text description
        """
        with self._lock:
            if step_key in self._process_info:
                info = self._process_info[step_key]
                info['progress_current'] = current
                info['progress_total'] = total
                info['progress_text'] = text
    
    def append_step_log(self, step_key: str, message: str) -> None:
        """Append a log message to a step's log.
        
        Args:
            step_key: Step identifier
            message: Log message to append
        """
        with self._lock:
            if step_key in self._process_info:
                self._process_info[step_key]['log'].append(message)
    
    def clear_step_log(self, step_key: str) -> None:
        """Clear the log for a step.
        
        Args:
            step_key: Step identifier
        """
        with self._lock:
            if step_key in self._process_info:
                self._process_info[step_key]['log'].clear()
    
    def update_step_info(self, step_key: str, **kwargs) -> None:
        """Update multiple fields for a step atomically.
        
        Args:
            step_key: Step identifier
            **kwargs: Fields to update
        """
        with self._lock:
            if step_key in self._process_info:
                self._process_info[step_key].update(kwargs)
                logger.debug(f"{step_key} updated with: {list(kwargs.keys())}")
    
    def get_step_status(self, step_key: str) -> Optional[str]:
        """Get current status of a step.
        
        Args:
            step_key: Step identifier
            
        Returns:
            Current status or None if step not found
        """
        with self._lock:
            if step_key in self._process_info:
                return self._process_info[step_key]['status']
            return None
    
    def is_step_running(self, step_key: str) -> bool:
        """Check if a step is currently running.
        
        Args:
            step_key: Step identifier
            
        Returns:
            True if step is running or starting
        """
        status = self.get_step_status(step_key)
        return status in ['running', 'starting', 'initiated']
    
    def is_any_step_running(self) -> bool:
        """Check if any step is currently running.
        
        Returns:
            True if any step is running
        """
        with self._lock:
            return any(
                info['status'] in ['running', 'starting', 'initiated']
                for info in self._process_info.values()
            )
    
    def set_step_process(self, step_key: str, process: Any) -> None:
        """Set the subprocess for a step.
        
        Args:
            step_key: Step identifier
            process: Subprocess instance (or None to clear)
        """
        with self._lock:
            if step_key in self._process_info:
                self._process_info[step_key]['process'] = process
    
    def get_step_process(self, step_key: str) -> Optional[Any]:
        """Get the subprocess for a step.
        
        Args:
            step_key: Step identifier
            
        Returns:
            Subprocess instance or None
        """
        with self._lock:
            if step_key in self._process_info:
                return self._process_info[step_key].get('process')
            return None
    
    def get_step_field(self, step_key: str, field_name: str, default: Any = None) -> Any:
        """Get a specific field from step info.
        
        Args:
            step_key: Step identifier
            field_name: Name of field to retrieve
            default: Default value if field not found
            
        Returns:
            Field value or default
        """
        with self._lock:
            if step_key in self._process_info:
                return self._process_info[step_key].get(field_name, default)
            return default
    
    def set_step_field(self, step_key: str, field_name: str, value: Any) -> None:
        """Set a specific field in step info.
        
        Args:
            step_key: Step identifier
            field_name: Name of field to set
            value: Value to set
        """
        with self._lock:
            if step_key in self._process_info:
                self._process_info[step_key][field_name] = value
    
    def get_step_log_deque(self, step_key: str) -> Optional[deque]:
        if step_key in self._process_info:
            return self._process_info[step_key]['log']
        return None
    
    def is_sequence_running(self) -> bool:
        """Check if a sequence is currently running.
        
        Returns:
            True if a sequence is running
        """
        with self._lock:
            return self._sequence_running
    
    def start_sequence(self, sequence_type: str) -> bool:
        """Mark sequence as started.
        
        Args:
            sequence_type: Type of sequence ('Full', 'Remote', etc.)
            
        Returns:
            True if sequence started, False if already running
        """
        with self._lock:
            if self._sequence_running:
                logger.warning(f"Cannot start {sequence_type} sequence: already running")
                return False
            
            self._sequence_running = True
            self._sequence_outcome = {
                "status": f"running_{sequence_type.lower()}",
                "type": sequence_type,
                "message": None,
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
            logger.info(f"{sequence_type} sequence started")
            return True
    
    def complete_sequence(self, success: bool, message: str = None, sequence_type: str = None) -> None:
        """Mark sequence as completed.
        
        Args:
            success: True if sequence succeeded, False otherwise
            message: Optional completion message
            sequence_type: Type of sequence
        """
        with self._lock:
            self._sequence_running = False
            status = "success" if success else "error"
            self._sequence_outcome = {
                "status": status,
                "type": sequence_type or self._sequence_outcome.get("type"),
                "message": message,
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
            logger.info(f"Sequence completed: {status}")
    
    def get_sequence_outcome(self) -> Dict[str, Any]:
        """Get last sequence outcome.
        
        Returns:
            Dictionary with sequence outcome information
        """
        with self._lock:
            return self._sequence_outcome.copy()
    
    def add_csv_download(self, download_id: str, download_info: Dict[str, Any]) -> None:
        """Add a CSV download to tracking.
        
        Args:
            download_id: Unique download identifier
            download_info: Download information dictionary
        """
        with self._lock:
            self._active_csv_downloads[download_id] = download_info.copy()
            logger.debug(f"CSV download added: {download_id}")
    
    def update_csv_download(self, download_id: str, status: str, 
                           progress: int = None, message: str = None, 
                           filename: str = None) -> None:
        """Update CSV download status.
        
        Args:
            download_id: Download identifier
            status: New status
            progress: Progress percentage (0-100)
            message: Status message
            filename: Filename (if changed)
        """
        with self._lock:
            if download_id in self._active_csv_downloads:
                download = self._active_csv_downloads[download_id]
                download['status'] = status
                if progress is not None:
                    download['progress'] = progress
                if message is not None:
                    download['message'] = message
                if filename is not None:
                    download['filename'] = filename
    
    def remove_csv_download(self, download_id: str, keep_in_history: bool = True) -> None:
        """Remove a CSV download from active tracking.
        
        Args:
            download_id: Download identifier
            keep_in_history: If True, move to kept downloads history
        """
        with self._lock:
            if download_id in self._active_csv_downloads:
                download = self._active_csv_downloads.pop(download_id)
                if keep_in_history:
                    self._kept_csv_downloads.append(download)
                logger.debug(f"CSV download removed: {download_id}")
    
    def get_csv_downloads_status(self) -> Dict[str, Any]:
        """Get status of all CSV downloads.
        
        Returns:
            Dictionary with active and kept downloads
        """
        with self._lock:
            active = [d.copy() for d in self._active_csv_downloads.values()]
            kept = list(self._kept_csv_downloads)
            
            return {
                "active": active,
                "kept": kept,
                "total_active": len(active),
                "total_kept": len(kept)
            }
    
    def get_active_csv_downloads_dict(self) -> Dict[str, Dict[str, Any]]:
        """Get active CSV downloads as a dictionary.
        
        Returns:
            Dictionary mapping download_id to download info
        """
        with self._lock:
            return {k: v.copy() for k, v in self._active_csv_downloads.items()}
    
    def get_kept_csv_downloads_list(self) -> List[Dict[str, Any]]:
        """Get list of kept (recent) CSV downloads.
        
        Returns:
            List of download info dictionaries
        """
        with self._lock:
            return list(self._kept_csv_downloads)
    
    def move_csv_download_to_history(self, download_id: str) -> None:
        """Move a CSV download from active to history.
        
        Args:
            download_id: Download identifier
        """
        with self._lock:
            if download_id in self._active_csv_downloads:
                download = self._active_csv_downloads.pop(download_id)
                self._kept_csv_downloads.append(download)
                logger.debug(f"CSV download moved to history: {download_id}")
    
    def get_csv_monitor_status(self) -> Dict[str, Any]:
        """Get CSV monitor status.
        
        Returns:
            Dictionary with monitor status
        """
        with self._lock:
            return self._csv_monitor_status.copy()
    
    def update_csv_monitor_status(self, status: str, last_check: str = None, error: str = None) -> None:
        """Update CSV monitor status.
        
        Args:
            status: Monitor status ('running', 'stopped', 'error')
            last_check: Timestamp of last check
            error: Error message if any
        """
        with self._lock:
            self._csv_monitor_status['status'] = status
            if last_check is not None:
                self._csv_monitor_status['last_check'] = last_check
            if error is not None:
                self._csv_monitor_status['error'] = error
    
    def reset_all(self) -> None:
        """Reset all state to initial values (useful for testing)."""
        with self._lock:
            self._process_info.clear()
            self._sequence_running = False
            self._sequence_outcome = {
                "status": "never_run",
                "type": None,
                "message": None,
                "timestamp": None
            }
            self._active_csv_downloads.clear()
            self._kept_csv_downloads.clear()
            self._csv_monitor_status = {
                "status": "stopped",
                "last_check": None,
                "error": None
            }
            logger.info("WorkflowState reset to initial values")
    
    def get_summary(self) -> Dict[str, Any]:
        """Get a summary of current workflow state.
        
        Returns:
            Dictionary with state summary
        """
        with self._lock:
            return {
                "steps_count": len(self._process_info),
                "running_steps": [
                    key for key, info in self._process_info.items()
                    if info['status'] in ['running', 'starting']
                ],
                "sequence_running": self._sequence_running,
                "sequence_outcome": self._sequence_outcome.copy(),
                "active_downloads": len(self._active_csv_downloads),
                "csv_monitor_status": self._csv_monitor_status['status']
            }


_workflow_state: Optional[WorkflowState] = None
_state_lock = threading.Lock()


def get_workflow_state() -> WorkflowState:
    """Get the global WorkflowState singleton instance.
    
    Returns:
        WorkflowState singleton instance
    """
    global _workflow_state
    
    if _workflow_state is None:
        with _state_lock:
            if _workflow_state is None:
                _workflow_state = WorkflowState()
    
    return _workflow_state


def reset_workflow_state() -> None:
    """Reset the global WorkflowState (useful for testing)."""
    global _workflow_state
    
    with _state_lock:
        if _workflow_state is not None:
            _workflow_state.reset_all()
        _workflow_state = None
```

## File: static/css/components/csv-workflow-prompt.css
```css
/**
 * CSV Workflow Prompt Styles
 * Engaging and playful styles for the CSV download completion prompt
 */

/* Main prompt container */
.csv-workflow-prompt .popup-content {
    max-width: 600px;
    min-width: 500px;
    border-radius: 16px;
    background: linear-gradient(135deg, #ffffff 0%, #f8f9fa 100%);
    border: 2px solid var(--accent-primary);
    box-shadow: 0 20px 40px rgba(0, 0, 0, 0.15);
    animation: csvPromptSlideIn 0.4s cubic-bezier(0.34, 1.56, 0.64, 1);
    position: relative;
    /* Ensure solid, opaque background for readability */
    opacity: 1;
}

/* Close button */
.csv-workflow-prompt .popup-close-button {
    position: absolute;
    top: 16px;
    right: 16px;
    background: none;
    border: none;
    font-size: 24px;
    color: #6c757d;
    cursor: pointer;
    width: 32px;
    height: 32px;
    display: flex;
    align-items: center;
    justify-content: center;
    border-radius: 50%;
    transition: all 0.2s ease;
}

.csv-workflow-prompt .popup-close-button:hover {
    background: #f8f9fa;
    color: #495057;
    transform: scale(1.1);
}

/* Title */
.csv-workflow-prompt .popup-title {
    margin: 0 0 16px 0;
    padding: 24px 24px 0 24px;
    font-size: 20px;
    font-weight: 600;
    color: #212529;
    text-align: center;
}

/* Slide-in animation */
@keyframes csvPromptSlideIn {
    0% {
        opacity: 0;
        transform: translateY(-30px) scale(0.9);
    }
    100% {
        opacity: 1;
        transform: translateY(0) scale(1);
    }
}

/* Prompt content container */
.csv-workflow-prompt-content {
    padding: 0;
}

/* Header section */
.prompt-header {
    display: flex;
    align-items: center;
    gap: 16px;
    padding: 24px 24px 16px 24px;
    border-bottom: 1px solid #e0e0e0;
    background: linear-gradient(90deg, rgba(121, 134, 203, 0.1) 0%, rgba(255, 255, 255, 0.95) 100%);
    /* Ensure solid background for text readability */
    backdrop-filter: blur(10px);
}

.download-icon {
    font-size: 48px;
    animation: csvIconBounce 2s ease-in-out infinite;
}

@keyframes csvIconBounce {
    0%, 100% { transform: translateY(0); }
    50% { transform: translateY(-8px); }
}

.download-info {
    flex: 1;
}

.download-title {
    margin: 0 0 4px 0;
    font-size: 18px;
    font-weight: 600;
    color: #212529;
    word-break: break-word;
}

.download-subtitle {
    margin: 0;
    font-size: 14px;
    color: #6c757d;
    opacity: 1;
}

/* Message section */
.prompt-message {
    padding: 20px 24px;
}

.main-message {
    margin: 0 0 20px 0;
    font-size: 16px;
    line-height: 1.5;
    color: #495057;
    text-align: center;
}

/* Workflow preview */
.workflow-preview {
    background: #f8f9fa;
    border-radius: 12px;
    padding: 16px;
    border: 1px solid #e0e0e0;
    /* Ensure solid background for content readability */
    opacity: 1;
}

.workflow-steps {
    display: flex;
    align-items: center;
    justify-content: center;
    flex-wrap: wrap;
    gap: 8px;
}

.step-badge {
    background: var(--accent-primary);
    color: white;
    padding: 6px 12px;
    border-radius: 20px;
    font-size: 12px;
    font-weight: 500;
    white-space: nowrap;
    animation: csvStepPulse 3s ease-in-out infinite;
}

.step-badge:nth-child(odd) {
    animation-delay: 0.5s;
}

@keyframes csvStepPulse {
    0%, 100% { transform: scale(1); opacity: 1; }
    50% { transform: scale(1.05); opacity: 0.9; }
}

.step-arrow {
    color: var(--accent-primary);
    font-weight: bold;
    font-size: 14px;
}

/* Actions section */
.prompt-actions {
    display: flex;
    gap: 12px;
    padding: 20px 24px 16px 24px;
    justify-content: center;
}

.workflow-launch-btn {
    background: linear-gradient(135deg, var(--accent-primary) 0%, var(--accent-secondary) 100%);
    color: white;
    border: none;
    padding: 14px 24px;
    border-radius: 12px;
    font-size: 16px;
    font-weight: 600;
    cursor: pointer;
    display: flex;
    align-items: center;
    gap: 8px;
    transition: all 0.3s ease;
    box-shadow: 0 4px 12px rgba(var(--accent-primary-rgb), 0.3);
    position: relative;
    overflow: hidden;
}

.workflow-launch-btn:hover {
    transform: translateY(-2px);
    box-shadow: 0 6px 20px rgba(var(--accent-primary-rgb), 0.4);
}

.workflow-launch-btn:active {
    transform: translateY(0);
}

.workflow-launch-btn::before {
    content: '';
    position: absolute;
    top: 0;
    left: -100%;
    width: 100%;
    height: 100%;
    background: linear-gradient(90deg, transparent, rgba(255, 255, 255, 0.2), transparent);
    transition: left 0.5s;
}

.workflow-launch-btn:hover::before {
    left: 100%;
}

.workflow-dismiss-btn {
    background: #ffffff;
    color: #6c757d;
    border: 2px solid #e0e0e0;
    padding: 12px 20px;
    border-radius: 12px;
    font-size: 14px;
    font-weight: 500;
    cursor: pointer;
    display: flex;
    align-items: center;
    gap: 6px;
    transition: all 0.3s ease;
    /* Ensure solid background for button readability */
    opacity: 1;
}

.workflow-dismiss-btn:hover {
    background: #f8f9fa;
    border-color: var(--accent-primary);
    color: #495057;
}

.btn-icon {
    font-size: 16px;
}

.btn-text {
    white-space: nowrap;
}

/* Footer section */
.prompt-footer {
    padding: 16px 24px 24px 24px;
    border-top: 1px solid #e0e0e0;
    background: #f8f9fa;
    border-radius: 0 0 14px 14px;
    /* Ensure solid background for footer readability */
    opacity: 1;
}

.footer-note {
    margin: 0;
    font-size: 13px;
    color: #6c757d;
    text-align: center;
    line-height: 1.4;
}

/* Responsive design */
@media (max-width: 768px) {
    .csv-workflow-prompt .popup-content {
        min-width: 90vw;
        max-width: 95vw;
        margin: 20px;
    }
    
    .prompt-header {
        flex-direction: column;
        text-align: center;
        gap: 12px;
    }
    
    .workflow-steps {
        flex-direction: column;
        gap: 12px;
    }
    
    .step-arrow {
        transform: rotate(90deg);
    }
    
    .prompt-actions {
        flex-direction: column;
        gap: 12px;
    }
    
    .workflow-launch-btn,
    .workflow-dismiss-btn {
        width: 100%;
        justify-content: center;
    }
}

/* Dark mode adjustments */
@media (prefers-color-scheme: dark) {
    .csv-workflow-prompt .popup-content {
        background: linear-gradient(135deg, #2c2c3e 0%, #1a1a1a 100%);
        border-color: var(--accent-primary);
        color: #e0e0e0;
    }

    .prompt-header {
        background: linear-gradient(90deg, rgba(121, 134, 203, 0.15) 0%, rgba(44, 44, 62, 0.95) 100%);
        border-bottom-color: #404040;
    }

    .workflow-preview {
        background: #3a3a4e;
        border-color: #404040;
    }

    .prompt-footer {
        background: #3a3a4e;
        border-top-color: #404040;
    }

    .workflow-dismiss-btn {
        background: #3a3a4e;
        color: #a0a0b0;
        border-color: #404040;
    }

    .workflow-dismiss-btn:hover {
        background: #4a4a5e;
        color: #e0e0e0;
    }

    .download-title {
        color: #e0e0e0;
    }

    .download-subtitle {
        color: #a0a0b0;
    }

    .main-message {
        color: #e0e0e0;
    }

    .footer-note {
        color: #a0a0b0;
    }
}

/* Accessibility improvements */
.workflow-launch-btn:focus,
.workflow-dismiss-btn:focus {
    outline: 2px solid var(--accent-primary);
    outline-offset: 2px;
}

/* High contrast mode support */
@media (prefers-contrast: high) {
    .csv-workflow-prompt .popup-content {
        border-width: 3px;
    }
    
    .step-badge {
        border: 2px solid currentColor;
    }
    
    .workflow-launch-btn,
    .workflow-dismiss-btn {
        border-width: 2px;
    }
}
```

## File: static/css/components/downloads.css
```css
/* Styles pour la nouvelle section des téléchargements locaux */
.local-downloads-section {
    background-color: var(--bg-card);
    border: 1px solid var(--border-color);
    padding: 25px;
    margin: 25px auto;
    border-radius: 12px;
    box-shadow: 0 5px 15px rgba(0,0,0,0.2);
    width: 100%;
    max-width: 900px;
    /* Animated visibility (GPU-friendly). Visible by default to avoid empty space before JS runs */
    opacity: 1;
    transform: none;
    transition: opacity 0.32s cubic-bezier(0.4, 0, 0.2, 1),
                transform 0.32s cubic-bezier(0.4, 0, 0.2, 1),
                box-shadow 0.32s ease;
    will-change: opacity, transform;
}

.local-downloads-section[data-visible="false"] {
    opacity: 0;
    transform: translateX(24px) scale(0.985);
}

.local-downloads-section[data-visible="true"] {
    opacity: 1;
    transform: translateX(0) scale(1);
    box-shadow: 0 10px 30px rgba(0,0,0,0.26);
}

@media (prefers-reduced-motion: reduce) {
    .local-downloads-section {
        transition: none !important;
        transform: none !important;
    }
}

.local-downloads-section h2 {
    color: var(--accent-secondary);
    margin-top: 0;
    border-bottom: 1px solid var(--border-color);
    padding-bottom: 10px;
    margin-bottom: 20px;
    font-size: 1.5em;
    display: flex;
    align-items: center;
    font-weight: 500;
}
.local-downloads-section h2 .section-icon {
    margin-right: 12px;
    font-size: 1.6em;
}

.local-downloads-list-container {
    max-height: 350px;
    overflow-y: auto;
    padding-right: 10px;
}

.status-list { 
    list-style-type: none;
    padding-left: 0;
    margin:0;
}

.status-list li {
    padding: 10px 5px;
    border-bottom: 1px solid var(--border-color-translucent);
    font-size: 0.95em;
    display: flex;
    flex-wrap: wrap;
    gap: 5px 10px;
}
.status-list li:last-child {
    border-bottom: none;
}
.status-list li.placeholder {
    color: var(--text-secondary);
    font-style: italic;
    justify-content: center;
}

.status-list li .timestamp {
    color: var(--text-secondary);
    font-size: 0.9em;
    margin-right: 8px;
    flex-shrink: 0;
}
.status-list li .filename {
    font-weight: 500;
    color: var(--text-primary);
    margin-right: 8px;
    overflow: hidden;
    text-overflow: ellipsis;
    white-space: nowrap;
    max-width: 300px;
}
.status-list li .status-text {
    font-weight: bold;
}
.status-list li .progress-percentage {
    color: var(--blue);
    font-weight: bold;
}
.status-list li .message {
    font-size: 0.85em;
    color: var(--text-secondary);
    font-style: italic;
    max-width: 300px;
    overflow: hidden;
    text-overflow: ellipsis;
    white-space: nowrap;
}

.download-status-pending .status-text { color: var(--blue); }
.download-status-starting .status-text { color: var(--blue); }
.download-status-downloading .status-text { color: var(--yellow); }
.download-status-completed .status-text { color: var(--green); }
.download-status-failed .status-text { color: var(--red); }


/* --- Unified Controls: Downloads Toggle Button --- */
.downloads-toggle {
    margin-left: 6px;
    padding: 6px 10px;
    border-radius: 8px;
    border: 1px solid var(--border-color);
    background: var(--bg-muted);
    color: var(--text-primary);
    cursor: pointer;
    transition: background-color 0.2s ease, color 0.2s ease, border-color 0.2s ease;
}
.downloads-toggle[aria-pressed="true"] {
    background: var(--accent-primary-light);
    border-color: var(--accent-primary);
}
.downloads-toggle--hidden {
    opacity: 0.85;
}
.downloads-toggle--alert {
    background: var(--red-translucent, rgba(220, 53, 69, 0.12));
    border-color: var(--red, #dc3545);
    color: var(--red, #dc3545);
}
```

## File: static/css/components/notifications.css
```css
/* --- Notification Area Styles --- */
#notifications-area {
    position: fixed;
    top: calc(var(--topbar-height) + 10px);
    right: 20px;
    z-index: 1200;
    width: 300px;
    max-height: 90vh;
    overflow-y: auto;
    display: flex;
    flex-direction: column;
    gap: 8px;
}

.notification {
    background-color: var(--bg-card);
    color: var(--text-primary);
    padding: 15px;
    margin-bottom: 10px;
    border-radius: 8px;
    box-shadow: 0 2px 10px rgba(0,0,0,0.3);
    font-size: 0.9em;
    opacity: 0.95;
    border-left: 5px solid var(--blue);
}

.notification.error {
    border-left-color: var(--red);
}
.notification.success {
    border-left-color: var(--green);
}
.notification.warning {
    border-left-color: var(--yellow);
    color: #333;
}
/* --- End Notification Area Styles --- */
```

## File: static/css/components/popups.css
```css
.popup-overlay {
    display: none; 
    position: fixed;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    background: rgba(0,0,0,0.7);
    justify-content: center;
    align-items: center;
    z-index: 1000;
    /* Transitions for smooth show/hide when data-visible toggles */
    opacity: 0;
    transform: scale(0.98);
    transition: opacity 0.25s cubic-bezier(0.4, 0, 0.2, 1),
                transform 0.25s cubic-bezier(0.4, 0, 0.2, 1);
    will-change: opacity, transform;
}

/* Respect users who prefer reduced motion */
@media (prefers-reduced-motion: reduce) {
    .popup-overlay {
        transition: none !important;
        transform: none !important;
    }
    .popup-content {
        animation: none !important;
    }
}
.popup-content {
    background: var(--bg-card);
    padding: 30px;
    border-radius: 10px;
    min-width: 300px;
    max-width: 500px;
    box-shadow: 0 0 20px rgba(0,0,0,0.5);
    color: var(--text-primary);
    /* Subtle entrance animation */
    animation: modalSlideIn 0.3s cubic-bezier(0.34, 1.56, 0.64, 1);
}
.popup-content h3 {
    color: var(--accent-primary);
    margin-top: 0;
    text-align: center;
}
.popup-list {
    list-style: none;
    padding: 0;
    margin-bottom: 20px;
}
.popup-list li {
    padding: 8px 0;
    border-bottom: 1px solid var(--border-color);
}
.popup-list li:last-child {
    border-bottom: none;
}
.popup-list li .order-prefix {
    font-weight: bold;
    color: var(--orange);
    margin-right: 10px;
}
.status-icon {
    margin-right: 8px;
}
.summary-popup li .duration {
    font-size: 0.9em;
    color: var(--text-secondary);
    margin-left: 10px;
}
.popup-buttons {
    display: flex;
    justify-content: space-around;
    margin-top: 20px;
}
.popup-buttons button {
    padding: 10px 20px;
    border: none;
    border-radius: 5px;
    cursor: pointer;
    font-weight: bold;
}
.popup-confirm-button { background-color: var(--green); color: white; }
.popup-cancel-button, #close-summary-popup { background-color: var(--red); color: white; }

#close-summary-popup {
     display: block;
     margin: 20px auto 0 auto;
}
#close-summary-popup:hover { background-color: #d9534f; }
.popup-confirm-button:hover { background-color: #5cb85c; }

/* Keyboard accessibility: focus visible ring */
.popup-buttons button:focus-visible,
.popup-confirm-button:focus-visible,
.popup-cancel-button:focus-visible,
#close-summary-popup:focus-visible {
    outline: none;
    box-shadow: 0 0 0 3px color-mix(in oklab, var(--accent-primary) 35%, transparent);
}

/* Visible state driven by data attribute for smoother transitions */
.popup-overlay[data-visible="true"] {
    opacity: 1;
    transform: scale(1);
}

@keyframes modalSlideIn {
    from {
        opacity: 0;
        transform: translateY(-16px) scale(0.97);
    }
    to {
        opacity: 1;
        transform: translateY(0) scale(1);
    }
}
```

## File: static/css/components/widgets.css
```css
/* --- Generic loader & button loading state --- */
.loading-spinner {
    display: inline-block;
    width: 16px;
    height: 16px;
    border: 2px solid var(--border-color);
    border-top-color: var(--accent-primary);
    border-radius: 50%;
    animation: spin 0.8s linear infinite;
    vertical-align: middle;
    margin-left: 8px;
}

button[data-loading="true"] {
    pointer-events: none;
    opacity: 0.75;
}

button[data-loading="true"]::before {
    content: '';
    display: inline-block;
    width: 14px;
    height: 14px;
    margin-right: 8px;
    border: 2px solid currentColor;
    border-top-color: transparent;
    border-radius: 50%;
    animation: spin 0.8s linear infinite;
    vertical-align: -2px;
}

/* --- Styles pour le Widget de Monitoring Système --- */
.system-monitor-widget {
    position: fixed;
    top: 24px;
    right: 24px;
    width: 260px;
    background-color: var(--bg-card);
    border: 1px solid var(--border-color);
    border-radius: 10px;
    padding: 15px;
    box-shadow: 0 5px 20px rgba(0,0,0,0.3);
    z-index: 1000;
    font-size: 0.9em;
    color: var(--text-secondary);
    transition: opacity 0.3s ease, transform 0.3s ease, height 0.25s ease, width 0.25s ease, padding 0.25s ease, right 0.25s ease, top 0.25s ease;
}

.workflow-wrapper.logs-active + .logs-column + #system-monitor-widget,
.workflow-wrapper.logs-active ~ #system-monitor-widget {
    transform: translateX(-12px);
}

.monitor-header {
    display: flex;
    align-items: center;
    margin-bottom: 15px;
    padding-bottom: 8px;
    border-bottom: 1px solid var(--border-color);
}

.monitor-icon {
    font-size: 1.5em;
    margin-right: 10px;
}

.monitor-title {
    font-weight: 600;
    color: var(--accent-primary);
}

.monitor-close {
    margin-left: auto;
    background: transparent;
    border: 0;
    color: var(--text-secondary);
    font-size: 1.2em;
    line-height: 1;
    cursor: pointer;
}

.monitor-close:hover,
.monitor-close:focus {
    color: var(--text-primary);
}

/* Keyboard accessibility: focus ring for widget controls */
.monitor-close:focus-visible,
.upload-button:focus-visible,
.btn-like-switch:focus-visible {
    outline: none;
    box-shadow: 0 0 0 3px color-mix(in oklab, var(--accent-primary) 35%, transparent);
    border-color: var(--accent-primary);
}

.monitor-item {
    display: flex;
    align-items: center;
    margin-bottom: 10px;
}

.monitor-label {
    width: 35px; /* Pour aligner les barres */
    font-weight: 500;
}

.monitor-bar-container {
    flex-grow: 1;
    height: 16px;
    background-color: var(--border-color);
    border-radius: 8px;
    padding: 2px;
    margin: 0 8px;
}

.monitor-bar {
    height: 100%;
    width: 0%;
    border-radius: 6px;
    transition: width 0.5s ease-out, background-color 0.5s ease;
    background-color: var(--green);
}

.monitor-value {
    width: 50px; /* Pour aligner */
    text-align: right;
    font-weight: bold;
    font-family: 'SFMono-Regular', Consolas, monospace;
}

.monitor-details {
    font-size: 0.8em;
    text-align: center;
    color: var(--text-secondary);
    margin-top: 5px;
}

/* Compact/minimized mode */
.system-monitor-widget.minimized {
    width: auto; /* auto-size to content */
    min-width: 220px;
    max-width: 360px; /* cap to avoid overflow */
    padding: 8px 10px;
}

.system-monitor-widget.minimized .monitor-header {
    margin-bottom: 6px;
    padding-bottom: 4px;
}

.system-monitor-widget.minimized .monitor-item,
.system-monitor-widget.minimized #ram-monitor-details,
.system-monitor-widget.minimized #gpu-monitor-section,
.system-monitor-widget.minimized #gpu-monitor-error {
    display: none !important;
}

.monitor-compact-line {
    display: flex;
    align-items: center;
    justify-content: flex-start;
    gap: 2px;
    font-family: 'SFMono-Regular', Consolas, monospace;
    font-size: 0.62em; /* tighter */
    letter-spacing: -0.04em; /* tighter */
    white-space: nowrap;
    overflow: hidden;
    text-overflow: clip; /* prefer full display within max-width */
    color: var(--text-primary);
}

/* Styles pour les couleurs des barres en fonction de l'utilisation */
#cpu-monitor-bar[data-usage-level="high"],
#ram-monitor-bar[data-usage-level="high"] {
    background-color: var(--red);
}

#cpu-monitor-bar[data-usage-level="medium"],
#ram-monitor-bar[data-usage-level="medium"] {
    background-color: var(--yellow);
}

#cpu-monitor-bar[data-usage-level="low"],
#ram-monitor-bar[data-usage-level="low"] {
    background-color: var(--green);
}

/* --- Styles pour le Widget de Contrôle Auto-Scroll --- */
.auto-scroll-widget {
    position: static;
    width: 100%;
    background-color: var(--bg-secondary);
    border: 1px solid var(--border-color);
    border-radius: 8px;
    padding: 12px;
    box-shadow: none;
    font-size: 0.9em;
    display: flex;
    flex-direction: column;
    gap: 8px;
}

.auto-scroll-header {
    display: flex;
    align-items: center;
    margin-bottom: 8px;
    font-weight: 600;
    color: var(--text-primary);
}

.auto-scroll-icon {
    margin-right: 6px;
    font-size: 1.1em;
}

.auto-scroll-title {
    font-size: 0.9em;
}

.auto-scroll-control {
    display: flex;
    align-items: center;
    justify-content: space-between;
}

.auto-scroll-switch {
    position: relative;
    display: inline-block;
    width: 44px;
    height: 24px;
}

.auto-scroll-switch input {
    opacity: 0;
    width: 0;
    height: 0;
}

.auto-scroll-slider {
    position: absolute;
    cursor: pointer;
    top: 0;
    left: 0;
    right: 0;
    bottom: 0;
    background-color: var(--border-color);
    transition: 0.3s;
    border-radius: 24px;
}

.auto-scroll-slider:before {
    position: absolute;
    content: "";
    height: 18px;
    width: 18px;
    left: 3px;
    bottom: 3px;
    background-color: white;
    transition: 0.3s;
    border-radius: 50%;
}

input:checked + .auto-scroll-slider {
    background-color: var(--accent-primary);
}

input:checked + .auto-scroll-slider:before {
    transform: translateX(20px);
}

.auto-scroll-status {
    font-size: 0.8em;
    color: var(--text-secondary);
    margin-left: 8px;
}

/* --- Styles pour le Widget de Contrôle Sonore --- */
.sound-control-widget {
    position: static;
    width: 100%;
    background-color: var(--bg-secondary);
    border: 1px solid var(--border-color);
    border-radius: 8px;
    padding: 12px;
    box-shadow: none;
    font-size: 0.9em;
    display: flex;
    flex-direction: column;
    gap: 8px;
}


.sound-control-header {
    display: flex;
    align-items: center;
    margin-bottom: 8px;
    font-weight: 600;
    color: var(--text-primary);
}

.sound-control-icon {
    margin-right: 6px;
    font-size: 1.1em;
}

.sound-control-title {
    font-size: 0.9em;
}

.sound-control-controls {
    display: flex;
    align-items: center;
    justify-content: space-between;
}

.sound-control-switch {
    position: relative;
    display: inline-block;
    width: 44px;
    height: 24px;
}

.sound-control-switch input {
    opacity: 0;
    width: 0;
    height: 0;
}

.sound-control-slider {
    position: absolute;
    cursor: pointer;
    top: 0;
    left: 0;
    right: 0;
    bottom: 0;
    background-color: var(--border-color);
    transition: 0.3s;
    border-radius: 24px;
}

.sound-control-slider:before {
    position: absolute;
    content: "";
    height: 18px;
    width: 18px;
    left: 3px;
    bottom: 3px;
    background-color: white;
    transition: 0.3s;
    border-radius: 50%;
}

input:checked + .sound-control-slider {
    background-color: var(--accent-primary);
}

input:checked + .sound-control-slider:before {
    transform: translateX(20px);
}

.sound-control-status {
    font-size: 0.8em;
    color: var(--text-secondary);
    margin-left: 8px;
}

/* ===== Overrides for topbar integration (no floating panels) ===== */
```

## File: static/css/features/reports.css
```css
/**
 * Reports Generator Styles
 * Visual report generation feature
 */

/* Report Modal */
.report-overlay {
    display: none;
    position: fixed;
    top: 0;
    left: 0;
    right: 0;
    bottom: 0;
    background: rgba(0, 0, 0, 0.7);
    backdrop-filter: blur(5px);
    z-index: 9999;
    align-items: center;
    justify-content: center;
    opacity: 0;
    transition: opacity 0.3s ease;
}

.report-overlay[data-visible="true"] {
    opacity: 1;
}

.report-modal {
    background: var(--card-bg);
    border-radius: 12px;
    box-shadow: 0 10px 40px rgba(0, 0, 0, 0.3);
    width: 90%;
    max-width: 1200px;
    max-height: 90vh;
    display: flex;
    flex-direction: column;
    transform: scale(0.95);
    transition: transform 0.3s ease;
}

.report-overlay[data-visible="true"] .report-modal {
    transform: scale(1);
}

/* Report Header */
.report-header {
    padding: 20px 30px;
    border-bottom: 2px solid var(--border-color);
    display: flex;
    align-items: center;
    justify-content: space-between;
}

.report-title {
    display: flex;
    align-items: center;
    gap: 12px;
    font-size: 24px;
    font-weight: 600;
    color: var(--text-primary);
}

.report-close {
    background: none;
    border: none;
    font-size: 28px;
    cursor: pointer;
    color: var(--text-secondary);
    transition: all 0.2s ease;
    padding: 5px 10px;
    border-radius: 6px;
}

.report-close:hover {
    background: var(--hover-bg);
    color: var(--text-primary);
}

/* Report Body */
.report-body {
    flex: 1;
    overflow-y: auto;
    padding: 30px;
}

/* Report Selectors */
.report-selectors {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
    gap: 20px;
    margin-bottom: 30px;
}

.report-selector-group {
    display: flex;
    flex-direction: column;
    gap: 8px;
}

.report-selector-label {
    font-size: 14px;
    font-weight: 600;
    color: var(--text-secondary);
    text-transform: uppercase;
    letter-spacing: 0.5px;
}

.report-selector {
    padding: 12px 16px;
    border: 2px solid var(--border-color);
    border-radius: 8px;
    background: var(--input-bg);
    color: var(--text-primary);
    font-size: 16px;
    cursor: pointer;
    transition: all 0.2s ease;
}

.report-selector:hover:not(:disabled) {
    border-color: var(--accent-color);
}

.report-selector:focus {
    outline: none;
    border-color: var(--accent-color);
    box-shadow: 0 0 0 3px rgba(var(--accent-rgb), 0.1);
}

.report-selector:disabled {
    opacity: 0.5;
    cursor: not-allowed;
}

/* Report Options */
.report-options {
    background: var(--section-bg);
    padding: 20px;
    border-radius: 8px;
    margin-bottom: 30px;
}

.report-options-title {
    font-size: 16px;
    font-weight: 600;
    margin-bottom: 15px;
    color: var(--text-primary);
}

.report-format-group {
    display: flex;
    gap: 15px;
    flex-wrap: wrap;
}

.report-format-option {
    display: flex;
    align-items: center;
    gap: 8px;
    padding: 10px 16px;
    border: 2px solid var(--border-color);
    border-radius: 8px;
    cursor: pointer;
    transition: all 0.2s ease;
    background: var(--card-bg);
}

.report-format-option:hover {
    border-color: var(--accent-color);
    background: var(--hover-bg);
}

.report-format-option input[type="radio"] {
    cursor: pointer;
}

.report-format-option.selected {
    border-color: var(--accent-color);
    background: rgba(var(--accent-rgb), 0.1);
}

/* Report Actions */
.report-actions {
    display: flex;
    gap: 15px;
    justify-content: flex-end;
    margin-bottom: 30px;
}

.report-button {
    padding: 12px 24px;
    border: none;
    border-radius: 8px;
    font-size: 16px;
    font-weight: 600;
    cursor: pointer;
    transition: all 0.2s ease;
    display: flex;
    align-items: center;
    gap: 8px;
}

.report-button-primary {
    background: var(--accent-color);
    color: white;
}

.report-button-primary:hover:not(:disabled) {
    background: var(--accent-hover);
    transform: translateY(-2px);
    box-shadow: 0 4px 12px rgba(var(--accent-rgb), 0.3);
}

.report-button-secondary {
    background: var(--section-bg);
    color: var(--text-primary);
    border: 2px solid var(--border-color);
}

.report-button-secondary:hover:not(:disabled) {
    background: var(--hover-bg);
    border-color: var(--accent-color);
}

.report-button:disabled {
    opacity: 0.5;
    cursor: not-allowed;
    transform: none;
}

/* Report Preview */
.report-preview {
    border: 2px solid var(--border-color);
    border-radius: 8px;
    background: white;
    min-height: 400px;
    overflow: hidden;
}

.report-preview-iframe {
    width: 100%;
    height: 600px;
    border: none;
}

.report-preview-empty {
    display: flex;
    flex-direction: column;
    align-items: center;
    justify-content: center;
    padding: 60px;
    color: var(--text-secondary);
}

.report-preview-icon {
    font-size: 64px;
    margin-bottom: 20px;
    opacity: 0.5;
}

.report-preview-message {
    font-size: 18px;
    font-weight: 600;
    margin-bottom: 10px;
}

.report-preview-detail {
    font-size: 14px;
    opacity: 0.7;
}

/* Loading State */
.report-loading {
    display: flex;
    flex-direction: column;
    align-items: center;
    justify-content: center;
    padding: 60px;
}

.report-loading-spinner {
    width: 50px;
    height: 50px;
    border: 4px solid var(--border-color);
    border-top-color: var(--accent-color);
    border-radius: 50%;
    animation: report-spin 1s linear infinite;
}

@keyframes report-spin {
    to {
        transform: rotate(360deg);
    }
}

.report-loading-text {
    margin-top: 20px;
    font-size: 16px;
    color: var(--text-secondary);
}

/* Download Info */
.report-download-info {
    background: var(--success-bg);
    border: 2px solid var(--success-color);
    border-radius: 8px;
    padding: 15px 20px;
    margin-top: 20px;
    display: flex;
    align-items: center;
    gap: 12px;
    color: var(--success-text);
}

.report-download-icon {
    font-size: 24px;
}

/* Responsive */
@media (max-width: 768px) {
    .report-modal {
        width: 95%;
        max-height: 95vh;
    }
    
    .report-header {
        padding: 15px 20px;
    }
    
    .report-body {
        padding: 20px;
    }
    
    .report-selectors {
        grid-template-columns: 1fr;
    }
    
    .report-actions {
        flex-direction: column;
    }
    
    .report-button {
        width: 100%;
        justify-content: center;
    }
}
```

## File: static/css/features/responsive.css
```css
/* Ajustements Responsive */
@media (max-width: 768px) {
    body {
      padding-top: 70px; 
      padding-left: 10px;
      padding-right: 10px;
    }
    #clear-cache-global-button {
        top:10px; left:10px;
        padding: 8px 12px;
        font-size: 0.8em;
    }
    /* Unified controls section mobile styles */
    .unified-controls-section {
        margin: 20px auto 30px auto;
        padding: 20px;
        max-width: 95%;
    }

    .auto-mode-container {
        flex-direction: column;
        gap: 8px;
        padding: 10px;
    }

    .workflow-controls {
        flex-direction: column;
        gap: 12px;
    }

    /* Enhanced mobile styling for workflow control buttons */
    .workflow-controls button {
        width: 100%;
        padding: 14px 20px;
        font-size: 1em;
        min-width: unset;
        border-radius: 25px;
        margin: 0;
    }

    #run-all-steps-button, #run-custom-sequence-button, #clear-custom-sequence-button {
        box-shadow: 0 3px 10px rgba(0, 0, 0, 0.15);
    }

    #run-all-steps-button:hover, #run-custom-sequence-button:hover, #clear-custom-sequence-button:hover {
        transform: translateY(-2px);
        box-shadow: 0 4px 15px rgba(0, 0, 0, 0.2);
    }
    .auto-mode-label { margin-right: 0;}

    .local-downloads-section {
        max-width: 95%;
        padding: 15px;
    }
    .local-downloads-section h2 {
        font-size: 1.3em;
    }
    .local-downloads-section h2 .section-icon {
        font-size: 1.4em;
    }
    .status-list li .filename,
    .status-list li .message {
        max-width: 100%;
        white-space: normal;
    }

    .system-monitor-widget {
        display: none; /* Cacher le widget sur les petits écrans pour ne pas surcharger */
    }

    .auto-scroll-widget {
        width: 160px;
        font-size: 0.8em;
        padding: 8px;
    }

    .auto-scroll-switch {
        width: 36px;
        height: 20px;
    }

    .auto-scroll-slider:before {
        height: 14px;
        width: 14px;
        left: 3px;
        bottom: 3px;
    }

    input:checked + .auto-scroll-slider:before {
        transform: translateX(16px);
    }

    .sound-control-widget {
        width: 160px;
        font-size: 0.8em;
        padding: 8px;
        top: 70px; /* Positioned below auto-scroll widget on smaller screens */
        left: 15px; /* Aligned with auto-scroll widget */
    }

    .sound-control-switch {
        width: 36px;
        height: 20px;
    }

    .sound-control-slider:before {
        height: 14px;
        width: 14px;
        left: 3px;
        bottom: 3px;
    }

    input:checked + .sound-control-slider:before {
        transform: translateX(16px);
    }
}
```

## File: static/css/features/stats-dashboard.css
```css
/* =============================================================================
   STATS DASHBOARD - Workflow MediaPipe v4.0
   Performance statistics dashboard with charts and metrics
   ============================================================================= */

/* Stats Dashboard Modal */
.stats-dashboard-overlay {
    position: fixed;
    top: 0;
    left: 0;
    right: 0;
    bottom: 0;
    background: rgba(0, 0, 0, 0.7);
    backdrop-filter: blur(5px);
    display: flex;
    align-items: center;
    justify-content: center;
    z-index: 10000;
    padding: 20px;
    opacity: 0;
    transition: opacity 0.3s ease;
}

.stats-dashboard-overlay[data-visible="true"] {
    opacity: 1;
}

.stats-dashboard-content {
    background: var(--bg-card);
    border: 2px solid var(--border-color);
    border-radius: 12px;
    max-width: 1200px;
    width: 100%;
    max-height: 90vh;
    overflow: hidden;
    display: flex;
    flex-direction: column;
    box-shadow: 0 10px 40px rgba(0, 0, 0, 0.3);
    transform: scale(0.95);
    transition: transform 0.3s ease;
}

.stats-dashboard-overlay[data-visible="true"] .stats-dashboard-content {
    transform: scale(1);
}

/* Dashboard Header */
.stats-dashboard-header {
    display: flex;
    justify-content: space-between;
    align-items: center;
    padding: 20px 24px;
    border-bottom: 2px solid var(--border-color);
    background: linear-gradient(135deg, var(--bg-secondary) 0%, var(--bg-card) 100%);
}

.stats-dashboard-title {
    display: flex;
    align-items: center;
    gap: 12px;
    font-size: 24px;
    font-weight: 600;
    color: var(--accent-primary);
}

.stats-dashboard-title::before {
    content: '📊';
    font-size: 28px;
}

.stats-dashboard-close {
    background: none;
    border: none;
    font-size: 32px;
    color: var(--text-secondary);
    cursor: pointer;
    padding: 4px 12px;
    line-height: 1;
    transition: all 0.2s ease;
    border-radius: 4px;
}

.stats-dashboard-close:hover {
    background: var(--bg-tertiary);
    color: var(--text-primary);
}

/* Dashboard Body */
.stats-dashboard-body {
    overflow-y: auto;
    padding: 24px;
    flex: 1;
}

/* Summary Cards */
.stats-summary-grid {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
    gap: 16px;
    margin-bottom: 24px;
}

.stats-summary-card {
    background: var(--bg-secondary);
    border: 1px solid var(--border-color);
    border-radius: 8px;
    padding: 20px;
    transition: all 0.3s ease;
}

.stats-summary-card:hover {
    border-color: var(--accent-primary);
    box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
    transform: translateY(-2px);
}

.stats-summary-card-label {
    font-size: 13px;
    color: var(--text-secondary);
    text-transform: uppercase;
    letter-spacing: 0.5px;
    margin-bottom: 8px;
    font-weight: 500;
}

.stats-summary-card-value {
    font-size: 32px;
    font-weight: 700;
    color: var(--text-primary);
    line-height: 1.2;
}

.stats-summary-card-unit {
    font-size: 16px;
    color: var(--text-secondary);
    margin-left: 4px;
}

.stats-summary-card-subtitle {
    font-size: 12px;
    color: var(--text-muted);
    margin-top: 8px;
}

/* Icon colors for different card types */
.stats-summary-card.api-calls { border-left: 4px solid var(--blue); }
.stats-summary-card.errors { border-left: 4px solid var(--red); }
.stats-summary-card.response-time { border-left: 4px solid var(--green); }
.stats-summary-card.error-rate { border-left: 4px solid var(--orange); }

/* Chart Sections */
.stats-chart-section {
    background: var(--bg-secondary);
    border: 1px solid var(--border-color);
    border-radius: 8px;
    padding: 20px;
    margin-bottom: 20px;
}

.stats-chart-header {
    display: flex;
    justify-content: space-between;
    align-items: center;
    margin-bottom: 16px;
    padding-bottom: 12px;
    border-bottom: 1px solid var(--border-color);
}

.stats-chart-title {
    font-size: 18px;
    font-weight: 600;
    color: var(--text-primary);
    display: flex;
    align-items: center;
    gap: 8px;
}

.stats-chart-controls {
    display: flex;
    gap: 8px;
}

.stats-chart-button {
    background: var(--bg-tertiary);
    border: 1px solid var(--border-color);
    color: var(--text-secondary);
    padding: 6px 12px;
    border-radius: 4px;
    font-size: 13px;
    cursor: pointer;
    transition: all 0.2s ease;
}

.stats-chart-button:hover {
    background: var(--bg-card);
    border-color: var(--accent-primary);
    color: var(--accent-primary);
}

.stats-chart-button.active {
    background: var(--accent-primary);
    border-color: var(--accent-primary);
    color: white;
}

.stats-chart-container {
    position: relative;
    height: 300px;
    margin-top: 12px;
}

.stats-chart-canvas {
    max-height: 300px;
}

/* Loading State */
.stats-loading {
    display: flex;
    flex-direction: column;
    align-items: center;
    justify-content: center;
    padding: 60px 20px;
    color: var(--text-secondary);
}

.stats-loading-spinner {
    width: 48px;
    height: 48px;
    border: 4px solid var(--bg-tertiary);
    border-top-color: var(--accent-primary);
    border-radius: 50%;
    animation: stats-spin 1s linear infinite;
    margin-bottom: 16px;
}

@keyframes stats-spin {
    to { transform: rotate(360deg); }
}

.stats-loading-text {
    font-size: 16px;
}

/* Error State */
.stats-error {
    background: rgba(239, 83, 80, 0.1);
    border: 1px solid var(--red);
    border-radius: 8px;
    padding: 20px;
    color: var(--red);
    text-align: center;
}

.stats-error-icon {
    font-size: 48px;
    margin-bottom: 12px;
}

.stats-error-message {
    font-size: 16px;
    margin-bottom: 8px;
}

.stats-error-detail {
    font-size: 13px;
    color: var(--text-secondary);
}

/* Empty State */
.stats-empty {
    text-align: center;
    padding: 60px 20px;
    color: var(--text-secondary);
}

.stats-empty-icon {
    font-size: 64px;
    margin-bottom: 16px;
    opacity: 0.5;
}

.stats-empty-message {
    font-size: 18px;
    margin-bottom: 8px;
}

.stats-empty-detail {
    font-size: 14px;
}

/* Alerts Section */
.stats-alerts-list {
    max-height: 200px;
    overflow-y: auto;
}

.stats-alert-item {
    background: var(--bg-card);
    border-left: 4px solid var(--orange);
    padding: 12px 16px;
    margin-bottom: 8px;
    border-radius: 4px;
    display: flex;
    align-items: flex-start;
    gap: 12px;
}

.stats-alert-item.severity-warning {
    border-left-color: var(--orange);
}

.stats-alert-item.severity-error {
    border-left-color: var(--red);
}

.stats-alert-icon {
    font-size: 20px;
    flex-shrink: 0;
}

.stats-alert-content {
    flex: 1;
}

.stats-alert-message {
    font-size: 14px;
    color: var(--text-primary);
    margin-bottom: 4px;
}

.stats-alert-timestamp {
    font-size: 12px;
    color: var(--text-muted);
}

/* Step History Table */
.stats-step-table {
    width: 100%;
    border-collapse: collapse;
}

.stats-step-table th,
.stats-step-table td {
    padding: 12px;
    text-align: left;
    border-bottom: 1px solid var(--border-color);
}

.stats-step-table th {
    background: var(--bg-tertiary);
    color: var(--text-secondary);
    font-weight: 600;
    font-size: 13px;
    text-transform: uppercase;
    letter-spacing: 0.5px;
}

.stats-step-table td {
    color: var(--text-primary);
    font-size: 14px;
}

.stats-step-table tr:hover td {
    background: var(--bg-tertiary);
}

.stats-step-name {
    font-weight: 500;
    color: var(--accent-primary);
}

/* Dashboard Button */
.stats-dashboard-button {
    background: var(--bg-secondary);
    border: 1px solid var(--border-color);
    color: var(--text-primary);
    padding: 8px 16px;
    border-radius: 6px;
    cursor: pointer;
    transition: all 0.3s ease;
    font-size: 14px;
    font-weight: 500;
    display: inline-flex;
    align-items: center;
    gap: 8px;
}

.stats-dashboard-button::before {
    content: '📊';
    font-size: 16px;
}

.stats-dashboard-button:hover {
    background: var(--bg-tertiary);
    border-color: var(--accent-primary);
    color: var(--accent-primary);
    box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
}

/* Responsive Design */
@media (max-width: 768px) {
    .stats-dashboard-content {
        max-width: 100%;
        max-height: 95vh;
        border-radius: 8px;
    }
    
    .stats-summary-grid {
        grid-template-columns: 1fr;
    }
    
    .stats-chart-container {
        height: 250px;
    }
    
    .stats-dashboard-header {
        padding: 16px;
    }
    
    .stats-dashboard-body {
        padding: 16px;
    }
    
    .stats-chart-controls {
        flex-wrap: wrap;
    }
}

/* Scrollbar Styling */
.stats-dashboard-body::-webkit-scrollbar {
    width: 8px;
}

.stats-dashboard-body::-webkit-scrollbar-track {
    background: var(--bg-tertiary);
    border-radius: 4px;
}

.stats-dashboard-body::-webkit-scrollbar-thumb {
    background: var(--border-color);
    border-radius: 4px;
}

.stats-dashboard-body::-webkit-scrollbar-thumb:hover {
    background: var(--accent-primary);
}

/* Refresh Button Animation */
.stats-refresh-button {
    position: relative;
}

.stats-refresh-button.refreshing::before {
    animation: stats-spin 1s linear infinite;
}
```

## File: static/css/utils/animations.css
```css
@keyframes pulseStatus {
    0%, 100% { opacity: 1; }
    50% { opacity: 0.6; }
}

/* Shine pass for active progress bar fill */
@keyframes progressShine {
    0% { left: -100%; }
    100% { left: 100%; }
}

/* Subtle pulse for processing filename text */
@keyframes textPulse {
    0%, 100% { opacity: 1; }
    50% { opacity: 0.7; }
}

/* Generic spinner */
@keyframes spin {
    0% { transform: rotate(0deg); }
    100% { transform: rotate(360deg); }
}

/* Micro-interactions for step cards */
@keyframes subtleOpacityPulse {
    0%, 100% { opacity: 0.22; }
    50% { opacity: 0.42; }
}

@keyframes cardBreath {
    0%, 100% { transform: scale(1); }
    50% { transform: scale(1.01); }
}

/* Brief highlight flash for newly opened sections/titles */
@keyframes highlightFlash {
    0% { box-shadow: 0 0 0 0 color-mix(in oklab, var(--accent-primary) 55%, transparent); }
    100% { box-shadow: 0 0 0 10px transparent; }
}

.section-focus-highlight {
    animation: highlightFlash 0.35s ease-out;
    border-radius: 6px;
}

@media (prefers-reduced-motion: reduce) {
    .section-focus-highlight { animation: none; }
}
```

## File: static/css/themes.css
```css
/* =============================================================================
   THEME SYSTEM - Workflow MediaPipe v4.0
   Dynamic theme definitions with smooth transitions
   ============================================================================= */

/* Base theme transition for smooth switching */
:root {
    transition: background-color 0.3s ease, color 0.3s ease;
}

body {
    transition: background-color 0.3s ease, color 0.3s ease;
}

/* Apply transitions to all theme-dependent elements */
.card-like-section,
.step,
.popup-content,
.unified-controls-section,
button,
input,
select,
textarea {
    transition: background-color 0.3s ease, 
                color 0.3s ease, 
                border-color 0.3s ease,
                box-shadow 0.3s ease;
}

/* =============================================================================
   THEME 1: DARK PRO (Default - Current)
   ============================================================================= */
[data-theme="dark-pro"],
:root:not([data-theme]) {
    --bg-dark: #1e1e2f;
    --bg-card: #2c2c3e;
    --bg-secondary: #2c2c3e;
    --bg-tertiary: #3a3a4e;
    --text-primary: #e0e0e0;
    --text-secondary: #a0a0b0;
    --text-muted: #707080;
    --accent-primary: #7986cb;
    --accent-secondary: #ff8a65;
    --border-color: #39394d;
    --green: #66bb6a;
    --red: #ef5350;
    --yellow: #ffee58;
    --blue: #42a5f5;
    --orange: #ffb74d;
    --log-bg: #161625;
    --log-text: #c0c0d0;
    --border-color-translucent: #39394d88;
}

/* =============================================================================
   THEME 2: LIGHT MODE (Professional Light)
   ============================================================================= */
[data-theme="light-mode"] {
    --bg-dark: #f5f7fa;
    --bg-card: #ffffff;
    --bg-secondary: #ffffff;
    --bg-tertiary: #e8eef3;
    --text-primary: #2c3e50;
    --text-secondary: #546e7a;
    --text-muted: #90a4ae;
    --accent-primary: #5c6bc0;
    --accent-secondary: #ff7043;
    --border-color: #cfd8dc;
    --green: #43a047;
    --red: #e53935;
    --yellow: #fbc02d;
    --blue: #1e88e5;
    --orange: #fb8c00;
    --log-bg: #f9fafb;
    --log-text: #37474f;
    --border-color-translucent: #cfd8dc88;
}

/* =============================================================================
   THEME 3: PASTEL ZEN (Soft and Calming)
   ============================================================================= */
[data-theme="pastel-zen"] {
    --bg-dark: #faf3f3;
    --bg-card: #fff5f5;
    --bg-secondary: #ffe8f0;
    --bg-tertiary: #f3e8ff;
    --text-primary: #4a4a5a;
    --text-secondary: #7a7a8a;
    --text-muted: #aaaaba;
    --accent-primary: #b39ddb;
    --accent-secondary: #f48fb1;
    --border-color: #e1d5e7;
    --green: #81c784;
    --red: #ef9a9a;
    --yellow: #fff59d;
    --blue: #90caf9;
    --orange: #ffcc80;
    --log-bg: #fef7ff;
    --log-text: #5a5a6a;
    --border-color-translucent: #e1d5e788;
}

/* =============================================================================
   THEME 4: NEON CYBERPUNK (Vibrant and Futuristic)
   ============================================================================= */
[data-theme="neon-cyberpunk"] {
    --bg-dark: #0a0e27;
    --bg-card: #1a1f3a;
    --bg-secondary: #1a1f3a;
    --bg-tertiary: #2a2f4a;
    --text-primary: #e0f7ff;
    --text-secondary: #a0d7ff;
    --text-muted: #6097bf;
    --accent-primary: #00d9ff;
    --accent-secondary: #ff006e;
    --border-color: #00ffff;
    --green: #00ff88;
    --red: #ff0066;
    --yellow: #ffff00;
    --blue: #00d4ff;
    --orange: #ff9500;
    --log-bg: #050816;
    --log-text: #b0e7ff;
    --border-color-translucent: #00ffff44;
}

/* Neon glow effects for cyberpunk theme */
[data-theme="neon-cyberpunk"] button:hover {
    box-shadow: 0 0 15px var(--accent-primary);
}

[data-theme="neon-cyberpunk"] .step.running {
    box-shadow: 0 0 20px var(--blue);
}

[data-theme="neon-cyberpunk"] .step.completed {
    box-shadow: 0 0 20px var(--green);
}

[data-theme="neon-cyberpunk"] .step.error {
    box-shadow: 0 0 20px var(--red);
}

/* =============================================================================
   THEME 5: FOREST NIGHT (Natural and Earthy)
   ============================================================================= */
[data-theme="forest-night"] {
    --bg-dark: #1a2421;
    --bg-card: #243530;
    --bg-secondary: #2a3f3a;
    --bg-tertiary: #344a44;
    --text-primary: #d4e8d4;
    --text-secondary: #a4c8a4;
    --text-muted: #748874;
    --accent-primary: #66bb6a;
    --accent-secondary: #ffb74d;
    --border-color: #3d5a4d;
    --green: #81c784;
    --red: #e57373;
    --yellow: #fff176;
    --blue: #64b5f6;
    --orange: #ffb74d;
    --log-bg: #151f1a;
    --log-text: #c4d8c4;
    --border-color-translucent: #3d5a4d88;
}

/* =============================================================================
   THEME 6: OCEAN DEPTH (Deep Blue Tones)
   ============================================================================= */
[data-theme="ocean-depth"] {
    --bg-dark: #0d1b2a;
    --bg-card: #1b263b;
    --bg-secondary: #1b263b;
    --bg-tertiary: #2a3f5f;
    --text-primary: #e0f4ff;
    --text-secondary: #a8dadc;
    --text-muted: #6d9bb5;
    --accent-primary: #457b9d;
    --accent-secondary: #f1faee;
    --border-color: #264653;
    --green: #52b788;
    --red: #ef476f;
    --yellow: #ffd166;
    --blue: #118ab2;
    --orange: #06d6a0;
    --log-bg: #081420;
    --log-text: #d0e4f0;
    --border-color-translucent: #26465388;
}

/* =============================================================================
   THEME SELECTOR STYLING
   ============================================================================= */
.theme-selector-container {
    display: inline-flex;
    align-items: center;
    gap: 8px;
    margin-left: 16px;
    padding: 8px 12px;
    background: var(--bg-card);
    border: 1px solid var(--border-color);
    border-radius: 6px;
    transition: all 0.3s ease;
}

.theme-selector-container:hover {
    border-color: var(--accent-primary);
    box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
}

.theme-selector-label {
    font-size: 14px;
    color: var(--text-secondary);
    font-weight: 500;
    white-space: nowrap;
}

.theme-selector {
    background: var(--bg-secondary);
    color: var(--text-primary);
    border: 1px solid var(--border-color);
    border-radius: 4px;
    padding: 6px 10px;
    font-size: 14px;
    cursor: pointer;
    outline: none;
    transition: all 0.2s ease;
    min-width: 140px;
}

.theme-selector:hover {
    border-color: var(--accent-primary);
    background: var(--bg-tertiary);
}

.theme-selector:focus {
    border-color: var(--accent-primary);
    box-shadow: 0 0 0 3px rgba(121, 134, 203, 0.3);
}

.theme-selector option {
    background: var(--bg-card);
    color: var(--text-primary);
    padding: 8px;
}

/* Icon in theme selector */
.theme-selector-container::before {
    content: '🎨';
    font-size: 18px;
}

/* =============================================================================
   RESPONSIVE ADJUSTMENTS
   ============================================================================= */
@media (max-width: 768px) {
    .theme-selector-container {
        margin-left: 0;
        margin-top: 8px;
        width: 100%;
        justify-content: space-between;
    }
    
    .theme-selector {
        flex: 1;
    }
}
```

## File: static/test-exports/fetchWithLoadingState.js
```javascript
// Standalone test export for fetchWithLoadingState (no optional chaining, no external deps)
export async function fetchWithLoadingState(url, options = {}, buttonElOrId = null) {
  let btn = null;
  if (typeof buttonElOrId === 'string' && typeof document !== 'undefined') {
    btn = document.getElementById(buttonElOrId);
  } else if (buttonElOrId && buttonElOrId.nodeType === 1) {
    btn = buttonElOrId;
  }
  try {
    if (btn) {
      btn.setAttribute('data-loading', 'true');
      btn.disabled = true;
    }
    const response = await fetch(url, options);
    let data = {};
    try { data = await response.json(); } catch (e) { data = {}; }
    if (!response.ok) {
      throw new Error((data && data.message) || ('Erreur HTTP ' + response.status));
    }
    return data;
  } finally {
    if (btn) {
      btn.removeAttribute('data-loading');
      btn.disabled = false;
    }
  }
}
```

## File: static/utils/DOMBatcher.js
```javascript
/**
 * DOM Update Batcher
 * Batches DOM updates using requestAnimationFrame for optimal performance.
 */

class DOMUpdateBatcher {
    constructor() {
        this.pendingUpdates = new Map();
        this.rafId = null;
        this.isDestroyed = false;
        this.updateCount = 0;
        this.lastFlushTime = 0;
        
        this.stats = {
            totalUpdates: 0,
            batchedUpdates: 0,
            averageBatchSize: 0,
            lastBatchSize: 0
        };
        
        console.debug('[DOMBatcher] Initialized');
    }
    
    /**
     * Schedule a DOM update to be batched.
     * @param {string} key - Unique key for the update (prevents duplicates)
     * @param {Function} updateFn - Function that performs DOM updates
     * @param {number} [priority] - Update priority (lower = higher priority)
     */
    scheduleUpdate(key, updateFn, priority = 0) {
        if (this.isDestroyed) {
            console.warn('[DOMBatcher] Attempted to schedule update on destroyed batcher');
            return;
        }
        
        if (typeof updateFn !== 'function') {
            console.error('[DOMBatcher] Update function must be a function');
            return;
        }
        
        this.pendingUpdates.set(key, {
            updateFn,
            priority,
            timestamp: performance.now()
        });
        
        this.updateCount++;
        
        if (!this.rafId) {
            this.rafId = requestAnimationFrame(() => {
                this.flushUpdates();
            });
        }
    }
    
    /**
     * Schedule multiple related updates as a group.
     * @param {string} groupKey - Key for the update group
     * @param {Object} updates - Object with update keys and functions
     * @param {number} [priority] - Priority for the entire group
     */
    scheduleUpdateGroup(groupKey, updates, priority = 0) {
        Object.entries(updates).forEach(([key, updateFn]) => {
            this.scheduleUpdate(`${groupKey}:${key}`, updateFn, priority);
        });
    }
    
    /**
     * Schedule a high-priority update that should be processed first.
     * @param {string} key - Unique key for the update
     * @param {Function} updateFn - Function that performs DOM updates
     */
    scheduleHighPriorityUpdate(key, updateFn) {
        this.scheduleUpdate(key, updateFn, -1);
    }
    
    /**
     * Cancel a scheduled update.
     * @param {string} key - Key of the update to cancel
     * @returns {boolean} True if update was cancelled
     */
    cancelUpdate(key) {
        return this.pendingUpdates.delete(key);
    }
    
    /**
     * Cancel all updates matching a pattern.
     * @param {RegExp|string} pattern - Pattern to match against keys
     * @returns {number} Number of cancelled updates
     */
    cancelUpdatesMatching(pattern) {
        let cancelled = 0;
        const regex = pattern instanceof RegExp ? pattern : new RegExp(pattern);
        
        for (const key of this.pendingUpdates.keys()) {
            if (regex.test(key)) {
                this.pendingUpdates.delete(key);
                cancelled++;
            }
        }
        
        return cancelled;
    }
    
    /**
     * Force immediate flush of all pending updates.
     */
    flushUpdates() {
        if (this.isDestroyed || this.pendingUpdates.size === 0) {
            this.rafId = null;
            return;
        }
        
        const startTime = performance.now();
        const batchSize = this.pendingUpdates.size;
        
        try {
            const sortedUpdates = Array.from(this.pendingUpdates.entries())
                .sort(([, a], [, b]) => a.priority - b.priority);
            
            for (const [key, { updateFn }] of sortedUpdates) {
                try {
                    updateFn();
                } catch (error) {
                    console.error(`[DOMBatcher] Update error for key "${key}":`, error);
                }
            }
            
            this.stats.totalUpdates += batchSize;
            this.stats.batchedUpdates++;
            this.stats.lastBatchSize = batchSize;
            this.stats.averageBatchSize = this.stats.totalUpdates / this.stats.batchedUpdates;
            
            const flushTime = performance.now() - startTime;
            this.lastFlushTime = flushTime;
            
            if (flushTime > 16) { // More than one frame
                console.warn(`[DOMBatcher] Slow batch update: ${flushTime.toFixed(2)}ms for ${batchSize} updates`);
            }
            
        } catch (error) {
            console.error('[DOMBatcher] Flush error:', error);
        } finally {
            this.pendingUpdates.clear();
            this.rafId = null;
        }
    }
    
    /**
     * Get current statistics about batching performance.
     * @returns {Object} Statistics object
     */
    getStats() {
        return {
            ...this.stats,
            pendingUpdates: this.pendingUpdates.size,
            isScheduled: this.rafId !== null,
            lastFlushTime: this.lastFlushTime,
            updateCount: this.updateCount
        };
    }
    
    /**
     * Reset statistics.
     */
    resetStats() {
        this.stats = {
            totalUpdates: 0,
            batchedUpdates: 0,
            averageBatchSize: 0,
            lastBatchSize: 0
        };
        this.updateCount = 0;
        this.lastFlushTime = 0;
        
        console.debug('[DOMBatcher] Statistics reset');
    }
    
    /**
     * Check if there are pending updates.
     * @returns {boolean} True if updates are pending
     */
    hasPendingUpdates() {
        return this.pendingUpdates.size > 0;
    }
    
    /**
     * Get list of pending update keys.
     * @returns {string[]} Array of pending update keys
     */
    getPendingUpdateKeys() {
        return Array.from(this.pendingUpdates.keys());
    }
    
    /**
     * Destroy the batcher and cleanup resources.
     */
    destroy() {
        if (this.rafId) {
            cancelAnimationFrame(this.rafId);
            this.rafId = null;
        }
        
        this.pendingUpdates.clear();
        this.isDestroyed = true;
        
        console.debug('[DOMBatcher] Destroyed');
    }
}

/**
 * Performance-optimized DOM update utilities.
 */
class DOMUpdateUtils {
    /**
     * Batch update multiple element properties.
     * @param {HTMLElement} element - Target element
     * @param {Object} properties - Properties to update
     */
    static updateElementProperties(element, properties) {
        if (!element) return;
        
        if (properties.style) {
            Object.assign(element.style, properties.style);
        }
        
        if (properties.attributes) {
            Object.entries(properties.attributes).forEach(([attr, value]) => {
                if (value === null || value === undefined) {
                    element.removeAttribute(attr);
                } else {
                    element.setAttribute(attr, value);
                }
            });
        }
        
        Object.entries(properties).forEach(([prop, value]) => {
            if (prop !== 'style' && prop !== 'attributes' && prop in element) {
                element[prop] = value;
            }
        });
    }
    
    /**
     * Efficiently update text content with HTML escaping.
     * @param {HTMLElement} element - Target element
     * @param {string} text - Text content
     * @param {boolean} [escapeHtml] - Whether to escape HTML
     */
    static updateTextContent(element, text, escapeHtml = true) {
        if (!element) return;
        
        const content = escapeHtml ? DOMUpdateUtils.escapeHtml(text) : text;
        
        if (escapeHtml) {
            element.textContent = content;
        } else {
            element.innerHTML = content;
        }
    }
    
    /**
     * Escape HTML characters.
     * @param {string} text - Text to escape
     * @returns {string} Escaped text
     */
    static escapeHtml(text) {
        const div = document.createElement('div');
        div.textContent = text;
        return div.innerHTML;
    }
    
    /**
     * Update progress bar with animation.
     * @param {HTMLElement} progressBar - Progress bar element
     * @param {number} percentage - Progress percentage (0-100)
     * @param {boolean} [animate] - Whether to animate the change
     */
    static updateProgressBar(progressBar, percentage, animate = true) {
        if (!progressBar) return;
        
        const clampedPercentage = Math.max(0, Math.min(100, percentage));
        
        if (animate) {
            progressBar.style.transition = 'width 0.3s ease-in-out';
        } else {
            progressBar.style.transition = 'none';
        }
        
        progressBar.style.width = `${clampedPercentage}%`;
        progressBar.setAttribute('aria-valuenow', clampedPercentage);
        
        const textElement = progressBar.querySelector('.progress-text');
        if (textElement) {
            textElement.textContent = `${Math.round(clampedPercentage)}%`;
        }
    }
    
    /**
     * Update element visibility with optional animation.
     * @param {HTMLElement} element - Target element
     * @param {boolean} visible - Whether element should be visible
     * @param {string} [animation] - Animation type ('fade', 'slide', 'none')
     */
    static updateVisibility(element, visible, animation = 'none') {
        if (!element) return;
        
        if (animation === 'fade') {
            element.style.transition = 'opacity 0.3s ease-in-out';
            element.style.opacity = visible ? '1' : '0';
            element.style.display = visible ? '' : 'none';
        } else if (animation === 'slide') {
            element.style.transition = 'max-height 0.3s ease-in-out';
            element.style.maxHeight = visible ? '1000px' : '0';
            element.style.overflow = 'hidden';
        } else {
            element.style.display = visible ? '' : 'none';
        }
    }
}

// Create and export singleton instance
export const domBatcher = new DOMUpdateBatcher();

// Cleanup on page unload
window.addEventListener('beforeunload', () => {
    domBatcher.destroy();
});

// Export utilities
export { DOMUpdateUtils };

// Development helpers
if (window.location.hostname === 'localhost' || window.location.hostname === '127.0.0.1') {
window.domBatcher = domBatcher;
    
    const originalFlush = domBatcher.flushUpdates;
    domBatcher.flushUpdates = function() {
        const startTime = performance.now();
        originalFlush.call(this);
        const duration = performance.now() - startTime;
        
        if (duration > 16) {
            console.warn(`[DOMBatcher] Performance warning: ${duration.toFixed(2)}ms flush time`);
        }
    };
}

export default domBatcher;
```

## File: static/utils/ErrorHandler.js
```javascript
/**
 * Comprehensive error handling utility for workflow_mediapipe frontend.
 * 
 * This module provides centralized error handling with user feedback,
 * exponential backoff for repeated errors, and proper error reporting.
 */

class ErrorHandler {
    constructor() {
        this.consecutiveErrors = new Map();
        this.errorHistory = [];
        this.maxHistorySize = 100;
        this.notificationTimeouts = new Map();
        
        this._bindGlobalErrorHandlers();
        
        console.debug('ErrorHandler initialized');
    }

    /**
     * Handle polling errors with exponential backoff and user feedback.
     * 
     * @param {string} operation - Name of the operation that failed
     * @param {Error} error - The error that occurred
     * @param {Object} context - Additional context information
     * @returns {number} - Delay in milliseconds before retry (0 = no delay)
     */
    async handlePollingError(operation, error, context = {}) {
        const errorKey = `${operation}_${context.stepKey || 'global'}`;
        const count = this.consecutiveErrors.get(errorKey) || 0;
        const newCount = count + 1;
        
        this.consecutiveErrors.set(errorKey, newCount);
        
        console.error(`Polling error in ${operation} (attempt ${newCount}):`, error);
        
        this._addToHistory({
            operation,
            error: error.message || error.toString(),
            context,
            count: newCount,
            timestamp: new Date().toISOString()
        });
        
        if (newCount >= 3) {
            this._showErrorNotification(
                operation,
                `Unable to update ${operation}. Retrying...`,
                'warning',
                context.elementId
            );
        }
        
        if (newCount >= 5) {
            this._showErrorNotification(
                operation,
                `${operation} is experiencing persistent issues. Please check your connection.`,
                'error',
                context.elementId
            );
        }
        
        let delay = 0;
        if (newCount >= 3) {
            delay = Math.min(30000, 2000 * Math.pow(2, newCount - 3));
        }
        
        this._dispatchErrorEvent(operation, error, newCount, delay);
        
        return delay;
    }

    /**
     * Clear error state for a successful operation.
     * 
     * @param {string} operation - Name of the operation that succeeded
     * @param {Object} context - Additional context information
     */
    clearErrors(operation, context = {}) {
        const errorKey = `${operation}_${context.stepKey || 'global'}`;
        const hadErrors = this.consecutiveErrors.has(errorKey);
        
        this.consecutiveErrors.delete(errorKey);
        
        if (hadErrors) {
            console.debug(`Cleared error state for ${operation}`);
            this._clearErrorNotification(operation);
            
            if (context.elementId) {
                this.clearErrorState(context.elementId);
            }
        }
    }

    /**
     * Handle API errors with proper user feedback.
     * 
     * @param {string} endpoint - API endpoint that failed
     * @param {Error} error - The error that occurred
     * @param {Object} context - Additional context information
     */
    handleApiError(endpoint, error, context = {}) {
        console.error(`API error for ${endpoint}:`, error);
        
        this._addToHistory({
            type: 'api',
            endpoint,
            error: error.message || error.toString(),
            context,
            timestamp: new Date().toISOString()
        });
        
        let message = 'An unexpected error occurred';
        let type = 'error';
        
        if (error.name === 'TypeError' && error.message.includes('fetch')) {
            message = 'Network connection error. Please check your internet connection.';
            type = 'warning';
        } else if (error.message.includes('401')) {
            message = 'Authentication error. Please refresh the page.';
            type = 'error';
        } else if (error.message.includes('404')) {
            message = 'Service not found. Please contact support.';
            type = 'error';
        } else if (error.message.includes('500')) {
            message = 'Server error. Please try again later.';
            type = 'error';
        } else if (error.message) {
            message = error.message;
        }
        
        this._showErrorNotification(
            `api-${endpoint}`,
            message,
            type
        );
        
        this._dispatchErrorEvent(`api-${endpoint}`, error, 1, 0);
    }

    /**
     * Show error state on a specific UI element.
     * 
     * @param {string} elementId - ID of the element to show error state
     * @param {string} message - Error message to display
     */
    showErrorState(elementId, message) {
        const element = document.getElementById(elementId);
        if (!element) {
            console.warn(`Element not found for error state: ${elementId}`);
            return;
        }
        
        element.classList.add('error-state');
        
        let errorElement = element.querySelector('.error-message');
        if (!errorElement) {
            errorElement = document.createElement('div');
            errorElement.className = 'error-message';
            element.appendChild(errorElement);
        }
        
        errorElement.textContent = message;
        errorElement.style.display = 'block';
        
        console.debug(`Showing error state for ${elementId}: ${message}`);
    }

    /**
     * Clear error state from a specific UI element.
     * 
     * @param {string} elementId - ID of the element to clear error state
     */
    clearErrorState(elementId) {
        const element = document.getElementById(elementId);
        if (!element) {
            return;
        }
        
        element.classList.remove('error-state');
        
        // Hide error message
        const errorElement = element.querySelector('.error-message');
        if (errorElement) {
            errorElement.style.display = 'none';
        }
        
        console.debug(`Cleared error state for ${elementId}`);
    }

    /**
     * Get error statistics for monitoring.
     * 
     * @returns {Object} - Error statistics
     */
    getErrorStats() {
        const now = Date.now();
        const recentErrors = this.errorHistory.filter(
            error => (now - new Date(error.timestamp).getTime()) < 300000
        );
        
        return {
            totalErrors: this.errorHistory.length,
            recentErrors: recentErrors.length,
            activeErrorOperations: Array.from(this.consecutiveErrors.keys()),
            errorsByOperation: this._groupErrorsByOperation(recentErrors)
        };
    }

    /**
     * Clear all error history and state.
     */
    clearAllErrors() {
        this.consecutiveErrors.clear();
        this.errorHistory = [];
        
        this.notificationTimeouts.forEach((timeout, key) => {
            clearTimeout(timeout);
            this._clearErrorNotification(key);
        });
        this.notificationTimeouts.clear();
        
        console.debug('Cleared all error state');
    }

    /**
     * Add error to history with size limit.
     * @private
     */
    _addToHistory(errorInfo) {
        this.errorHistory.push(errorInfo);
        
        // Maintain history size limit
        if (this.errorHistory.length > this.maxHistorySize) {
            this.errorHistory.shift();
        }
    }

    /**
     * Show error notification with deduplication.
     * @private
     */
    _showErrorNotification(key, message, type = 'error', elementId = null) {
        const existingTimeout = this.notificationTimeouts.get(key);
        if (existingTimeout) {
            clearTimeout(existingTimeout);
        }
        
        if (typeof window.showNotification === 'function') {
            window.showNotification(message, type, 5000);
        } else {
            console.warn('showNotification function not available');
        }
        
        if (elementId) {
            this.showErrorState(elementId, message);
        }
        
        const timeout = setTimeout(() => {
            this._clearErrorNotification(key);
        }, 10000);
        
        this.notificationTimeouts.set(key, timeout);
    }

    /**
     * Clear error notification.
     * @private
     */
    _clearErrorNotification(key) {
        const timeout = this.notificationTimeouts.get(key);
        if (timeout) {
            clearTimeout(timeout);
            this.notificationTimeouts.delete(key);
        }
    }

    /**
     * Dispatch custom error event.
     * @private
     */
    _dispatchErrorEvent(operation, error, count, delay) {
        const event = new CustomEvent('applicationError', {
            detail: {
                operation,
                error: error.message || error.toString(),
                count,
                delay,
                timestamp: new Date().toISOString()
            }
        });
        
        window.dispatchEvent(event);
    }

    /**
     * Group errors by operation for statistics.
     * @private
     */
    _groupErrorsByOperation(errors) {
        const grouped = {};
        
        errors.forEach(error => {
            const operation = error.operation || error.endpoint || 'unknown';
            if (!grouped[operation]) {
                grouped[operation] = 0;
            }
            grouped[operation]++;
        });
        
        return grouped;
    }

    /**
     * Bind global error handlers.
     * @private
     */
    _bindGlobalErrorHandlers() {
        window.addEventListener('unhandledrejection', (event) => {
            console.error('Unhandled promise rejection:', event.reason);
            this.handleApiError('unhandled-promise', event.reason);
        });
        
        window.addEventListener('error', (event) => {
            console.error('Global JavaScript error:', event.error);
            this._addToHistory({
                type: 'javascript',
                error: event.error?.message || event.message,
                filename: event.filename,
                lineno: event.lineno,
                colno: event.colno,
                timestamp: new Date().toISOString()
            });
        });
    }
}

// Create and export global error handler instance
const errorHandler = new ErrorHandler();

// Export for use in other modules
export { ErrorHandler, errorHandler };

// Also make available globally for legacy code
window.errorHandler = errorHandler;
```

## File: static/utils/PerformanceMonitor.js
```javascript
/**
 * Frontend Performance Monitor
 * Monitors and reports frontend performance metrics.
 */

import { performanceOptimizer } from './PerformanceOptimizer.js';
import { domBatcher } from './DOMBatcher.js';
import { pollingManager } from './PollingManager.js';
import { errorHandler } from './ErrorHandler.js';

class PerformanceMonitor {
    constructor() {
        this.isMonitoring = false;
        this.metrics = {
            pageLoad: null,
            apiCalls: [],
            domUpdates: [],
            memoryUsage: [],
            userInteractions: []
        };
        this.observers = new Map();
        this.startTime = performance.now();
        
        console.debug('[PerformanceMonitor] Initialized');
    }
    
    /**
     * Start performance monitoring.
     */
    startMonitoring() {
        if (this.isMonitoring) {
            console.warn('[PerformanceMonitor] Already monitoring');
            return;
        }
        
        this.isMonitoring = true;
        
        // Monitor page load performance
        this.monitorPageLoad();
        
        // Monitor API calls
        this.monitorApiCalls();
        
        // Monitor DOM updates
        this.monitorDomUpdates();
        
        // Monitor memory usage
        this.monitorMemoryUsage();
        
        // Monitor user interactions
        this.monitorUserInteractions();
        
        // Monitor long tasks
        this.monitorLongTasks();
        
        // Start periodic reporting
        this.startPeriodicReporting();
        
        console.info('[PerformanceMonitor] Monitoring started');
    }
    
    /**
     * Stop performance monitoring.
     */
    stopMonitoring() {
        if (!this.isMonitoring) return;
        
        this.isMonitoring = false;
        
        // Disconnect all observers
        this.observers.forEach(observer => {
            if (observer.disconnect) observer.disconnect();
        });
        this.observers.clear();
        
        // Stop periodic reporting
        if (this.reportingInterval) {
            clearInterval(this.reportingInterval);
            this.reportingInterval = null;
        }
        
        console.info('[PerformanceMonitor] Monitoring stopped');
    }
    
    /**
     * Monitor page load performance.
     */
    monitorPageLoad() {
        if (document.readyState === 'complete') {
            this.recordPageLoadMetrics();
        } else {
            window.addEventListener('load', () => {
                this.recordPageLoadMetrics();
            });
        }
    }
    
    /**
     * Record page load metrics.
     */
    recordPageLoadMetrics() {
        const navigation = performance.getEntriesByType('navigation')[0];
        if (!navigation) return;
        
        this.metrics.pageLoad = {
            domContentLoaded: navigation.domContentLoadedEventEnd - navigation.domContentLoadedEventStart,
            loadComplete: navigation.loadEventEnd - navigation.loadEventStart,
            domInteractive: navigation.domInteractive - navigation.navigationStart,
            firstPaint: this.getFirstPaint(),
            firstContentfulPaint: this.getFirstContentfulPaint(),
            timestamp: Date.now()
        };
        
        console.debug('[PerformanceMonitor] Page load metrics recorded:', this.metrics.pageLoad);
    }
    
    /**
     * Get First Paint timing.
     * @returns {number|null} First Paint time or null
     */
    getFirstPaint() {
        const paintEntries = performance.getEntriesByType('paint');
        const firstPaint = paintEntries.find(entry => entry.name === 'first-paint');
        return firstPaint ? firstPaint.startTime : null;
    }
    
    /**
     * Get First Contentful Paint timing.
     * @returns {number|null} First Contentful Paint time or null
     */
    getFirstContentfulPaint() {
        const paintEntries = performance.getEntriesByType('paint');
        const fcp = paintEntries.find(entry => entry.name === 'first-contentful-paint');
        return fcp ? fcp.startTime : null;
    }
    
    /**
     * Monitor API calls by wrapping fetch.
     */
    monitorApiCalls() {
        const originalFetch = window.fetch;
        
        window.fetch = async (...args) => {
            const startTime = performance.now();
            const url = args[0];
            
            try {
                const response = await originalFetch(...args);
                const duration = performance.now() - startTime;
                
                this.recordApiCall({
                    url,
                    method: args[1]?.method || 'GET',
                    status: response.status,
                    duration,
                    success: response.ok,
                    timestamp: Date.now()
                });
                
                return response;
            } catch (error) {
                const duration = performance.now() - startTime;
                
                this.recordApiCall({
                    url,
                    method: args[1]?.method || 'GET',
                    status: 0,
                    duration,
                    success: false,
                    error: error.message,
                    timestamp: Date.now()
                });
                
                throw error;
            }
        };
    }
    
    /**
     * Record API call metrics.
     * @param {Object} metric - API call metric
     */
    recordApiCall(metric) {
        this.metrics.apiCalls.push(metric);
        
        // Keep only last 50 API calls
        if (this.metrics.apiCalls.length > 50) {
            this.metrics.apiCalls.shift();
        }
        
        // Log slow API calls
        if (metric.duration > 1000) {
            console.warn(`[PerformanceMonitor] Slow API call: ${metric.url} took ${metric.duration.toFixed(2)}ms`);
        }
    }
    
    /**
     * Monitor DOM updates using MutationObserver.
     */
    monitorDomUpdates() {
        if (!window.MutationObserver) return;
        
        const observer = new MutationObserver((mutations) => {
            const updateCount = mutations.length;
            const timestamp = Date.now();
            
            this.recordDomUpdate({
                mutationCount: updateCount,
                timestamp
            });
        });
        
        observer.observe(document.body, {
            childList: true,
            subtree: true,
            attributes: true,
            attributeOldValue: false,
            characterData: true,
            characterDataOldValue: false
        });
        
        this.observers.set('domUpdates', observer);
    }
    
    /**
     * Record DOM update metrics.
     * @param {Object} metric - DOM update metric
     */
    recordDomUpdate(metric) {
        this.metrics.domUpdates.push(metric);
        
        // Keep only last 100 DOM updates
        if (this.metrics.domUpdates.length > 100) {
            this.metrics.domUpdates.shift();
        }
    }
    
    /**
     * Monitor memory usage periodically.
     */
    monitorMemoryUsage() {
        if (!performance.memory) return;
        
        const recordMemory = () => {
            if (!this.isMonitoring) return;
            
            const memory = performance.memory;
            this.metrics.memoryUsage.push({
                usedJSHeapSize: memory.usedJSHeapSize,
                totalJSHeapSize: memory.totalJSHeapSize,
                jsHeapSizeLimit: memory.jsHeapSizeLimit,
                usagePercent: (memory.usedJSHeapSize / memory.jsHeapSizeLimit) * 100,
                timestamp: Date.now()
            });
            
            // Keep only last 20 memory measurements
            if (this.metrics.memoryUsage.length > 20) {
                this.metrics.memoryUsage.shift();
            }
            
            // Check for memory leaks
            const currentUsage = memory.usedJSHeapSize / memory.jsHeapSizeLimit;
            if (currentUsage > 0.8) {
                console.warn(`[PerformanceMonitor] High memory usage: ${(currentUsage * 100).toFixed(1)}%`);
            }
        };
        
        // Record immediately and then every 10 seconds
        recordMemory();
        const memoryInterval = setInterval(recordMemory, 10000);
        this.observers.set('memoryUsage', { disconnect: () => clearInterval(memoryInterval) });
    }
    
    /**
     * Monitor user interactions.
     */
    monitorUserInteractions() {
        const interactionTypes = ['click', 'keydown', 'scroll', 'resize'];
        
        interactionTypes.forEach(type => {
            const handler = performanceOptimizer.throttle(`interaction_${type}`, (event) => {
                this.recordUserInteraction({
                    type,
                    target: event.target.tagName,
                    timestamp: Date.now()
                });
            }, 100);
            
            document.addEventListener(type, handler, { passive: true });
            
            this.observers.set(`interaction_${type}`, {
                disconnect: () => document.removeEventListener(type, handler)
            });
        });
    }
    
    /**
     * Record user interaction metrics.
     * @param {Object} metric - User interaction metric
     */
    recordUserInteraction(metric) {
        this.metrics.userInteractions.push(metric);
        
        // Keep only last 50 interactions
        if (this.metrics.userInteractions.length > 50) {
            this.metrics.userInteractions.shift();
        }
    }
    
    /**
     * Monitor long tasks using PerformanceObserver.
     */
    monitorLongTasks() {
        if (!window.PerformanceObserver) return;
        
        try {
            const observer = new PerformanceObserver((list) => {
                for (const entry of list.getEntries()) {
                    if (entry.duration > 50) { // Tasks longer than 50ms
                        console.warn(`[PerformanceMonitor] Long task detected: ${entry.duration.toFixed(2)}ms`);
                        
                        this.recordLongTask({
                            duration: entry.duration,
                            startTime: entry.startTime,
                            timestamp: Date.now()
                        });
                    }
                }
            });
            
            observer.observe({ entryTypes: ['longtask'] });
            this.observers.set('longTasks', observer);
            
        } catch (error) {
            console.debug('[PerformanceMonitor] Long task monitoring not supported');
        }
    }
    
    /**
     * Record long task metrics.
     * @param {Object} metric - Long task metric
     */
    recordLongTask(metric) {
        if (!this.metrics.longTasks) {
            this.metrics.longTasks = [];
        }
        
        this.metrics.longTasks.push(metric);
        
        // Keep only last 20 long tasks
        if (this.metrics.longTasks.length > 20) {
            this.metrics.longTasks.shift();
        }
    }
    
    /**
     * Start periodic performance reporting.
     */
    startPeriodicReporting() {
        this.reportingInterval = setInterval(() => {
            this.sendPerformanceReport();
        }, 60000); // Every minute
    }
    
    /**
     * Send performance report to backend.
     */
    async sendPerformanceReport() {
        if (!this.isMonitoring) return;
        
        try {
            const report = this.generatePerformanceReport();
            
            // Send to backend performance API
            await fetch('/api/performance/frontend', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json'
                },
                body: JSON.stringify(report)
            });
            
        } catch (error) {
            console.debug('[PerformanceMonitor] Failed to send performance report:', error);
        }
    }
    
    /**
     * Generate comprehensive performance report.
     * @returns {Object} Performance report
     */
    generatePerformanceReport() {
        const optimizerStats = performanceOptimizer.getPerformanceStats();
        const batcherStats = domBatcher.getStats();
        const pollingStats = pollingManager.getStats();
        const errorStats = errorHandler.getStats();
        
        return {
            timestamp: Date.now(),
            uptime: performance.now() - this.startTime,
            pageLoad: this.metrics.pageLoad,
            apiCalls: this.getApiCallSummary(),
            domUpdates: this.getDomUpdateSummary(),
            memoryUsage: this.getMemoryUsageSummary(),
            userInteractions: this.getUserInteractionSummary(),
            longTasks: this.metrics.longTasks || [],
            components: {
                optimizer: optimizerStats,
                batcher: batcherStats,
                polling: pollingStats,
                errorHandler: errorStats
            }
        };
    }
    
    /**
     * Get API call summary.
     * @returns {Object} API call summary
     */
    getApiCallSummary() {
        const calls = this.metrics.apiCalls;
        if (calls.length === 0) return { count: 0 };
        
        const durations = calls.map(c => c.duration);
        const errors = calls.filter(c => !c.success);
        
        return {
            count: calls.length,
            averageDuration: durations.reduce((a, b) => a + b, 0) / durations.length,
            maxDuration: Math.max(...durations),
            errorRate: errors.length / calls.length,
            slowCalls: calls.filter(c => c.duration > 1000).length
        };
    }
    
    /**
     * Get DOM update summary.
     * @returns {Object} DOM update summary
     */
    getDomUpdateSummary() {
        const updates = this.metrics.domUpdates;
        return {
            count: updates.length,
            totalMutations: updates.reduce((sum, u) => sum + u.mutationCount, 0)
        };
    }
    
    /**
     * Get memory usage summary.
     * @returns {Object} Memory usage summary
     */
    getMemoryUsageSummary() {
        const usage = this.metrics.memoryUsage;
        if (usage.length === 0) return { available: false };
        
        const latest = usage[usage.length - 1];
        const usagePercents = usage.map(u => u.usagePercent);
        
        return {
            current: latest,
            average: usagePercents.reduce((a, b) => a + b, 0) / usagePercents.length,
            peak: Math.max(...usagePercents)
        };
    }
    
    /**
     * Get user interaction summary.
     * @returns {Object} User interaction summary
     */
    getUserInteractionSummary() {
        const interactions = this.metrics.userInteractions;
        const byType = {};
        
        interactions.forEach(interaction => {
            byType[interaction.type] = (byType[interaction.type] || 0) + 1;
        });
        
        return {
            total: interactions.length,
            byType
        };
    }
    
    /**
     * Get current performance metrics.
     * @returns {Object} Current metrics
     */
    getMetrics() {
        return this.generatePerformanceReport();
    }
    
    /**
     * Reset all metrics.
     */
    resetMetrics() {
        this.metrics = {
            pageLoad: null,
            apiCalls: [],
            domUpdates: [],
            memoryUsage: [],
            userInteractions: []
        };
        
        console.debug('[PerformanceMonitor] Metrics reset');
    }
    
    /**
     * Destroy the monitor and cleanup resources.
     */
    destroy() {
        this.stopMonitoring();
        this.resetMetrics();
        
        console.debug('[PerformanceMonitor] Destroyed');
    }
}

// Create and export singleton instance
export const performanceMonitor = new PerformanceMonitor();

// Auto-start monitoring when DOM is ready
if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', () => {
        performanceMonitor.startMonitoring();
    });
} else {
    performanceMonitor.startMonitoring();
}

// Cleanup on page unload
window.addEventListener('beforeunload', () => {
    performanceMonitor.destroy();
});

// Development helpers
if (window.location.hostname === 'localhost' || window.location.hostname === '127.0.0.1') {
    window.performanceMonitor = performanceMonitor; // Expose for debugging
}

export default performanceMonitor;
```

## File: static/utils/PerformanceOptimizer.js
```javascript
/**
 * Performance Optimizer
 * Provides debouncing, throttling, and performance monitoring utilities.
 */

class PerformanceOptimizer {
    constructor() {
        this.debounceTimers = new Map();
        this.throttleTimers = new Map();
        this.performanceMetrics = {
            apiCalls: [],
            domUpdates: [],
            errors: []
        };
        this.isDestroyed = false;
        
        console.debug('[PerformanceOptimizer] Initialized');
    }
    
    /**
     * Debounce a function call.
     * @param {string} key - Unique key for the debounced function
     * @param {Function} func - Function to debounce
     * @param {number} delay - Delay in milliseconds
     * @param {boolean} [immediate] - Execute immediately on first call
     * @returns {Function} Debounced function
     */
    debounce(key, func, delay = 300, immediate = false) {
        return (...args) => {
            if (this.isDestroyed) return;
            
            const callNow = immediate && !this.debounceTimers.has(key);
            
            // Clear existing timer
            if (this.debounceTimers.has(key)) {
                clearTimeout(this.debounceTimers.get(key));
            }
            
            // Set new timer
            const timerId = setTimeout(() => {
                this.debounceTimers.delete(key);
                if (!immediate) func.apply(this, args);
            }, delay);
            
            this.debounceTimers.set(key, timerId);
            
            // Execute immediately if requested
            if (callNow) func.apply(this, args);
        };
    }
    
    /**
     * Throttle a function call.
     * @param {string} key - Unique key for the throttled function
     * @param {Function} func - Function to throttle
     * @param {number} interval - Minimum interval between calls in milliseconds
     * @returns {Function} Throttled function
     */
    throttle(key, func, interval = 100) {
        return (...args) => {
            if (this.isDestroyed) return;
            
            if (!this.throttleTimers.has(key)) {
                func.apply(this, args);
                
                this.throttleTimers.set(key, setTimeout(() => {
                    this.throttleTimers.delete(key);
                }, interval));
            }
        };
    }
    
    /**
     * Create a debounced update function for DOM elements.
     * @param {string} key - Unique key
     * @param {Function} updateFn - Update function
     * @param {number} [delay] - Debounce delay
     * @returns {Function} Debounced update function
     */
    debouncedUpdate(key, updateFn, delay = 100) {
        return this.debounce(`update_${key}`, updateFn, delay);
    }
    
    /**
     * Create a throttled scroll handler.
     * @param {string} key - Unique key
     * @param {Function} scrollFn - Scroll handler function
     * @param {number} [interval] - Throttle interval
     * @returns {Function} Throttled scroll handler
     */
    throttledScroll(key, scrollFn, interval = 16) { // ~60fps
        return this.throttle(`scroll_${key}`, scrollFn, interval);
    }
    
    /**
     * Create a throttled resize handler.
     * @param {string} key - Unique key
     * @param {Function} resizeFn - Resize handler function
     * @param {number} [interval] - Throttle interval
     * @returns {Function} Throttled resize handler
     */
    throttledResize(key, resizeFn, interval = 100) {
        return this.throttle(`resize_${key}`, resizeFn, interval);
    }
    
    /**
     * Measure and record API call performance.
     * @param {string} endpoint - API endpoint name
     * @param {Function} apiCall - Function that makes the API call
     * @returns {Promise} Promise that resolves with API call result
     */
    async measureApiCall(endpoint, apiCall) {
        const startTime = performance.now();
        let success = true;
        let error = null;
        
        try {
            const result = await apiCall();
            return result;
        } catch (err) {
            success = false;
            error = err;
            throw err;
        } finally {
            const duration = performance.now() - startTime;
            
            this.recordApiMetric({
                endpoint,
                duration,
                success,
                error: error ? error.message : null,
                timestamp: Date.now()
            });
        }
    }
    
    /**
     * Measure DOM update performance.
     * @param {string} operation - Operation name
     * @param {Function} updateFn - DOM update function
     * @returns {*} Result of update function
     */
    measureDomUpdate(operation, updateFn) {
        const startTime = performance.now();
        
        try {
            const result = updateFn();
            const duration = performance.now() - startTime;
            
            this.recordDomMetric({
                operation,
                duration,
                success: true,
                timestamp: Date.now()
            });
            
            return result;
        } catch (error) {
            const duration = performance.now() - startTime;
            
            this.recordDomMetric({
                operation,
                duration,
                success: false,
                error: error.message,
                timestamp: Date.now()
            });
            
            throw error;
        }
    }
    
    /**
     * Record API performance metric.
     * @param {Object} metric - Metric data
     */
    recordApiMetric(metric) {
        this.performanceMetrics.apiCalls.push(metric);
        
        // Keep only last 100 metrics
        if (this.performanceMetrics.apiCalls.length > 100) {
            this.performanceMetrics.apiCalls.shift();
        }
        
        // Log slow API calls
        if (metric.duration > 1000) {
            console.warn(`[PerformanceOptimizer] Slow API call: ${metric.endpoint} took ${metric.duration.toFixed(2)}ms`);
        }
    }
    
    /**
     * Record DOM update performance metric.
     * @param {Object} metric - Metric data
     */
    recordDomMetric(metric) {
        this.performanceMetrics.domUpdates.push(metric);
        
        // Keep only last 100 metrics
        if (this.performanceMetrics.domUpdates.length > 100) {
            this.performanceMetrics.domUpdates.shift();
        }
        
        // Log slow DOM updates
        if (metric.duration > 16) { // More than one frame
            console.warn(`[PerformanceOptimizer] Slow DOM update: ${metric.operation} took ${metric.duration.toFixed(2)}ms`);
        }
    }
    
    /**
     * Get performance statistics.
     * @returns {Object} Performance statistics
     */
    getPerformanceStats() {
        const apiCalls = this.performanceMetrics.apiCalls;
        const domUpdates = this.performanceMetrics.domUpdates;
        
        const apiStats = this.calculateStats(apiCalls.map(m => m.duration));
        const domStats = this.calculateStats(domUpdates.map(m => m.duration));
        
        return {
            api: {
                ...apiStats,
                totalCalls: apiCalls.length,
                errorRate: apiCalls.filter(m => !m.success).length / apiCalls.length,
                slowCalls: apiCalls.filter(m => m.duration > 1000).length
            },
            dom: {
                ...domStats,
                totalUpdates: domUpdates.length,
                errorRate: domUpdates.filter(m => !m.success).length / domUpdates.length,
                slowUpdates: domUpdates.filter(m => m.duration > 16).length
            },
            memory: this.getMemoryStats()
        };
    }
    
    /**
     * Calculate basic statistics for an array of numbers.
     * @param {number[]} values - Array of values
     * @returns {Object} Statistics object
     */
    calculateStats(values) {
        if (values.length === 0) {
            return { avg: 0, min: 0, max: 0, median: 0 };
        }
        
        const sorted = [...values].sort((a, b) => a - b);
        const sum = values.reduce((a, b) => a + b, 0);
        
        return {
            avg: sum / values.length,
            min: sorted[0],
            max: sorted[sorted.length - 1],
            median: sorted[Math.floor(sorted.length / 2)]
        };
    }
    
    /**
     * Get memory usage statistics (if available).
     * @returns {Object} Memory statistics
     */
    getMemoryStats() {
        if (performance.memory) {
            return {
                usedJSHeapSize: performance.memory.usedJSHeapSize,
                totalJSHeapSize: performance.memory.totalJSHeapSize,
                jsHeapSizeLimit: performance.memory.jsHeapSizeLimit,
                usagePercent: (performance.memory.usedJSHeapSize / performance.memory.jsHeapSizeLimit) * 100
            };
        }
        
        return { available: false };
    }
    
    /**
     * Clear all timers and reset metrics.
     */
    reset() {
        // Clear all debounce timers
        this.debounceTimers.forEach(timerId => clearTimeout(timerId));
        this.debounceTimers.clear();
        
        // Clear all throttle timers
        this.throttleTimers.forEach(timerId => clearTimeout(timerId));
        this.throttleTimers.clear();
        
        // Reset metrics
        this.performanceMetrics = {
            apiCalls: [],
            domUpdates: [],
            errors: []
        };
        
        console.debug('[PerformanceOptimizer] Reset completed');
    }
    
    /**
     * Get current timer counts.
     * @returns {Object} Timer counts
     */
    getTimerCounts() {
        return {
            debounceTimers: this.debounceTimers.size,
            throttleTimers: this.throttleTimers.size,
            totalTimers: this.debounceTimers.size + this.throttleTimers.size
        };
    }
    
    /**
     * Cancel specific timer.
     * @param {string} key - Timer key
     * @param {string} type - Timer type ('debounce' or 'throttle')
     * @returns {boolean} True if timer was cancelled
     */
    cancelTimer(key, type) {
        const timers = type === 'debounce' ? this.debounceTimers : this.throttleTimers;
        
        if (timers.has(key)) {
            clearTimeout(timers.get(key));
            timers.delete(key);
            return true;
        }
        
        return false;
    }
    
    /**
     * Destroy the optimizer and cleanup resources.
     */
    destroy() {
        this.reset();
        this.isDestroyed = true;
        
        console.debug('[PerformanceOptimizer] Destroyed');
    }
}

// Create and export singleton instance
export const performanceOptimizer = new PerformanceOptimizer();

// Cleanup on page unload
window.addEventListener('beforeunload', () => {
    performanceOptimizer.destroy();
});

// Development helpers
if (window.location.hostname === 'localhost' || window.location.hostname === '127.0.0.1') {
    window.performanceOptimizer = performanceOptimizer; // Expose for debugging
    
    // Log performance stats periodically in development
    setInterval(() => {
        const stats = performanceOptimizer.getPerformanceStats();
        if (stats.api.totalCalls > 0 || stats.dom.totalUpdates > 0) {
            console.debug('[PerformanceOptimizer] Stats:', stats);
        }
    }, 30000); // Every 30 seconds
}

export default performanceOptimizer;
```

## File: static/constants.js
```javascript
export const POLLING_INTERVAL = 2000; // Increased from 500ms to 2000ms to reduce logging

// --- MODIFICATION: La liste des étapes est mise à jour pour correspondre au backend ---
export const defaultSequenceableStepsKeys = [
    "STEP1",
    "STEP2",
    "STEP3",
    "STEP4",
    "STEP5",
    "STEP6",
    "STEP7"
];
```

## File: static/csvDownloadMonitor.js
```javascript
/**
 * CSV Download Monitor
 * Monitors CSV download completions and triggers auto-workflow prompts
 */

import { appState } from './state/AppState.js';
import { soundEvents } from './soundManager.js';
import { showCSVWorkflowPrompt } from './csvWorkflowPrompt.js';

/**
 * CSV download monitoring state
 */
let previousDownloads = new Map();
let isMonitoringEnabled = true;
const processedDownloads = new Set();

/**
 * Handle download status updates from the polling system
 * @param {Array} newDownloads - New download list from API
 * @param {Array} oldDownloads - Previous download list
 */
const handleDownloadStatusUpdate = (newDownloads, oldDownloads) => {
    if (!isMonitoringEnabled) {
        console.log('[CSV_MONITOR] Monitoring disabled, skipping update');
        return;
    }

    if (!Array.isArray(newDownloads)) {
        console.warn('[CSV_MONITOR] Invalid data format - expected array, got:', typeof newDownloads, newDownloads);
        return;
    }

    try {
        const csvDownloads = newDownloads.filter(download => isCSVTriggeredDownload(download));

        if (csvDownloads.length > 0) {
            console.log('[CSV_MONITOR] CSV downloads found:', csvDownloads.length, csvDownloads.map(d => ({ id: d.id, status: d.status })));
        }

        const newlyCompletedDownloads = detectNewlyCompletedCSVDownloads(newDownloads);
        console.log('[CSV_MONITOR] Newly completed CSV downloads:', newlyCompletedDownloads.length);

        newlyCompletedDownloads.forEach(download => {
            console.log('[CSV_MONITOR] Processing completion for:', download.id, download.filename);
            handleCSVDownloadCompletion(download);
        });

        updateDownloadTrackingState(newDownloads);

    } catch (error) {
        console.error('[CSV_MONITOR] Error handling download status update:', error);
    }
};

/**
 * Detect newly completed CSV downloads
 * @param {Array} currentDownloads - Current download list
 * @returns {Array} Newly completed CSV downloads
 */
function detectNewlyCompletedCSVDownloads(currentDownloads) {
    const newlyCompleted = [];

    currentDownloads.forEach(download => {
        const downloadId = download.id;
        const isCSVDownload = isCSVTriggeredDownload(download);
        const isCompleted = download.status === 'completed';
        
        if (isCSVDownload && isCompleted) {
            const previousDownload = previousDownloads.get(downloadId);
            const alreadyProcessed = processedDownloads.has(downloadId);

            if (!alreadyProcessed && (!previousDownload || previousDownload.status !== 'completed')) {
                newlyCompleted.push(download);
                console.log('[CSV_MONITOR] Detected newly completed CSV download:', {
                    id: downloadId,
                    filename: download.filename,
                    previousStatus: previousDownload?.status || 'unknown',
                    alreadyProcessed: false
                });
            } else if (alreadyProcessed) {
                console.log('[CSV_MONITOR] Skipping already processed download:', downloadId);
            }
        }
    });

    return newlyCompleted;
}

/**
 * Check if a download was triggered by CSV monitoring
 * @param {Object} download - Download object
 * @returns {boolean} True if CSV-triggered
 */
function isCSVTriggeredDownload(download) {
    return (
        download.id && download.id.startsWith('csv_') ||
        download.csv_timestamp ||
        (download.message && download.message.includes('CSV'))
    );
}

/**
 * Handle a completed CSV download
 * @param {Object} download - Completed download object
 */
function handleCSVDownloadCompletion(download) {
    console.log('[CSV_MONITOR] Processing completed CSV download:', download.filename);

    try {
        processedDownloads.add(download.id);
        console.log('[CSV_MONITOR] Marked download as processed:', download.id);

        soundEvents.csvDownloadCompletion();

        showCSVWorkflowPrompt(download);

        appState.setState({
            lastCSVDownloadCompletion: {
                downloadId: download.id,
                filename: download.filename,
                timestamp: new Date().toISOString(),
                promptShown: true
            }
        }, 'csv_download_completed');

    } catch (error) {
        console.error('[CSV_MONITOR] Error handling CSV download completion:', error);
        processedDownloads.add(download.id);
    }
}

/**
 * Update our internal tracking state
 * @param {Array} currentDownloads - Current download list
 */
function updateDownloadTrackingState(currentDownloads) {

    currentDownloads.forEach(download => {
        if (download.id) {
            previousDownloads.set(download.id, {
                id: download.id,
                status: download.status,
                filename: download.filename,
                timestamp: new Date().toISOString()
            });
        }
    });

    const oneHourAgo = Date.now() - (60 * 60 * 1000);
    for (const [downloadId, downloadInfo] of previousDownloads.entries()) {
        const downloadTime = new Date(downloadInfo.timestamp).getTime();
        if (downloadTime < oneHourAgo) {
            previousDownloads.delete(downloadId);
            processedDownloads.delete(downloadId);
        }
    }
}

/**
 * Initialize CSV download monitoring
 */
export function initializeCSVDownloadMonitor() {
    console.log('[CSV_MONITOR] CSV download completion monitor initializing...');

    console.log('[CSV_MONITOR] handleDownloadStatusUpdate type:', typeof handleDownloadStatusUpdate);
    console.log('[CSV_MONITOR] handleDownloadStatusUpdate function:', handleDownloadStatusUpdate);

    if (typeof handleDownloadStatusUpdate !== 'function') {
        console.error('[CSV_MONITOR] ERROR: handleDownloadStatusUpdate is not a function!');
        return;
    }

    try {
        appState.subscribeToProperty('csvDownloads', handleDownloadStatusUpdate);
        console.log('[CSV_MONITOR] Successfully subscribed to csvDownloads state changes');
    } catch (error) {
        console.error('[CSV_MONITOR] Failed to subscribe to appState:', error);
        return;
    }

    console.log('[CSV_MONITOR] CSV download completion monitor initialized successfully');
}

/**
 * Enable or disable CSV download monitoring
 * @param {boolean} enabled - Whether monitoring should be enabled
 */
export function setCSVMonitoringEnabled(enabled) {
    isMonitoringEnabled = enabled;
    console.log(`[CSV_MONITOR] CSV download monitoring ${enabled ? 'enabled' : 'disabled'}`);
}

/**
 * Get current monitoring state
 * @returns {boolean} Whether monitoring is enabled
 */
export function isCSVMonitoringEnabled() {
    return isMonitoringEnabled;
}

/**
 * Get statistics about monitored downloads
 * @returns {Object} Monitoring statistics
 */
export function getMonitoringStats() {
    return {
        isEnabled: isMonitoringEnabled,
        trackedDownloads: previousDownloads.size,
        lastUpdate: new Date().toISOString()
    };
}
```

## File: static/csvWorkflowPrompt.js
```javascript
/**
 * CSV Workflow Prompt
 * Engaging popup that prompts users to process completed CSV downloads
 */

import { openPopupUI, closePopupUI } from './popupManager.js';
import { soundEvents } from './soundManager.js';
import { appState } from './state/AppState.js';
import { runStepSequence } from './sequenceManager.js';
import { defaultSequenceableStepsKeys } from './constants.js';
import { showNotification } from './utils.js';
import { DOMUpdateUtils } from './utils/DOMBatcher.js';

// Global variables to track popup state and prevent duplicates
let currentCSVPopup = null;
const shownPopups = new Set(); // Track downloads that have already shown popups
const POPUP_COOLDOWN_MS = 5000; // 5 second cooldown between popups
let lastPopupTime = 0;

// Helper: detect Dropbox hostnames
function isDropboxUrl(rawUrl) {
    try {
        const u = new URL(String(rawUrl || '').trim());
        const host = (u.hostname || '').toLowerCase();
        return host === 'dropbox.com' || host === 'www.dropbox.com' || host === 'dl.dropboxusercontent.com';
    } catch {
        return false;
    }
}

// Helper: detect Dropbox proxy URLs (R2/Worker) like https://<host>.workers.dev/dropbox/<...>/file
function isDropboxProxyUrl(rawUrl) {
    try {
        const u = new URL(String(rawUrl || '').trim());
        const host = (u.hostname || '').toLowerCase();
        const path = (u.pathname || '').toLowerCase();
        return (host.includes('workers.dev') || host.includes('worker')) && path.includes('/dropbox/');
    } catch {
        return false;
    }
}

function isDropboxLikeDownload(download) {
    try {
        const isManualOpen = Boolean(download && download.manual_open);
        if (isManualOpen) return false;
        const urlStr = (download && (download.original_url || download.url)) || '';
        const urlType = (download && String(download.url_type || '').toLowerCase()) || '';
        return urlType === 'dropbox' || isDropboxUrl(urlStr) || isDropboxProxyUrl(urlStr);
    } catch {
        return false;
    }
}

/**
 * Show the CSV workflow prompt popup
 * @param {Object} download - Completed download object
 */
export function showCSVWorkflowPrompt(download) {
    console.log('[CSV_WORKFLOW_PROMPT] Showing workflow prompt for:', download.filename);

    if (!isDropboxLikeDownload(download)) {
        console.log('[CSV_WORKFLOW_PROMPT] Ignoring non-Dropbox download:', download && download.id);
        return;
    }

    // Rate limiting: Check if we've already shown a popup for this download
    if (shownPopups.has(download.id)) {
        console.log('[CSV_WORKFLOW_PROMPT] Popup already shown for download:', download.id);
        return;
    }

    // Rate limiting: Check cooldown period
    const now = Date.now();
    if (now - lastPopupTime < POPUP_COOLDOWN_MS) {
        console.log('[CSV_WORKFLOW_PROMPT] Popup cooldown active, skipping:', download.id);
        return;
    }

    // Close any existing popup first
    if (currentCSVPopup) {
        console.log('[CSV_WORKFLOW_PROMPT] Closing existing popup before showing new one');
        closeCSVWorkflowPrompt();
    }

    // Mark this download as having shown a popup
    shownPopups.add(download.id);
    lastPopupTime = now;

    // Create the popup overlay
    currentCSVPopup = createPopupOverlay(download);

    // Add to document body
    document.body.appendChild(currentCSVPopup);

    // Show the popup
    openPopupUI(currentCSVPopup);

    // Add event listeners
    setupPromptEventListeners(download);

    // Play notification sound
    soundEvents.workflowCompletion();
}

/**
 * Create the popup overlay element
 * @param {Object} download - Completed download object
 * @returns {HTMLElement} Popup overlay element
 */
function createPopupOverlay(download) {
    const overlay = document.createElement('div');
    overlay.className = 'popup-overlay csv-workflow-prompt';
    overlay.style.display = 'none';
    overlay.setAttribute('role', 'dialog');
    overlay.setAttribute('aria-modal', 'true');

    const popupContent = document.createElement('div');
    popupContent.className = 'popup-content';

    // Add close button
    const closeButton = document.createElement('button');
    closeButton.className = 'popup-close-button';
    closeButton.innerHTML = '×';
    closeButton.setAttribute('aria-label', 'Fermer');
    closeButton.onclick = () => closeCSVWorkflowPrompt();

    // Add title (conditional for FromSmash)
    const title = document.createElement('h2');
    title.className = 'popup-title';
    const titleId = `csv-workflow-prompt-title-${Date.now()}`;
    title.id = titleId;
    overlay.setAttribute('aria-labelledby', titleId);
    const urlStr = (download && (download.original_url || download.url)) || '';
    const isFromSmash = (download && (download.url_type === 'fromsmash' || urlStr.toLowerCase().includes('fromsmash.com')));
    const isSwissTransfer = (download && (download.url_type === 'swisstransfer' || urlStr.toLowerCase().includes('swisstransfer.com')));
    const isDropbox = isDropboxLikeDownload(download);
    title.textContent = (!isDropbox) ? '🚀 Nouveau lien disponible !' : '🎉 Téléchargement Terminé !';

    // Add main content
    const contentDiv = document.createElement('div');
    contentDiv.innerHTML = createWorkflowPromptContent(download);

    // Assemble popup
    popupContent.appendChild(closeButton);
    popupContent.appendChild(title);
    popupContent.appendChild(contentDiv);
    overlay.appendChild(popupContent);

    return overlay;
}

/**
 * Create the HTML content for the workflow prompt
 * @param {Object} download - Completed download object
 * @returns {string} HTML content
 */
function createWorkflowPromptContent(download) {
    const filename = download.filename || 'Fichier téléchargé';
    const downloadTime = download.display_timestamp || 'maintenant';
    const urlStr = (download && (download.original_url || download.url || ''));
    const isFromSmash = (download && (download.url_type === 'fromsmash' || urlStr.toLowerCase().includes('fromsmash.com')));
    const isSwissTransfer = (download && (download.url_type === 'swisstransfer' || urlStr.toLowerCase().includes('swisstransfer.com')));
    const isDropbox = isDropboxLikeDownload(download);
    const isDropboxByTypeOrUrl = (
        (download && String(download.url_type || '').toLowerCase() === 'dropbox')
        || isDropboxUrl(urlStr)
        || isDropboxProxyUrl(urlStr)
    );
    
    if (!isDropbox) {
        const safeUrl = sanitizeExternalUrl(download.original_url || download.url || '');
        const providerLabel = isFromSmash ? 'FromSmash' : (isSwissTransfer ? 'SwissTransfer' : (isDropboxByTypeOrUrl ? 'Dropbox' : 'Lien Externe'));
        const hiddenId = isFromSmash ? 'csv-fromsmash-hidden-link' : (isSwissTransfer ? 'csv-swisstransfer-hidden-link' : 'csv-external-hidden-link');
        const openBtnId = isFromSmash ? 'csv-open-fromsmash-btn' : (isSwissTransfer ? 'csv-open-swisstransfer-btn' : 'csv-open-external-btn');
        const mainMsg = isFromSmash
            ? '🚀 Un nouveau lien FromSmash est disponible. Cliquez sur « Ouvrir et télécharger » pour l’ouvrir dans un nouvel onglet et démarrer le téléchargement.'
            : (isSwissTransfer
                ? '🚀 Un nouveau lien SwissTransfer est disponible. Cliquez sur « Ouvrir et télécharger » pour l’ouvrir dans un nouvel onglet et démarrer le téléchargement.'
                : (isDropboxByTypeOrUrl
                    ? '🚀 Un nouveau lien Dropbox est disponible. Cliquez sur « Ouvrir et télécharger » pour l’ouvrir dans un nouvel onglet et démarrer le téléchargement.'
                    : '🚀 Un nouveau lien externe est disponible. Cliquez sur « Ouvrir manuellement » pour l’ouvrir dans un nouvel onglet.'));
        return `
        <div class="csv-workflow-prompt-content">
            <div class="prompt-header">
                <div class="download-icon">🔗</div>
                <div class="download-info">
                    <h3 class="download-title">${DOMUpdateUtils.escapeHtml(filename)}</h3>
                    <p class="download-subtitle">Reçu à ${downloadTime}</p>
                </div>
            </div>
            
            <div class="prompt-message">
                <p class="main-message">${mainMsg}</p>
                ${!safeUrl ? '<p class="warning">⚠️ Domaine non autorisé. L’ouverture automatique est bloquée pour des raisons de sécurité.</p>' : ''}
            </div>
            
            <div class="prompt-actions">
                <button id="${openBtnId}" class="btn-primary" ${!safeUrl ? 'disabled' : ''}>
                    <span class="btn-icon">🌐</span>
                    <span class="btn-text">${isFromSmash || isSwissTransfer || isDropboxByTypeOrUrl ? 'Ouvrir et télécharger' : 'Ouvrir manuellement'}</span>
                </button>
                <button id="csv-workflow-dismiss-btn" class="btn-secondary workflow-dismiss-btn">
                    <span class="btn-icon">⏭️</span>
                    <span class="btn-text">Plus tard</span>
                </button>
            </div>
            
            <div class="prompt-footer">
                <p class="footer-note">
                    💡 Aucun traitement automatique ne sera lancé pour ce lien (${providerLabel}). Revenez lancer le workflow une fois le fichier téléchargé et disponible localement.
                </p>
            </div>
            
            <a id="${hiddenId}" href="${safeUrl || '#'}" target="_blank" rel="noopener noreferrer" style="display:none;">open</a>
        </div>
        `;
    }
    
    return `
        <div class="csv-workflow-prompt-content">
            <div class="prompt-header">
                <div class="download-icon">📥</div>
                <div class="download-info">
                    <h3 class="download-title">${DOMUpdateUtils.escapeHtml(filename)}</h3>
                    <p class="download-subtitle">Téléchargé à ${downloadTime}</p>
                </div>
            </div>
            
            <div class="prompt-message">
                <p class="main-message">
                    🚀 Votre fichier est prêt ! Voulez-vous lancer le workflow complet 
                    pour traiter automatiquement ce contenu ?
                </p>
                <div class="workflow-preview">
                    <div class="workflow-steps">
                        <span class="step-badge">1️⃣ Extraction</span>
                        <span class="step-arrow">→</span>
                        <span class="step-badge">2️⃣ Analyse</span>
                        <span class="step-arrow">→</span>
                        <span class="step-badge">3️⃣ Détection</span>
                        <span class="step-arrow">→</span>
                        <span class="step-badge">4️⃣ Audio</span>
                        <span class="step-arrow">→</span>
                        <span class="step-badge">5️⃣ Tracking</span>
                        <span class="step-arrow">→</span>
                        <span class="step-badge">6️⃣ Finalisation</span>
                    </div>
                </div>
            </div>
            
            <div class="prompt-actions">
                <button id="csv-workflow-launch-btn" class="btn-primary workflow-launch-btn">
                    <span class="btn-icon">✨</span>
                    <span class="btn-text">Oui, Lancer le Workflow !</span>
                </button>
                <button id="csv-workflow-dismiss-btn" class="btn-secondary workflow-dismiss-btn">
                    <span class="btn-icon">⏭️</span>
                    <span class="btn-text">Plus tard</span>
                </button>
            </div>
            
            <div class="prompt-footer">
                <p class="footer-note">
                    💡 Le workflow traitera automatiquement votre fichier selon la séquence complète (étapes 1-6)
                </p>
            </div>
        </div>
    `;
}

/**
 * Close the CSV workflow prompt popup
 */
function closeCSVWorkflowPrompt() {
    if (currentCSVPopup) {
        closePopupUI(currentCSVPopup);
        document.body.removeChild(currentCSVPopup);
        currentCSVPopup = null;
    }
}

/**
 * Setup event listeners for the prompt buttons
 * @param {Object} download - Download object
 */
function setupPromptEventListeners(download) {
    const urlStr = (download && (download.original_url || download.url || ''));
    const isFromSmash = (download && (download.url_type === 'fromsmash' || urlStr.toLowerCase().includes('fromsmash.com')));
    const isSwissTransfer = (download && (download.url_type === 'swisstransfer' || urlStr.toLowerCase().includes('swisstransfer.com')));
    const isDropbox = isDropboxLikeDownload(download);
    const dismissBtn = document.getElementById('csv-workflow-dismiss-btn');

    if (dismissBtn) {
        dismissBtn.addEventListener('click', () => handleWorkflowDismiss(download));
    }

    if (!isDropbox) {
        const btnId = isFromSmash ? 'csv-open-fromsmash-btn' : (isSwissTransfer ? 'csv-open-swisstransfer-btn' : 'csv-open-external-btn');
        const openBtn = document.getElementById(btnId);
        if (openBtn) {
            openBtn.addEventListener('click', () => openExternalLink(download, { isFromSmash, isSwissTransfer }));
        }
    } else {
        const launchBtn = document.getElementById('csv-workflow-launch-btn');
        if (launchBtn) {
            launchBtn.addEventListener('click', () => handleWorkflowLaunch(download));
        }
    }
}

/**
 * Open FromSmash link in a new tab with basic URL sanitization
 * @param {Object} download
 */
function openExternalLink(download, { isFromSmash = false, isSwissTransfer = false } = {}) {
    try {
        const rawUrl = (download && (download.original_url || download.url)) || '';
        const safeUrl = sanitizeExternalUrl(rawUrl);
        if (!safeUrl) {
            showNotification('Lien invalide ou domaine non autorisé.', 'error');
            return;
        }

        // Close the popup first
        closeCSVWorkflowPrompt();

        // Attempt to open via hidden anchor to respect browser policies
        const hiddenId = isFromSmash ? 'csv-fromsmash-hidden-link' : (isSwissTransfer ? 'csv-swisstransfer-hidden-link' : 'csv-external-hidden-link');
        const hiddenA = document.getElementById(hiddenId);
        if (hiddenA && hiddenA.href) {
            hiddenA.click();
        } else {
            window.open(safeUrl, '_blank', 'noopener');
        }

        const provider = isFromSmash ? 'FromSmash' : (isSwissTransfer ? 'SwissTransfer' : 'externe');
        showNotification(`Ouverture du lien ${provider} dans un nouvel onglet...`, 'success');
        soundEvents.workflowStart();
    } catch (e) {
        console.error('[CSV_WORKFLOW_PROMPT] Failed to open external link:', e);
        showNotification("Impossible d'ouvrir le lien.", 'error');
        soundEvents.errorEvent();
    }
}

/**
 * Open SwissTransfer link in a new tab with basic URL sanitization
 * @param {Object} download
 */
function openSwissTransferLink(download) {
    try {
        const rawUrl = (download && (download.original_url || download.url)) || '';
        const safeUrl = sanitizeExternalUrl(rawUrl);
        if (!safeUrl) {
            showNotification("Lien invalide ou non supporté.", 'error');
            return;
        }

        // Close the popup first
        closeCSVWorkflowPrompt();

        // Attempt to open via hidden anchor to respect browser policies
        const hiddenA = document.getElementById('csv-swisstransfer-hidden-link');
        if (hiddenA && hiddenA.href) {
            hiddenA.click();
        } else {
            window.open(safeUrl, '_blank', 'noopener');
        }

        showNotification("Ouverture du lien SwissTransfer dans un nouvel onglet...", 'success');
        soundEvents.workflowStart();
    } catch (e) {
        console.error('[CSV_WORKFLOW_PROMPT] Failed to open SwissTransfer link:', e);
        showNotification("Impossible d'ouvrir le lien.", 'error');
        soundEvents.errorEvent();
    }
}

/**
 * Sanitize external URL (very basic allowlist)
 * @param {string} url
 * @returns {string|null}
 */
function sanitizeExternalUrl(url) {
    if (typeof url !== 'string') return null;
    const trimmed = url.trim();
    if (!trimmed) return null;
    try {
        const u = new URL(trimmed);
        const hostname = (u.hostname || '').toLowerCase();
        // Allow any valid HTTP(S) URL. Non-Dropbox links are opened manually in a new tab.
        if (!['http:', 'https:'].includes(u.protocol)) {
            return null;
        }
        return u.toString();
    } catch {
        return null;
    }
}

/**
 * Handle workflow launch button click
 * @param {Object} download - Download object
 */
async function handleWorkflowLaunch(download) {
    console.log('[CSV_WORKFLOW_PROMPT] User chose to launch workflow for:', download.filename);
    
    try {
        // Close the popup first
        closeCSVWorkflowPrompt();
        
        // Check if a sequence is already running
        if (appState.getStateProperty('isAnySequenceRunning')) {
            showNotification("Une séquence est déjà en cours. Veuillez attendre qu'elle se termine.", 'warning');
            return;
        }
        
        // Play workflow start sound
        soundEvents.workflowStart();
        
        // Show success notification
        showNotification(`🚀 Lancement du workflow pour "${download.filename}"`, 'success');
        
        // Launch the complete workflow sequence
        await runStepSequence(defaultSequenceableStepsKeys, "Séquence CSV Auto");
        
        // Update app state to track the auto-launch
        appState.setState({
            lastCSVAutoLaunch: {
                downloadId: download.id,
                filename: download.filename,
                timestamp: new Date().toISOString(),
                sequenceType: 'CSV Auto'
            }
        }, 'csv_workflow_auto_launched');
        
    } catch (error) {
        console.error('[CSV_WORKFLOW_PROMPT] Error launching workflow:', error);
        showNotification("Erreur lors du lancement du workflow. Veuillez réessayer.", 'error');
        soundEvents.errorEvent();
    }
}

/**
 * Handle workflow dismiss button click
 * @param {Object} download - Download object
 */
function handleWorkflowDismiss(download) {
    console.log('[CSV_WORKFLOW_PROMPT] User dismissed workflow prompt for:', download.filename);
    
    // Close the popup
    closeCSVWorkflowPrompt();
    
    // Show a gentle notification
    showNotification("Workflow reporté. Vous pouvez le lancer manuellement quand vous le souhaitez.", 'info');
    
    // Update app state to track the dismissal
    appState.setState({
        lastCSVPromptDismissal: {
            downloadId: download.id,
            filename: download.filename,
            timestamp: new Date().toISOString()
        }
    }, 'csv_workflow_prompt_dismissed');
}

// Using DOMUpdateUtils.escapeHtml from DOMBatcher for XSS safety (project standard)
```

## File: static/reportViewer.js
```javascript
/**
 * Report Viewer - Visual Analysis Report Generator
 * Workflow MediaPipe v4.0
 * 
 * Generates comprehensive HTML reports with statistics and infographics
 */

import { domBatcher } from './utils/DOMBatcher.js';

class ReportViewer {
    constructor() {
        this.overlay = null;
        this.isVisible = false;
        this.projects = [];
        this.currentReport = null;
        this.selectedFormat = 'html';
        this.availableMonths = new Set();
        this.prevFocusEl = null;
        this._keydownHandler = null;
    }

    _getFocusableElements(container) {
        if (!container || typeof container.querySelectorAll !== 'function') return [];
        const selectors = [
            'a[href]',
            'button:not([disabled])',
            'textarea:not([disabled])',
            'input:not([disabled])',
            'select:not([disabled])',
            '[tabindex]:not([tabindex="-1"])'
        ];
        return Array.from(container.querySelectorAll(selectors.join(',')))
            .filter(el => el && typeof el.focus === 'function' && (el.offsetParent !== null || (typeof el.getAttribute === 'function' && el.getAttribute('aria-hidden') !== 'true')));
    }

    _enableModalFocusTrap() {
        const overlay = this.overlay;
        if (!overlay) return;

        try {
            const currentFocused = document.activeElement;
            if (currentFocused && currentFocused !== document.body && currentFocused !== document.documentElement && typeof currentFocused.focus === 'function') {
                this.prevFocusEl = currentFocused;
            } else {
                this.prevFocusEl = null;
            }
        } catch {
            this.prevFocusEl = null;
        }

        if (typeof overlay.getAttribute === 'function' && overlay.getAttribute('tabindex') === null && typeof overlay.setAttribute === 'function') {
            overlay.setAttribute('tabindex', '-1');
        }

        const focusables = this._getFocusableElements(overlay);
        const first = focusables[0] || overlay;
        if (first && typeof first.focus === 'function') {
            first.focus();
        }

        this._keydownHandler = (e) => {
            if (e.key === 'Escape') {
                e.preventDefault();
                this.close();
                return;
            }
            if (e.key === 'Tab') {
                const focusEls = this._getFocusableElements(overlay);
                if (focusEls.length === 0) {
                    e.preventDefault();
                    overlay.focus && overlay.focus();
                    return;
                }
                const currentIndex = focusEls.indexOf(document.activeElement);
                let nextIndex = currentIndex;
                if (e.shiftKey) {
                    nextIndex = currentIndex <= 0 ? focusEls.length - 1 : currentIndex - 1;
                } else {
                    nextIndex = currentIndex === focusEls.length - 1 ? 0 : currentIndex + 1;
                }
                e.preventDefault();
                focusEls[nextIndex].focus();
            }
        };

        overlay.addEventListener('keydown', this._keydownHandler);
    }

    _disableModalFocusTrap() {
        const overlay = this.overlay;
        if (overlay && this._keydownHandler) {
            overlay.removeEventListener('keydown', this._keydownHandler);
        }
        this._keydownHandler = null;
        const prev = this.prevFocusEl;
        this.prevFocusEl = null;

        if (prev && typeof prev.focus === 'function') {
            try {
                prev.focus();
            } catch (_) {}
        }
    }

    /**
     * Generate monthly archive report
     */
    async generateMonthlyReport() {
        try {
            const monthInput = document.getElementById('report-month-select');
            const previewContainer = document.getElementById('report-preview-container');
            if (!monthInput) return;

            const month = (monthInput.value || '').trim();
            if (!month) {
                this.showError('Veuillez sélectionner un mois au format YYYY-MM.');
                return;
            }

            this.showLoading(previewContainer);

            const response = await fetch('/api/reports/generate/monthly', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({ month })
            });

            if (!response.ok) {
                let backendMsg = '';
                try {
                    const err = await response.json();
                    backendMsg = err && err.error ? ` — ${err.error}` : '';
                } catch {}
                const message = `HTTP ${response.status}: ${response.statusText}${backendMsg}`;
                throw new Error(message);
            }

            const data = await response.json();
            if (data.error) {
                throw new Error(data.error);
            }

            // Render the monthly HTML into the preview container
            this.currentReport = { ...data, project_name: null, video_name: null };
            this.renderReport({ html: data.html, format: 'html' }, previewContainer);

            // Enable download button
            const downloadButton = document.getElementById('report-download-button');
            if (downloadButton) downloadButton.disabled = false;

        } catch (error) {
            console.error('[ReportViewer] Error generating monthly report:', error);
            let hint = '';
            if (this.availableMonths && this.availableMonths.size > 0) {
                hint = `\nMois disponibles: ${Array.from(this.availableMonths).sort().join(', ')}`;
            }
            this.showError('Erreur de génération du rapport mensuel: ' + error.message + hint);
        }
    }

    /**
     * Parse a project name that may contain a timestamp suffix
     * Pattern: "<base> YYYY-MM-DD_HH-MM-SS"
     * Returns { baseName, timestampRaw, timestampPretty }
     */
    parseArchiveName(name) {
        try {
            const m = name.match(/^(.*) (\d{4}-\d{2}-\d{2}_)\b(\d{2}-\d{2}-\d{2})$/);
            if (!m) {
                return { baseName: name, timestampRaw: null, timestampPretty: null };
            }
            const base = m[1];
            const datePart = m[2].slice(0, -1); // remove trailing underscore
            const timePart = m[3].replace(/-/g, ':');
            const pretty = `${datePart} ${timePart}`; // YYYY-MM-DD HH:MM:SS
            return { baseName: base, timestampRaw: `${datePart}_${m[3]}`, timestampPretty: pretty };
        } catch {
            return { baseName: name, timestampRaw: null, timestampPretty: null };
        }
    }

    /**
     * Initialize the report viewer
     */
    init() {
        console.log('[ReportViewer] Initializing...');
        
        this.overlay = document.getElementById('report-overlay');
        
        if (!this.overlay) {
            console.warn('[ReportViewer] Report overlay not found');
            return;
        }

        this.setupEventHandlers();
        
        console.log('[ReportViewer] Initialized successfully');
    }

    /**
     * Setup event handlers
     */
    setupEventHandlers() {
        // Open button
        const openButton = document.getElementById('open-report-button');
        if (openButton) {
            openButton.addEventListener('click', () => this.open());
        }

        // Close button
        const closeButton = document.getElementById('report-close');
        if (closeButton) {
            closeButton.addEventListener('click', () => this.close());
        }

        // Close on overlay click
        this.overlay.addEventListener('click', (e) => {
            if (e.target === this.overlay) {
                this.close();
            }
        });

        // Escape key to close
        document.addEventListener('keydown', (e) => {
            if (e.key === 'Escape' && this.isVisible) {
                this.close();
            }
        });

        // Generate button
        const generateButton = document.getElementById('report-generate-button');
        if (generateButton) {
            generateButton.addEventListener('click', () => this.generateReport());
        }

        // Download button
        const downloadButton = document.getElementById('report-download-button');
        if (downloadButton) {
            downloadButton.addEventListener('click', () => this.downloadReport());
        }

        // Project selector change
        const projectSelect = document.getElementById('report-project-select');
        if (projectSelect) {
            projectSelect.addEventListener('change', (e) => this.onProjectChange(e.target.value));
        }

        // No format radios anymore (HTML-only)

        // Optional: project-only checkbox to generate consolidated project report
        const projectOnly = document.getElementById('report-project-only');
        if (projectOnly) {
            projectOnly.addEventListener('change', () => this.updateGenerateButtonState());
        }

        // Monthly report button
        const monthlyBtn = document.getElementById('report-generate-monthly-button');
        if (monthlyBtn) {
            monthlyBtn.addEventListener('click', () => this.generateMonthlyReport());
        }

        // Analyze uploaded monthly report
        const analyzeBtn = document.getElementById('report-analyze-upload-button');
        const uploadInput = document.getElementById('monthly-report-upload');
        const resultEl = document.getElementById('report-analyze-result');
        if (analyzeBtn && uploadInput && resultEl) {
            analyzeBtn.addEventListener('click', async () => {
                try {
                    if (!uploadInput.files || uploadInput.files.length === 0) {
                        resultEl.textContent = 'Veuillez sélectionner un fichier HTML de rapport.';
                        resultEl.className = 'form-feedback error';
                        return;
                    }
                    const file = uploadInput.files[0];
                    const formData = new FormData();
                    formData.append('file', file);
                    resultEl.textContent = 'Analyse en cours...';
                    resultEl.className = 'form-feedback info';

                    const resp = await fetch('/api/reports/analyze/monthly_upload', {
                        method: 'POST',
                        body: formData
                    });
                    if (!resp.ok) {
                        let backendMsg = '';
                        try { const j = await resp.json(); backendMsg = j && j.error ? ` — ${j.error}` : ''; } catch {}
                        throw new Error(`HTTP ${resp.status}: ${resp.statusText}${backendMsg}`);
                    }
                    const data = await resp.json();
                    if (data.error) throw new Error(data.error);

                    const mp4 = data.by_extension?.[".mp4"] ?? 0;
                    const other = data.by_extension?.other ?? 0;
                    const noext = data.by_extension?.noext ?? 0;
                    const parts = [];
                    parts.push(`Total listé: ${data.total_listed}`);
                    parts.push(`.mp4: ${mp4}`);
                    parts.push(`autres: ${other}`);
                    parts.push(`sans extension: ${noext}`);
                    if (typeof data.projects === 'number') parts.push(`projets: ${data.projects}`);
                    if (data.month) parts.push(`mois: ${data.month}`);
                    if (data.build_id) parts.push(`build: ${data.build_id}`);

                    resultEl.textContent = parts.join(' · ');
                    resultEl.className = 'form-feedback success';
                } catch (err) {
                    console.error('[ReportViewer] analyze upload error:', err);
                    resultEl.textContent = `Erreur d\'analyse: ${err.message || err}`;
                    resultEl.className = 'form-feedback error';
                }
            });
        }
    }

    /**
     * Open the report viewer
     */
    async open() {
        console.log('[ReportViewer] Opening report viewer...');

        if (!this.overlay) {
            this.overlay = document.getElementById('report-overlay');
        }
        if (!this.overlay) {
            return;
        }
        
        this.isVisible = true;
        this.overlay.style.display = 'flex';
        this._enableModalFocusTrap();
        
        // Trigger reflow for animation
        setTimeout(() => {
            this.overlay.setAttribute('data-visible', 'true');
        }, 10);

        // Load projects list
        await this.loadProjects();
    }

    /**
     * Close the report viewer
     */
    close() {
        console.log('[ReportViewer] Closing report viewer...');

        if (!this.overlay) {
            this.overlay = document.getElementById('report-overlay');
        }
        if (!this.overlay) {
            this.isVisible = false;
            return;
        }
        
        this.isVisible = false;
        this.overlay.setAttribute('data-visible', 'false');
        
        setTimeout(() => {
            this.overlay.style.display = 'none';
            this._disableModalFocusTrap();
        }, 300);
    }

    /**
     * Load available projects from API
     */
    async loadProjects() {
        try {
            const response = await fetch('/api/visualization/projects');
            
            if (!response.ok) {
                throw new Error(`HTTP ${response.status}: ${response.statusText}`);
            }

            const data = await response.json();

            if (data.error) {
                throw new Error(data.error);
            }

            this.projects = data.projects || [];
            // Build available months set from archive timestamps
            this.availableMonths = new Set(
                (this.projects || [])
                    .map(p => (p.archive_timestamp || '').slice(0, 7))
                    .filter(m => /\d{4}-\d{2}/.test(m))
            );
            // Prefill month selector to current month if empty
            const monthInput = document.getElementById('report-month-select');
            if (monthInput && !monthInput.value) {
                const currentMonth = new Date().toISOString().slice(0, 7);
                monthInput.value = currentMonth;
                if (!this.availableMonths.has(monthInput.value) && this.availableMonths.size > 0) {
                    // Pick any available month to help the user
                    const first = Array.from(this.availableMonths)[0];
                    monthInput.value = first;
                }
            }
            this.renderProjectSelector();

            console.log(`[ReportViewer] Loaded ${this.projects.length} projects`);

        } catch (error) {
            console.error('[ReportViewer] Error loading projects:', error);
            this.showError('Impossible de charger les projets: ' + error.message);
        }
    }

    /**
     * Update generate button enabled/disabled state based on selections
     */
    updateGenerateButtonState() {
        const projectSelect = document.getElementById('report-project-select');
        const videoSelect = document.getElementById('report-video-select');
        const generateButton = document.getElementById('report-generate-button');
        const projectOnly = document.getElementById('report-project-only');

        if (!generateButton || !projectSelect) return;

        const hasProject = !!projectSelect.value;
        const isProjectOnly = !!(projectOnly && projectOnly.checked);
        const hasVideo = !!(videoSelect && videoSelect.value);

        generateButton.disabled = !(hasProject && (isProjectOnly || hasVideo));
    }

    /**
     * Render project selector dropdown
     */
    renderProjectSelector() {
        const projectSelect = document.getElementById('report-project-select');
        const videoSelect = document.getElementById('report-video-select');
        
        if (!projectSelect) return;

        domBatcher.scheduleUpdate('report-projects-select', () => {
            // Clear and populate project selector
            projectSelect.innerHTML = '<option value="">-- Sélectionner un projet --</option>';

            this.projects.forEach(project => {
                const option = document.createElement('option');
                option.value = project.name;
                // Prefer backend-provided fields; fallback to local parsing
                const baseLabel = project.display_base || this.parseArchiveName(project.name).baseName;
                const tsPretty = project.archive_timestamp || this.parseArchiveName(project.name).timestampPretty;
                const countLabel = `(${project.video_count} vidéo${project.video_count > 1 ? 's' : ''})`;
                const tsLabel = tsPretty ? ` — archivé le ${tsPretty}` : '';

                // Accessibility: expose full timestamp on hover
                if (tsPretty) {
                    option.title = `Archivé le ${tsPretty}`;
                }

                // Use textContent to avoid HTML injection
                option.textContent = `${baseLabel} ${countLabel}${tsLabel}`;
                projectSelect.appendChild(option);
            });
        });

        // Reset video selector
        if (videoSelect) {
            videoSelect.innerHTML = '<option value="">-- Sélectionner une vidéo --</option>';
            videoSelect.disabled = true;
        }

        // Disable generate button
        const generateButton = document.getElementById('report-generate-button');
        if (generateButton) {
            generateButton.disabled = true;
        }
    }

    /**
     * Handle project selection change
     */
    onProjectChange(projectName) {
        const videoSelect = document.getElementById('report-video-select');
        const generateButton = document.getElementById('report-generate-button');
        const projectOnly = document.getElementById('report-project-only');
        
        if (!projectName || !videoSelect) return;

        const project = this.projects.find(p => p.name === projectName);
        
        if (!project) return;

        // Populate video selector
        videoSelect.innerHTML = '<option value="">-- Sélectionner une vidéo --</option>';
        
        project.videos.forEach(video => {
            const option = document.createElement('option');
            option.value = video;
            option.textContent = video;
            videoSelect.appendChild(option);
        });

        videoSelect.disabled = false;

        // Enable generate button when video is selected (unless project-only mode)
        videoSelect.onchange = () => this.updateGenerateButtonState();
        this.updateGenerateButtonState();
    }

    /**
     * Generate report for selected project/video
     */
    async generateReport() {
        const projectSelect = document.getElementById('report-project-select');
        const videoSelect = document.getElementById('report-video-select');
        const previewContainer = document.getElementById('report-preview-container');
        const projectOnly = document.getElementById('report-project-only');
        
        if (!projectSelect || !videoSelect) return;

        const projectName = projectSelect.value;
        const videoName = videoSelect.value;

        if (!projectName) return;

        try {
            this.showLoading(previewContainer);

            const isProjectOnly = !!(projectOnly && projectOnly.checked);
            const endpoint = isProjectOnly ? '/api/reports/generate/project' : '/api/reports/generate';
            const body = isProjectOnly
                ? { project: projectName, format: 'html' }
                : { project: projectName, video: videoName, format: 'html' };

            const response = await fetch(endpoint, {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify(body)
            });
            
            if (!response.ok) {
                throw new Error(`HTTP ${response.status}: ${response.statusText}`);
            }

            const data = await response.json();

            if (data.error) {
                throw new Error(data.error);
            }

            this.currentReport = data;
            this.renderReport(data, previewContainer);

            console.log('[ReportViewer] Report generated successfully');

        } catch (error) {
            console.error('[ReportViewer] Error generating report:', error);
            this.showError('Erreur de génération: ' + error.message);
        }
    }

    /**
     * Show loading state
     */
    showLoading(container) {
        domBatcher.scheduleUpdate('report-loading', () => {
            container.innerHTML = `
                <div class="report-loading">
                    <div class="report-loading-spinner"></div>
                    <div class="report-loading-text">Génération du rapport en cours...</div>
                </div>
            `;
        });
    }

    /**
     * Show error state
     */
    showError(message) {
        const previewContainer = document.getElementById('report-preview-container');
        if (!previewContainer) return;

        domBatcher.scheduleUpdate('report-error', () => {
            previewContainer.innerHTML = `
                <div class="report-preview-empty">
                    <div class="report-preview-icon">⚠️</div>
                    <div class="report-preview-message">Erreur</div>
                    <div class="report-preview-detail">${this.escapeHtml(message)}</div>
                </div>
            `;
        });
    }

    /**
     * Render generated report
     */
    renderReport(data, container) {
        const { html, format } = data;

        let content = '';

        if (format === 'html' && html) {
            // Create sandboxed iframe for HTML preview. We do NOT escape srcdoc to allow proper rendering.
            content = `
                <iframe class="report-preview-iframe" id="report-iframe" sandbox="allow-same-origin" srcdoc='${html.replace(/'/g, "&#39;")}'></iframe>
                <div class="report-download-info">
                    <div class="report-download-icon">✅</div>
                    <div>
                        <div style="font-weight: 600;">Rapport généré avec succès</div>
                        <div style="font-size: 14px; opacity: 0.9;">Utilisez le bouton "Télécharger" pour sauvegarder</div>
                    </div>
                </div>
            `;
        } else {
            content = `
                <div class="report-preview-empty">
                    <div class="report-preview-icon">📊</div>
                    <div class="report-preview-message">Aucun contenu disponible</div>
                </div>
            `;
        }

        domBatcher.scheduleUpdate('report-render', () => {
            container.innerHTML = content;
        });

        // Enable download button
        const downloadButton = document.getElementById('report-download-button');
        if (downloadButton) {
            downloadButton.disabled = false;
        }
    }

    /**
     * Download current report
     */
    downloadReport() {
        if (!this.currentReport) return;

        const { html, format, video_name, project_name } = this.currentReport;
        const timestamp = new Date().toISOString().split('T')[0];
        const baseName = video_name
            ? video_name.replace(/\.[^/.]+$/, "")
            : (project_name || 'projet');
        
        if (format === 'html' && html) {
            // Download HTML
            const blob = new Blob([html], { type: 'text/html' });
            const url = URL.createObjectURL(blob);
            const a = document.createElement('a');
            a.href = url;
            a.download = `rapport_${baseName}_${timestamp}.html`;
            document.body.appendChild(a);
            a.click();
            document.body.removeChild(a);
            URL.revokeObjectURL(url);
        }
    }

    /**
     * Update format selection UI
     */
    updateFormatSelection() {
        const formatOptions = document.querySelectorAll('.report-format-option');
        formatOptions.forEach(option => {
            const radio = option.querySelector('input[type="radio"]');
            if (radio && radio.value === this.selectedFormat) {
                option.classList.add('selected');
            } else {
                option.classList.remove('selected');
            }
        });
    }

    /**
     * Escape HTML to prevent XSS
     */
    escapeHtml(text) {
        const div = document.createElement('div');
        div.textContent = text;
        return div.innerHTML;
    }
}

// Create and export singleton instance
const reportViewer = new ReportViewer();
export { reportViewer };
```

## File: static/soundManager.js
```javascript
// ===== SOUND MANAGER =====
// Manages audio feedback for user interactions and workflow events

/**
 * Sound event types and their corresponding audio files
 */
const SOUND_EVENTS = {
    WORKFLOW_START: 'DM_20250703182122_001.mp3',
    CHECKBOX_INTERACTION: 'DM_20250703182143_001.mp3',
    WORKFLOW_COMPLETION: 'DM_20250703182159_001.mp3',
    ERROR_EVENT: 'DM_20250703182219_001.mp3',
    AUTO_MODE_TOGGLE: 'DM_20250703182234_001.mp3',
    STEP_SUCCESS: 'DM_20250703182315_001.mp3',
    CSV_DOWNLOAD_INITIATION: 'DM_20250703194305_001.mp3',
    CSV_DOWNLOAD_COMPLETION: 'DM_20250703182159_001.mp3' // Reuse workflow completion sound
};

/**
 * Sound manager state
 */
let soundEnabled = true;
let audioCache = new Map();
let masterVolume = 0.7;

/**
 * Initialize the sound manager
 */
export function initializeSoundManager() {
    // Load user preferences from localStorage
    const savedSoundEnabled = localStorage.getItem('soundEnabled');
    if (savedSoundEnabled !== null) {
        soundEnabled = savedSoundEnabled === 'true';
    }

    const savedVolume = localStorage.getItem('soundVolume');
    if (savedVolume !== null) {
        masterVolume = parseFloat(savedVolume);
    }

    // Preload audio files
    preloadAudioFiles();

    console.log('[SOUND] Sound manager initialized', {
        soundEnabled,
        masterVolume,
        availableSounds: Object.keys(SOUND_EVENTS)
    });
}

/**
 * Preload all audio files for better performance
 */
function preloadAudioFiles() {
    Object.entries(SOUND_EVENTS).forEach(([eventType, filename]) => {
        try {
            const audio = new Audio(`/sound-design/${filename}`);
            audio.preload = 'auto';
            audio.volume = masterVolume;
            
            // Handle loading events
            audio.addEventListener('canplaythrough', () => {
                console.log(`[SOUND] Preloaded: ${filename}`);
            });
            
            audio.addEventListener('error', (e) => {
                console.warn(`[SOUND] Failed to preload: ${filename}`, e);
            });
            
            audioCache.set(eventType, audio);
        } catch (error) {
            console.warn(`[SOUND] Error creating audio object for ${filename}:`, error);
        }
    });
}

// Rate limiting for sound events to prevent spam
const soundRateLimit = new Map();
const SOUND_RATE_LIMIT_MS = 1000; // Minimum time between same sound events

/**
 * Play a sound for a specific event type with rate limiting and proper cleanup
 * @param {string} eventType - The type of event (from SOUND_EVENTS keys)
 * @param {Object} options - Optional parameters
 * @param {number} options.volume - Volume override (0.0 to 1.0)
 * @param {boolean} options.force - Force play even if sounds are disabled
 */
export function playSound(eventType, options = {}) {
    if (!soundEnabled && !options.force) {
        return;
    }

    if (!SOUND_EVENTS[eventType]) {
        console.warn(`[SOUND] Unknown event type: ${eventType}`);
        return;
    }

    // Rate limiting - prevent rapid-fire sound events
    const now = Date.now();
    const lastPlayed = soundRateLimit.get(eventType) || 0;
    if (now - lastPlayed < SOUND_RATE_LIMIT_MS) {
        return; // Skip this sound event to prevent spam
    }
    soundRateLimit.set(eventType, now);

    try {
        const audio = audioCache.get(eventType);
        if (!audio) {
            console.warn(`[SOUND] Audio not found in cache for event: ${eventType}`);
            return;
        }

        // Reuse the same audio element instead of cloning to prevent WebMediaPlayer accumulation
        audio.currentTime = 0; // Reset to beginning
        audio.volume = options.volume !== undefined ? options.volume : masterVolume;

        // Play the sound
        const playPromise = audio.play();

        if (playPromise !== undefined) {
            playPromise
                .then(() => {
                    // Reduced logging - only log success once per audio element
                    if (!audio.hasLoggedSuccess) {
                        console.log(`[SOUND] Audio system working for ${eventType}`);
                        audio.hasLoggedSuccess = true;
                    }
                })
                .catch((error) => {
                    // Handle autoplay restrictions gracefully with reduced logging
                    if (error.name === 'NotAllowedError') {
                        if (!audio.hasLoggedAutoplayBlock) {
                            console.log(`[SOUND] Autoplay blocked - user interaction required`);
                            audio.hasLoggedAutoplayBlock = true;
                        }
                    } else {
                        console.warn(`[SOUND] Error playing ${eventType}:`, error);
                    }
                });
        }
    } catch (error) {
        console.warn(`[SOUND] Exception playing sound for ${eventType}:`, error);
    }
}

/**
 * Enable or disable sound effects
 * @param {boolean} enabled - Whether sounds should be enabled
 */
export function setSoundEnabled(enabled) {
    soundEnabled = enabled;
    localStorage.setItem('soundEnabled', enabled.toString());
    console.log(`[SOUND] Sound ${enabled ? 'enabled' : 'disabled'}`);
}

/**
 * Get current sound enabled state
 * @returns {boolean} Whether sounds are enabled
 */
export function isSoundEnabled() {
    return soundEnabled;
}

/**
 * Set master volume for all sounds
 * @param {number} volume - Volume level (0.0 to 1.0)
 */
export function setMasterVolume(volume) {
    masterVolume = Math.max(0, Math.min(1, volume));
    localStorage.setItem('soundVolume', masterVolume.toString());
    
    // Update volume for all cached audio objects
    audioCache.forEach((audio) => {
        audio.volume = masterVolume;
    });
    
    console.log(`[SOUND] Master volume set to: ${masterVolume}`);
}

/**
 * Get current master volume
 * @returns {number} Current master volume (0.0 to 1.0)
 */
export function getMasterVolume() {
    return masterVolume;
}

/**
 * Convenience functions for specific event types
 */
export const soundEvents = {
    workflowStart: () => playSound('WORKFLOW_START'),
    checkboxInteraction: () => playSound('CHECKBOX_INTERACTION'),
    workflowCompletion: () => playSound('WORKFLOW_COMPLETION'),
    errorEvent: () => playSound('ERROR_EVENT'),
    autoModeToggle: () => playSound('AUTO_MODE_TOGGLE'),
    stepSuccess: () => playSound('STEP_SUCCESS'),
    csvDownloadInitiation: () => playSound('CSV_DOWNLOAD_INITIATION'),
    csvDownloadCompletion: () => playSound('CSV_DOWNLOAD_COMPLETION')
};
```

## File: static/stepDetailsPanel.js
```javascript
import { domBatcher } from './utils/DOMBatcher.js';
import { appState } from './state/AppState.js';

let lastFocusedStepElement = null;

function getEl(id) {
    const el = document.getElementById(id);
    if (!el) {
        console.warn(`[StepDetails] Missing element: #${id}`);
    }
    return el;
}

export function refreshStepDetailsPanelIfOpen(stepKey) {
    const open = !!appState.getStateProperty('ui.stepDetailsOpen');
    const selectedStepKey = appState.getStateProperty('ui.selectedStepKey');
    if (!open) return;
    if (!selectedStepKey) return;
    if (stepKey && stepKey !== selectedStepKey) return;
    updatePanelFromStep(selectedStepKey);
}

function getStepKeyFromElement(el) {
    if (!el) return null;
    const stepKey = el.dataset ? el.dataset.stepKey : null;
    return stepKey || null;
}

function setStepsExpandedState(selectedStepKey, expanded) {
    const steps = document.querySelectorAll('.timeline-step');
    steps.forEach((stepEl) => {
        const stepKey = getStepKeyFromElement(stepEl);
        const isSelected = !!selectedStepKey && stepKey === selectedStepKey;
        stepEl.setAttribute('aria-expanded', expanded && isSelected ? 'true' : 'false');
        if (isSelected && expanded) {
            stepEl.classList.add('is-selected');
        } else {
            stepEl.classList.remove('is-selected');
        }
    });
}

function updatePanelFromStep(stepKey) {
    const panel = getEl('step-details-panel');
    if (!panel) return;

    const titleEl = getEl('step-details-title');
    const statusEl = getEl('step-details-status');
    const timerEl = getEl('step-details-timer');
    const progressTextEl = getEl('step-details-progress-text');

    const runBtn = getEl('step-details-run');
    const cancelBtn = getEl('step-details-cancel');
    const logsBtn = getEl('step-details-open-logs');

    const stepCard = document.getElementById(`step-${stepKey}`);
    const stepName = stepCard ? (stepCard.dataset.stepName || stepKey) : stepKey;

    const statusBadge = document.getElementById(`status-${stepKey}`);
    const timerSource = document.getElementById(`timer-${stepKey}`);
    const progressTextSource = document.getElementById(`progress-text-${stepKey}`);

    const stepRunButton = document.querySelector(`.run-button[data-step="${stepKey}"]`);
    const stepCancelButton = document.querySelector(`.cancel-button[data-step="${stepKey}"]`);

    domBatcher.scheduleUpdate(`step-details-update:${stepKey}`, () => {
        if (titleEl) {
            titleEl.textContent = `Détails — ${stepName}`;
        }

        if (statusEl) {
            if (statusBadge) {
                statusEl.textContent = statusBadge.textContent || 'Prêt';
                statusEl.className = statusBadge.className || 'status-badge status-idle';
            } else {
                statusEl.textContent = 'Prêt';
                statusEl.className = 'status-badge status-idle';
            }
        }

        if (timerEl) {
            timerEl.textContent = timerSource ? (timerSource.textContent || '') : '';
        }

        if (progressTextEl) {
            progressTextEl.textContent = progressTextSource ? (progressTextSource.textContent || '') : '';
        }

        if (runBtn) {
            runBtn.disabled = !stepRunButton || !!stepRunButton.disabled;
        }

        if (cancelBtn) {
            cancelBtn.disabled = !stepCancelButton || !!stepCancelButton.disabled;
        }

        if (logsBtn) {
            logsBtn.disabled = !stepKey;
        }

        panel.dataset.stepKey = stepKey;
    });
}

function openPanel(stepKey) {
    const wrapper = getEl('workflow-wrapper');
    const panel = getEl('step-details-panel');
    const closeBtn = getEl('close-step-details');

    if (!wrapper || !panel) return;

    lastFocusedStepElement = document.getElementById(`step-${stepKey}`);

    appState.setState({ ui: { stepDetailsOpen: true, selectedStepKey: stepKey } }, 'step_details_open');

    domBatcher.scheduleUpdate('step-details-open', () => {
        panel.hidden = false;
        wrapper.classList.add('details-active');
        setStepsExpandedState(stepKey, true);
    });

    updatePanelFromStep(stepKey);

    requestAnimationFrame(() => {
        try {
            if (closeBtn && typeof closeBtn.focus === 'function') {
                closeBtn.focus();
            }
        } catch (_) {}
    });
}

function closePanel() {
    const wrapper = getEl('workflow-wrapper');
    const panel = getEl('step-details-panel');

    if (!wrapper || !panel) return;

    const selectedStepKey = appState.getStateProperty('ui.selectedStepKey');
    appState.setState({ ui: { stepDetailsOpen: false, selectedStepKey: null } }, 'step_details_close');

    domBatcher.scheduleUpdate('step-details-close', () => {
        wrapper.classList.remove('details-active');
        panel.hidden = true;
        setStepsExpandedState(selectedStepKey, false);
        panel.removeAttribute('data-step-key');
    });

    requestAnimationFrame(() => {
        try {
            if (lastFocusedStepElement && typeof lastFocusedStepElement.focus === 'function') {
                lastFocusedStepElement.focus();
            }
        } catch (_) {}
        lastFocusedStepElement = null;
    });
}

function isLogsPanelOpen() {
    const wrapper = getEl('workflow-wrapper');
    return !!(wrapper && wrapper.classList.contains('logs-active'));
}

function attachStepSelectionListeners() {
    const steps = document.querySelectorAll('.timeline-step');

    steps.forEach((stepEl) => {
        const handleSelect = () => {
            if (isLogsPanelOpen()) return;
            const stepKey = getStepKeyFromElement(stepEl);
            if (!stepKey) return;
            openPanel(stepKey);
        };

        stepEl.addEventListener('click', (e) => {
            if (e && e.target && e.target.closest) {
                const interactive = e.target.closest('button, a, input, select, textarea');
                if (interactive) return;
            }
            handleSelect();
        });

        stepEl.addEventListener('keydown', (e) => {
            if (!e) return;

            if (e.target && e.target.closest) {
                const interactive = e.target.closest('button, a, input, select, textarea');
                if (interactive) return;
            }

            if (e.key === 'Enter' || e.key === ' ') {
                e.preventDefault();
                handleSelect();
            }
        });
    });
}

function attachPanelListeners() {
    const closeBtn = getEl('close-step-details');
    const runBtn = getEl('step-details-run');
    const cancelBtn = getEl('step-details-cancel');
    const logsBtn = getEl('step-details-open-logs');
    const panel = getEl('step-details-panel');

    if (closeBtn) {
        closeBtn.addEventListener('click', () => {
            closePanel();
        });
    }

    if (runBtn) {
        runBtn.addEventListener('click', () => {
            const stepKey = panel ? panel.dataset.stepKey : null;
            if (!stepKey) return;
            const btn = document.querySelector(`.run-button[data-step="${stepKey}"]`);
            if (btn && typeof btn.click === 'function') {
                btn.click();
            }
            updatePanelFromStep(stepKey);
        });
    }

    if (cancelBtn) {
        cancelBtn.addEventListener('click', () => {
            const stepKey = panel ? panel.dataset.stepKey : null;
            if (!stepKey) return;
            const btn = document.querySelector(`.cancel-button[data-step="${stepKey}"]`);
            if (btn && typeof btn.click === 'function') {
                btn.click();
            }
            updatePanelFromStep(stepKey);
        });
    }

    if (logsBtn) {
        logsBtn.addEventListener('click', async () => {
            const stepKey = panel ? panel.dataset.stepKey : null;
            if (!stepKey) return;
            closePanel();
            try {
                const mod = await import('./uiUpdater.js');
                if (mod && typeof mod.openLogPanelUI === 'function') {
                    mod.openLogPanelUI(stepKey, true);
                }
            } catch (e) {
                console.error('[StepDetails] Failed to open logs panel:', e);
            }
        });
    }

    document.addEventListener('keydown', (e) => {
        if (!e || e.key !== 'Escape') return;
        const open = !!appState.getStateProperty('ui.stepDetailsOpen');
        if (!open) return;
        closePanel();
    });
}

function attachLogsPanelObserver() {
    const wrapper = getEl('workflow-wrapper');
    if (!wrapper || typeof MutationObserver === 'undefined') return;

    const observer = new MutationObserver(() => {
        if (wrapper.classList.contains('logs-active')) {
            const open = !!appState.getStateProperty('ui.stepDetailsOpen');
            if (open) {
                closePanel();
            }
        }
    });

    observer.observe(wrapper, { attributes: true, attributeFilter: ['class'] });
}

export function initializeStepDetailsPanel() {
    attachStepSelectionListeners();
    attachPanelListeners();
    attachLogsPanelObserver();
}
```

## File: static/themeManager.js
```javascript
/**
 * Theme Manager - Workflow MediaPipe v4.0
 * Handles dynamic theme switching with localStorage persistence
 */

const THEME_STORAGE_KEY = 'workflow-theme-preference';

const THEMES = {
    'dark-pro': {
        name: 'Dark Pro',
        description: 'Professional dark theme (default)'
    },
    'light-mode': {
        name: 'Light Mode',
        description: 'Clean and bright professional theme'
    },
    'pastel-zen': {
        name: 'Pastel Zen',
        description: 'Soft and calming pastel colors'
    },
    'neon-cyberpunk': {
        name: 'Neon Cyberpunk',
        description: 'Vibrant futuristic theme with glows'
    },
    'forest-night': {
        name: 'Forest Night',
        description: 'Natural earthy tones'
    },
    'ocean-depth': {
        name: 'Ocean Depth',
        description: 'Deep blue oceanic theme'
    }
};

class ThemeManager {
    constructor() {
        this.currentTheme = this.loadTheme();
        this.themeSelector = null;
    }

    /**
     * Initialize the theme system
     */
    init() {
        console.log('[ThemeManager] Initializing theme system...');
        
        // Apply saved theme immediately
        this.applyTheme(this.currentTheme);
        
        // Setup theme selector if it exists
        this.setupThemeSelector();
        
        console.log(`[ThemeManager] Theme system initialized with: ${this.currentTheme}`);
    }

    /**
     * Load theme preference from localStorage
     * @returns {string} Theme identifier
     */
    loadTheme() {
        try {
            const savedTheme = localStorage.getItem(THEME_STORAGE_KEY);
            if (savedTheme && THEMES[savedTheme]) {
                console.log(`[ThemeManager] Loaded saved theme: ${savedTheme}`);
                return savedTheme;
            }
        } catch (error) {
            console.error('[ThemeManager] Error loading theme from localStorage:', error);
        }
        
        // Default to dark-pro
        return 'dark-pro';
    }

    /**
     * Save theme preference to localStorage
     * @param {string} themeId - Theme identifier
     */
    saveTheme(themeId) {
        try {
            localStorage.setItem(THEME_STORAGE_KEY, themeId);
            console.log(`[ThemeManager] Saved theme preference: ${themeId}`);
        } catch (error) {
            console.error('[ThemeManager] Error saving theme to localStorage:', error);
        }
    }

    /**
     * Apply a theme to the document
     * @param {string} themeId - Theme identifier
     */
    applyTheme(themeId) {
        if (!THEMES[themeId]) {
            console.warn(`[ThemeManager] Unknown theme: ${themeId}, falling back to dark-pro`);
            themeId = 'dark-pro';
        }

        console.log(`[ThemeManager] Applying theme: ${themeId}`);
        
        // Apply theme to document root
        document.documentElement.setAttribute('data-theme', themeId);
        
        // Update current theme
        this.currentTheme = themeId;
        
        // Save preference
        this.saveTheme(themeId);
        
        // Update selector if available
        if (this.themeSelector) {
            this.themeSelector.value = themeId;
        }

        // Dispatch custom event for other components
        window.dispatchEvent(new CustomEvent('themeChanged', {
            detail: { theme: themeId, themeName: THEMES[themeId].name }
        }));
        
        console.log(`[ThemeManager] Theme applied successfully: ${THEMES[themeId].name}`);
    }

    /**
     * Setup the theme selector dropdown
     */
    setupThemeSelector() {
        this.themeSelector = document.getElementById('theme-selector');
        
        if (!this.themeSelector) {
            console.warn('[ThemeManager] Theme selector element not found');
            return;
        }

        // Populate options
        this.themeSelector.innerHTML = '';
        Object.entries(THEMES).forEach(([id, theme]) => {
            const option = document.createElement('option');
            option.value = id;
            option.textContent = theme.name;
            option.title = theme.description;
            this.themeSelector.appendChild(option);
        });

        // Set current selection
        this.themeSelector.value = this.currentTheme;

        // Add change event listener
        this.themeSelector.addEventListener('change', (e) => {
            const selectedTheme = e.target.value;
            console.log(`[ThemeManager] User selected theme: ${selectedTheme}`);
            this.applyTheme(selectedTheme);
        });

        console.log('[ThemeManager] Theme selector setup complete');
    }

    /**
     * Get current theme info
     * @returns {Object} Current theme information
     */
    getCurrentTheme() {
        return {
            id: this.currentTheme,
            ...THEMES[this.currentTheme]
        };
    }

    /**
     * Get all available themes
     * @returns {Object} All themes
     */
    getAvailableThemes() {
        return THEMES;
    }
}

// Create singleton instance
const themeManager = new ThemeManager();

// Export for module usage
export { themeManager, THEMES };

// Also expose globally for non-module scripts
window.themeManager = themeManager;
```

## File: templates/reports/analysis_report.html
```html
<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Rapport d'Analyse - {{ video_name }}</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, sans-serif;
            line-height: 1.6;
            color: #333;
            background: #f5f5f5;
            padding: 20px;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        
        .header {
            border-bottom: 3px solid #007bff;
            padding-bottom: 20px;
            margin-bottom: 30px;
        }
        
        .header h1 {
            color: #007bff;
            margin-bottom: 10px;
        }
        
        .meta-info {
            color: #666;
            font-size: 14px;
        }
        
        .section {
            margin-bottom: 40px;
        }
        
        .section-title {
            color: #007bff;
            border-left: 4px solid #007bff;
            padding-left: 15px;
            margin-bottom: 20px;
            font-size: 24px;
        }
        
        .stats-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin-bottom: 20px;
        }
        
        .stat-card {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            border-left: 4px solid #007bff;
        }
        
        .stat-label {
            color: #666;
            font-size: 12px;
            text-transform: uppercase;
            letter-spacing: 0.5px;
            margin-bottom: 5px;
        }
        
        .stat-value {
            font-size: 28px;
            font-weight: bold;
            color: #007bff;
        }
        
        .stat-unit {
            font-size: 14px;
            color: #666;
            font-weight: normal;
        }
        
        .badge {
            display: inline-block;
            padding: 4px 12px;
            border-radius: 12px;
            font-size: 12px;
            font-weight: 600;
            margin-right: 10px;
        }
        
        .badge-success {
            background: #28a745;
            color: white;
        }
        
        .badge-warning {
            background: #ffc107;
            color: #333;
        }
        
        .badge-info {
            background: #17a2b8;
            color: white;
        }
        
        .badge-archive {
            background: #6c757d;
            color: white;
        }
        
        .progress-bar {
            width: 100%;
            height: 30px;
            background: #e9ecef;
            border-radius: 15px;
            overflow: hidden;
            margin-top: 10px;
        }
        
        .progress-fill {
            height: 100%;
            background: linear-gradient(90deg, #007bff, #0056b3);
            display: flex;
            align-items: center;
            justify-content: center;
            color: white;
            font-weight: bold;
            font-size: 14px;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #dee2e6;
            text-align: center;
            color: #666;
            font-size: 14px;
        }
        
        @media print {
            body {
                background: white;
                padding: 0;
            }
            .container {
                box-shadow: none;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>📊 Rapport d'Analyse Vidéo</h1>
            <div class="meta-info">
                <strong>Projet:</strong> {{ project_name }} | 
                <strong>Vidéo:</strong> {{ video_name }}<br>
                <strong>Généré le:</strong> {{ generated_at }}
                {% if archive_probe_source.metadata.provenance == 'archives' %}
                <br><span class="badge badge-archive">📦 Données archivées</span>
                {% endif %}
            </div>
        </div>
        
        <div class="section">
            <h2 class="section-title">📹 Métadonnées Vidéo</h2>
            <div class="stats-grid">
                <div class="stat-card">
                    <div class="stat-label">Durée</div>
                    <div class="stat-value">{{ statistics.video.duration_seconds|format_duration }}</div>
                </div>
                <div class="stat-card">
                    <div class="stat-label">Frames Totales</div>
                    <div class="stat-value">{{ statistics.video.total_frames|int }}</div>
                </div>
                <div class="stat-card">
                    <div class="stat-label">FPS</div>
                    <div class="stat-value">{{ statistics.video.fps|round(2) }} <span class="stat-unit">fps</span></div>
                </div>
            </div>
        </div>
        
        {% if scenes.available %}
        <div class="section">
            <h2 class="section-title">✂️ Analyse des Scènes</h2>
            <span class="badge badge-success">{{ statistics.scenes.total_count }} scènes détectées</span>
            {% if archive_probe_source.scenes.provenance == 'archives' %}
            <span class="badge badge-archive">📦 Données archivées</span>
            {% endif %}
            
            <div class="stats-grid" style="margin-top: 20px;">
                <div class="stat-card">
                    <div class="stat-label">Durée Moyenne</div>
                    <div class="stat-value">{{ statistics.scenes.average_duration|round(1) }} <span class="stat-unit">s</span></div>
                </div>
                <div class="stat-card">
                    <div class="stat-label">Scène la Plus Courte</div>
                    <div class="stat-value">{{ statistics.scenes.shortest_duration|round(1) }} <span class="stat-unit">s</span></div>
                </div>
                <div class="stat-card">
                    <div class="stat-label">Scène la Plus Longue</div>
                    <div class="stat-value">{{ statistics.scenes.longest_duration|round(1) }} <span class="stat-unit">s</span></div>
                </div>
            </div>
        </div>
        {% endif %}
        
        {% if audio.available %}
        <div class="section">
            <h2 class="section-title">🔊 Analyse Audio</h2>
            <span class="badge badge-info">{{ statistics.audio.total_segments }} segments de parole</span>
            <span class="badge badge-info">{{ statistics.audio.unique_speakers }} locuteurs</span>
            {% if archive_probe_source.audio.provenance == 'archives' %}
            <span class="badge badge-archive">📦 Données archivées</span>
            {% endif %}
            
            <div class="stats-grid" style="margin-top: 20px;">
                <div class="stat-card">
                    <div class="stat-label">Durée Totale de Parole</div>
                    <div class="stat-value">{{ statistics.audio.total_speech_duration|format_duration }}</div>
                </div>
                <div class="stat-card">
                    <div class="stat-label">Couverture Audio</div>
                    <div class="stat-value">{{ statistics.audio.speech_coverage_percent|format_percentage }}</div>
                    <div class="progress-bar">
                        <div class="progress-fill" style="width: {{ statistics.audio.speech_coverage_percent }}%">
                            {{ statistics.audio.speech_coverage_percent|format_percentage }}
                        </div>
                    </div>
                </div>
            </div>
        </div>
        {% endif %}
        
        {% if tracking.available %}
        <div class="section">
            <h2 class="section-title">👤 Suivi des Visages</h2>
            <span class="badge badge-warning">{{ statistics.tracking.max_faces_detected }} visages max détectés</span>
            {% if archive_probe_source.tracking.provenance == 'archives' %}
            <span class="badge badge-archive">📦 Données archivées</span>
            {% endif %}
            
            <div class="stats-grid" style="margin-top: 20px;">
                <div class="stat-card">
                    <div class="stat-label">Frames avec Visages</div>
                    <div class="stat-value">{{ statistics.tracking.frames_with_faces|int }}</div>
                </div>
                <div class="stat-card">
                    <div class="stat-label">Frames avec Parole Détectée</div>
                    <div class="stat-value">{{ statistics.tracking.frames_with_speaking|int }}</div>
                </div>
                <div class="stat-card">
                    <div class="stat-label">Couverture Visages</div>
                    <div class="stat-value">{{ statistics.tracking.face_coverage_percent|format_percentage }}</div>
                    <div class="progress-bar">
                        <div class="progress-fill" style="width: {{ statistics.tracking.face_coverage_percent }}%">
                            {{ statistics.tracking.face_coverage_percent|format_percentage }}
                        </div>
                    </div>
                </div>
            </div>
        </div>
        {% endif %}
        
        <div class="footer">
            <p>Rapport généré par MediaPipe Workflow Analysis System</p>
            <p>{{ generated_at }}</p>
        </div>
    </div>
</body>
</html>
```

## File: templates/reports/monthly_archive_report.html
```html
<!doctype html>
<html lang="fr">
  <meta charset="utf-8" />
  <title>Rapport Mensuel Archives - {{ month }}</title>
  <style>
    body { font-family: Arial, sans-serif; margin: 20px; }
    h1 { margin-bottom: 6px; }
    .meta { color: #666; font-size: 12px; margin-bottom: 12px; }
    table { border-collapse: collapse; width: 100%; margin-top: 10px; }
    th, td { border: 1px solid #ddd; padding: 8px; font-size: 13px; }
    th { background: #fafafa; text-align: left; }
    .section { margin-top: 20px; }
    ul { margin: 6px 0 12px 18px; }
    li { margin: 2px 0; }
    .video-names { margin: 4px 0 0 0; }
    .video-name { display: block; padding-left: 4px; }
  </style>
  </head>
<body>
  <h1>Rapport Mensuel Archives — {{ month }}</h1>
  <div class="meta">Généré le {{ generated_at }} · Build {{ build_id }}</div>
  <table>
    <thead>
      <tr><th>Projet</th><th>Horodatage</th><th>Vidéos</th><th>Scènes</th><th>Parole (s)</th><th>Faces (frames)</th></tr>
    </thead>
    <tbody>
      {% for p in projects %}
      <tr>
        <td>{{ p.display_base }}</td>
        <td>{{ p.archive_timestamp or '-' }}</td>
        <td style="text-align:right">{{ p.video_count }}</td>
        <td style="text-align:right">{{ p.videos | sum(attribute='statistics.scenes.total_count') }}</td>
        <td style="text-align:right">{{ p.videos | sum(attribute='statistics.audio.total_speech_duration') | round(1) }}</td>
        <td style="text-align:right">{{ p.videos | sum(attribute='statistics.tracking.frames_with_faces') }}</td>
      </tr>
      {% endfor %}
    </tbody>
  </table>

  <div class="section">
    <h2>Répartition des Durées par Projet</h2>
    {% for p in projects %}
      <div>
        <strong>{{ p.display_base }}</strong> :
        <ul>
          <li>
            moins de 2 minutes : {{ p.duration_counts.lt_2m }}
            {% if p.duration_names and p.duration_names.lt_2m %}
              : <div class="video-names">
                  {% for nm in p.duration_names.lt_2m %}
                    <div class="video-name">{{ nm }}</div>
                  {% endfor %}
                </div>
            {% endif %}
          </li>
          <li>
            entre 2 et 5 minutes : {{ p.duration_counts.between_2_5m }}
            {% if p.duration_names and p.duration_names.between_2_5m %}
              : <div class="video-names">
                  {% for nm in p.duration_names.between_2_5m %}
                    <div class="video-name">{{ nm }}</div>
                  {% endfor %}
                </div>
            {% endif %}
          </li>
          <li>
            plus de 5 minutes : {{ p.duration_counts.gt_5m }}
            {% if p.duration_names and p.duration_names.gt_5m %}
              : <div class="video-names">
                  {% for nm in p.duration_names.gt_5m %}
                    <div class="video-name">{{ nm }}</div>
                  {% endfor %}
                </div>
            {% endif %}
          </li>
        </ul>
      </div>
    {% endfor %}
  </div>
</body>
</html>
```

## File: templates/reports/project_report.html
```html
<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Rapport de Projet - {{ project_name }}</title>
    <style>
        body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, sans-serif; background:#f5f5f5; color:#333; padding:20px; }
        .container { max-width: 1200px; margin:0 auto; background:#fff; border-radius:8px; box-shadow:0 2px 10px rgba(0,0,0,0.1); padding:40px; }
        .header { border-bottom:3px solid #007bff; padding-bottom:16px; margin-bottom:24px; }
        .header h1 { color:#007bff; margin:0 0 6px; }
        .section { margin:28px 0; }
        .section-title { color:#007bff; border-left:4px solid #007bff; padding-left:12px; font-size:22px; margin-bottom:12px; }
        .grid { display:grid; grid-template-columns: repeat(auto-fit, minmax(260px,1fr)); gap:16px; }
        .card { background:#f8f9fa; border-left:4px solid #007bff; border-radius:8px; padding:16px; }
        .badge { display:inline-block; padding:4px 10px; border-radius:12px; font-size:12px; font-weight:600; margin-right:8px; }
        .badge-info { background:#17a2b8; color:#fff; }
        .table { width:100%; border-collapse:collapse; }
        .table th, .table td { border-bottom:1px solid #e9ecef; padding:10px; text-align:left; }
        .table th { background:#f1f3f5; }
        .muted { color:#666; font-size:13px; }
    </style>
</head>
<body>
<div class="container">
  <div class="header">
    <h1>📁 Rapport Consolidé du Projet</h1>
    <div class="muted">Projet: <strong>{{ project_name }}</strong> · Généré le {{ generated_at }}</div>
  </div>

  <div class="section">
    <h2 class="section-title">📊 Statistiques Globales</h2>
    <div class="grid">
      <div class="card"><div class="muted">Durée Totale</div><div style="font-size:28px;color:#007bff;">{{ consolidated.video.duration_seconds|format_duration }}</div></div>
      <div class="card"><div class="muted">Frames Totales</div><div style="font-size:28px;color:#007bff;">{{ consolidated.video.total_frames|int }}</div></div>
      <div class="card"><div class="muted">FPS Moyen</div><div style="font-size:28px;color:#007bff;">{{ consolidated.video.fps|round(2) }}</div></div>
      <div class="card"><div class="muted">Scènes Totales</div><div style="font-size:28px;color:#007bff;">{{ consolidated.scenes.total_count|int }}</div></div>
      <div class="card"><div class="muted">Segments de Parole</div><div style="font-size:28px;color:#007bff;">{{ consolidated.audio.total_segments|int }}</div></div>
      <div class="card"><div class="muted">Locuteurs Uniques</div><div style="font-size:28px;color:#007bff;">{{ consolidated.audio.unique_speakers|int }}</div></div>
      <div class="card"><div class="muted">Couverture Parole</div><div style="font-size:28px;color:#007bff;">{{ consolidated.audio.speech_coverage_percent|format_percentage }}</div></div>
      <div class="card"><div class="muted">Frames avec Visages</div><div style="font-size:28px;color:#007bff;">{{ consolidated.tracking.frames_with_faces|int }}</div></div>
    </div>
  </div>

  <div class="section">
    <h2 class="section-title">🎬 Détails par Vidéo</h2>
    <table class="table">
      <thead>
        <tr>
          <th>Vidéo</th>
          <th>Durée</th>
          <th>Scènes</th>
          <th>Locuteurs</th>
          <th>Segments</th>
          <th>Faces (max)</th>
          <th>Couverture Visages</th>
          <th>Statut</th>
        </tr>
      </thead>
      <tbody>
        {% for v in videos %}
        <tr>
          <td>{{ v.video_name }}</td>
          {% if v.error %}
            <td colspan="6" class="muted">—</td>
            <td><span class="badge badge-info">Erreur</span> {{ v.error }}</td>
          {% else %}
            <td>{{ v.statistics.video.duration_seconds|format_duration }}</td>
            <td>{{ v.statistics.scenes.total_count|int }}</td>
            <td>{{ v.metadata.get('speaker_count', 0) or v.statistics.audio.unique_speakers|int }}</td>
            <td>{{ v.statistics.audio.total_segments|int }}</td>
            <td>{{ v.statistics.tracking.max_faces_detected|int }}</td>
            <td>{{ v.statistics.tracking.face_coverage_percent|format_percentage }}</td>
            <td><span class="badge badge-info">OK</span></td>
          {% endif %}
        </tr>
        {% endfor %}
      </tbody>
    </table>
  </div>

  <div class="section" style="text-align:center;color:#666;font-size:13px;">
    Rapport généré par MediaPipe Workflow Analysis System · {{ generated_at }}
  </div>
</div>
</body>
</html>
```

## File: utils/enhanced_speaking_detection.py
```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Enhanced Speaking Detection System
Implements multi-source validation for speaking detection using:
1. Face blendshapes (jaw movement)
2. Audio diarization data
3. CSV segment analysis
4. Temporal consistency validation
"""

import json
import csv
import logging
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class SegmentInfo:
    """Information about a video segment from CSV data."""
    segment_id: int
    frame_start: int
    frame_end: int
    timecode_start: str
    timecode_end: str

@dataclass
class AudioInfo:
    """Audio analysis information for a frame."""
    is_speech_present: bool
    num_speakers: int
    active_speakers: List[str]
    timecode_sec: float

@dataclass
class SpeakingDetectionResult:
    """Result of enhanced speaking detection."""
    is_speaking: bool
    confidence: float
    sources: Dict[str, Any]  # Evidence from different sources
    method: str  # Primary detection method used

class EnhancedSpeakingDetector:
    """Enhanced speaking detection using multiple data sources."""
    
    def __init__(self, video_path: str, jaw_threshold: float = 0.08):
        """
        Initialize the enhanced speaking detector.
        
        Args:
            video_path: Path to the video file
            jaw_threshold: Threshold for jaw movement detection
        """
        self.video_path = Path(video_path)
        self.jaw_threshold = jaw_threshold
        
        # Load auxiliary data
        self.segments = self._load_csv_segments()
        self.audio_data = self._load_audio_analysis()
        
        # Detection parameters
        self.audio_weight = 0.6  # Weight for audio evidence
        self.visual_weight = 0.4  # Weight for visual evidence
        self.min_confidence_threshold = 0.3
        
        logger.info(f"Enhanced speaking detector initialized for {self.video_path.name}")
        logger.info(f"Loaded {len(self.segments)} segments, audio data: {self.audio_data is not None}")
    
    def _load_csv_segments(self) -> List[SegmentInfo]:
        """Load CSV segmentation data."""
        csv_path = self.video_path.with_suffix('.csv')
        segments = []

        if not csv_path.exists():
            logger.warning(f"CSV file not found: {csv_path}")
            return segments

        try:
            zero_duration_count = 0
            invalid_duration_count = 0
            with open(csv_path, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    frame_start = int(row['Frame In'])
                    frame_end = int(row['Frame Out'])
                    duration = frame_end - frame_start

                    # Check for problematic segments
                    if duration == 0:
                        zero_duration_count += 1
                        logger.warning(f"Zero-duration segment detected: ID {row['No']}, frame {frame_start}")
                    elif duration < 0:
                        invalid_duration_count += 1
                        logger.warning(f"Invalid segment with negative duration: ID {row['No']}, frames {frame_start}-{frame_end}")

                    segment = SegmentInfo(
                        segment_id=int(row['No']),
                        frame_start=frame_start,
                        frame_end=frame_end,
                        timecode_start=row['Timecode In'],
                        timecode_end=row['Timecode Out']
                    )
                    segments.append(segment)

            logger.info(f"Loaded {len(segments)} segments from {csv_path.name}")
            if zero_duration_count > 0:
                logger.warning(f"Found {zero_duration_count} zero-duration segments that will be handled safely")
            if invalid_duration_count > 0:
                logger.warning(f"Found {invalid_duration_count} segments with invalid negative durations that will be handled safely")

        except Exception as e:
            logger.error(f"Error loading CSV segments: {e}")

        return segments
    
    def _load_audio_analysis(self) -> Optional[Dict[int, AudioInfo]]:
        """Load audio analysis data."""
        # Construct audio JSON path: video_name_audio.json
        video_stem = self.video_path.stem  # e.g., "AMERICAINES"
        audio_json_path = self.video_path.parent / f"{video_stem}_audio.json"
        
        if not audio_json_path.exists():
            logger.warning(f"Audio analysis file not found: {audio_json_path}")
            return None
        
        try:
            with open(audio_json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            audio_data = {}
            warned_schema_once = False
            for frame_data in data.get('frames_analysis', []):
                try:
                    frame_num = frame_data.get('frame')
                    audio_info = frame_data.get('audio_info', {}) or {}

                    # Fallbacks for schema differences
                    is_speech_present = bool(audio_info.get('is_speech_present', False))
                    if 'num_distinct_speakers_audio' in audio_info:
                        num_speakers = int(audio_info.get('num_distinct_speakers_audio', 0) or 0)
                    elif 'num_speakers' in audio_info:
                        num_speakers = int(audio_info.get('num_speakers', 0) or 0)
                    else:
                        # derive from list length if available
                        num_speakers = len(audio_info.get('active_speaker_labels', []) or [])
                        if not warned_schema_once:
                            logger.debug("Audio schema using simplified format (active_speaker_labels only); deriving num_speakers from list length")
                            warned_schema_once = True

                    active_speakers = list(audio_info.get('active_speaker_labels', []) or [])
                    timecode_sec = float(audio_info.get('timecode_sec', 0.0) or 0.0)

                    if frame_num is None:
                        continue

                    audio_data[int(frame_num)] = AudioInfo(
                        is_speech_present=is_speech_present,
                        num_speakers=num_speakers,
                        active_speakers=active_speakers,
                        timecode_sec=timecode_sec
                    )
                except Exception as e:
                    logger.warning(f"Skipping malformed audio frame entry: {e}")

            logger.info(f"Loaded audio data for {len(audio_data)} frames")
            return audio_data
            
        except Exception as e:
            logger.error(f"Error loading audio analysis: {e}")
            return None
    
    def get_segment_for_frame(self, frame_num: int) -> Optional[SegmentInfo]:
        """Get the segment that contains the given frame."""
        for segment in self.segments:
            if segment.frame_start <= frame_num <= segment.frame_end:
                return segment
        return None
    
    def detect_speaking(self, frame_num: int, blendshapes: Optional[Dict[str, float]] = None,
                       source_detector: str = "face_landmarker") -> SpeakingDetectionResult:
        """
        Enhanced speaking detection using multiple sources.
        
        Args:
            frame_num: Frame number (1-based)
            blendshapes: Face blendshapes data (if available)
            source_detector: Source of detection ("face_landmarker" or "object_detector")
            
        Returns:
            SpeakingDetectionResult with confidence and evidence
        """
        sources = {}
        confidence_scores = []
        
        # 1. Audio-based detection (primary source)
        audio_confidence = 0.0
        if self.audio_data and frame_num in self.audio_data:
            audio_info = self.audio_data[frame_num]
            if audio_info.is_speech_present:
                # Higher confidence with more speakers
                audio_confidence = min(0.8 + (audio_info.num_speakers * 0.1), 1.0)
            sources['audio'] = {
                'is_speech_present': audio_info.is_speech_present,
                'num_speakers': audio_info.num_speakers,
                'active_speakers': audio_info.active_speakers,
                'confidence': audio_confidence
            }
            confidence_scores.append(audio_confidence * self.audio_weight)
        
        # 2. Visual-based detection (blendshapes)
        visual_confidence = 0.0
        if source_detector == "face_landmarker" and blendshapes and "jawOpen" in blendshapes:
            jaw_open = blendshapes["jawOpen"]
            
            # Enhanced jaw movement analysis
            if jaw_open > self.jaw_threshold:
                # Scale confidence based on jaw opening amount
                visual_confidence = min(jaw_open / self.jaw_threshold * 0.7, 1.0)
                
                # Additional blendshape analysis for better accuracy
                mouth_indicators = [
                    blendshapes.get("mouthOpen", 0.0),
                    blendshapes.get("mouthShrugUpper", 0.0),
                    blendshapes.get("mouthShrugLower", 0.0)
                ]
                mouth_activity = sum(mouth_indicators) / len(mouth_indicators)
                visual_confidence = min(visual_confidence + mouth_activity * 0.2, 1.0)
            
            sources['visual'] = {
                'jaw_open': jaw_open,
                'jaw_threshold': self.jaw_threshold,
                'mouth_activity': mouth_activity if 'mouth_activity' in locals() else 0.0,
                'confidence': visual_confidence
            }
            confidence_scores.append(visual_confidence * self.visual_weight)
        
        # 3. Segment-based context
        segment = self.get_segment_for_frame(frame_num)
        if segment:
            # Calculate frame position safely, handling zero-duration segments
            segment_duration = segment.frame_end - segment.frame_start
            if segment_duration > 0:
                frame_position = (frame_num - segment.frame_start) / segment_duration
            elif segment_duration == 0:
                # Zero-duration segment: frame is at the single point
                frame_position = 0.0
                logger.debug(f"Zero-duration segment {segment.segment_id} at frame {frame_num}, using position 0.0")
            else:
                # Negative duration (invalid segment): treat as zero-duration
                frame_position = 0.0
                logger.warning(f"Invalid segment {segment.segment_id} with negative duration {segment_duration}, using position 0.0")

            sources['segment'] = {
                'segment_id': segment.segment_id,
                'frame_position': frame_position,
                'segment_duration': segment_duration,
                'is_zero_duration': segment_duration == 0
            }
        
        # 4. Calculate final result
        if confidence_scores:
            final_confidence = sum(confidence_scores)
        else:
            final_confidence = 0.0
        
        # Determine speaking status and method
        is_speaking = final_confidence > self.min_confidence_threshold
        
        # Determine primary method
        if audio_confidence > visual_confidence:
            method = "audio_primary"
        elif visual_confidence > 0:
            method = "visual_primary"
        elif source_detector == "object_detector":
            method = "object_detection_fallback"
        else:
            method = "no_detection"
        
        return SpeakingDetectionResult(
            is_speaking=is_speaking,
            confidence=final_confidence,
            sources=sources,
            method=method
        )
    
    def get_detection_stats(self) -> Dict[str, Any]:
        """Get statistics about available detection sources."""
        return {
            'segments_available': len(self.segments) > 0,
            'audio_data_available': self.audio_data is not None,
            'total_segments': len(self.segments),
            'total_audio_frames': len(self.audio_data) if self.audio_data else 0,
            'jaw_threshold': self.jaw_threshold,
            'audio_weight': self.audio_weight,
            'visual_weight': self.visual_weight
        }
```

## File: utils/filename_security.py
```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Comprehensive filename sanitization and security utilities for archive extraction.
Provides secure filename handling to prevent filesystem attacks and ensure compatibility.
"""

import os
import re
import unicodedata
import logging
from pathlib import Path
from typing import Tuple, Set, Optional, List
from dataclasses import dataclass

logger = logging.getLogger(__name__)

WINDOWS_FORBIDDEN_CHARS = set('<>:"|?*')
UNIX_FORBIDDEN_CHARS = set('\x00')  # Null byte
CONTROL_CHARS = set(chr(i) for i in range(32))  # ASCII control characters
WINDOWS_RESERVED_NAMES = {
    'CON', 'PRN', 'AUX', 'NUL',
    'COM1', 'COM2', 'COM3', 'COM4', 'COM5', 'COM6', 'COM7', 'COM8', 'COM9',
    'LPT1', 'LPT2', 'LPT3', 'LPT4', 'LPT5', 'LPT6', 'LPT7', 'LPT8', 'LPT9'
}

MAX_FILENAME_LENGTH = 255
MAX_PATH_LENGTH = 4096
MAX_COMPONENT_LENGTH = 255

DANGEROUS_PATTERNS = [
    r'\.\.[\\/]',  # Path traversal
    r'^[\\/]',     # Absolute paths
    r'[\\/]\.\.[\\/]',  # Path traversal in middle
    r'[\\/]\.\.$',      # Path traversal at end
]


@dataclass
class SanitizationResult:
    """Result of filename sanitization operation."""
    original_name: str
    sanitized_name: str
    was_modified: bool
    security_issues: List[str]
    warnings: List[str]


class FilenameSanitizer:
    """
    Comprehensive filename sanitizer for secure archive extraction.
    
    Handles:
    - Path traversal prevention
    - Platform-specific forbidden characters
    - Unicode normalization
    - Length limits
    - Reserved names
    - Control character filtering
    """
    
    def __init__(self, max_filename_length: int = MAX_FILENAME_LENGTH):
        """
        Initialize the sanitizer.
        
        Args:
            max_filename_length: Maximum allowed filename length
        """
        self.max_filename_length = max_filename_length
        self.sanitization_stats = {
            'total_processed': 0,
            'modified_count': 0,
            'security_issues_found': 0
        }
    
    def sanitize_archive_member_path(self, member_path: str) -> SanitizationResult:
        """
        Sanitize a complete path from an archive member.
        
        Args:
            member_path: Original path from archive
            
        Returns:
            SanitizationResult with sanitized path and security info
        """
        self.sanitization_stats['total_processed'] += 1
        original_path = member_path
        security_issues = []
        warnings = []
        
        for pattern in DANGEROUS_PATTERNS:
            if re.search(pattern, member_path):
                security_issues.append(f"Path traversal pattern detected: {pattern}")
        
        member_path = unicodedata.normalize('NFKC', member_path)
        
        path_parts = []
        for part in member_path.split('/'):
            if part in ('', '.', '..'):
                security_issues.append(f"Dangerous path component: '{part}'")
                continue
            
            sanitized_part = self._sanitize_filename_component(part)
            if sanitized_part.was_modified:
                warnings.extend(sanitized_part.warnings)
                security_issues.extend(sanitized_part.security_issues)
            
            if sanitized_part.sanitized_name:  # Only add non-empty parts
                path_parts.append(sanitized_part.sanitized_name)
        
        sanitized_path = '/'.join(path_parts) if path_parts else 'sanitized_file'
        
        if len(sanitized_path) > MAX_PATH_LENGTH:
            warnings.append(f"Path length {len(sanitized_path)} exceeds limit {MAX_PATH_LENGTH}")
            sanitized_path = self._truncate_path(sanitized_path)
        
        was_modified = original_path != sanitized_path
        if was_modified:
            self.sanitization_stats['modified_count'] += 1
        
        if security_issues:
            self.sanitization_stats['security_issues_found'] += 1
            logger.warning(f"Security issues in path '{original_path}': {security_issues}")
        
        return SanitizationResult(
            original_name=original_path,
            sanitized_name=sanitized_path,
            was_modified=was_modified,
            security_issues=security_issues,
            warnings=warnings
        )
    
    def _sanitize_filename_component(self, filename: str) -> SanitizationResult:
        """
        Sanitize a single filename component.
        
        Args:
            filename: Original filename
            
        Returns:
            SanitizationResult with sanitized filename and security info
        """
        original_filename = filename
        security_issues = []
        warnings = []
        
        for char in CONTROL_CHARS | UNIX_FORBIDDEN_CHARS:
            if char in filename:
                security_issues.append(f"Control/null character detected: {repr(char)}")
                filename = filename.replace(char, '')
        
        for char in WINDOWS_FORBIDDEN_CHARS:
            if char in filename:
                warnings.append(f"Windows forbidden character replaced: '{char}'")
                filename = filename.replace(char, '_')
        
        original_stripped = filename
        filename = filename.strip('. ')
        if filename != original_stripped:
            warnings.append("Removed leading/trailing dots or spaces")
        
        name_without_ext = Path(filename).stem.upper()
        if name_without_ext in WINDOWS_RESERVED_NAMES:
            security_issues.append(f"Windows reserved name detected: {name_without_ext}")
            filename = f"safe_{filename}"
        
        if len(filename) > self.max_filename_length:
            warnings.append(f"Filename length {len(filename)} exceeds limit {self.max_filename_length}")
            filename = self._truncate_filename(filename)
        
        if not filename or filename in ('.', '..'):
            warnings.append("Empty or invalid filename, using fallback")
            filename = 'sanitized_file'
        
        was_modified = original_filename != filename
        
        return SanitizationResult(
            original_name=original_filename,
            sanitized_name=filename,
            was_modified=was_modified,
            security_issues=security_issues,
            warnings=warnings
        )
    
    def _truncate_filename(self, filename: str) -> str:
        """
        Truncate filename while preserving extension.
        
        Args:
            filename: Filename to truncate
            
        Returns:
            Truncated filename
        """
        if len(filename) <= self.max_filename_length:
            return filename
        
        path_obj = Path(filename)
        extension = path_obj.suffix
        name_part = path_obj.stem
        
        max_name_length = self.max_filename_length - len(extension)
        
        if max_name_length <= 0:
            return filename[:self.max_filename_length]
        
        truncated_name = name_part[:max_name_length]
        return truncated_name + extension
    
    def _truncate_path(self, path: str) -> str:
        """
        Truncate path while preserving structure.
        
        Args:
            path: Path to truncate
            
        Returns:
            Truncated path
        """
        if len(path) <= MAX_PATH_LENGTH:
            return path
        
        excess = len(path) - MAX_PATH_LENGTH + 10  # +10 for "..." marker
        middle = len(path) // 2
        start = middle - excess // 2
        end = middle + excess // 2
        
        return path[:start] + "..." + path[end:]
    
    def get_stats(self) -> dict:
        """Get sanitization statistics."""
        return self.sanitization_stats.copy()
    
    def reset_stats(self):
        """Reset sanitization statistics."""
        self.sanitization_stats = {
            'total_processed': 0,
            'modified_count': 0,
            'security_issues_found': 0
        }


def validate_extraction_path(member_path: str, base_extraction_dir: Path) -> bool:
    """
    Validate that an extraction path is safe relative to base directory.
    
    Args:
        member_path: Path from archive member
        base_extraction_dir: Base directory for extraction
        
    Returns:
        True if path is safe, False otherwise
    """
    try:
        full_path = (base_extraction_dir / member_path).resolve()
        base_resolved = base_extraction_dir.resolve()
        
        try:
            full_path.relative_to(base_resolved)
            return True
        except ValueError:
            logger.warning(f"Path outside base directory: {member_path}")
            return False
            
    except Exception as e:
        logger.error(f"Error validating extraction path '{member_path}': {e}")
        return False


def sanitize_filename(filename: str, max_length: int = MAX_FILENAME_LENGTH) -> str:
    """
    Simple filename sanitization function.
    
    Args:
        filename: Original filename
        max_length: Maximum allowed length
        
    Returns:
        Sanitized filename
    """
    sanitizer = FilenameSanitizer(max_length)
    result = sanitizer._sanitize_filename_component(filename)
    return result.sanitized_name
```

## File: utils/mediapipe_asset_helper.py
```python
import os
import shutil
import time
import uuid
import mediapipe as mp
import sys

def _get_mediapipe_assets_dir():
    """
    Finds the 'assets' directory within the installed MediaPipe package.
    This is the most reliable way to locate it.
    """
    try:
        # The __file__ attribute of the mediapipe package gives us the path to its __init__.py
        mp_package_path = os.path.dirname(mp.__file__)
        assets_path = os.path.join(mp_package_path, 'tasks', 'python', 'vision', 'pybind', 'assets')
        
        # A fallback path seen in some installations
        if not os.path.exists(assets_path):
            assets_path = os.path.join(mp_package_path, 'tasks', 'python', 'assets')
            
        if os.path.exists(assets_path):
            return assets_path
        else:
            # If all else fails, create a local assets dir as a last resort
            local_assets = os.path.join(os.getcwd(), 'mediapipe_assets')
            os.makedirs(local_assets, exist_ok=True)
            print(f"WARNING: MediaPipe assets directory not found. Using local fallback: {local_assets}", file=sys.stderr)
            return local_assets
            
    except Exception as e:
        print(f"ERROR: Could not determine MediaPipe assets path: {e}", file=sys.stderr)
        return None

def create_mediapipe_asset_copy(original_model_path):
    """
    Copies a model file to the MediaPipe assets directory to ensure it can be loaded.
    MediaPipe tasks often require models to be in this specific 'assets' folder.
    
    Args:
        original_model_path (str): The absolute path to the model file.

    Returns:
        dict: A dictionary with path information, or None on failure.
              {'asset_path': '...', 'asset_name': '...'}
    """
    assets_dir = _get_mediapipe_assets_dir()
    if not assets_dir:
        print("ERROR: Cannot create model copy because MediaPipe assets directory was not found.", file=sys.stderr)
        return None

    if not os.path.exists(original_model_path):
        print(f"ERROR: Original model file does not exist: {original_model_path}", file=sys.stderr)
        return None
        
    try:
        original_filename = os.path.basename(original_model_path)
        # Create a unique name to avoid conflicts if multiple processes run
        unique_suffix = uuid.uuid4().hex[:8]
        temp_filename = f"temp_{unique_suffix}_{original_filename}"
        
        destination_path = os.path.join(assets_dir, temp_filename)
        
        # Copy the file
        shutil.copy2(original_model_path, destination_path)
        
        print(f"INFO: Model '{original_filename}' copied to MediaPipe assets as '{temp_filename}' for processing.", file=sys.stderr)
        
        return {
            "asset_path": os.path.abspath(destination_path),
            "asset_name": temp_filename
        }
    except Exception as e:
        print(f"ERROR: Failed to copy model to MediaPipe assets directory: {e}", file=sys.stderr)
        return None

def cleanup_mediapipe_temp_assets(max_age_hours=1):
    """
    Deletes temporary model files from the MediaPipe assets directory that are older
    than a specified age. This prevents the folder from filling up with old temp files.

    Args:
        max_age_hours (int): The maximum age of a file in hours to be kept.
    
    Returns:
        tuple: (number_of_files_deleted, total_space_freed_in_bytes)
    """
    assets_dir = _get_mediapipe_assets_dir()
    if not assets_dir:
        print("WARNING: Cannot clean up assets, directory not found.", file=sys.stderr)
        return 0, 0
        
    now = time.time()
    max_age_seconds = max_age_hours * 3600
    deleted_count = 0
    freed_space = 0
    
    try:
        for filename in os.listdir(assets_dir):
            if filename.startswith("temp_"):
                file_path = os.path.join(assets_dir, filename)
                try:
                    if os.path.isfile(file_path):
                        file_age = now - os.path.getmtime(file_path)
                        if file_age > max_age_seconds:
                            file_size = os.path.getsize(file_path)
                            os.remove(file_path)
                            deleted_count += 1
                            freed_space += file_size
                            print(f"CLEANUP: Removed old temp asset: {filename}", file=sys.stderr)
                except Exception as e_inner:
                    # This can happen if another process deletes the file in the meantime
                    print(f"WARNING: Could not process asset '{filename}' during cleanup: {e_inner}", file=sys.stderr)

    except Exception as e:
        print(f"ERROR: An error occurred during asset cleanup: {e}", file=sys.stderr)
        
    return deleted_count, freed_space
```

## File: utils/performance.py
```python
"""
Performance Utilities
Comprehensive performance optimization utilities for backend operations.
"""

import time
import logging
import threading
from functools import lru_cache, wraps
from typing import Dict, Any, Optional, Callable, List
from contextlib import contextmanager
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path

logger = logging.getLogger(__name__)

# Performance tracking
performance_stats = {
    "function_calls": {},
    "cache_hits": 0,
    "cache_misses": 0,
    "parallel_operations": 0
}
stats_lock = threading.Lock()


def track_performance(func_name: str = None):
    """
    Decorator to track function performance.
    
    Args:
        func_name: Optional custom name for the function
        
    Returns:
        Decorator function
    """
    def decorator(func: Callable) -> Callable:
        name = func_name or f"{func.__module__}.{func.__name__}"
        
        @wraps(func)
        def wrapper(*args, **kwargs):
            start_time = time.perf_counter()
            
            try:
                result = func(*args, **kwargs)
                success = True
                error = None
            except Exception as e:
                success = False
                error = str(e)
                raise
            finally:
                duration = (time.perf_counter() - start_time) * 1000  # Convert to ms
                
                with stats_lock:
                    if name not in performance_stats["function_calls"]:
                        performance_stats["function_calls"][name] = {
                            "total_time": 0,
                            "calls": 0,
                            "errors": 0,
                            "avg_time": 0
                        }
                    
                    stats = performance_stats["function_calls"][name]
                    stats["total_time"] += duration
                    stats["calls"] += 1
                    if not success:
                        stats["errors"] += 1
                    stats["avg_time"] = stats["total_time"] / stats["calls"]
                
                # Log slow operations
                if duration > 1000:  # More than 1 second
                    logger.warning(f"Slow operation: {name} took {duration:.2f}ms")
            
            return result
        
        return wrapper
    return decorator


@contextmanager
def profile_section(section_name: str):
    """
    Context manager for profiling code sections.
    
    Args:
        section_name: Name of the section being profiled
        
    Usage:
        with profile_section("video_processing"):
            # Code to profile
            process_video()
    """
    start_time = time.perf_counter()
    try:
        yield
    finally:
        elapsed_time = (time.perf_counter() - start_time) * 1000  # Convert to ms
        
        with stats_lock:
            if section_name not in performance_stats["function_calls"]:
                performance_stats["function_calls"][section_name] = {
                    "total_time": 0,
                    "calls": 0,
                    "errors": 0,
                    "avg_time": 0
                }
            
            stats = performance_stats["function_calls"][section_name]
            stats["total_time"] += elapsed_time
            stats["calls"] += 1
            stats["avg_time"] = stats["total_time"] / stats["calls"]


@lru_cache(maxsize=128)
def get_video_metadata_cached(video_path: str) -> Dict[str, Any]:
    """
    Get video metadata with caching to avoid repeated file reads.
    
    Args:
        video_path: Path to video file
        
    Returns:
        Video metadata dictionary
    """
    try:
        import cv2
        
        with stats_lock:
            performance_stats["cache_misses"] += 1
        
        cap = cv2.VideoCapture(video_path)
        try:
            if not cap.isOpened():
                raise ValueError(f"Cannot open video: {video_path}")
            
            metadata = {
                'frame_count': int(cap.get(cv2.CAP_PROP_FRAME_COUNT)),
                'fps': cap.get(cv2.CAP_PROP_FPS),
                'width': int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),
                'height': int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)),
                'duration_seconds': None
            }
            
            # Calculate duration
            if metadata['fps'] > 0 and metadata['frame_count'] > 0:
                metadata['duration_seconds'] = metadata['frame_count'] / metadata['fps']
            
            return metadata
        finally:
            cap.release()
            
    except Exception as e:
        logger.error(f"Video metadata error for {video_path}: {e}")
        return {
            'frame_count': 0,
            'fps': 0,
            'width': 0,
            'height': 0,
            'duration_seconds': None,
            'error': str(e)
        }


def cached_with_stats(maxsize: int = 128, typed: bool = False):
    """
    LRU cache decorator with statistics tracking.
    
    Args:
        maxsize: Maximum cache size
        typed: Whether to consider argument types
        
    Returns:
        Decorator function
    """
    def decorator(func: Callable) -> Callable:
        cached_func = lru_cache(maxsize=maxsize, typed=typed)(func)
        
        @wraps(func)
        def wrapper(*args, **kwargs):
            # Check if result is in cache
            cache_info = cached_func.cache_info()
            initial_hits = cache_info.hits
            
            result = cached_func(*args, **kwargs)
            
            # Update statistics
            new_cache_info = cached_func.cache_info()
            if new_cache_info.hits > initial_hits:
                with stats_lock:
                    performance_stats["cache_hits"] += 1
            else:
                with stats_lock:
                    performance_stats["cache_misses"] += 1
            
            return result
        
        # Expose cache methods
        wrapper.cache_info = cached_func.cache_info
        wrapper.cache_clear = cached_func.cache_clear
        
        return wrapper
    
    return decorator


@track_performance("parallel_video_processing")
def process_videos_parallel(video_list: List[Path], 
                          process_func: Callable,
                          max_workers: int = 4,
                          **kwargs) -> List[bool]:
    """
    Process multiple videos with controlled parallelism.
    
    Args:
        video_list: List of video file paths
        process_func: Function to process each video
        max_workers: Maximum number of parallel workers
        **kwargs: Additional arguments for process_func
        
    Returns:
        List of success/failure results
    """
    if not video_list:
        return []
    
    results = []
    
    with stats_lock:
        performance_stats["parallel_operations"] += 1
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Submit all tasks
        future_to_video = {
            executor.submit(process_func, video, **kwargs): video
            for video in video_list
        }
        
        # Collect results as they complete
        for future in as_completed(future_to_video):
            video = future_to_video[future]
            try:
                result = future.result()
                results.append(result)
                logger.info(f"Completed processing: {video.name}")
            except Exception as e:
                logger.error(f"Failed processing {video.name}: {e}")
                results.append(False)
    
    return results


@track_performance("batch_file_operations")
def batch_file_operations(file_paths: List[Path], 
                         operation: Callable,
                         batch_size: int = 10,
                         **kwargs) -> List[Any]:
    """
    Process files in batches for memory efficiency.
    
    Args:
        file_paths: List of file paths to process
        operation: Function to apply to each file
        batch_size: Number of files to process in each batch
        **kwargs: Additional arguments for operation
        
    Returns:
        List of operation results
    """
    results = []
    
    for i in range(0, len(file_paths), batch_size):
        batch = file_paths[i:i + batch_size]
        
        with profile_section(f"batch_{i//batch_size + 1}"):
            batch_results = []
            for file_path in batch:
                try:
                    result = operation(file_path, **kwargs)
                    batch_results.append(result)
                except Exception as e:
                    logger.error(f"Batch operation failed for {file_path}: {e}")
                    batch_results.append(None)
            
            results.extend(batch_results)
    
    return results


class PerformanceTimer:
    """
    Context manager for timing operations with automatic logging.
    """
    
    def __init__(self, operation_name: str, log_threshold_ms: float = 100.0):
        """
        Initialize performance timer.
        
        Args:
            operation_name: Name of the operation being timed
            log_threshold_ms: Log warning if operation takes longer than this
        """
        self.operation_name = operation_name
        self.log_threshold_ms = log_threshold_ms
        self.start_time = None
        self.end_time = None
    
    def __enter__(self):
        self.start_time = time.perf_counter()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.end_time = time.perf_counter()
        duration_ms = (self.end_time - self.start_time) * 1000
        
        if duration_ms > self.log_threshold_ms:
            logger.warning(f"Slow operation: {self.operation_name} took {duration_ms:.2f}ms")
        else:
            logger.debug(f"Operation: {self.operation_name} took {duration_ms:.2f}ms")
    
    @property
    def duration_ms(self) -> Optional[float]:
        """Get operation duration in milliseconds."""
        if self.start_time and self.end_time:
            return (self.end_time - self.start_time) * 1000
        return None


def optimize_memory_usage():
    """
    Perform memory optimization operations.
    """
    import gc
    
    # Clear function caches
    get_video_metadata_cached.cache_clear()
    
    # Force garbage collection
    collected = gc.collect()
    
    logger.info(f"Memory optimization: collected {collected} objects")


def get_performance_stats() -> Dict[str, Any]:
    """
    Get comprehensive performance statistics.
    
    Returns:
        Performance statistics dictionary
    """
    with stats_lock:
        stats = {
            "function_calls": dict(performance_stats["function_calls"]),
            "cache_stats": {
                "hits": performance_stats["cache_hits"],
                "misses": performance_stats["cache_misses"],
                "hit_rate": (
                    performance_stats["cache_hits"] / 
                    (performance_stats["cache_hits"] + performance_stats["cache_misses"])
                    if (performance_stats["cache_hits"] + performance_stats["cache_misses"]) > 0
                    else 0
                )
            },
            "parallel_operations": performance_stats["parallel_operations"]
        }
    
    # Add cache info for cached functions
    try:
        stats["video_metadata_cache"] = {
            "info": get_video_metadata_cached.cache_info()._asdict()
        }
    except AttributeError:
        pass
    
    return stats


def reset_performance_stats():
    """Reset all performance statistics."""
    global performance_stats
    
    with stats_lock:
        performance_stats = {
            "function_calls": {},
            "cache_hits": 0,
            "cache_misses": 0,
            "parallel_operations": 0
        }
    
    # Clear function caches
    get_video_metadata_cached.cache_clear()
    
    logger.info("Performance statistics reset")


def print_performance_summary(logger_fn: Callable = logger.info):
    """
    Print a formatted summary of performance statistics.
    
    Args:
        logger_fn: Function to use for logging output
    """
    stats = get_performance_stats()
    
    logger_fn("\n--- PERFORMANCE SUMMARY ---")
    
    # Function call statistics
    if stats["function_calls"]:
        logger_fn("Function Call Statistics:")
        sorted_functions = sorted(
            stats["function_calls"].items(),
            key=lambda x: x[1]["total_time"],
            reverse=True
        )
        
        for func_name, func_stats in sorted_functions[:10]:  # Top 10
            logger_fn(
                f"  {func_name}: {func_stats['calls']} calls, "
                f"{func_stats['total_time']:.2f}ms total, "
                f"{func_stats['avg_time']:.2f}ms avg"
            )
    
    # Cache statistics
    cache_stats = stats["cache_stats"]
    logger_fn(f"Cache Statistics: {cache_stats['hit_rate']:.1%} hit rate "
             f"({cache_stats['hits']} hits, {cache_stats['misses']} misses)")
    
    # Parallel operations
    logger_fn(f"Parallel Operations: {stats['parallel_operations']}")
    
    logger_fn("--- END PERFORMANCE SUMMARY ---\n")


# Utility functions for common performance patterns

def memoize_with_ttl(ttl_seconds: int = 300):
    """
    Memoization decorator with time-to-live.
    
    Args:
        ttl_seconds: Time to live for cached results
        
    Returns:
        Decorator function
    """
    def decorator(func: Callable) -> Callable:
        cache = {}
        
        @wraps(func)
        def wrapper(*args, **kwargs):
            key = str(args) + str(sorted(kwargs.items()))
            current_time = time.time()
            
            # Check if result is cached and not expired
            if key in cache:
                result, timestamp = cache[key]
                if current_time - timestamp < ttl_seconds:
                    with stats_lock:
                        performance_stats["cache_hits"] += 1
                    return result
                else:
                    del cache[key]
            
            # Compute and cache result
            result = func(*args, **kwargs)
            cache[key] = (result, current_time)
            
            with stats_lock:
                performance_stats["cache_misses"] += 1
            
            return result
        
        wrapper.cache_clear = lambda: cache.clear()
        wrapper.cache_info = lambda: {"size": len(cache)}
        
        return wrapper
    
    return decorator
```

## File: utils/resource_manager.py
```python
"""
Resource management utilities for workflow_mediapipe.

This module provides context managers and utilities for proper resource cleanup,
preventing memory leaks and ensuring resources are properly released.
"""

import logging
import cv2
import tempfile
from pathlib import Path
from typing import Optional, List, Any, Union
from contextlib import contextmanager

logger = logging.getLogger(__name__)


class VideoResourceManager:
    """
    Context manager for safe video capture resource management.
    
    Ensures that cv2.VideoCapture objects are properly released
    even if exceptions occur during processing.
    """
    
    def __init__(self, video_path: Union[str, Path]):
        """
        Initialize video resource manager.
        
        Args:
            video_path: Path to the video file to open
        """
        self.video_path = Path(video_path)
        self.capture: Optional[cv2.VideoCapture] = None
        self.is_opened = False
        
    def __enter__(self) -> cv2.VideoCapture:
        """
        Enter the context manager and open video capture.
        
        Returns:
            cv2.VideoCapture: Opened video capture object
            
        Raises:
            ValueError: If video file cannot be opened
            FileNotFoundError: If video file doesn't exist
        """
        if not self.video_path.exists():
            raise FileNotFoundError(f"Video file not found: {self.video_path}")
        
        try:
            self.capture = cv2.VideoCapture(str(self.video_path))
            
            if not self.capture.isOpened():
                raise ValueError(f"Cannot open video file: {self.video_path}")
            
            self.is_opened = True
            logger.debug(f"Video capture opened successfully: {self.video_path.name}")
            
            return self.capture
            
        except Exception as e:
            # Ensure cleanup if opening fails
            if self.capture:
                self.capture.release()
            logger.error(f"Failed to open video capture for {self.video_path}: {e}")
            raise
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """
        Exit the context manager and release video capture.
        
        Args:
            exc_type: Exception type if an exception occurred
            exc_val: Exception value if an exception occurred
            exc_tb: Exception traceback if an exception occurred
        """
        if self.capture and self.is_opened:
            try:
                self.capture.release()
                logger.debug(f"Video capture released: {self.video_path.name}")
            except Exception as e:
                logger.error(f"Error releasing video capture for {self.video_path}: {e}")
            finally:
                self.capture = None
                self.is_opened = False
        
        # Log any exceptions that occurred during processing
        if exc_type:
            logger.error(f"Exception in video processing for {self.video_path}: {exc_val}")
        
        # Don't suppress exceptions
        return False


class TempFileManager:
    """
    Context manager for temporary file management.
    
    Ensures temporary files are cleaned up even if exceptions occur.
    """
    
    def __init__(self, suffix: str = "", prefix: str = "workflow_", dir: Optional[Path] = None):
        """
        Initialize temporary file manager.
        
        Args:
            suffix: File suffix/extension
            prefix: File prefix
            dir: Directory to create temp file in (defaults to system temp)
        """
        self.suffix = suffix
        self.prefix = prefix
        self.dir = dir
        self.temp_files: List[Path] = []
        
    def create_temp_file(self, suffix: Optional[str] = None, prefix: Optional[str] = None) -> Path:
        """
        Create a temporary file and track it for cleanup.
        
        Args:
            suffix: Override default suffix
            prefix: Override default prefix
            
        Returns:
            Path: Path to the created temporary file
        """
        actual_suffix = suffix or self.suffix
        actual_prefix = prefix or self.prefix
        
        # Create temporary file
        fd, temp_path = tempfile.mkstemp(
            suffix=actual_suffix,
            prefix=actual_prefix,
            dir=self.dir
        )
        
        # Close the file descriptor (we just need the path)
        import os
        os.close(fd)
        
        temp_path = Path(temp_path)
        self.temp_files.append(temp_path)
        
        logger.debug(f"Created temporary file: {temp_path}")
        return temp_path
    
    def __enter__(self):
        """Enter the context manager."""
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Exit the context manager and cleanup temporary files."""
        self.cleanup()
        
        # Don't suppress exceptions
        return False
    
    def cleanup(self):
        """Clean up all tracked temporary files."""
        for temp_file in self.temp_files:
            try:
                if temp_file.exists():
                    temp_file.unlink()
                    logger.debug(f"Cleaned up temporary file: {temp_file}")
            except Exception as e:
                logger.error(f"Failed to cleanup temporary file {temp_file}: {e}")
        
        self.temp_files.clear()


@contextmanager
def safe_video_processing(video_path: Union[str, Path], cleanup_temp_files: bool = True):
    """
    Context manager for safe video processing with automatic resource cleanup.
    
    Args:
        video_path: Path to video file
        cleanup_temp_files: Whether to automatically cleanup temporary files
        
    Yields:
        tuple: (video_capture, temp_file_manager)
        
    Example:
        with safe_video_processing("video.mp4") as (cap, temp_mgr):
            # Process video frames
            while cap.isOpened():
                ret, frame = cap.read()
                if not ret:
                    break
                
                # Create temporary files if needed
                temp_output = temp_mgr.create_temp_file(suffix=".json")
                
                # Process frame...
    """
    video_manager = VideoResourceManager(video_path)
    temp_manager = TempFileManager() if cleanup_temp_files else None
    
    try:
        with video_manager as capture:
            if temp_manager:
                with temp_manager:
                    yield capture, temp_manager
            else:
                yield capture, None
                
    except Exception as e:
        logger.error(f"Error in safe video processing for {video_path}: {e}")
        raise


def get_video_metadata(video_path: Union[str, Path]) -> dict:
    """
    Safely get video metadata with proper resource cleanup.
    
    Args:
        video_path: Path to video file
        
    Returns:
        dict: Video metadata including frame count, fps, width, height
        
    Raises:
        ValueError: If video cannot be opened
        FileNotFoundError: If video file doesn't exist
    """
    with VideoResourceManager(video_path) as capture:
        metadata = {
            'frame_count': int(capture.get(cv2.CAP_PROP_FRAME_COUNT)),
            'fps': capture.get(cv2.CAP_PROP_FPS),
            'width': int(capture.get(cv2.CAP_PROP_FRAME_WIDTH)),
            'height': int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT)),
            'duration_seconds': None
        }
        
        # Calculate duration if fps is available
        if metadata['fps'] > 0:
            metadata['duration_seconds'] = metadata['frame_count'] / metadata['fps']
        
        logger.debug(f"Retrieved metadata for {Path(video_path).name}: {metadata}")
        return metadata


class ResourceTracker:
    """
    Track and monitor resource usage during processing.
    
    Useful for debugging resource leaks and monitoring performance.
    """
    
    def __init__(self):
        """Initialize resource tracker."""
        self.active_resources = {}
        self.resource_counter = 0
    
    def register_resource(self, resource: Any, resource_type: str, description: str = "") -> str:
        """
        Register a resource for tracking.
        
        Args:
            resource: The resource object to track
            resource_type: Type of resource (e.g., 'video_capture', 'file_handle')
            description: Optional description
            
        Returns:
            str: Resource ID for later reference
        """
        resource_id = f"{resource_type}_{self.resource_counter}"
        self.resource_counter += 1
        
        self.active_resources[resource_id] = {
            'resource': resource,
            'type': resource_type,
            'description': description,
            'created_at': logger.handlers[0].formatter.formatTime(logger.makeRecord(
                '', 0, '', 0, '', (), None
            )) if logger.handlers else 'unknown'
        }
        
        logger.debug(f"Registered resource {resource_id}: {resource_type} - {description}")
        return resource_id
    
    def unregister_resource(self, resource_id: str):
        """
        Unregister a resource.
        
        Args:
            resource_id: ID of resource to unregister
        """
        if resource_id in self.active_resources:
            resource_info = self.active_resources.pop(resource_id)
            logger.debug(f"Unregistered resource {resource_id}: {resource_info['type']}")
        else:
            logger.warning(f"Attempted to unregister unknown resource: {resource_id}")
    
    def get_active_resources(self) -> dict:
        """
        Get information about currently active resources.
        
        Returns:
            dict: Information about active resources
        """
        return {
            'count': len(self.active_resources),
            'resources': {rid: {k: v for k, v in info.items() if k != 'resource'} 
                         for rid, info in self.active_resources.items()}
        }
    
    def cleanup_all(self):
        """
        Attempt to cleanup all tracked resources.
        
        This is a last-resort cleanup method.
        """
        logger.warning(f"Emergency cleanup of {len(self.active_resources)} resources")
        
        for resource_id, resource_info in list(self.active_resources.items()):
            try:
                resource = resource_info['resource']
                resource_type = resource_info['type']
                
                # Attempt cleanup based on resource type
                if resource_type == 'video_capture' and hasattr(resource, 'release'):
                    resource.release()
                elif resource_type == 'file_handle' and hasattr(resource, 'close'):
                    resource.close()
                elif hasattr(resource, '__exit__'):
                    resource.__exit__(None, None, None)
                
                self.unregister_resource(resource_id)
                
            except Exception as e:
                logger.error(f"Failed to cleanup resource {resource_id}: {e}")


# Global resource tracker instance
resource_tracker = ResourceTracker()
```

## File: utils/simple_profiling.py
```python
import time
from collections import defaultdict
from contextlib import contextmanager

# Dictionnaire global pour stocker les statistiques de profilage
PROFILING_STATS = defaultdict(lambda: {"total_time": 0, "calls": 0})


@contextmanager
def profile_section(section_name):
    """Un context manager pour profiler une section de code."""
    start_time = time.perf_counter()
    try:
        yield
    finally:
        elapsed_time = (time.perf_counter() - start_time) * 1000  # en ms
        PROFILING_STATS[section_name]["total_time"] += elapsed_time
        PROFILING_STATS[section_name]["calls"] += 1


def reset_profiling_stats():
    """Réinitialise les statistiques de profilage."""
    global PROFILING_STATS
    PROFILING_STATS = defaultdict(lambda: {"total_time": 0, "calls": 0})


# Créer un alias pour maintenir la compatibilité avec le worker
reset_profiling = reset_profiling_stats


def get_profiling_stats_dict():
    """Retourne une copie des statistiques actuelles."""
    return {k: dict(v) for k, v in PROFILING_STATS.items()}


def print_profiling_stats(logger_fn=print):
    """Affiche un résumé formaté des statistiques de profilage."""
    if not PROFILING_STATS:
        logger_fn("Aucune donnée de profilage collectée.")
        return

    logger_fn("\n--- PROFILING RESULTS (Detailed) ---")

    # Calcul du temps total pour le pourcentage
    total_time_all_sections = sum(
        stats["total_time"] for stats in PROFILING_STATS.values()
    )
    if total_time_all_sections == 0:
        logger_fn(
            "Temps total de profilage est zéro. Impossible de calculer les pourcentages."
        )
        total_time_all_sections = 1  # Pour éviter la division par zéro

    # Tri par temps total décroissant
    sorted_stats = sorted(
        PROFILING_STATS.items(), key=lambda item: item[1]["total_time"], reverse=True
    )

    logger_fn(
        f"{'Section':<30} | {'Total Time (ms)':>15} | {'Calls':>10} | {'Avg Time (ms)':>15} | {'% of Total':>10}"
    )
    logger_fn("-" * 85)

    for name, stats in sorted_stats:
        total_ms = stats["total_time"]
        calls = stats["calls"]
        avg_ms = total_ms / calls if calls > 0 else 0
        percentage = (total_ms / total_time_all_sections) * 100

        logger_fn(
            f"{name:<30} | {total_ms:>15.2f} | {calls:>10} | {avg_ms:>15.4f} | {percentage:>9.2f}%"
        )
    logger_fn("-" * 85)
```

## File: utils/tracking_optimizations.py
```python
# Fichier : tracking_optimizations.py

import os
import numpy as np
from scipy.spatial import KDTree
import sys
from typing import Dict, List, Tuple, Any, Optional


def _filter_blendshapes_for_export(blendshapes: Any) -> Any:
    profile = os.environ.get("STEP5_BLENDSHAPES_PROFILE", "full").strip().lower()
    if not blendshapes or not isinstance(blendshapes, dict):
        return blendshapes

    if profile in {"", "full", "all"}:
        return blendshapes

    if profile in {"none", "off", "0", "false", "no"}:
        return None

    if profile == "mouth":
        filtered = {
            k: v
            for k, v in blendshapes.items()
            if k.startswith("mouth") or k.startswith("jaw")
        }
        include_tongue = os.environ.get("STEP5_BLENDSHAPES_INCLUDE_TONGUE", "0").strip().lower() in {
            "1",
            "true",
            "yes",
        }
        if include_tongue and "tongueOut" in blendshapes:
            filtered["tongueOut"] = blendshapes["tongueOut"]
        return filtered or None

    if profile == "mediapipe":
        filtered = dict(blendshapes)
        filtered.pop("tongueOut", None)
        filtered.setdefault("_neutral", 0.0)
        return filtered or None

    if profile == "custom":
        keys_raw = os.environ.get("STEP5_BLENDSHAPES_EXPORT_KEYS", "").strip()
        if not keys_raw:
            return blendshapes
        keys = [k.strip() for k in keys_raw.split(",") if k.strip()]
        filtered = {k: blendshapes[k] for k in keys if k in blendshapes}
        return filtered or None

    return blendshapes


def get_next_id(counter_ref, prefix="obj_"):
    """Incrémente et retourne un nouvel ID."""
    counter_ref["value"] += 1
    return f"{prefix}{counter_ref['value']}"


def apply_tracking_and_management(
    active_objects,
    current_detections,
    next_id_counter,
    distance_threshold,
    frames_unseen_to_deregister,
    speaking_detection_jaw_open_threshold=0.08,
    enhanced_speaking_detector=None,
    current_frame_num=None,
):
    """
    Gère le cycle de vie complet du tracking pour une frame en utilisant KDTree.
    Version optimisée et centralisée pour toute l'application.
    """
    # --- Phase 1: Incrémentation des 'frames_unseen' pour tous les objets actifs ---
    for obj_id in active_objects:
        active_objects[obj_id]["frames_unseen"] += 1

    matched_detection_indices = set()

    # --- Phase 2: Appariement si des objets actifs ET des détections existent ---
    if active_objects and current_detections:
        active_obj_ids = list(active_objects.keys())
        tracked_centroids = np.array([obj["centroid"] for obj in active_objects.values()])
        detection_centroids = np.array([det["centroid"] for det in current_detections])

        kdtree = KDTree(tracked_centroids)
        distances, indices = kdtree.query(detection_centroids, k=1)

        potential_matches = sorted(zip(distances, indices, range(len(current_detections))))
        
        matched_tracked_indices = set()

        for dist, tracked_idx, det_idx in potential_matches:
            if (
                dist > distance_threshold
                or det_idx in matched_detection_indices
                or tracked_idx in matched_tracked_indices
            ):
                continue

            obj_id = active_obj_ids[tracked_idx]
            matched_det = current_detections[det_idx]

            active_objects[obj_id].update({**matched_det, "frames_unseen": 0})
            
            matched_detection_indices.add(det_idx)
            matched_tracked_indices.add(tracked_idx)

    # --- Phase 3: Enregistrement des nouvelles détections non appariées ---
    # Ce bloc est maintenant exécuté que active_objects soit vide ou non.
    if current_detections:
        for i, det_info in enumerate(current_detections):
            if i not in matched_detection_indices:
                new_id = get_next_id(next_id_counter)
                active_objects[new_id] = {
                    "id": new_id,
                    **det_info,
                    "frames_unseen": 0,
                }

    # --- Phase 4: Préparation de la sortie et Nettoyage ---
    output_objects_for_json = []
    ids_to_remove = []

    for obj_id, obj_data in active_objects.items():
        if obj_data["frames_unseen"] > frames_unseen_to_deregister:
            ids_to_remove.append(obj_id)
            continue

        if obj_data["frames_unseen"] == 0:
            confidence_val = obj_data.get("confidence")
            output_obj = {
                "id": obj_id,
                "centroid_x": obj_data["centroid"][0],
                "centroid_y": obj_data["centroid"][1],
                "bbox_xmin": obj_data["bbox"][0],
                "bbox_xmax": obj_data["bbox"][0] + obj_data["bbox"][2],
                "source": obj_data.get("source_detector", "unknown"),
                "label": obj_data.get("label", ""),
                "confidence": (
                    round(float(confidence_val), 3) if confidence_val is not None else 0.0
                ),
                "blendshapes": _filter_blendshapes_for_export(obj_data.get("blendshapes")),
            }

            # Always include bbox size for face_landmarker before speaking detection branching
            try:
                bbox_tuple = obj_data.get("bbox")
                if bbox_tuple and len(bbox_tuple) >= 4:
                    output_obj["bbox_ymin"] = int(bbox_tuple[1])
                    output_obj["bbox_ymax"] = int(bbox_tuple[1] + bbox_tuple[3])
                    output_obj["bbox_width"] = int(bbox_tuple[2])
                    output_obj["bbox_height"] = int(bbox_tuple[3])
            except Exception:
                # Non-blocking if bbox is missing or malformed
                pass

            # Enhanced speaking detection
            if enhanced_speaking_detector and current_frame_num:
                # Use enhanced multi-source speaking detection
                detection_result = enhanced_speaking_detector.detect_speaking(
                    frame_num=current_frame_num,
                    blendshapes=output_obj.get("blendshapes"),
                    source_detector=output_obj["source"]
                )

                output_obj["is_speaking"] = detection_result.is_speaking
                output_obj["speaking_confidence"] = detection_result.confidence
                output_obj["speaking_method"] = detection_result.method
                output_obj["speaking_sources"] = detection_result.sources

            elif output_obj["source"] == "face_landmarker":
                # Fallback to original jaw-based detection
                if output_obj["blendshapes"] and "jawOpen" in output_obj["blendshapes"]:
                    jaw_open_score = output_obj["blendshapes"].get("jawOpen", 0.0)
                    output_obj["is_speaking"] = (
                        jaw_open_score > speaking_detection_jaw_open_threshold
                    )
                    output_obj["speaking_confidence"] = min(jaw_open_score / speaking_detection_jaw_open_threshold, 1.0)
                    output_obj["speaking_method"] = "jaw_threshold_fallback"
                else:
                    # S'assurer que la clé existe pour les visages, même sans blendshapes
                    output_obj["is_speaking"] = False
                    output_obj["speaking_confidence"] = 0.0
                    output_obj["speaking_method"] = "no_blendshapes"
            else:
                # For object detection, try enhanced detection or set defaults
                if enhanced_speaking_detector and current_frame_num:
                    detection_result = enhanced_speaking_detector.detect_speaking(
                        frame_num=current_frame_num,
                        blendshapes=None,
                        source_detector=output_obj["source"]
                    )
                    output_obj["is_speaking"] = detection_result.is_speaking
                    output_obj["speaking_confidence"] = detection_result.confidence
                    output_obj["speaking_method"] = detection_result.method
                    output_obj["speaking_sources"] = detection_result.sources
                else:
                    # Remove speaking-related fields for objects without enhanced detection
                    output_obj.pop("blendshapes", None)
                    output_obj.pop("is_speaking", None)
                    output_obj.pop("speaking_confidence", None)
                    output_obj.pop("speaking_method", None)
                    output_obj.pop("speaking_sources", None)

            # Optional exports for face engines (not present for object detector)
            # Controlled by STEP5_EXPORT_VERBOSE_FIELDS to reduce JSON size
            try:
                if output_obj.get("source") == "face_landmarker":
                    export_verbose = os.environ.get("STEP5_EXPORT_VERBOSE_FIELDS", "false").strip().lower()
                    should_export_verbose = export_verbose in {"true", "1", "yes", "on", "all"}
                    
                    if should_export_verbose:
                        landmarks_val = obj_data.get("landmarks")
                        if landmarks_val:
                            output_obj["landmarks"] = landmarks_val

                        eos_val = obj_data.get("eos")
                        if eos_val and isinstance(eos_val, dict):
                            output_obj["eos"] = eos_val
            except Exception:
                pass

            output_objects_for_json.append(output_obj)

    for obj_id in ids_to_remove:
        del active_objects[obj_id]

    output_objects_for_json.sort(key=lambda o: o.get("centroid_x", 0))

    return output_objects_for_json
```

## File: utils/transnetv2_library.py
```python
import os
import numpy as np
import tensorflow as tf


class TransNetV2:

    def __init__(self, model_dir=None):
        if model_dir is None:
            model_dir = os.path.join(os.path.dirname(__file__), "transnetv2-weights/")
            if not os.path.isdir(model_dir):
                raise FileNotFoundError(f"[TransNetV2] ERROR: {model_dir} is not a directory.")
            else:
                print(f"[TransNetV2] Using weights from {model_dir}.")

        self._input_size = (27, 48, 3)
        try:
            self._model = tf.saved_model.load(model_dir)
        except OSError as exc:
            raise IOError(f"[TransNetV2] It seems that files in {model_dir} are corrupted or missing. "
                          f"Re-download them manually and retry. For more info, see: "
                          f"https://github.com/soCzech/TransNetV2/issues/1#issuecomment-647357796") from exc

    def predict_raw(self, frames: np.ndarray):
        assert len(frames.shape) == 5 and frames.shape[2:] == self._input_size, \
            "[TransNetV2] Input shape must be [batch, frames, height, width, 3]."
        frames = tf.cast(frames, tf.float32)

        logits, dict_ = self._model(frames)
        single_frame_pred = tf.sigmoid(logits)
        all_frames_pred = tf.sigmoid(dict_["many_hot"])

        return single_frame_pred, all_frames_pred

    def predict_frames(self, frames: np.ndarray):
        assert len(frames.shape) == 4 and frames.shape[1:] == self._input_size, \
            "[TransNetV2] Input shape must be [frames, height, width, 3]."

        def input_iterator():
            no_padded_frames_start = 25
            no_padded_frames_end = 25 + 50 - (len(frames) % 50 if len(frames) % 50 != 0 else 50)  # 25 - 74

            start_frame = np.expand_dims(frames[0], 0)
            end_frame = np.expand_dims(frames[-1], 0)
            padded_inputs = np.concatenate(
                [start_frame] * no_padded_frames_start + [frames] + [end_frame] * no_padded_frames_end, 0
            )

            ptr = 0
            while ptr + 100 <= len(padded_inputs):
                out = padded_inputs[ptr:ptr + 100]
                ptr += 50
                yield out[np.newaxis]

        predictions = []

        for inp in input_iterator():
            single_frame_pred, all_frames_pred = self.predict_raw(inp)
            predictions.append((single_frame_pred.numpy()[0, 25:75, 0],
                                all_frames_pred.numpy()[0, 25:75, 0]))

            print("\r[TransNetV2] Processing video frames {}/{}".format(
                min(len(predictions) * 50, len(frames)), len(frames)
            ), end="")
        print("")

        single_frame_pred = np.concatenate([single_ for single_, all_ in predictions])
        all_frames_pred = np.concatenate([all_ for single_, all_ in predictions])

        return single_frame_pred[:len(frames)], all_frames_pred[:len(frames)]  # remove extra padded frames

    def predict_video(self, video_fn: str):
        try:
            import ffmpeg
        except ModuleNotFoundError:
            raise ModuleNotFoundError("For `predict_video` function `ffmpeg` needs to be installed in order to extract "
                                      "individual frames from video file. Install `ffmpeg` command line tool and then "
                                      "install python wrapper by `pip install ffmpeg-python`.")

        print("[TransNetV2] Extracting frames from {}".format(video_fn))
        video_stream, err = ffmpeg.input(video_fn).output(
            "pipe:", format="rawvideo", pix_fmt="rgb24", s="48x27"
        ).run(capture_stdout=True, capture_stderr=True)

        video = np.frombuffer(video_stream, np.uint8).reshape([-1, 27, 48, 3])
        return (video, *self.predict_frames(video))

    @staticmethod
    def predictions_to_scenes(predictions: np.ndarray, threshold: float = 0.5):
        predictions = (predictions > threshold).astype(np.uint8)

        scenes = []
        t, t_prev, start = -1, 0, 0
        for i, t in enumerate(predictions):
            if t_prev == 1 and t == 0:
                start = i
            if t_prev == 0 and t == 1 and i != 0:
                scenes.append([start, i])
            t_prev = t
        if t == 0:
            scenes.append([start, i])

        if len(scenes) == 0:
            return np.array([[0, len(predictions) - 1]], dtype=np.int32)

        return np.array(scenes, dtype=np.int32)

    @staticmethod
    def visualize_predictions(frames: np.ndarray, predictions):
        from PIL import Image, ImageDraw

        if isinstance(predictions, np.ndarray):
            predictions = [predictions]

        ih, iw, ic = frames.shape[1:]
        width = 25

        pad_with = width - len(frames) % width if len(frames) % width != 0 else 0
        frames = np.pad(frames, [(0, pad_with), (0, 1), (0, len(predictions)), (0, 0)])

        predictions = [np.pad(x, (0, pad_with)) for x in predictions]
        height = len(frames) // width

        img = frames.reshape([height, width, ih + 1, iw + len(predictions), ic])
        img = np.concatenate(np.split(
            np.concatenate(np.split(img, height), axis=2)[0], width
        ), axis=2)[0, :-1]

        img = Image.fromarray(img)
        draw = ImageDraw.Draw(img)

        for i, pred in enumerate(zip(*predictions)):
            x, y = i % width, i // width
            x, y = x * (iw + len(predictions)) + iw, y * (ih + 1) + ih - 1

            for j, p in enumerate(pred):
                color = [0, 0, 0]
                color[(j + 1) % 3] = 255

                value = round(p * (ih - 1))
                if value != 0:
                    draw.line((x + j, y, x + j, y - value), fill=tuple(color), width=1)
        return img


def main():
    import sys
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("files", type=str, nargs="+", help="path to video files to process")
    parser.add_argument("--weights", type=str, default=None,
                        help="path to TransNet V2 weights, tries to infer the location if not specified")
    parser.add_argument('--visualize', action="store_true",
                        help="save a png file with prediction visualization for each extracted video")
    args = parser.parse_args()

    model = TransNetV2(args.weights)
    for file in args.files:
        if os.path.exists(file + ".predictions.txt") or os.path.exists(file + ".scenes.txt"):
            print(f"[TransNetV2] {file}.predictions.txt or {file}.scenes.txt already exists. "
                  f"Skipping video {file}.", file=sys.stderr)
            continue

        video_frames, single_frame_predictions, all_frame_predictions = \
            model.predict_video(file)

        predictions = np.stack([single_frame_predictions, all_frame_predictions], 1)
        np.savetxt(file + ".predictions.txt", predictions, fmt="%.6f")

        scenes = model.predictions_to_scenes(single_frame_predictions)
        np.savetxt(file + ".scenes.txt", scenes, fmt="%d")

        if args.visualize:
            if os.path.exists(file + ".vis.png"):
                print(f"[TransNetV2] {file}.vis.png already exists. "
                      f"Skipping visualization of video {file}.", file=sys.stderr)
                continue

            pil_image = model.visualize_predictions(
                video_frames, predictions=(single_frame_predictions, all_frame_predictions))
            pil_image.save(file + ".vis.png")


if __name__ == "__main__":
    main()
```

## File: utils/worker_wrapper.py
```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Script wrapper qui adapte les arguments de run_with_monitoring.py aux arguments attendus
par process_video_worker_blendshapes_good_backup3.py
"""

import os
import sys
import argparse
import subprocess

def main():
    """
    Prend tous les arguments reçus et les passe directement à process_video_worker.
    """
    # Chemin vers le script worker final
    worker_script_path = os.path.join(
        os.path.dirname(__file__),
        "process_video_worker_blendshapes_good_backup3.py"
    )

    # La commande est simplement l'exécutable python, le script worker, et tous les arguments reçus
    # sys.argv[1:] contient tous les arguments passés au wrapper, y compris video_path.
    command = [
        sys.executable,
        worker_script_path,
    ] + sys.argv[1:]

    print(f"Wrapper: Lancement du worker avec la commande : {' '.join(command)}", file=sys.stderr)
    
    # Exécute le worker et retourne son code de sortie
    # Cela assure que la sortie (stdout/stderr) du worker est directement visible par le processus parent (run_with_monitoring)
    process = subprocess.Popen(command, stdout=sys.stdout, stderr=sys.stderr)
    process.wait()
    return process.returncode

if __name__ == "__main__":
    sys.exit(main())
```

## File: workflow_scripts/step1/extract_archives.py
```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Script d'extraction d'archives pour le workflow de traitement vidéo
Version Ubuntu - Étape 1 (Logique métier de la version originale préservée)
"""

import os
import sys
import shutil
import zipfile
import rarfile
import tarfile
import logging
import argparse
import re
import unicodedata
import time
from pathlib import Path
from datetime import datetime

sys.path.insert(0, str(Path(__file__).parent.parent.parent))
from utils.filename_security import FilenameSanitizer, validate_extraction_path

# --- Configuration des chemins et du logger ---
BASE_DIR = Path(os.path.dirname(os.path.abspath(__file__))).parent.parent
LOG_DIR = BASE_DIR / "logs" / "step1"
LOG_DIR.mkdir(parents=True, exist_ok=True)
log_file = LOG_DIR / f"extract_archives_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file, encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)

# --- Configuration du traitement ---
PROCESSED_ARCHIVES_FILE = LOG_DIR / "processed_archives.txt"
PROCESSED_ARCHIVES_RESET_MARKER = LOG_DIR / "processed_archives.last_reset"
WORK_DIR = BASE_DIR / "projets_extraits"
DELETE_ARCHIVE_AFTER_SUCCESS = True


def get_processed_archives():
    """Récupère la liste des archives déjà traitées."""
    if not PROCESSED_ARCHIVES_FILE.exists():
        return set()
    try:
        with open(PROCESSED_ARCHIVES_FILE, 'r', encoding='utf-8') as f:
            return {line.strip() for line in f if line.strip()}
    except Exception as e:
        logging.error(f"Erreur lecture du fichier des archives traitées: {e}")
        return set()


def reset_processed_archives_if_needed(now: datetime | None = None) -> bool:
    """Réinitialise mensuellement le fichier des archives traitées.

    Cette fonction s'assure qu'au changement de mois, le fichier
    `processed_archives.txt` est vidé pour éviter qu'une archive ayant
    le même nom de fichier soit ignorée d'un mois sur l'autre. Un
    marqueur de mois (`processed_archives.last_reset`) est utilisé pour
    savoir si une réinitialisation a déjà eu lieu pour le mois courant.

    Args:
        now: Date/heure à utiliser (principalement pour les tests). Si None, utilise l'heure courante.

    Returns:
        bool: True si une réinitialisation a eu lieu, False sinon (erreur ou déjà à jour).
    """
    try:
        current_dt = now or datetime.now()
        current_month = current_dt.strftime("%Y-%m")

        last_month_value = None
        if PROCESSED_ARCHIVES_RESET_MARKER.exists():
            try:
                last_month_value = PROCESSED_ARCHIVES_RESET_MARKER.read_text(encoding='utf-8').strip()
            except Exception as e:
                logging.warning(f"Impossible de lire le marqueur de réinitialisation: {e}")

        if last_month_value == current_month:
            return False

        if PROCESSED_ARCHIVES_FILE.exists() and PROCESSED_ARCHIVES_FILE.stat().st_size > 0:
            backup_name = LOG_DIR / (
                f"processed_archives_{last_month_value or 'previous'}_backup_"
                f"{current_dt.strftime('%Y%m%d_%H%M%S')}.txt"
            )
            try:
                shutil.copy2(PROCESSED_ARCHIVES_FILE, backup_name)
                logging.info(f"Réinitialisation mensuelle: sauvegarde créée '{backup_name.name}'.")
            except Exception as e:
                logging.error(f"Échec de sauvegarde avant réinitialisation: {e}")

            try:
                with open(PROCESSED_ARCHIVES_FILE, 'w', encoding='utf-8'):
                    pass
                logging.info("Réinitialisation mensuelle: fichier 'processed_archives.txt' vidé.")
            except Exception as e:
                logging.error(f"Impossible de vider le fichier processed_archives.txt: {e}")
        else:
            logging.info("Réinitialisation mensuelle: aucun contenu existant à sauvegarder.")

        try:
            PROCESSED_ARCHIVES_RESET_MARKER.write_text(current_month, encoding='utf-8')
            logging.info(f"Marqueur de réinitialisation mis à jour pour le mois: {current_month}")
        except Exception as e:
            logging.error(f"Impossible d'écrire le marqueur de réinitialisation: {e}")

        return True

    except Exception as e:
        logging.error(f"Erreur inattendue lors de la réinitialisation mensuelle: {e}")
        return False


def mark_archive_as_processed(archive_path):
    """Marque une archive comme traitée."""
    try:
        with open(PROCESSED_ARCHIVES_FILE, 'a', encoding='utf-8') as f:
            f.write(f"{archive_path}\n")
    except Exception as e:
        logging.error(f"Erreur d'écriture dans le fichier des archives traitées: {e}")


def get_project_folder_name(archive_filename_str):
    """Dérive un nom de dossier de projet propre à partir du nom de l'archive."""
    name_without_ext = Path(archive_filename_str).stem
    parts = name_without_ext.split('_')
    folder_name_candidate = ""

    if parts[0].isdigit() and len(parts) > 1 and "camille" in parts[1].lower():
        folder_name_candidate = f"{parts[0]} {parts[1]}"
    elif "camille" in parts[0].lower():
        if len(parts) > 1 and parts[1].isalpha() and not parts[1].lower() == "camille":
            folder_name_candidate = f"{parts[0]} {parts[1]}"
        else:
            folder_name_candidate = parts[0]
    elif parts[0].isdigit() and len(parts) > 1 and parts[1].isalpha():
        folder_name_candidate = f"{parts[0]} {parts[1]}"
    else:
        folder_name_candidate = name_without_ext.replace("_", " ")

    clean_name = folder_name_candidate.strip()
    clean_name = re.sub(r'[<>:"/\\|?*&]', '_', clean_name)
    clean_name = re.sub(r'\s\s+', ' ', clean_name)
    clean_name = clean_name.strip('_ .')

    if not clean_name:
        logging.warning(f"Le nom de dossier pour '{archive_filename_str}' est vide. Utilisation d'un fallback.")
        fallback_name = re.sub(r'[<>:"/\\|?*&\s]', '_', Path(archive_filename_str).stem).strip('_')
        clean_name = f"projet_{fallback_name}" if fallback_name else "projet_sans_nom"
    return clean_name


def _format_timestamp(now: datetime | None = None) -> str:
    """Retourne un horodatage sans caractères interdits pour un nom de dossier.

    Format: YYYY-MM-DD_HH-MM-SS (ex: 2025-10-06_07-51-55)

    Args:
        now: Datetime à utiliser (principalement pour les tests). Si None, utilise datetime.now().

    Returns:
        str: Horodatage formaté.
    """
    dt = now or datetime.now()
    return dt.strftime("%Y-%m-%d_%H-%M-%S")


def compute_unique_project_dir(base_name: str, destination_base_dir: Path, now: datetime | None = None) -> Path:
    """Calcule un nom de dossier projet unique à créer sous destination_base_dir.

    - Ajoute un suffixe horodaté pour éviter les collisions entre projets portant le même nom logique (ex: "13 Camille").
    - En cas d'ultra-collision (même seconde), incrémente avec un compteur (-2, -3...).

    Exemple: base_name="13 Camille" => "13 Camille 2025-10-06_07-51-55"

    Args:
        base_name: Nom de base dérivé du fichier d'archive (déjà nettoyé).
        destination_base_dir: Dossier parent où créer le projet (ex: `projets_extraits/`).
        now: Datetime optionnelle pour tests.

    Returns:
        Path: Chemin complet du dossier unique (sans le sous-dossier "docs").
    """
    ts = _format_timestamp(now)
    candidate = destination_base_dir / f"{base_name} {ts}"

    if not candidate.exists():
        return candidate

        
    counter = 2
    while True:
        with_counter = destination_base_dir / f"{base_name} {ts}-{counter}"
        if not with_counter.exists():
            return with_counter
        counter += 1


def secure_extract_zip(zip_path, temp_extract_dir, sanitizer):
    """
    Securely extract ZIP archive with filename validation and path traversal protection.

    Args:
        zip_path: Path to ZIP file
        temp_extract_dir: Temporary extraction directory
        sanitizer: FilenameSanitizer instance

    Returns:
        tuple: (success: bool, security_issues_count: int)
    """
    security_issues_count = 0

    try:
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            for member_info in zip_ref.infolist():
                original_path = member_info.filename

                if original_path.endswith('/'):
                    continue

                sanitization_result = sanitizer.sanitize_archive_member_path(original_path)

                if sanitization_result.security_issues:
                    security_issues_count += len(sanitization_result.security_issues)
                    logging.warning(f"Security issues in ZIP member '{original_path}': {sanitization_result.security_issues}")

                if sanitization_result.was_modified:
                    logging.info(f"Sanitized ZIP member: '{original_path}' -> '{sanitization_result.sanitized_name}'")

                final_path = temp_extract_dir / sanitization_result.sanitized_name
                if not validate_extraction_path(sanitization_result.sanitized_name, temp_extract_dir):
                    logging.error(f"Unsafe extraction path detected, skipping: {original_path}")
                    security_issues_count += 1
                    continue

                final_path.parent.mkdir(parents=True, exist_ok=True)

                try:
                    with zip_ref.open(member_info) as source, open(final_path, 'wb') as target:
                        shutil.copyfileobj(source, target)
                except Exception as e:
                    logging.error(f"Failed to extract ZIP member '{original_path}': {e}")
                    continue

        return True, security_issues_count

    except Exception as e:
        logging.error(f"Error during secure ZIP extraction: {e}")
        return False, security_issues_count


def secure_extract_rar(rar_path, temp_extract_dir, sanitizer):
    """
    Securely extract RAR archive with filename validation and path traversal protection.

    Args:
        rar_path: Path to RAR file
        temp_extract_dir: Temporary extraction directory
        sanitizer: FilenameSanitizer instance

    Returns:
        tuple: (success: bool, security_issues_count: int)
    """
    security_issues_count = 0

    try:
        with rarfile.RarFile(rar_path, 'r') as rar_ref:
            for member_info in rar_ref.infolist():
                original_path = member_info.filename

                if member_info.is_dir():
                    continue

                sanitization_result = sanitizer.sanitize_archive_member_path(original_path)

                if sanitization_result.security_issues:
                    security_issues_count += len(sanitization_result.security_issues)
                    logging.warning(f"Security issues in RAR member '{original_path}': {sanitization_result.security_issues}")

                if sanitization_result.was_modified:
                    logging.info(f"Sanitized RAR member: '{original_path}' -> '{sanitization_result.sanitized_name}'")

                final_path = temp_extract_dir / sanitization_result.sanitized_name
                if not validate_extraction_path(sanitization_result.sanitized_name, temp_extract_dir):
                    logging.error(f"Unsafe extraction path detected, skipping: {original_path}")
                    security_issues_count += 1
                    continue

                final_path.parent.mkdir(parents=True, exist_ok=True)

                try:
                    with rar_ref.open(member_info) as source, open(final_path, 'wb') as target:
                        shutil.copyfileobj(source, target)
                except Exception as e:
                    logging.error(f"Failed to extract RAR member '{original_path}': {e}")
                    continue

        return True, security_issues_count

    except Exception as e:
        logging.error(f"Error during secure RAR extraction: {e}")
        return False, security_issues_count


def secure_extract_tar(tar_path, temp_extract_dir, sanitizer):
    """
    Securely extract TAR archive with filename validation and path traversal protection.

    Args:
        tar_path: Path to TAR file
        temp_extract_dir: Temporary extraction directory
        sanitizer: FilenameSanitizer instance

    Returns:
        tuple: (success: bool, security_issues_count: int)
    """
    security_issues_count = 0

    try:
        with tarfile.open(tar_path, 'r:*') as tar_ref:
            for member_info in tar_ref.getmembers():
                original_path = member_info.name

                if member_info.isdir():
                    continue

                if not member_info.isfile():
                    logging.warning(f"Skipping non-regular file in TAR: {original_path}")
                    security_issues_count += 1
                    continue

                sanitization_result = sanitizer.sanitize_archive_member_path(original_path)

                if sanitization_result.security_issues:
                    security_issues_count += len(sanitization_result.security_issues)
                    logging.warning(f"Security issues in TAR member '{original_path}': {sanitization_result.security_issues}")

                if sanitization_result.was_modified:
                    logging.info(f"Sanitized TAR member: '{original_path}' -> '{sanitization_result.sanitized_name}'")

                final_path = temp_extract_dir / sanitization_result.sanitized_name
                if not validate_extraction_path(sanitization_result.sanitized_name, temp_extract_dir):
                    logging.error(f"Unsafe extraction path detected, skipping: {original_path}")
                    security_issues_count += 1
                    continue

                final_path.parent.mkdir(parents=True, exist_ok=True)

                try:
                    with tar_ref.extractfile(member_info) as source, open(final_path, 'wb') as target:
                        if source:  # extractfile can return None for some members
                            shutil.copyfileobj(source, target)
                except Exception as e:
                    logging.error(f"Failed to extract TAR member '{original_path}': {e}")
                    continue

        return True, security_issues_count

    except Exception as e:
        logging.error(f"Error during secure TAR extraction: {e}")
        return False, security_issues_count


def extract_archive(archive_path, destination_base_dir):
    """Extrait une archive de manière sécurisée, gère les sous-dossiers et nettoie."""
    project_folder_name = get_project_folder_name(archive_path.name)
    # Le dossier final contiendra un sous-dossier "docs" pour la cohérence avec les étapes suivantes
    final_destination = destination_base_dir / project_folder_name / "docs"
    temp_extract_dir = destination_base_dir / f"_temp_{project_folder_name}_{int(time.time())}"

    # Initialize security sanitizer
    sanitizer = FilenameSanitizer()
    total_security_issues = 0

    try:
        logging.info(f"Extraction sécurisée de {archive_path.name} vers {final_destination}")

        # 1. Extraire dans un dossier temporaire avec validation de sécurité
        temp_extract_dir.mkdir(parents=True, exist_ok=True)

        suffix = archive_path.suffix.lower()
        extraction_success = False

        if suffix in ('.zip', '.zipx'):
            logging.info(f"Extraction ZIP sécurisée de {archive_path.name}")
            extraction_success, security_issues = secure_extract_zip(archive_path, temp_extract_dir, sanitizer)
            total_security_issues += security_issues
        elif suffix == '.rar':
            logging.info(f"Extraction RAR sécurisée de {archive_path.name}")
            extraction_success, security_issues = secure_extract_rar(archive_path, temp_extract_dir, sanitizer)
            total_security_issues += security_issues
        elif suffix in ('.tar', '.gz', '.bz2', '.xz', '.tgz'):
            logging.info(f"Extraction TAR sécurisée de {archive_path.name}")
            extraction_success, security_issues = secure_extract_tar(archive_path, temp_extract_dir, sanitizer)
            total_security_issues += security_issues
        else:
            logging.warning(f"Format non supporté: {archive_path}")
            return False

        if not extraction_success:
            logging.error(f"Échec de l'extraction sécurisée pour {archive_path.name}")
            return False

        # Log security statistics
        stats = sanitizer.get_stats()
        logging.info(f"Statistiques de sécurité pour {archive_path.name}: "
                    f"{stats['total_processed']} fichiers traités, "
                    f"{stats['modified_count']} modifiés, "
                    f"{total_security_issues} problèmes de sécurité détectés")

        if total_security_issues > 0:
            logging.warning(f"ATTENTION: {total_security_issues} problèmes de sécurité détectés dans {archive_path.name}")

        # Vérifier qu'il y a des fichiers extraits
        extracted_items = list(temp_extract_dir.rglob('*'))
        if not extracted_items:
            logging.warning(f"Aucun fichier extrait de {archive_path.name}")
            return False

        # 2. Nettoyer les fichiers inutiles de l'extraction (ex: __MACOSX)
        macosx_junk = temp_extract_dir / "__MACOSX"
        if macosx_junk.exists() and macosx_junk.is_dir():
            logging.info("Suppression du dossier __MACOSX.")
            shutil.rmtree(macosx_junk)

        # 3. Gérer les cas où le ZIP contient un seul dossier racine
        items_in_temp = list(temp_extract_dir.iterdir())
        source_content_root = temp_extract_dir
        if len(items_in_temp) == 1 and items_in_temp[0].is_dir():
            logging.info(f"Le ZIP contient un seul dossier racine '{items_in_temp[0].name}'. Utilisation comme source.")
            source_content_root = items_in_temp[0]

        # 4. Déplacer le contenu vers la destination finale
        final_destination.mkdir(parents=True, exist_ok=True)
        for item_to_move in source_content_root.iterdir():
            target_path = final_destination / item_to_move.name
            if target_path.exists():
                logging.warning(f"'{target_path.name}' existe déjà dans la destination. Il sera écrasé.")
                if target_path.is_dir():
                    shutil.rmtree(target_path)
                else:
                    target_path.unlink()
            shutil.move(str(item_to_move), str(target_path))

        logging.info(f"Extraction terminée pour {archive_path.name}")
        return True

    except (zipfile.BadZipFile, rarfile.BadRarFile, tarfile.ReadError) as e:
        logging.error(f"Erreur: Fichier archive corrompu ou invalide - {archive_path.name}: {e}")
        return False
    except Exception as e:
        logging.error(f"Erreur inattendue lors de l'extraction de {archive_path.name}: {e}")
        return False
    finally:
        # 5. Nettoyer le dossier temporaire
        if temp_extract_dir.exists():
            shutil.rmtree(temp_extract_dir)


def find_archives_to_process(source_dir):
    """Trouve les archives non encore traitées et contenant le mot-clé 'Camille'."""
    processed = get_processed_archives()
    archives = []
    keyword = "Camille"

    if not source_dir.exists():
        logging.warning(f"Le dossier d'archives '{source_dir}' n'existe pas.")
        return []

    logging.info(f"Recherche des archives dans '{source_dir}' avec le mot-clé '{keyword}'...")

    archive_extensions = ('.zip', '.zipx', '.rar', '.tar.gz', '.tgz', '.tar.bz2', '.tar.xz', '.7z', '.tar')
    for archive in source_dir.iterdir():
        if archive.is_file() and archive.suffix.lower() in archive_extensions:
            if keyword.lower() in archive.name.lower() and str(archive.resolve()) not in processed:
                archives.append(archive)
                logging.info(f"Archive correspondante trouvée: {archive.name}")

    return archives


def main():
    parser = argparse.ArgumentParser(description="Script d'extraction d'archives intelligent.")
    parser.add_argument('--source-dir', type=str, required=True,
                        help="Spécifie le répertoire où chercher les archives.")
    args = parser.parse_args()

    source_archives_dir = Path(args.source_dir)
    logging.info(f"--- Démarrage du script d'extraction d'archives ---")
    logging.info(f"Dossier source: {source_archives_dir.resolve()}")
    logging.info(f"Dossier de destination des projets: {WORK_DIR.resolve()}")

    WORK_DIR.mkdir(parents=True, exist_ok=True)

    # Réinitialisation mensuelle du fichier des archives traitées (si nécessaire)
    try:
        did_reset = reset_processed_archives_if_needed()
        if did_reset:
            logging.info("Réinitialisation mensuelle exécutée (ou marqueur mis à jour).")
        else:
            logging.info("Aucune réinitialisation mensuelle nécessaire.")
    except Exception as e:
        logging.error(f"Échec de la vérification de réinitialisation mensuelle: {e}")

    archives = find_archives_to_process(source_archives_dir)
    total_to_process = len(archives)
    logging.info(f"Trouvé {total_to_process} nouvelle(s) archive(s) à traiter.")
    print(f"Trouvé {total_to_process} archive(s) à traiter")  # Pour l'UI

    if total_to_process == 0:
        logging.info("Aucune nouvelle archive à traiter. Fin du script.")
        return

    successful_count = 0
    for i, archive in enumerate(archives):
        logging.info(f"--- Traitement {i + 1}/{total_to_process}: {archive.name} ---")

        success = extract_archive(archive, WORK_DIR)

        if success:
            successful_count += 1
            mark_archive_as_processed(str(archive.resolve()))
            if DELETE_ARCHIVE_AFTER_SUCCESS:
                try:
                    archive.unlink()
                    logging.info(f"Archive source '{archive.name}' supprimée avec succès.")
                except Exception as e:
                    logging.error(f"Impossible de supprimer l'archive source '{archive.name}': {e}")
        else:
            logging.error(f"L'extraction de '{archive.name}' a échoué. Voir les logs précédents.")

    logging.info(f"--- Traitement terminé ---")
    logging.info(f"Résumé: {successful_count}/{total_to_process} archive(s) extraite(s) avec succès.")

    if successful_count < total_to_process:
        sys.exit(1)  # Quitter avec un code d'erreur s'il y a eu des échecs


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        logging.critical(f"Erreur critique non gérée dans le script: {e}")
        sys.exit(1)
```

## File: workflow_scripts/step2/convert_videos.py
```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Script de conversion vidéo pour le workflow de traitement
Version Ubuntu - Étape 2 (Logique optimisée et parallèle)
"""

import os
import sys
import shutil
import subprocess
import json
import logging
from pathlib import Path
from datetime import datetime
import concurrent.futures
import threading
import queue
import time

# --- Configuration ---
WORK_DIR = Path(os.getcwd())
TARGET_FPS = 25.0
VIDEO_EXTENSIONS = ('.mp4', '.mov', '.avi', '.mkv', '.webm', '.flv', '.wmv')
MAX_CPU_WORKERS = max(1, os.cpu_count() - 2)
FFMPEG_PATH = "ffmpeg"
FFPROBE_PATH = "ffprobe"

# GPU worker queue system for continuous processing
GPU_QUEUE = queue.Queue()
GPU_RESULTS_QUEUE = queue.Queue()
GPU_WORKER_SHUTDOWN = threading.Event()
PROGRESS_LOCK = threading.Lock()
COMPLETED_VIDEOS = 0
TOTAL_VIDEOS_COUNT = 0
COMPRESS_COMPLETED = 0
COMPRESS_TOTAL = 0

BASE_DIR = Path(os.path.dirname(os.path.abspath(__file__))).parent.parent
LOG_DIR = BASE_DIR / "logs" / "step2"
LOG_DIR.mkdir(parents=True, exist_ok=True)
log_file = LOG_DIR / f"convert_videos_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(threadName)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file, encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)


def _parse_ffprobe_fraction(value: str):
    if not value:
        return None
    value = str(value).strip()
    if not value or value == "0/0":
        return None
    try:
        if "/" in value:
            num_str, den_str = value.split("/", 1)
            num = float(num_str)
            den = float(den_str)
            if den == 0:
                return None
            return num / den
        return float(value)
    except Exception:
        return None


def _parse_ffprobe_fps(payload: dict):
    try:
        streams = payload.get("streams") or []
        if not streams:
            return None
        stream = streams[0] or {}

        avg_fps = _parse_ffprobe_fraction(stream.get("avg_frame_rate"))
        r_fps = _parse_ffprobe_fraction(stream.get("r_frame_rate"))

        nb_frames_raw = stream.get("nb_frames")
        duration_raw = stream.get("duration")
        nb_frames = None
        duration = None
        try:
            if nb_frames_raw not in (None, "N/A", ""):
                nb_frames = int(float(nb_frames_raw))
        except Exception:
            nb_frames = None
        try:
            if duration_raw not in (None, "N/A", ""):
                duration = float(duration_raw)
        except Exception:
            duration = None

        fps_from_counts = None
        if nb_frames and duration and duration > 0:
            fps_from_counts = float(nb_frames) / float(duration)

        for candidate in (fps_from_counts, avg_fps, r_fps):
            if candidate is None:
                continue
            if candidate <= 0 or candidate > 240:
                continue
            return float(candidate)
        return None
    except Exception:
        return None


def get_video_framerate(video_path):
    """Récupère le framerate effectif d'une vidéo."""
    try:
        command = [
            FFPROBE_PATH, "-v", "error",
            "-select_streams", "v:0",
            "-show_entries", "stream=avg_frame_rate,r_frame_rate,nb_frames,duration",
            "-of", "json",
            str(video_path),
        ]
        result = subprocess.run(command, capture_output=True, text=True, check=True, encoding='utf-8')
        payload = json.loads(result.stdout or "{}")
        fps = _parse_ffprobe_fps(payload)
        if fps is None:
            raise ValueError("ffprobe fps parse returned None")
        return fps
    except Exception as e:
        logging.error(f"Impossible de lire le framerate de {video_path.name}: {e}")
        return None


def find_videos_to_convert():
    """Trouve toutes les vidéos à convertir dans le répertoire de travail."""
    videos_to_check = []
    logging.info(f"Recherche de vidéos ({', '.join(VIDEO_EXTENSIONS)}) dans {WORK_DIR}...")

    for root, _, files in os.walk(WORK_DIR):
        for file in files:
            if "_temp_conversion" in file or "_converted" in file:
                continue
            if file.lower().endswith(VIDEO_EXTENSIONS):
                videos_to_check.append(Path(root) / file)

    logging.info(f"{len(videos_to_check)} vidéo(s) trouvée(s). Vérification du framerate...")

    videos_requiring_conversion = []
    for video_path in videos_to_check:
        current_fps = get_video_framerate(video_path)
        if current_fps is not None and abs(current_fps - TARGET_FPS) > 0.1:
            logging.info(f"Conversion requise pour {video_path.name} (FPS actuel: {current_fps:.2f})")
            videos_requiring_conversion.append(video_path)
        elif current_fps is not None:
            logging.info(f"Conversion non requise pour {video_path.name} (FPS actuel: {current_fps:.2f})")

    return videos_requiring_conversion


def find_mp4_videos():
    """Liste toutes les vidéos .mp4 à compresser (exclut les fichiers temporaires)."""
    mp4_videos = []
    logging.info(f"Recherche des vidéos .mp4 dans {WORK_DIR} pour compression…")

    for root, _, files in os.walk(WORK_DIR):
        for file in files:
            if "_temp_conversion" in file or "_converted" in file or ".temp_compress" in file:
                continue
            if file.lower().endswith('.mp4'):
                mp4_videos.append(Path(root) / file)

    logging.info(f"{len(mp4_videos)} fichier(s) .mp4 détecté(s) pour la compression.")
    return mp4_videos


def convert_single_video(video_path, use_gpu=False):
    """Convertit une seule vidéo avec l'encodeur spécifié."""
    worker_type = 'GPU' if use_gpu else 'CPU'

    try:
        logging.info(f"Conversion ({worker_type}) démarrée pour {video_path.name}")

        temp_output_path = video_path.with_suffix(f".temp_conversion{video_path.suffix}")

        command = [FFMPEG_PATH, '-y', '-hide_banner', '-i', str(video_path), '-vf', f'fps={TARGET_FPS}']

        if use_gpu:
            command.extend(['-c:v', 'h264_nvenc', '-preset', 'p5', '-tune', 'hq', '-cq', '23', '-pix_fmt', 'yuv420p'])
        else:
            command.extend(['-c:v', 'libx264', '-preset', 'medium', '-crf', '23', '-pix_fmt', 'yuv420p'])

        command_with_audio_copy = command + ['-c:a', 'copy', str(temp_output_path)]

        result = subprocess.run(command_with_audio_copy, capture_output=True, text=True, check=False, encoding='utf-8')

        if result.returncode != 0:
            logging.warning(f"La copie audio a échoué pour {video_path.name}, tentative de ré-encodage audio...")
            command_with_audio_reencode = command + ['-c:a', 'aac', '-b:a', '192k', str(temp_output_path)]
            result = subprocess.run(command_with_audio_reencode, capture_output=True, text=True, check=False,
                                    encoding='utf-8')

        if result.returncode != 0:
            logging.error(f"Erreur FFmpeg ({worker_type}) pour {video_path.name}.\nStderr: {result.stderr.strip()}")
            if temp_output_path.exists(): temp_output_path.unlink()
            return False

        shutil.move(str(temp_output_path), str(video_path))
        logging.info(f"Succès ({worker_type}): {video_path.name} a été converti et mis à jour.")
        return True

    except Exception as e:
        logging.error(f"Erreur inattendue dans le worker ({worker_type}) pour {video_path.name}: {e}")
        return False


def compress_single_video(video_path, use_gpu=False):
    """Compresse une vidéo .mp4 sans changer la résolution ni le framerate.

    Utilise des paramètres FFmpeg conservateurs pour réduire la taille tout en préservant la qualité visuelle.
    - GPU: h264_nvenc avec cq=28 (qualité élevée, réduction notable)
    - CPU: libx264 avec crf=28
    L'audio est copié si possible, sinon ré-encodé en AAC 192k.
    """
    worker_type = 'GPU' if use_gpu else 'CPU'

    try:
        logging.info(f"Compression ({worker_type}) démarrée pour {video_path.name}")

        temp_output_path = video_path.with_suffix(f".temp_compress{video_path.suffix}")

        command = [FFMPEG_PATH, '-y', '-hide_banner', '-i', str(video_path)]

        if use_gpu:
            command.extend(['-c:v', 'h264_nvenc', '-preset', 'p5', '-tune', 'hq', '-cq', '28', '-pix_fmt', 'yuv420p'])
        else:
            command.extend(['-c:v', 'libx264', '-preset', 'medium', '-crf', '28', '-pix_fmt', 'yuv420p'])

        command_with_audio_copy = command + ['-c:a', 'copy', str(temp_output_path)]
        result = subprocess.run(command_with_audio_copy, capture_output=True, text=True, check=False, encoding='utf-8')

        if result.returncode != 0:
            logging.warning(f"La copie audio a échoué pour {video_path.name}, tentative de ré-encodage audio…")
            command_with_audio_reencode = command + ['-c:a', 'aac', '-b:a', '192k', str(temp_output_path)]
            result = subprocess.run(command_with_audio_reencode, capture_output=True, text=True, check=False, encoding='utf-8')

        if result.returncode != 0:
            logging.error(f"Erreur FFmpeg ({worker_type}) lors de la compression de {video_path.name}.\nStderr: {result.stderr.strip()}")
            if temp_output_path.exists():
                try:
                    temp_output_path.unlink()
                except Exception:
                    pass
            return False

        shutil.move(str(temp_output_path), str(video_path))
        logging.info(f"Succès ({worker_type}): {video_path.name} a été compressé.")
        return True

    except Exception as e:
        logging.error(f"Erreur inattendue dans le worker ({worker_type}) pour {video_path.name} (compression): {e}")
        return False



def gpu_worker_thread():
    """Thread dédié pour le traitement GPU continu."""
    global COMPLETED_VIDEOS

    logging.info("GPU worker thread démarré")

    while not GPU_WORKER_SHUTDOWN.is_set():
        try:
            video_path = GPU_QUEUE.get(timeout=1.0)

            with PROGRESS_LOCK:
                current_index = COMPLETED_VIDEOS + 1
                logging.info(f"--- Traitement de la vidéo ({current_index}/{TOTAL_VIDEOS_COUNT}): {video_path.name} ---")
                print(f"--- Traitement de la vidéo ({current_index}/{TOTAL_VIDEOS_COUNT}): {video_path.name} ---")

            success = convert_single_video(video_path, use_gpu=True)

            GPU_RESULTS_QUEUE.put((success, video_path))

            with PROGRESS_LOCK:
                COMPLETED_VIDEOS += 1

            GPU_QUEUE.task_done()

        except queue.Empty:
            continue
        except Exception as e:
            logging.error(f"Erreur dans le GPU worker thread: {e}")
            try:
                GPU_QUEUE.task_done()
            except ValueError:
                pass

    logging.info("GPU worker thread terminé")


def gpu_compress_worker_thread():
    """Thread dédié pour la compression GPU séquentielle."""
    global COMPRESS_COMPLETED

    logging.info("GPU compression worker thread démarré")

    while not GPU_WORKER_SHUTDOWN.is_set():
        try:
            video_path = GPU_QUEUE.get(timeout=1.0)

            # Émission d'une ligne de progression compatible STEP2 avant traitement (compression)
            with PROGRESS_LOCK:
                current_index = COMPRESS_COMPLETED + 1
                logging.info(f"--- Traitement de la vidéo ({current_index}/{COMPRESS_TOTAL}): {video_path.name} ---")
                print(f"--- Traitement de la vidéo ({current_index}/{COMPRESS_TOTAL}): {video_path.name} ---")

            success = compress_single_video(video_path, use_gpu=True)

            GPU_RESULTS_QUEUE.put((success, video_path))

            with PROGRESS_LOCK:
                COMPRESS_COMPLETED += 1

            GPU_QUEUE.task_done()

        except queue.Empty:
            continue
        except Exception as e:
            logging.error(f"Erreur dans le GPU compression worker thread: {e}")
            try:
                GPU_QUEUE.task_done()
            except ValueError:
                pass

    logging.info("GPU compression worker thread terminé")




def main():
    global TOTAL_VIDEOS_COUNT, COMPLETED_VIDEOS, COMPRESS_TOTAL, COMPRESS_COMPLETED

    logging.info("--- Démarrage du script de conversion vidéo ---")

    try:
        subprocess.run([FFMPEG_PATH, "-version"], capture_output=True, check=True)
        subprocess.run([FFPROBE_PATH, "-version"], capture_output=True, check=True)
    except (FileNotFoundError, subprocess.CalledProcessError):
        logging.critical("ffmpeg ou ffprobe n'est pas installé ou non accessible dans le PATH. Arrêt.")
        sys.exit(1)

    videos = find_videos_to_convert()
    total_videos = len(videos)
    TOTAL_VIDEOS_COUNT = total_videos
    COMPLETED_VIDEOS = 0

    logging.info(f"TOTAL_VIDEOS_TO_PROCESS: {total_videos}")
    print(f"TOTAL_VIDEOS_TO_PROCESS: {total_videos}")

    if total_videos == 0:
        logging.info("Aucune vidéo à convertir. Passage direct à la compression.")

    total_successful = 0
    if total_videos > 0:
        gpu_videos = videos
        cpu_videos = []

        logging.info(f"Allocation GPU exclusive: {len(gpu_videos)} vidéo(s) pour traitement GPU séquentiel")

        logging.info(f"Lancement de la conversion avec 1 worker GPU dédié pour traitement séquentiel.")
        gpu_thread = threading.Thread(target=gpu_worker_thread, name='GPU-Worker', daemon=True)
        gpu_thread.start()

        for video in gpu_videos:
            GPU_QUEUE.put(video)

        GPU_QUEUE.join()

        gpu_successful_count = 0
        while not GPU_RESULTS_QUEUE.empty():
            try:
                success, video_path = GPU_RESULTS_QUEUE.get_nowait()
                if success:
                    gpu_successful_count += 1
            except queue.Empty:
                break

        GPU_WORKER_SHUTDOWN.set()
        gpu_thread.join(timeout=5.0)

        total_successful = gpu_successful_count

        logging.info("--- Conversion de toutes les vidéos terminée ---")
        logging.info(f"Résumé: {total_successful}/{total_videos} conversion(s) réussie(s) (traitement GPU exclusif).")

    mp4_videos = find_mp4_videos()
    COMPRESS_TOTAL = len(mp4_videos)
    COMPRESS_COMPLETED = 0

    logging.info(f"TOTAL_VIDEOS_TO_PROCESS: {COMPRESS_TOTAL} (compression)")
    print(f"TOTAL_VIDEOS_TO_PROCESS: {COMPRESS_TOTAL}")

    if COMPRESS_TOTAL == 0:
        logging.info("Aucune vidéo .mp4 à compresser. Fin du script.")
        if total_successful < total_videos:
            sys.exit(1)
        return

    GPU_WORKER_SHUTDOWN.clear()
    gpu_compress_thread = threading.Thread(target=gpu_compress_worker_thread, name='GPU-Compress-Worker', daemon=True)
    gpu_compress_thread.start()

    for video in mp4_videos:
        GPU_QUEUE.put(video)

    GPU_QUEUE.join()

    compress_successful_count = 0
    while not GPU_RESULTS_QUEUE.empty():
        try:
            success, video_path = GPU_RESULTS_QUEUE.get_nowait()
            if success:
                compress_successful_count += 1
        except queue.Empty:
            break

    GPU_WORKER_SHUTDOWN.set()
    gpu_compress_thread.join(timeout=5.0)

    logging.info("--- Compression de toutes les vidéos .mp4 terminée ---")
    logging.info(f"Résumé: {compress_successful_count}/{COMPRESS_TOTAL} compression(s) réussie(s) (GPU séquentiel).")

    if total_successful < total_videos:
        sys.exit(1)


if __name__ == "__main__":
    main()
```

## File: workflow_scripts/step3/run_transnet.py
```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Script d'analyse des transitions vidéo avec TransNetV2 (PyTorch)
Version Ubuntu - Étape 3
"""

import os
import sys
import csv
import argparse
import logging
import json
import numpy as np
import torch
import torch.nn as nn
import ffmpeg
from pathlib import Path
from datetime import datetime
from scenedetect import FrameTimecode
import multiprocessing as mp
import time

# --- Configuration ---
WORK_DIR = Path(os.getcwd())
VIDEO_EXTENSIONS = ('.mp4', '.avi', '.mov', '.mkv', '.webm')
BASE_DIR = Path(os.path.dirname(os.path.abspath(__file__))).parent.parent

# --- Configuration du Logger ---
LOG_DIR = BASE_DIR / "logs" / "step3"
LOG_DIR.mkdir(parents=True, exist_ok=True)
log_file = LOG_DIR / f"transnet_pytorch_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file, encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)

# --- Logique de TransNetV2 PyTorch ---
# On s'attend à ce que transnetv2_pytorch.py soit dans le même dossier
try:
    from transnetv2_pytorch import TransNetV2 as TransNetV2_PyTorch_Model
except ImportError:
    logging.critical("ERREUR: Le module 'transnetv2_pytorch.py' n'a pas pu être importé. "
                     "Assurez-vous qu'il est dans le dossier 'workflow_scripts/step3/'.")
    sys.exit(1)


def get_video_fps(video_path):
    """Retourne toujours 25.0 FPS pour stabiliser les timecodes (Étape 2 force 25 FPS)."""
    return 25.0


def detect_scenes_with_pytorch(video_path, model, device, threshold=0.5):
    """Détection de scènes avec lecture streaming (chunked) et batching glissant.

    - Décodage FFmpeg en streaming (48x27, fps=25) avec run_async.
    - Fenêtre de taille WINDOW_SIZE, pas WINDOW_STRIDE, padding PADDING_FRAMES
      en répétant les frames de bord pour les fenêtres au début/à la fin.
    - Retourne des segments [start_frame, end_frame] basés sur un seuil.
    """
    try:
        process = (
            ffmpeg
            .input(str(video_path))
            .output(
                'pipe:',
                format='rawvideo',
                pix_fmt='rgb24',
                s='48x27',
                r=25  # forcer 25 FPS
            )
            .global_args('-threads', str(FFMPEG_THREADS) if FFMPEG_THREADS is not None else '0')
            .run_async(pipe_stdout=True, pipe_stderr=True, quiet=True)
        )

        FRAME_H, FRAME_W, FRAME_C = 27, 48, 3
        FRAME_SIZE = FRAME_H * FRAME_W * FRAME_C

        def read_n_frames(n):
            """Lit n frames depuis stdout et retourne une liste de np.ndarray shape (27,48,3)."""
            buf = bytearray()
            target = n * FRAME_SIZE
            while len(buf) < target:
                chunk = process.stdout.read(target - len(buf))
                if not chunk:
                    break
                buf.extend(chunk)
            if not buf:
                return []
            total_bytes = len(buf)
            frames_count = total_bytes // FRAME_SIZE
            if frames_count == 0:
                return []
            arr = np.frombuffer(bytes(buf[:frames_count * FRAME_SIZE]), np.uint8)
            return list(arr.reshape([frames_count, FRAME_H, FRAME_W, FRAME_C]))

        frames = []  # tampon de frames décodées
        predictions = []
        total_batches = 0
        batch_count = 0

        with torch.inference_mode():
            # Remplir suffisamment pour la première fenêtre (WINDOW_SIZE-PADDING_FRAMES) + PADDING_FRAMES à droite
            # On lit au moins WINDOW_SIZE frames réelles pour démarrer
            if len(frames) < WINDOW_SIZE:
                frames.extend(read_n_frames(WINDOW_SIZE - len(frames)))

            # Si aucune frame
            if len(frames) == 0:
                logging.warning(f"Aucune frame extraite pour {video_path.name}.")
                return []

            # Calculer une estimation du nombre de batches (approx, peut ajuster à la fin)
            # Impossible de connaître la longueur totale à l'avance en streaming.
            # On loguera la progression basée sur le compteur de batches.

            start_idx = 0

            def build_window(idx, available_right):
                """Construit une fenêtre de longueur WINDOW_SIZE autour de idx avec padding bords.

                idx est l'index de départ des frames réelles (sans le padding gauche).
                available_right indique combien de frames réelles existent à droite à partir de idx.
                """
                left_needed = PADDING_FRAMES
                right_needed = PADDING_FRAMES

                # gauche
                if idx >= left_needed:
                    left_part = frames[idx - left_needed: idx]
                else:
                    pad = [frames[0]] * (left_needed - idx)
                    left_part = pad + frames[0:idx]

                # milieu
                mid_len = WINDOW_SIZE - PADDING_FRAMES - PADDING_FRAMES
                mid_part = frames[idx: idx + mid_len]

                # droite
                right_have = max(0, available_right - mid_len)
                if right_have >= right_needed:
                    right_part = frames[idx + mid_len: idx + mid_len + right_needed]
                else:
                    # compléter avec la dernière frame disponible
                    base = frames[idx + mid_len - 1] if (idx + mid_len - 1) < len(frames) else frames[-1]
                    right_part = frames[idx + mid_len: idx + mid_len + right_have] + [base] * (right_needed - right_have)

                window = left_part + mid_part + right_part
                return np.asarray(window, dtype=np.uint8)

            while True:
                # S'assurer qu'on a assez de frames pour avancer d'un STRIDE
                to_read = max(0, (start_idx + WINDOW_SIZE) - len(frames))
                if to_read > 0:
                    new_frames = read_n_frames(to_read)
                    if new_frames:
                        frames.extend(new_frames)
                    else:
                        # fin du flux, on traitera ce qui reste avec padding à droite
                        pass

                if start_idx >= len(frames):
                    break

                # combien de frames réelles dispo à droite de idx
                available_right = max(0, len(frames) - start_idx)
                # Construire la fenêtre avec padding si nécessaire
                window_np = build_window(start_idx, available_right)
                if window_np.shape[0] != WINDOW_SIZE:
                    # cas limite si frames < 1
                    if len(frames) == 0:
                        break
                    # compléter strictement à WINDOW_SIZE
                    lastf = frames[-1]
                    add = WINDOW_SIZE - window_np.shape[0]
                    window_np = np.concatenate([window_np, np.repeat(np.expand_dims(lastf, 0), add, axis=0)], axis=0)

                batch_torch = torch.from_numpy(window_np).unsqueeze(0).to(device, dtype=torch.uint8)

                if USE_AMP and device.type == 'cuda':
                    try:
                        with torch.amp.autocast('cuda', dtype=AMP_DTYPE):
                            out = model(batch_torch)
                    except AttributeError:
                        with torch.cuda.amp.autocast(dtype=AMP_DTYPE):
                            out = model(batch_torch)
                else:
                    out = model(batch_torch)

                # Normaliser la sortie: prendre le tenseur principal si tuple/list
                single_frame_pred_logits = out[0] if isinstance(out, (tuple, list)) else out

                pred_slice = torch.sigmoid(single_frame_pred_logits).cpu().numpy()[0, PADDING_FRAMES:WINDOW_SIZE - PADDING_FRAMES, 0]
                predictions.append(pred_slice)
                batch_count += 1

                # progression (approx)
                if batch_count % 10 == 0:
                    logging.info(f"INTERNAL_PROGRESS: {batch_count} batches - {video_path.name}")
                    print(f"INTERNAL_PROGRESS: {batch_count} batches - {video_path.name}")

                # avancer
                start_idx += WINDOW_STRIDE

                # condition d'arrêt: si on est à la fin et qu'aucune nouvelle frame lue
                if start_idx >= len(frames):
                    # tenter de lire plus pour une dernière fenêtre sinon sortir
                    more = read_n_frames(WINDOW_STRIDE)
                    if more:
                        frames.extend(more)
                    else:
                        break

        # concat predictions and trim to actual frame count
        if not predictions:
            return [[0, len(frames) - 1]] if len(frames) > 0 else []

        final_predictions = np.concatenate(predictions)[:len(frames)]
        shot_boundaries = np.where(final_predictions > threshold)[0]

        if len(shot_boundaries) == 0:
            return [[0, len(frames) - 1]] if len(frames) > 0 else []

        # Création des scènes
        detected_scenes = []
        last_cut = -1
        for cut in shot_boundaries:
            if cut > last_cut:
                detected_scenes.append([last_cut + 1, cut])
            last_cut = cut
        if last_cut < len(frames) - 1:
            detected_scenes.append([last_cut + 1, len(frames) - 1])

        return detected_scenes

    except Exception as e:
        logging.error(f"Erreur lors de la détection de scènes pour {video_path.name}: {e}", exc_info=True)
        return None


def main():
    # L'argument `weights_dir` de app_new.py n'est pas utilisé, on utilise un chemin fixe
    # pour le fichier .pth pour simplifier.
    parser = argparse.ArgumentParser(description="Analyse des transitions vidéo avec TransNetV2 (PyTorch).")
    parser.add_argument("--weights_dir", help="Argument ignoré, présent pour compatibilité.")
    parser.add_argument("--config", type=str, help="Chemin du fichier de configuration JSON (par défaut: config/step3_transnet.json si présent)")
    # Mettre None comme défaut pour permettre au JSON de définir la valeur quand le flag n'est pas fourni
    parser.add_argument("--threshold", type=float, default=None, help="Seuil de détection (0-1). Défaut: 0.5")
    parser.add_argument("--window", type=int, default=None, help="Taille de fenêtre (frames). Défaut: 100")
    parser.add_argument("--stride", type=int, default=None, help="Pas entre fenêtres (frames). Défaut: 50")
    parser.add_argument("--padding", type=int, default=None, help="Padding au début/fin (frames). Défaut: 25")
    parser.add_argument("--device", choices=["auto", "cpu", "cuda"], default=None, help="Sélection du device. Défaut: auto")
    parser.add_argument("--ffmpeg_threads", type=int, default=None, help="Nombre de threads FFmpeg (0 = auto)")
    parser.add_argument("--mixed_precision", action="store_true", help="Active l'AMP (CUDA seulement)")
    parser.add_argument("--num_workers", type=int, default=None, help="Nombre de workers parallèles (1 par défaut, 1 forcé en CUDA)")
    parser.add_argument("--torchscript", action="store_true", help="Active TorchScript (expérimental)")
    parser.add_argument("--warmup", action="store_true", help="Effectue un warm-up du modèle avant traitement")
    parser.add_argument("--warmup_batches", type=int, default=None, help="Nombre de passes de warm-up (défaut: 1)")
    parser.add_argument("--torchscript_auto_fallback", action="store_true", help="En cas d'échec par vidéo avec TorchScript, retenter en Eager")
    args = parser.parse_args()

    # Chargement du fichier de configuration JSON (optionnel)
    # Ordre de priorité: défauts internes < fichier JSON < CLI
    defaults = {
        "threshold": 0.5,
        "window": 100,
        "stride": 50,
        "padding": 25,
        "device": "auto",
        "ffmpeg_threads": 0,
        "mixed_precision": False,
        "amp_dtype": "float16",
        "num_workers": 1,
        "torchscript": False,
        "warmup": True,
        "warmup_batches": 1,
        "torchscript_auto_fallback": True,
    }

    # Déterminer le chemin de config effectif
    config_path = None
    if args.config:
        config_path = Path(args.config)
    else:
        candidate = BASE_DIR / "config" / "step3_transnet.json"
        if candidate.exists():
            config_path = candidate

    file_cfg = {}
    if config_path is not None:
        try:
            with open(config_path, "r", encoding="utf-8") as f:
                file_cfg = json.load(f)
            logging.info(f"Configuration chargée depuis: {config_path}")
        except Exception as e:
            logging.error(f"Impossible de charger le fichier de configuration {config_path}: {e}")

    # Fusion des configurations
    effective_cfg = dict(defaults)
    effective_cfg.update({k: v for k, v in file_cfg.items() if v is not None})

    # Appliquer les overrides CLI seulement si fournis (None signifie non fourni)
    if args.threshold is not None:
        effective_cfg["threshold"] = float(args.threshold)
    if args.window is not None:
        effective_cfg["window"] = int(args.window)
    if args.stride is not None:
        effective_cfg["stride"] = int(args.stride)
    if args.padding is not None:
        effective_cfg["padding"] = int(args.padding)
    if args.device is not None:
        effective_cfg["device"] = args.device
    if args.ffmpeg_threads is not None:
        effective_cfg["ffmpeg_threads"] = int(args.ffmpeg_threads)
    # mixed_precision: le flag CLI l'active si présent, sinon on garde config/défaut
    if args.mixed_precision:
        effective_cfg["mixed_precision"] = True
    if args.num_workers is not None:
        effective_cfg["num_workers"] = int(args.num_workers)
    if args.torchscript:
        effective_cfg["torchscript"] = True
    if args.warmup:
        effective_cfg["warmup"] = True
    if args.warmup_batches is not None:
        effective_cfg["warmup_batches"] = int(args.warmup_batches)
    if args.torchscript_auto_fallback:
        effective_cfg["torchscript_auto_fallback"] = True

    # Propager les hyperparamètres globaux (utilisés dans detect_scenes_with_pytorch)
    global WINDOW_SIZE, WINDOW_STRIDE, PADDING_FRAMES, FFMPEG_THREADS, USE_AMP, AMP_DTYPE
    WINDOW_SIZE = int(effective_cfg["window"])
    WINDOW_STRIDE = int(effective_cfg["stride"])
    PADDING_FRAMES = int(effective_cfg["padding"])
    FFMPEG_THREADS = int(effective_cfg["ffmpeg_threads"]) if effective_cfg["ffmpeg_threads"] is not None else 0
    USE_AMP = bool(effective_cfg["mixed_precision"])
    # Convertir amp_dtype string en torch.dtype
    if effective_cfg["amp_dtype"] == "bfloat16":
        AMP_DTYPE = torch.bfloat16
    else:
        AMP_DTYPE = torch.float16

    # Log de la configuration effective
    logging.info(
        "CONFIG_EFFECTIVE: "
        f"threshold={effective_cfg['threshold']}, window={WINDOW_SIZE}, stride={WINDOW_STRIDE}, padding={PADDING_FRAMES}, "
        f"device={effective_cfg['device']}, ffmpeg_threads={FFMPEG_THREADS}, mixed_precision={USE_AMP}, amp_dtype={effective_cfg['amp_dtype']}, "
        f"num_workers={effective_cfg['num_workers']}, torchscript={effective_cfg['torchscript']}, warmup={effective_cfg['warmup']}, warmup_batches={effective_cfg['warmup_batches']}, "
        f"torchscript_auto_fallback={effective_cfg['torchscript_auto_fallback']}"
    )

    # Chemin vers le modèle PyTorch
    pytorch_weights_path = BASE_DIR / "assets" / "transnetv2-pytorch-weights.pth"

    logging.info("--- Démarrage de l'analyse des transitions (PyTorch) ---")

    # Préparer la liste des vidéos

    # Trouver les vidéos à traiter
    videos = [p for ext in VIDEO_EXTENSIONS for p in WORK_DIR.rglob(f'*{ext}') if not p.with_suffix('.csv').exists()]
    total_videos = len(videos)
    logging.info(f"TOTAL_VIDEOS_TO_PROCESS: {total_videos}")
    print(f"TOTAL_VIDEOS_TO_PROCESS: {total_videos}")

    if total_videos == 0:
        return

    # En CUDA, FORCER 1 worker pour éviter contention GPU (critique)
    device_mode = effective_cfg["device"]
    if device_mode == "cuda" or (device_mode == "auto" and torch.cuda.is_available()):
        if effective_cfg["num_workers"] and effective_cfg["num_workers"] > 1:
            logging.warning(f"CUDA mode détecté: limitation forcée des workers de {effective_cfg['num_workers']} à 1 pour éviter la contention GPU.")
            effective_cfg["num_workers"] = 1

    # Préparer les tâches
    tasks = [(str(p), effective_cfg, str(pytorch_weights_path)) for p in videos]

    successful_count = 0
    if effective_cfg["num_workers"] <= 1:
        for i, (vpath, cfg, wpath) in enumerate(tasks):
            ok = _process_single_video(i, total_videos, vpath, cfg, wpath)
            successful_count += 1 if ok else 0
    else:
        with mp.Pool(processes=effective_cfg["num_workers"]) as pool:
            for i, ok in enumerate(pool.imap_unordered(_pool_worker_wrapper, tasks), start=1):
                successful_count += 1 if ok else 0

    logging.info(f"--- Analyse terminée. {successful_count}/{total_videos} réussie(s). ---")
    if successful_count < total_videos:
        sys.exit(1)


def _pool_worker_wrapper(task):
    """Wrapper compatible Pool pour traiter une vidéo."""
    return _process_single_video(None, None, *task)


def _load_model_for_cfg(device, weights_path, use_torchscript=False):
    """Charge le modèle, optionnellement TorchScript, sur le device donné.
    
    Args:
        device: torch.device où charger le modèle
        weights_path: chemin vers les poids .pth
        use_torchscript: si True, compile avec TorchScript
    
    Returns:
        Le modèle chargé ou None en cas d'échec
    """
    if not Path(weights_path).exists():
        logging.critical(f"Fichier de poids PyTorch non trouvé: {weights_path}")
        logging.critical("Téléchargez les poids depuis https://github.com/soCzech/TransNetV2 et placez-les dans assets/")
        return None
    
    try:
        model = TransNetV2_PyTorch_Model()
        # Charger d'abord en CPU pour éviter les erreurs CUDA au chargement
        state = torch.load(str(weights_path), map_location='cpu')
        model.load_state_dict(state)
        model.eval()
        # Puis déplacer sur le device cible
        model.to(device)
    except RuntimeError as e:
        if "CUDA" in str(e):
            logging.error(f"Erreur CUDA lors du chargement du modèle: {e}")
            logging.warning("Tentative de fallback sur CPU...")
            try:
                # Fallback CPU
                cpu_device = torch.device('cpu')
                model = TransNetV2_PyTorch_Model()
                state = torch.load(str(weights_path), map_location='cpu')
                model.load_state_dict(state)
                model.eval().to(cpu_device)
                logging.info("Modèle chargé avec succès sur CPU (fallback)")
                return model
            except Exception as cpu_err:
                logging.error(f"Échec du fallback CPU: {cpu_err}")
                return None
        else:
            logging.error(f"Erreur lors du chargement du modèle: {e}", exc_info=True)
            return None
    except Exception as e:
        logging.error(f"Erreur inattendue lors du chargement du modèle: {e}", exc_info=True)
        return None

    if device.type == 'cuda':
        torch.backends.cudnn.benchmark = True

    if use_torchscript:
        try:
            class InferenceWrapper(nn.Module):
                def __init__(self, base: nn.Module):
                    super().__init__()
                    self.base = base

                def forward(self, x: torch.Tensor) -> torch.Tensor:
                    out = self.base(x)
                    # Le modèle peut retourner (tensor, {"many_hot": tensor})
                    if isinstance(out, (tuple, list)):
                        return out[0]
                    return out

            wrapper = InferenceWrapper(model).to(device).eval()
            example = torch.zeros((1, WINDOW_SIZE, 27, 48, 3), dtype=torch.uint8, device=device)
            scripted = torch.jit.trace(wrapper, example)
            scripted = torch.jit.freeze(scripted)
            logging.info("TorchScript activé via wrapper pour TransNetV2 (sortie tensor-only).")
            return scripted
        except Exception as e:
            logging.warning(f"TorchScript a échoué, fallback modèle Eager: {e}")
            return model
    return model


def _process_single_video(idx, total, video_path_str, cfg, weights_path_str):
    """Traite une seule vidéo: charge le modèle, warm-up, détecte, écrit CSV.

    Args:
      idx: index (peut être None en pool)
      total: total de vidéos (peut être None en pool)
      video_path_str: chemin de la vidéo
      cfg: dict de configuration effectif
      weights_path_str: chemin vers les poids du modèle
    Returns: bool succès
    """
    try:
        video_path = Path(video_path_str)
        # Propager globals dans le worker
        global WINDOW_SIZE, WINDOW_STRIDE, PADDING_FRAMES, FFMPEG_THREADS, USE_AMP, AMP_DTYPE
        WINDOW_SIZE = int(cfg["window"])
        WINDOW_STRIDE = int(cfg["stride"])
        PADDING_FRAMES = int(cfg["padding"])
        FFMPEG_THREADS = int(cfg["ffmpeg_threads"]) if cfg["ffmpeg_threads"] is not None else 0
        USE_AMP = bool(cfg["mixed_precision"])
        AMP_DTYPE = torch.bfloat16 if cfg.get("amp_dtype") == "bfloat16" else torch.float16

        # Device selection avec fallback intelligent
        if cfg["device"] == "cpu":
            device = torch.device("cpu")
            logging.info(f"Device sélectionné: CPU (forcé par config)")
        elif cfg["device"] == "cuda":
            if torch.cuda.is_available():
                device = torch.device("cuda")
                logging.info(f"Device sélectionné: CUDA (GPU disponible)")
            else:
                device = torch.device("cpu")
                logging.warning("CUDA demandé mais non disponible, fallback sur CPU")
        else:  # auto
            if torch.cuda.is_available():
                device = torch.device("cuda")
                logging.info(f"Device sélectionné: CUDA (auto-détection)")
            else:
                device = torch.device("cpu")
                logging.info(f"Device sélectionné: CPU (auto-détection)")

        if idx is not None and total is not None:
            logging.info(f"PROCESSING_VIDEO: {idx + 1}/{total}: {video_path.name}")
            print(f"PROCESSING_VIDEO: {idx + 1}/{total}: {video_path.name}")
        else:
            logging.info(f"PROCESSING_VIDEO: {video_path.name}")

        # Charger modèle par worker
        use_ts = bool(cfg.get("torchscript"))
        model = _load_model_for_cfg(device, weights_path_str, use_torchscript=use_ts)
        if model is None:
            return False

        # Warm-up optionnel
        if bool(cfg.get("warmup", True)):
            warm_batches = int(cfg.get("warmup_batches", 1))
            for _ in range(max(1, warm_batches)):
                dummy = torch.zeros((1, WINDOW_SIZE, 27, 48, 3), dtype=torch.uint8, device=device)
                if USE_AMP and device.type == 'cuda':
                    try:
                        with torch.amp.autocast('cuda', dtype=AMP_DTYPE):
                            _ = model(dummy)
                    except AttributeError:
                        with torch.cuda.amp.autocast(dtype=AMP_DTYPE):
                            _ = model(dummy)
                else:
                    _ = model(dummy)

        # Détection avec fallback multi-niveaux
        scenes = detect_scenes_with_pytorch(video_path, model, device, threshold=float(cfg["threshold"]))
        
        # Niveau 1: Si TorchScript activé et échec, retenter en Eager
        if scenes is None and use_ts and bool(cfg.get("torchscript_auto_fallback", True)):
            logging.warning(f"TorchScript a échoué pour {video_path.name}, tentative de fallback Eager...")
            model = _load_model_for_cfg(device, weights_path_str, use_torchscript=False)
            if model is None:
                return False
            scenes = detect_scenes_with_pytorch(video_path, model, device, threshold=float(cfg["threshold"]))
        
        # Niveau 2: Si CUDA et échec, retenter en CPU
        if scenes is None and device.type == 'cuda':
            logging.warning(f"Erreur avec CUDA pour {video_path.name}, tentative de fallback CPU...")
            cpu_device = torch.device('cpu')
            model = _load_model_for_cfg(cpu_device, weights_path_str, use_torchscript=False)
            if model is None:
                logging.error(f"Échec du fallback CPU pour {video_path.name}")
                return False
            scenes = detect_scenes_with_pytorch(video_path, model, cpu_device, threshold=float(cfg["threshold"]))
            if scenes is None:
                logging.error(f"Échec définitif du traitement de {video_path.name} (CUDA et CPU)")
                return False
            logging.info(f"Succès avec fallback CPU pour {video_path.name}")
        
        if scenes is None:
            logging.error(f"Échec du traitement de {video_path.name}")
            return False

        # Écriture CSV
        output_csv_path = video_path.with_suffix('.csv')
        fps = get_video_fps(video_path)
        with open(output_csv_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(['No', 'Timecode In', 'Timecode Out', 'Frame In', 'Frame Out'])
            for j, (start, end) in enumerate(scenes):
                start_frame = int(start)
                end_frame = int(end)
                timecode_in = FrameTimecode(start_frame, fps)
                timecode_out = FrameTimecode(end_frame, fps)
                writer.writerow([
                    j + 1,
                    timecode_in.get_timecode(),
                    timecode_out.get_timecode(),
                    start_frame + 1,
                    end_frame + 1
                ])
        logging.info(f"Succès: {output_csv_path.name} créé avec {len(scenes)} scènes.")
        return True
    except Exception as e:
        logging.error(f"Erreur worker pour {video_path_str}: {e}", exc_info=True)
        return False


if __name__ == "__main__":
    main()
```

## File: workflow_scripts/step3/transnetv2_pytorch.py
```python
"""
This work was downloaded from https://github.com/soCzech/TransNetV2
All credit to https://github.com/soCzech/
"""

import torch
import torch.nn as nn
import torch.nn.functional as functional

import random


class TransNetV2(nn.Module):

    def __init__(self,
                 F=16, L=3, S=2, D=1024,
                 use_many_hot_targets=True,
                 use_frame_similarity=True,
                 use_color_histograms=True,
                 use_mean_pooling=False,
                 dropout_rate=0.5,
                 use_convex_comb_reg=False,
                 use_resnet_features=False,
                 use_resnet_like_top=False,
                 frame_similarity_on_last_layer=False):
        super(TransNetV2, self).__init__()

        if use_resnet_features or use_resnet_like_top or use_convex_comb_reg or frame_similarity_on_last_layer:
            raise NotImplemented("Some options not implemented in Pytorch version of Transnet!")

        self.SDDCNN = nn.ModuleList(
            [StackedDDCNNV2(in_filters=3, n_blocks=S, filters=F, stochastic_depth_drop_prob=0.)] +
            [StackedDDCNNV2(in_filters=(F * 2 ** (i - 1)) * 4, n_blocks=S, filters=F * 2 ** i) for i in range(1, L)]
        )

        self.frame_sim_layer = FrameSimilarity(
            sum([(F * 2 ** i) * 4 for i in range(L)]), lookup_window=101, output_dim=128, similarity_dim=128, use_bias=True
        ) if use_frame_similarity else None
        self.color_hist_layer = ColorHistograms(
            lookup_window=101, output_dim=128
        ) if use_color_histograms else None

        self.dropout = nn.Dropout(dropout_rate) if dropout_rate is not None else None

        output_dim = ((F * 2 ** (L - 1)) * 4) * 3 * 6  # 3x6 for spatial dimensions
        if use_frame_similarity: output_dim += 128
        if use_color_histograms: output_dim += 128

        self.fc1 = nn.Linear(output_dim, D)
        self.cls_layer1 = nn.Linear(D, 1)
        self.cls_layer2 = nn.Linear(D, 1) if use_many_hot_targets else None

        self.use_mean_pooling = use_mean_pooling
        self.eval()

    def forward(self, inputs):
        if not isinstance(inputs, torch.Tensor):
            raise TypeError(f"Expected torch.Tensor, got {type(inputs)}")
        if inputs.dtype != torch.uint8:
            raise TypeError(f"Expected dtype torch.uint8, got {inputs.dtype}")
        if inputs.dim() != 5 or inputs.shape[2] != 27 or inputs.shape[3] != 48 or inputs.shape[4] != 3:
            raise ValueError(f"Expected shape [B, T, 27, 48, 3], got {inputs.shape}")
        
        x = inputs.permute([0, 4, 1, 2, 3]).float()
        x = x.div_(255.)

        block_features = []
        for block in self.SDDCNN:
            x = block(x)
            block_features.append(x)

        if self.use_mean_pooling:
            x = torch.mean(x, dim=[3, 4])
            x = x.permute(0, 2, 1)
        else:
            x = x.permute(0, 2, 3, 4, 1)
            x = x.reshape(x.shape[0], x.shape[1], -1)

        if self.frame_sim_layer is not None:
            x = torch.cat([self.frame_sim_layer(block_features), x], 2)

        if self.color_hist_layer is not None:
            x = torch.cat([self.color_hist_layer(inputs), x], 2)

        x = self.fc1(x)
        x = functional.relu(x)

        if self.dropout is not None:
            x = self.dropout(x)

        one_hot = self.cls_layer1(x)

        if self.cls_layer2 is not None:
            return one_hot, {"many_hot": self.cls_layer2(x)}

        return one_hot


class StackedDDCNNV2(nn.Module):

    def __init__(self,
                 in_filters,
                 n_blocks,
                 filters,
                 shortcut=True,
                 use_octave_conv=False,
                 pool_type="avg",
                 stochastic_depth_drop_prob=0.0):
        super(StackedDDCNNV2, self).__init__()

        if use_octave_conv:
            raise NotImplemented("Octave convolution not implemented in Pytorch version of Transnet!")

        assert pool_type == "max" or pool_type == "avg"
        if use_octave_conv and pool_type == "max":
            print("WARN: Octave convolution was designed with average pooling, not max pooling.")

        self.shortcut = shortcut
        self.DDCNN = nn.ModuleList([
            DilatedDCNNV2(in_filters if i == 1 else filters * 4, filters, octave_conv=use_octave_conv,
                          activation=functional.relu if i != n_blocks else None) for i in range(1, n_blocks + 1)
        ])
        self.pool = nn.MaxPool3d(kernel_size=(1, 2, 2)) if pool_type == "max" else nn.AvgPool3d(kernel_size=(1, 2, 2))
        self.stochastic_depth_drop_prob = stochastic_depth_drop_prob

    def forward(self, inputs):
        x = inputs
        shortcut = None

        for block in self.DDCNN:
            x = block(x)
            if shortcut is None:
                shortcut = x

        x = functional.relu(x)

        if self.shortcut is not None:
            if self.stochastic_depth_drop_prob != 0.:
                if self.training:
                    if random.random() < self.stochastic_depth_drop_prob:
                        x = shortcut
                    else:
                        x = x + shortcut
                else:
                    x = (1 - self.stochastic_depth_drop_prob) * x + shortcut
            else:
                x += shortcut

        x = self.pool(x)
        return x


class DilatedDCNNV2(nn.Module):

    def __init__(self,
                 in_filters,
                 filters,
                 batch_norm=True,
                 activation=None,
                 octave_conv=False):
        super(DilatedDCNNV2, self).__init__()

        if octave_conv:
            raise NotImplemented("Octave convolution not implemented in Pytorch version of Transnet!")

        assert not (octave_conv and batch_norm)

        self.Conv3D_1 = Conv3DConfigurable(in_filters, filters, 1, use_bias=not batch_norm)
        self.Conv3D_2 = Conv3DConfigurable(in_filters, filters, 2, use_bias=not batch_norm)
        self.Conv3D_4 = Conv3DConfigurable(in_filters, filters, 4, use_bias=not batch_norm)
        self.Conv3D_8 = Conv3DConfigurable(in_filters, filters, 8, use_bias=not batch_norm)

        self.bn = nn.BatchNorm3d(filters * 4, eps=1e-3) if batch_norm else None
        self.activation = activation

    def forward(self, inputs):
        conv1 = self.Conv3D_1(inputs)
        conv2 = self.Conv3D_2(inputs)
        conv3 = self.Conv3D_4(inputs)
        conv4 = self.Conv3D_8(inputs)

        x = torch.cat([conv1, conv2, conv3, conv4], dim=1)

        if self.bn is not None:
            x = self.bn(x)

        if self.activation is not None:
            x = self.activation(x)

        return x


class Conv3DConfigurable(nn.Module):

    def __init__(self,
                 in_filters,
                 filters,
                 dilation_rate,
                 separable=True,
                 octave=False,
                 use_bias=True,
                 kernel_initializer=None):  # not supported
        super(Conv3DConfigurable, self).__init__()

        if octave:
            raise NotImplemented("Octave convolution not implemented in Pytorch version of Transnet!")
        if kernel_initializer is not None:
            raise NotImplemented("Kernel initializers are not implemented in Pytorch version of Transnet!")

        assert not (separable and octave)

        if separable:
            # (2+1)D convolution https://arxiv.org/pdf/1711.11248.pdf
            conv1 = nn.Conv3d(in_filters, 2 * filters, kernel_size=(1, 3, 3),
                              dilation=(1, 1, 1), padding=(0, 1, 1), bias=False)
            conv2 = nn.Conv3d(2 * filters, filters, kernel_size=(3, 1, 1),
                              dilation=(dilation_rate, 1, 1), padding=(dilation_rate, 0, 0), bias=use_bias)
            self.layers = nn.ModuleList([conv1, conv2])
        else:
            conv = nn.Conv3d(in_filters, filters, kernel_size=3,
                             dilation=(dilation_rate, 1, 1), padding=(dilation_rate, 1, 1), bias=use_bias)
            self.layers = nn.ModuleList([conv])

    def forward(self, inputs):
        x = inputs
        for layer in self.layers:
            x = layer(x)
        return x


class FrameSimilarity(nn.Module):

    def __init__(self,
                 in_filters,
                 similarity_dim=128,
                 lookup_window=101,
                 output_dim=128,
                 stop_gradient=False,
                 use_bias=False):
        super(FrameSimilarity, self).__init__()

        if stop_gradient:
            raise NotImplemented("Stop gradient not implemented in Pytorch version of Transnet!")

        self.projection = nn.Linear(in_filters, similarity_dim, bias=use_bias)
        self.fc = nn.Linear(lookup_window, output_dim)

        self.lookup_window = lookup_window
        assert lookup_window % 2 == 1, "`lookup_window` must be odd integer"

    def forward(self, inputs):
        x = torch.cat([torch.mean(x, dim=[3, 4]) for x in inputs], dim=1)
        x = torch.transpose(x, 1, 2)

        x = self.projection(x)
        x = functional.normalize(x, p=2, dim=2)

        batch_size, time_window = x.shape[0], x.shape[1]
        similarities = torch.bmm(x, x.transpose(1, 2))  # [batch_size, time_window, time_window]
        similarities_padded = functional.pad(similarities, [(self.lookup_window - 1) // 2, (self.lookup_window - 1) // 2])

        batch_indices = torch.arange(0, batch_size, device=x.device).view([batch_size, 1, 1]).repeat(
            [1, time_window, self.lookup_window])
        time_indices = torch.arange(0, time_window, device=x.device).view([1, time_window, 1]).repeat(
            [batch_size, 1, self.lookup_window])
        lookup_indices = torch.arange(0, self.lookup_window, device=x.device).view([1, 1, self.lookup_window]).repeat(
            [batch_size, time_window, 1]) + time_indices

        similarities = similarities_padded[batch_indices, time_indices, lookup_indices]
        return functional.relu(self.fc(similarities))


class ColorHistograms(nn.Module):

    def __init__(self,
                 lookup_window=101,
                 output_dim=None):
        super(ColorHistograms, self).__init__()

        self.fc = nn.Linear(lookup_window, output_dim) if output_dim is not None else None
        self.lookup_window = lookup_window
        assert lookup_window % 2 == 1, "`lookup_window` must be odd integer"

    @staticmethod
    def compute_color_histograms(frames):
        frames = frames.int()

        def get_bin(frames):
            R, G, B = frames[:, :, 0], frames[:, :, 1], frames[:, :, 2]
            R, G, B = R >> 5, G >> 5, B >> 5
            return (R << 6) + (G << 3) + B

        batch_size, time_window, height, width, no_channels = frames.shape
        if no_channels != 3:
            raise ValueError(f"Expected 3 color channels, got {no_channels}")
        frames_flatten = frames.view(batch_size * time_window, height * width, 3)

        binned_values = get_bin(frames_flatten)
        frame_bin_prefix = (torch.arange(0, batch_size * time_window, device=frames.device) << 9).view(-1, 1)
        binned_values = (binned_values + frame_bin_prefix).view(-1)

        histograms = torch.zeros(batch_size * time_window * 512, dtype=torch.int32, device=frames.device)
        histograms.scatter_add_(0, binned_values, torch.ones(binned_values.shape[0], dtype=torch.int32, device=frames.device))

        histograms = histograms.view(batch_size, time_window, 512).float()
        histograms_normalized = functional.normalize(histograms, p=2, dim=2)
        return histograms_normalized

    def forward(self, inputs):
        x = self.compute_color_histograms(inputs)

        batch_size, time_window = x.shape[0], x.shape[1]
        similarities = torch.bmm(x, x.transpose(1, 2))  # [batch_size, time_window, time_window]
        similarities_padded = functional.pad(similarities, [(self.lookup_window - 1) // 2, (self.lookup_window - 1) // 2])

        batch_indices = torch.arange(0, batch_size, device=x.device).view([batch_size, 1, 1]).repeat(
            [1, time_window, self.lookup_window])
        time_indices = torch.arange(0, time_window, device=x.device).view([1, time_window, 1]).repeat(
            [batch_size, 1, self.lookup_window])
        lookup_indices = torch.arange(0, self.lookup_window, device=x.device).view([1, 1, self.lookup_window]).repeat(
            [batch_size, time_window, 1]) + time_indices

        similarities = similarities_padded[batch_indices, time_indices, lookup_indices]

        if self.fc is not None:
            return functional.relu(self.fc(similarities))
        return similarities
```

## File: workflow_scripts/step4/run_audio_analysis_lemonfox.py
```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import argparse
import importlib.util
import logging
import os
import subprocess
import sys
from datetime import datetime
from pathlib import Path


def _configure_file_logger(log_dir: Path) -> None:
    log_dir.mkdir(parents=True, exist_ok=True)
    log_file = log_dir / f"audio_analysis_lemonfox_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
    logging.getLogger().addHandler(file_handler)


def _find_videos_for_audio_analysis(work_dir: Path) -> list[Path]:
    video_extensions = ('.mp4', '.mov', '.avi', '.mkv', '.webm')
    videos_to_process: list[Path] = []

    all_videos = [p for ext in video_extensions for p in work_dir.rglob(f'*{ext}')]

    for video_path in all_videos:
        if video_path.suffix.lower() == '.mov':
            continue

        output_json_path = video_path.with_name(f"{video_path.stem}_audio.json")
        if not output_json_path.exists():
            videos_to_process.append(video_path)

    return videos_to_process


def _resolve_project_and_video_name(work_dir: Path, video_path: Path) -> tuple[str, str]:
    rel = video_path.relative_to(work_dir)
    parts = rel.parts
    if len(parts) < 2:
        raise ValueError(f"Video path is not inside a project folder: {rel}")
    project_name = parts[0]
    video_name = str(Path(*parts[1:]))
    return project_name, video_name


def _ensure_repo_root_on_sys_path() -> None:
    repo_root = Path(__file__).resolve().parents[2]
    repo_root_str = str(repo_root)
    if repo_root_str not in sys.path:
        sys.path.insert(0, repo_root_str)


def _import_lemonfox_audio_service():
    _ensure_repo_root_on_sys_path()

    repo_root = Path(__file__).resolve().parents[2]
    service_path = repo_root / "services" / "lemonfox_audio_service.py"
    spec = importlib.util.spec_from_file_location("lemonfox_audio_service", service_path)
    if spec is None or spec.loader is None:
        raise ImportError(f"Unable to load Lemonfox service module from {service_path}")
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    return module.LemonfoxAudioService


def _run_pyannote_fallback(log_dir: Path) -> int:
    repo_root = Path(__file__).resolve().parents[2]
    original_script = repo_root / "workflow_scripts" / "step4" / "run_audio_analysis.py"

    cmd = [
        sys.executable,
        str(original_script),
        "--log_dir",
        str(log_dir),
    ]

    logging.error("Fallback STEP4: exécution de la méthode originale (Pyannote) suite à une erreur Lemonfox")
    completed = subprocess.run(cmd)
    return int(completed.returncode)


def main() -> int:
    parser = argparse.ArgumentParser()
    parser.add_argument("--log_dir", type=str, required=True)
    args = parser.parse_args()

    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[logging.StreamHandler(sys.stdout)]
    )

    log_dir = Path(args.log_dir)
    _configure_file_logger(log_dir)

    work_dir = Path(os.getcwd())

    try:
        lemonfox_service = _import_lemonfox_audio_service()
    except Exception as e:
        logging.error(f"Impossible d'importer LemonfoxAudioService: {e}")
        return _run_pyannote_fallback(log_dir)

    videos_to_process = _find_videos_for_audio_analysis(work_dir)
    logging.info(f"TOTAL_AUDIO_TO_ANALYZE: {len(videos_to_process)}")

    if not videos_to_process:
        logging.info("Aucune vidéo à analyser (tous les _audio.json existent déjà).")
        return 0

    for idx, video_path in enumerate(videos_to_process, start=1):
        logging.info(f"ANALYZING_AUDIO: {idx}/{len(videos_to_process)}: {video_path.name}")

        try:
            project_name, video_name = _resolve_project_and_video_name(work_dir, video_path)
        except Exception as e:
            logging.error(f"Erreur résolution projet/vidéo pour {video_path}: {e}")
            return _run_pyannote_fallback(log_dir)

        try:
            duration_sec = lemonfox_service._get_video_duration_ffprobe(video_path)
            fps = 25.0
            total_frames = int(round((duration_sec or 0.0) * fps))
            if total_frames > 0:
                logging.info(f"INTERNAL_PROGRESS: 0/{total_frames} frames (0%) - Lemonfox API call")

            result = lemonfox_service.process_video_with_lemonfox(
                project_name=project_name,
                video_name=video_name,
            )

            if not result.success:
                logging.error(f"Erreur Lemonfox pour {video_path.name}: {result.error}")
                return _run_pyannote_fallback(log_dir)

            if result.total_frames > 0:
                logging.info(
                    f"INTERNAL_PROGRESS: {result.total_frames}/{result.total_frames} frames (100%) - Lemonfox done"
                )

            logging.info(f"Succès: analyse audio terminée pour {video_path.name}")

        except Exception as e:
            logging.error(f"Erreur inattendue Lemonfox pour {video_path.name}: {e}", exc_info=True)
            return _run_pyannote_fallback(log_dir)

    return 0


if __name__ == "__main__":
    raise SystemExit(main())
```

## File: workflow_scripts/step4/run_audio_analysis.py
```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Script d'analyse audio (diarisation) avec Pyannote.audio
Version Ubuntu - Étape 4

Optimisations clés:
- Extraction audio via ffmpeg (remplace MoviePy) vers tmpfs si disponible
- Suppression d'OpenCV/MoviePy pour les métadonnées (utilisation ffprobe + fallback FPS=25)
- Écriture JSON en streaming (évite le stockage complet en mémoire)
- Mapping segments->frames sans matérialiser toute la diarisation
- Journalisation unique (suppression des prints dupliqués)
- Inference PyTorch optimisée (CUDA prioritaire, CPU fallback; no_grad/inference_mode)
- Politique device/workers configurable via variables d'environnement
- Nettoyage robuste des répertoires temporaires
- Compression gzip optionnelle (désactivée par défaut pour compatibilité STEP5)
"""

import os
import sys
import json
import argparse
import logging
import subprocess
import tempfile
import gzip
import time
import gc
from contextlib import nullcontext
import torch
from pathlib import Path
from datetime import datetime

# --- Configuration ---
WORK_DIR = Path(os.getcwd())
VIDEO_EXTENSIONS = ('.mp4', '.mov', '.avi', '.mkv', '.webm')
OUTPUT_SUFFIX = "_audio.json"
DEFAULT_FPS = 25

LOG_DIR_PATH = None


def _load_optimal_tv_config() -> dict:
    try:
        repo_root = Path(__file__).resolve().parents[2]
        config_path = repo_root / "config" / "optimal_tv_config.json"
        if not config_path.exists():
            return {}
        with open(config_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        if not isinstance(data, dict):
            logging.warning("optimal_tv_config.json ignoré (JSON racine non-objet).")
            return {}
        logging.info(f"optimal_tv_config.json chargé: {config_path}")
        return data
    except Exception as e:
        logging.warning(f"Impossible de charger optimal_tv_config.json: {e}")
        return {}

# Le dossier de log est AUDIO_ANALYSIS_LOG_DIR, passé par argument
# On configure un logger de base qui sera complété dans main()
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)


def find_videos_for_audio_analysis():
    """Trouve toutes les vidéos à analyser qui n'ont pas encore de fichier _audio.json."""
    videos_to_process = []
    logging.info(f"Recherche de vidéos dans {WORK_DIR}...")

    all_videos = [p for ext in VIDEO_EXTENSIONS for p in WORK_DIR.rglob(f'*{ext}')]

    skipped_mov = 0
    filtered_videos = []
    for video_path in all_videos:
        if video_path.suffix.lower() == '.mov':
            skipped_mov += 1
            continue
        filtered_videos.append(video_path)

    for video_path in filtered_videos:
        output_json_path = video_path.with_name(f"{video_path.stem}{OUTPUT_SUFFIX}")
        if not output_json_path.exists():
            videos_to_process.append(video_path)
    
    return videos_to_process


def _run_ffprobe_duration(video_path: Path) -> float:
    """Retourne la durée (en secondes) via ffprobe, ou -1 en cas d'échec."""
    try:
        result = subprocess.run(
            [
                "ffprobe", "-v", "error", "-show_entries", "format=duration",
                "-of", "default=nw=1:nk=1", str(video_path)
            ],
            stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True, text=True
        )
        return float(result.stdout.strip())
    except Exception as e:
        logging.warning(f"ffprobe a échoué pour {video_path.name}: {e}")
        return -1.0


def _extract_audio_ffmpeg(input_video: Path, output_wav: Path) -> bool:
    """Extrait l'audio en WAV mono 16kHz via ffmpeg. Retourne True si OK."""
    try:
        cmd = [
            "ffmpeg", "-y", "-hide_banner", "-loglevel", "error",
            "-i", str(input_video),
            "-vn", "-ac", "1", "-ar", "16000", "-f", "wav", "-acodec", "pcm_s16le",
            str(output_wav)
        ]
        subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        return True
    except subprocess.CalledProcessError as e:
        logging.error(f"ffmpeg extraction audio a échoué pour {input_video.name}: {e}")
        return False


def _write_empty_audio_json_streaming(output_json_path: Path, video_name: str, total_frames: int, fps: float) -> None:
    """Écrit un JSON vide compatible STEP5 en streaming."""
    with open(output_json_path, 'w', encoding='utf-8') as f:
        f.write('{\n')
        f.write(f'  "video_filename": "{video_name}",\n')
        f.write(f'  "total_frames": {total_frames},\n')
        f.write(f'  "fps": {round(fps, 2)},\n')
        f.write('  "frames_analysis": [')
        if total_frames > 0:
            f.write('\n')
            for frame_num in range(1, total_frames + 1):
                timecode = round((frame_num - 1) / fps, 3)
                obj = {
                    "frame": frame_num,
                    "audio_info": {
                        "is_speech_present": False,
                        "num_distinct_speakers_audio": 0,
                        "active_speaker_labels": [],
                        "timecode_sec": timecode,
                    },
                }
                if frame_num > 1:
                    f.write(',\n')
                f.write(json.dumps(obj))
        f.write('\n  ]\n')
        f.write('}\n')


def _cleanup_cuda_memory() -> None:
    try:
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    except Exception:
        pass
    try:
        gc.collect()
    except Exception:
        pass


def _get_total_vram_gb() -> float:
    try:
        if not torch.cuda.is_available():
            return 0.0
        props = torch.cuda.get_device_properties(0)
        return float(props.total_memory) / (1024**3)
    except Exception:
        return 0.0


def _is_low_vram_gpu(threshold_gb: float = 6.0) -> bool:
    total_gb = _get_total_vram_gb()
    return total_gb > 0.0 and total_gb <= threshold_gb


def _should_enable_amp(device: str) -> bool:
    if device != "cuda":
        return False
    env_value = os.getenv("AUDIO_ENABLE_AMP")
    if env_value is not None:
        return env_value == "1"
    return _is_low_vram_gpu()


def _get_pyannote_batch_size(device: str) -> int | None:
    raw = os.getenv("AUDIO_PYANNOTE_BATCH_SIZE")
    if raw is not None:
        try:
            value = int(raw)
            return value if value > 0 else None
        except Exception:
            return None
    if device == "cuda" and _is_low_vram_gpu():
        return 1
    return None


def _import_pyannote_pipeline():
    from pyannote.audio import Pipeline
    return Pipeline


def _load_pyannote_pipeline(model_id: str, hf_token: str, pipeline_cls=None):
    """
    Load pyannote Pipeline while handling token/use_auth_token compatibility.
    """
    pipeline_cls = pipeline_cls or _import_pyannote_pipeline()
    try:
        return pipeline_cls.from_pretrained(model_id, token=hf_token)
    except TypeError as type_err:
        if "token" not in str(type_err):
            raise
        logging.info(
            "Pipeline.from_pretrained(%s) ne supporte pas 'token'. "
            "Tentative avec use_auth_token.",
            model_id,
        )
        return pipeline_cls.from_pretrained(model_id, use_auth_token=hf_token)


def _run_diarization_and_extract_segments(diarization_pipeline, wav_path: Path, device: str) -> list:
    use_amp = _should_enable_amp(device)
    if use_amp and hasattr(torch, "cuda") and hasattr(torch.cuda, "amp"):
        amp_ctx = torch.cuda.amp.autocast(dtype=torch.float16)
    else:
        amp_ctx = nullcontext()

    pyannote_batch_size = _get_pyannote_batch_size(device)
    diarization_kwargs = {"num_speakers": None}
    if pyannote_batch_size is not None:
        diarization_kwargs["batch_size"] = pyannote_batch_size
        logging.info(f"Diarisation paramètres: batch_size={pyannote_batch_size}")

    with torch.inference_mode():
        with amp_ctx:
            try:
                diarization = diarization_pipeline(str(wav_path), **diarization_kwargs)
            except TypeError as e:
                if "batch_size" in str(e):
                    diarization_kwargs.pop("batch_size", None)
                    diarization = diarization_pipeline(str(wav_path), **diarization_kwargs)
                else:
                    raise
    segments = [(t.start, t.end, spk) for t, _, spk in diarization.itertracks(yield_label=True)]
    del diarization
    return segments


def _apply_audio_profile_from_env() -> None:
    profile = (os.getenv("AUDIO_PROFILE") or "").strip().lower()
    if not profile:
        return

    if profile == "gpu_optimized":
        os.environ["AUDIO_DISABLE_GPU"] = "0"
        os.environ["AUDIO_ENABLE_AMP"] = "1"
        os.environ["AUDIO_PYANNOTE_BATCH_SIZE"] = "1"
        logging.info("AUDIO_PROFILE=gpu_optimized appliqué (AMP=1, batch_size=1)")
        logging.warning("ATTENTION: AMP peut réduire significativement la qualité de diarisation (faux négatifs).")
        return

    if profile == "gpu_fp32":
        os.environ["AUDIO_DISABLE_GPU"] = "0"
        os.environ["AUDIO_ENABLE_AMP"] = "0"
        os.environ["AUDIO_PYANNOTE_BATCH_SIZE"] = "1"
        logging.info("AUDIO_PROFILE=gpu_fp32 appliqué (AMP=0, batch_size=1, FP32 pur - cohérence CPU)")
        return

    if profile == "gpu_no_amp":
        os.environ["AUDIO_DISABLE_GPU"] = "0"
        os.environ["AUDIO_ENABLE_AMP"] = "0"
        os.environ["AUDIO_PYANNOTE_BATCH_SIZE"] = "1"
        logging.info("AUDIO_PROFILE=gpu_no_amp appliqué (AMP=0, batch_size=1)")
        return

    if profile == "cpu_only":
        os.environ["AUDIO_DISABLE_GPU"] = "1"
        os.environ["AUDIO_ENABLE_AMP"] = "0"
        os.environ.pop("AUDIO_PYANNOTE_BATCH_SIZE", None)
        logging.info("AUDIO_PROFILE=cpu_only appliqué (GPU désactivé)")
        return

    logging.warning(
        f"AUDIO_PROFILE inconnu: '{profile}'. Valeurs supportées: gpu_optimized, gpu_fp32, gpu_no_amp, cpu_only"
    )


def _run_cpu_diarization_subprocess(wav_path: Path, output_segments_json: Path, hf_token: str) -> None:
    """Fallback CPU avec mêmes paramètres que GPU (sauf AMP) pour cohérence."""
    env = os.environ.copy()
    env["AUDIO_DISABLE_GPU"] = "1"
    env["CUDA_VISIBLE_DEVICES"] = ""
    env["HUGGINGFACE_HUB_TOKEN"] = hf_token
    # Forcer FP32 (pas d'AMP) pour cohérence avec mode GPU sans AMP
    env["AUDIO_ENABLE_AMP"] = "0"
    pyannote_batch_size = os.getenv("AUDIO_PYANNOTE_BATCH_SIZE")
    if pyannote_batch_size:
        env["AUDIO_PYANNOTE_BATCH_SIZE"] = pyannote_batch_size

    cmd = [
        sys.executable,
        str(Path(__file__).resolve()),
        "--log_dir",
        str(LOG_DIR_PATH or Path(tempfile.gettempdir())),
        "--disable_gpu",
        "--cpu_diarize_wav",
        str(wav_path),
        "--cpu_diarize_out",
        str(output_segments_json),
    ]
    subprocess.run(
        cmd,
        check=True,
        env=env,
        stdout=subprocess.DEVNULL,
        stderr=subprocess.PIPE,
        text=True,
    )


def _load_segments_from_json(segments_json: Path) -> list:
    with open(segments_json, "r", encoding="utf-8") as f:
        data = json.load(f)
    segments = []
    for item in data:
        start = float(item["start"])
        end = float(item["end"])
        speaker = str(item["speaker"])
        segments.append((start, end, speaker))
    return segments


def analyze_audio_file(video_path, diarization_pipeline, hf_token, device: str):
    """Analyse une vidéo, extrait l'audio via ffmpeg, effectue la diarisation et sauvegarde le JSON (streaming)."""
    output_json_path = video_path.with_name(f"{video_path.stem}{OUTPUT_SUFFIX}")

    duration_sec = _run_ffprobe_duration(video_path)
    video_fps = DEFAULT_FPS
    if duration_sec > 0:
        total_frames = int(round(duration_sec * video_fps))
    else:
        logging.warning(f"Durée inconnue, fallback frames basé sur DEFAULT_FPS={DEFAULT_FPS} pour {video_path.name}")
        # On fixera total_frames après avoir déterminé le max de frame touché par la timeline.
        total_frames = -1

    tmp_dir_root = "/dev/shm" if os.path.isdir("/dev/shm") else None
    try:
        with tempfile.TemporaryDirectory(dir=tmp_dir_root) as tmp_dir:
            tmp_wav = Path(tmp_dir) / f"{video_path.stem}_temp.wav"

            logging.info(f"Extraction audio (ffmpeg) de {video_path.name} -> {tmp_wav.name}...")
            if not _extract_audio_ffmpeg(video_path, tmp_wav):
                if total_frames < 0:
                    total_frames = 0
                _write_empty_audio_json_streaming(output_json_path, video_path.name, total_frames, video_fps)
                return True

            logging.info(f"Diarisation en cours sur {tmp_wav.name}...")
            start_infer_t = time.time()
            segments = None
            try:
                if device == "cuda":
                    _cleanup_cuda_memory()
                segments = _run_diarization_and_extract_segments(diarization_pipeline, tmp_wav, device)
                logging.info(f"Diarisation: {len(segments)} segment(s) détecté(s)")
            except RuntimeError as e_oom:
                if "CUDA out of memory" in str(e_oom):
                    logging.warning("CUDA OOM durant la diarisation, tentative de fallback CPU pour ce fichier...")
                    _cleanup_cuda_memory()
                    try:
                        cpu_segments_json = Path(tmp_dir) / f"{video_path.stem}_cpu_segments.json"
                        logging.info("Fallback CPU: mêmes paramètres que GPU (batch_size) sauf AMP, pour cohérence")
                        _run_cpu_diarization_subprocess(tmp_wav, cpu_segments_json, hf_token)
                        segments = _load_segments_from_json(cpu_segments_json)
                        logging.info(f"Fallback CPU: {len(segments)} segment(s) détecté(s)")
                    except subprocess.CalledProcessError as cpu_e:
                        safe_stderr = (cpu_e.stderr or "")
                        logging.error(
                            "Echec du fallback CPU (subprocess). "
                            f"returncode={cpu_e.returncode}. stderr:\n{safe_stderr}"
                        )
                        raise
                    except Exception:
                        logging.error("Echec du fallback CPU (erreur inattendue).", exc_info=True)
                        raise
                else:
                    segments = _run_diarization_and_extract_segments(diarization_pipeline, tmp_wav, device)
            infer_ms = int((time.time() - start_infer_t) * 1000)
            logging.info(f"Diarisation terminée en ~{infer_ms} ms")

            audio_timeline = {}
            max_frame_seen = 0
            for start_sec, end_sec, speaker_label in (segments or []):
                start_frame = max(1, int(start_sec * video_fps))
                end_frame = int(end_sec * video_fps)
                if end_frame < start_frame:
                    continue
                max_frame_seen = max(max_frame_seen, end_frame)
                for frame_num in range(start_frame, end_frame + 1):
                    frame_entry = audio_timeline.get(frame_num)
                    if frame_entry is None:
                        frame_entry = set()
                        audio_timeline[frame_num] = frame_entry
                    frame_entry.add(speaker_label)
            
            num_speech_frames = len(audio_timeline)
            speech_pct = (num_speech_frames / total_frames * 100) if total_frames > 0 else 0
            logging.info(f"Timeline audio: {num_speech_frames}/{total_frames} frames avec parole ({speech_pct:.1f}%)")

            if total_frames < 0:
                total_frames = max_frame_seen

            # Écriture JSON streaming (préserve le schéma pour STEP5)
            use_gzip = os.getenv("AUDIO_JSON_GZIP", "0") == "1"
            if use_gzip:
                json_path = str(output_json_path) + ".gz"
                open_fn = lambda p: gzip.open(p, mode="wt", encoding="utf-8")
            else:
                json_path = str(output_json_path)
                open_fn = lambda p: open(p, mode="w", encoding="utf-8")

            with open_fn(json_path) as f:
                f.write("{\n")
                f.write(f"  \"video_filename\": \"{video_path.name}\",\n")
                f.write(f"  \"total_frames\": {total_frames},\n")
                f.write(f"  \"fps\": {round(video_fps, 2)},\n")
                f.write("  \"frames_analysis\": [\n")
    
                frames_processed = 0
                last_log_t = time.time()
                for frame_num in range(1, total_frames + 1):
                    speakers = sorted(audio_timeline.get(frame_num, []))
                    is_speech = len(speakers) > 0
                    timecode = round((frame_num - 1) / video_fps, 3)
                    obj = {
                        "frame": frame_num,
                        "audio_info": {
                            "is_speech_present": is_speech,
                            "num_distinct_speakers_audio": len(speakers),
                            "active_speaker_labels": speakers,
                            "timecode_sec": timecode,
                        },
                    }
                    # Écriture streaming avec virgules correctes
                    if frame_num > 1:
                        f.write(",\n")
                    f.write(json.dumps(obj))
    
                    frames_processed += 1
                    if time.time() - last_log_t >= 2 or frames_processed == total_frames:
                        progress_percent = int((frames_processed / total_frames) * 100) if total_frames else 100
                        logging.info(
                            f"INTERNAL_PROGRESS: {frames_processed}/{total_frames} frames ({progress_percent}%) - {video_path.name}")
                        last_log_t = time.time()
    
                f.write("\n  ]\n")
                f.write("}\n")
    
            logging.info(f"Succès: analyse audio terminée pour {video_path.name}")
            return True

    except Exception as e:
        logging.error(f"Erreur inattendue lors de l'analyse de {video_path.name}: {e}", exc_info=True)
        return False


def main():
    parser = argparse.ArgumentParser(description="Analyse audio (diarisation) des vidéos.")
    parser.add_argument("--log_dir", type=str, required=True, help="Répertoire de logs")
    parser.add_argument("--hf_auth_token", type=str, help="Token d'authentification HuggingFace")
    parser.add_argument("--disable_gpu", action="store_true", help="Forcer l'utilisation CPU")
    parser.add_argument("--cpu_diarize_wav", type=str, help="Mode interne: diarisation CPU-only d'un WAV")
    parser.add_argument("--cpu_diarize_out", type=str, help="Mode interne: sortie JSON segments")
    args = parser.parse_args()

    log_dir_path = Path(args.log_dir)
    log_dir_path.mkdir(parents=True, exist_ok=True)
    global LOG_DIR_PATH
    LOG_DIR_PATH = log_dir_path
    log_file = log_dir_path / f"audio_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
    logging.getLogger().addHandler(file_handler)

    logging.info("--- Démarrage du script d'analyse audio (Diarisation) ---")

    _apply_audio_profile_from_env()

    try:
        os.environ.setdefault("PYTORCH_CUDA_ALLOC_CONF", "max_split_size_mb:32")

        env_disable_gpu = os.getenv("AUDIO_DISABLE_GPU", "0") == "1"
        use_cuda = torch.cuda.is_available() and not env_disable_gpu and not args.disable_gpu
        device = "cuda" if use_cuda else "cpu"
        if device == "cpu":
            cpu_workers_env = os.getenv("AUDIO_CPU_WORKERS")
            try:
                if cpu_workers_env:
                    n_threads = max(1, int(cpu_workers_env))
                    torch.set_num_threads(n_threads)
                    os.environ.setdefault("OMP_NUM_THREADS", str(n_threads))
                    os.environ.setdefault("MKL_NUM_THREADS", str(n_threads))
            except Exception:
                pass
        logging.info(f"Utilisation du device: {device}")

        hf_token = None
        if args.hf_auth_token:
            hf_token = args.hf_auth_token
            try:
                os.environ["HUGGINGFACE_HUB_TOKEN"] = args.hf_auth_token
            except Exception:
                pass
        else:
            hf_token = os.getenv("HUGGINGFACE_HUB_TOKEN") or os.getenv("HF_AUTH_TOKEN")

        if not hf_token:
            logging.critical(
                "Aucun token Hugging Face fourni. Passez --hf_auth_token ou définissez HUGGINGFACE_HUB_TOKEN/HF_AUTH_TOKEN."
            )
            sys.exit(1)
        try:
            from huggingface_hub import HfApi
            try:
                from huggingface_hub.hf_api import HfFolder
                HfFolder.save_token(hf_token)
            except Exception:
                pass
            _id = HfApi().whoami(token=hf_token)
            safe_tail = hf_token[-6:] if len(hf_token) >= 6 else "***"
            logging.info(
                "Authentifié sur Hugging Face (token tail=***%s) en tant que: %s",
                safe_tail,
                _id.get("name") or _id.get("email"),
            )
        except Exception as auth_e:
            logging.warning(
                "Impossible de valider le token Hugging Face: %s. On tente quand même le téléchargement.",
                auth_e,
            )

        pipeline = None
        try:
            pipeline = _load_pyannote_pipeline("pyannote/speaker-diarization-3.1", hf_token)
        except Exception as e_v3:
            logging.warning(f"Impossible de charger la pipeline v3.1: {e_v3}. Tentative avec v2...")
            try:
                pipeline = _load_pyannote_pipeline("pyannote/speaker-diarization", hf_token)
            except Exception as e_v2:
                logging.critical(
                    "Echec du chargement des pipelines pyannote (v3.1 et v2). "
                    "Le modèle peut être privé/gated. Assurez-vous que votre token HF a accès "
                    "(accepter les conditions sur la page du modèle) et réessayez."
                )
                logging.critical(f"Détails v3.1: {e_v3}")
                logging.critical(f"Détails v2: {e_v2}")
                sys.exit(1)
        optimal_tv_config = _load_optimal_tv_config()
        pyannote_batch_size = _get_pyannote_batch_size(device)
        if pyannote_batch_size is not None:
            for key in ("segmentation", "embedding"):
                section = optimal_tv_config.get(key)
                if not isinstance(section, dict):
                    optimal_tv_config[key] = {}
                optimal_tv_config[key]["batch_size"] = pyannote_batch_size

        if optimal_tv_config and hasattr(pipeline, "instantiate"):
            try:
                pipeline.instantiate(optimal_tv_config)
                if pyannote_batch_size is not None:
                    logging.info(
                        f"Pyannote configuration appliquée (optimal_tv_config + batch_size={pyannote_batch_size})"
                    )
                else:
                    logging.info("Pyannote configuration appliquée (optimal_tv_config)")
            except Exception as e:
                msg = str(e)
                if pyannote_batch_size is not None and "batch_size" in msg and "does not exist" in msg:
                    logging.info(
                        f"Pipeline.instantiate ne supporte pas batch_size (AUDIO_PYANNOTE_BATCH_SIZE={pyannote_batch_size}). "
                        "Le batch_size sera appliqué lors de l'appel diarization si possible."
                    )
                else:
                    logging.warning(f"Impossible d'appliquer optimal_tv_config.json via pipeline.instantiate: {e}")

                    if pyannote_batch_size is not None:
                        try:
                            pipeline.instantiate(
                                {
                                    "segmentation": {"batch_size": pyannote_batch_size},
                                    "embedding": {"batch_size": pyannote_batch_size},
                                }
                            )
                            logging.info(
                                "Fallback: configuration minimale appliquée (batch_size seulement) suite à un échec optimal_tv_config"
                            )
                        except Exception as fallback_e:
                            logging.warning(
                                "Fallback: impossible d'appliquer la configuration minimale (batch_size seulement): "
                                f"{fallback_e}"
                            )

        if _should_enable_amp(device):
            total_gb = _get_total_vram_gb()
            logging.info(f"AMP activé pour l'inférence (VRAM total ~{total_gb:.2f} GiB)")

        if args.cpu_diarize_wav:
            if not args.cpu_diarize_out:
                logging.critical("Mode cpu_diarize_wav: --cpu_diarize_out est requis")
                sys.exit(1)
            wav_path = Path(args.cpu_diarize_wav)
            out_path = Path(args.cpu_diarize_out)
            out_path.parent.mkdir(parents=True, exist_ok=True)
            
            diarization_kwargs = {"num_speakers": None}
            pyannote_batch_size = _get_pyannote_batch_size("cpu")
            if pyannote_batch_size is not None:
                diarization_kwargs["batch_size"] = pyannote_batch_size
                logging.info(f"CPU subprocess: batch_size={pyannote_batch_size}")
            
            with torch.inference_mode():
                try:
                    diarization = pipeline(str(wav_path), **diarization_kwargs)
                except TypeError as e:
                    if "batch_size" in str(e):
                        diarization_kwargs.pop("batch_size", None)
                        diarization = pipeline(str(wav_path), **diarization_kwargs)
                    else:
                        raise
            
            segments = [
                {"start": float(t.start), "end": float(t.end), "speaker": str(spk)}
                for t, _, spk in diarization.itertracks(yield_label=True)
            ]
            logging.info(f"CPU subprocess: {len(segments)} segment(s) extrait(s)")
            with open(out_path, "w", encoding="utf-8") as f:
                json.dump(segments, f)
            return

        if pipeline is None:
            logging.critical(
                "Impossible de charger la pipeline pyannote (pipeline=None). Le modèle peut être privé/gated. "
                "Vérifiez votre token Hugging Face (HUGGINGFACE_HUB_TOKEN) et acceptez les conditions du modèle."
            )
            sys.exit(1)

        try:
            if hasattr(pipeline, "to"):
                pipeline.to(torch.device(device))
                logging.info(f"Pipeline de diarisation chargée avec succès sur {device}.")
            else:
                logging.info("Pipeline pyannote ne supporte pas .to(); continuation sans déplacement explicite de device.")
        except RuntimeError as e:
            if "NVIDIA driver" in str(e) or "CUDA" in str(e):
                logging.warning(f"GPU incompatible ({e}), fallback sur CPU.")
                device = "cpu"
                if hasattr(pipeline, "to"):
                    pipeline.to(torch.device("cpu"))
                logging.info("Pipeline de diarisation chargée avec succès sur CPU (fallback).")
            else:
                raise
    except ImportError as e:
        logging.critical(f"Erreur lors du chargement de la pipeline Pyannote: {e}")
        sys.exit(1)

    videos = find_videos_for_audio_analysis()
    total_videos = len(videos)
    logging.info(f"TOTAL_AUDIO_TO_ANALYZE: {total_videos}")

    if total_videos == 0:
        logging.info("Aucune nouvelle vidéo à analyser.")
        return

    successful_count = 0
    for i, video_path in enumerate(videos):
        logging.info(f"ANALYZING_AUDIO: {i + 1}/{total_videos}: {video_path.name}")

        success = analyze_audio_file(video_path, pipeline, hf_token, device)
        if success:
            successful_count += 1

        try:
            if device == "cuda" and torch.cuda.is_available():
                torch.cuda.empty_cache()
        except Exception:
            pass

    logging.info("--- Analyse audio terminée ---")
    logging.info(f"Résumé: {successful_count}/{total_videos} analyse(s) réussie(s).")

    if successful_count < total_videos:
        # Permettre un succès partiel si demandé (utile pour pipelines tolérantes)
        allow_partial = os.getenv("AUDIO_PARTIAL_SUCCESS_OK", "0") == "1"
        if allow_partial and successful_count > 0:
            logging.warning(
                f"Partial success autorisé: {successful_count}/{total_videos} analyses ont réussi. Code de sortie 0."
            )
            return
        sys.exit(1)


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        logging.critical(f"Erreur critique non gérée: {e}", exc_info=True)
        sys.exit(1)
```

## File: workflow_scripts/step5/face_engines.py
```python
import logging
import os
import threading
import time
from pathlib import Path
from typing import Optional

import cv2
import numpy as np

# Load .env file BEFORE reading any environment variables (critical for multiprocessing workers)
try:
    from dotenv import load_dotenv
    _env_path = Path(__file__).resolve().parent.parent.parent / '.env'
    if _env_path.exists():
        load_dotenv(_env_path)
except ImportError:
    pass  # dotenv not available, rely on system env vars

logger = logging.getLogger(__name__)

ARKIT_52_BLENDSHAPE_NAMES = [
    "browDownLeft",
    "browDownRight",
    "browInnerUp",
    "browOuterUpLeft",
    "browOuterUpRight",
    "cheekPuff",
    "cheekSquintLeft",
    "cheekSquintRight",
    "eyeBlinkLeft",
    "eyeBlinkRight",
    "eyeLookDownLeft",
    "eyeLookDownRight",
    "eyeLookInLeft",
    "eyeLookInRight",
    "eyeLookOutLeft",
    "eyeLookOutRight",
    "eyeLookUpLeft",
    "eyeLookUpRight",
    "eyeSquintLeft",
    "eyeSquintRight",
    "eyeWideLeft",
    "eyeWideRight",
    "jawForward",
    "jawLeft",
    "jawOpen",
    "jawRight",
    "mouthClose",
    "mouthDimpleLeft",
    "mouthDimpleRight",
    "mouthFrownLeft",
    "mouthFrownRight",
    "mouthFunnel",
    "mouthLeft",
    "mouthLowerDownLeft",
    "mouthLowerDownRight",
    "mouthPressLeft",
    "mouthPressRight",
    "mouthPucker",
    "mouthRight",
    "mouthRollLower",
    "mouthRollUpper",
    "mouthShrugLower",
    "mouthShrugUpper",
    "mouthSmileLeft",
    "mouthSmileRight",
    "mouthStretchLeft",
    "mouthStretchRight",
    "mouthUpperUpLeft",
    "mouthUpperUpRight",
    "noseSneerLeft",
    "noseSneerRight",
    "tongueOut",
]


def _parse_optional_positive_int(raw: Optional[str]) -> Optional[int]:
    if raw is None:
        return None
    raw = raw.strip()
    if not raw:
        return None
    try:
        value = int(raw)
    except Exception:
        return None
    if value <= 0:
        return None
    return value


def _apply_jawopen_scale(blendshapes: Optional[dict], scale: float) -> Optional[dict]:
    if not blendshapes or not isinstance(blendshapes, dict):
        return blendshapes
    try:
        jaw_open_raw = blendshapes.get("jawOpen")
        if jaw_open_raw is None:
            return blendshapes
        jaw_open_scaled = float(jaw_open_raw) * float(scale)
        jaw_open_scaled = float(np.clip(jaw_open_scaled, 0.0, 1.0))
        out = dict(blendshapes)
        out["jawOpen"] = jaw_open_scaled
        return out
    except Exception:
        return blendshapes


def _openseeface_logit_arr(p: np.ndarray, factor: float = 16.0) -> np.ndarray:
    p = np.clip(p, 0.0000001, 0.9999999)
    return np.log(p / (1 - p)) / float(factor)


class OpenSeeFaceEngine:
    def __init__(
        self,
        models_dir: Optional[str] = None,
        model_id: Optional[int] = None,
        detection_threshold: Optional[float] = None,
        max_faces: Optional[int] = None,
        use_gpu: bool = False,
    ):
        self._lock = threading.Lock()

        try:
            import onnxruntime as ort
        except ImportError as e:
            raise RuntimeError(
                "OpenSeeFace engine requires onnxruntime. Install it in tracking_env."
            ) from e

        self._ort = ort
        self._use_gpu = use_gpu
        self._frame_counter = 0
        self._last_detections = []

        self._enable_profiling = os.environ.get("STEP5_ENABLE_PROFILING", "0").strip().lower() in {
            "1",
            "true",
            "yes",
        }
        self._profiling_stats = {
            "resize_total": 0.0,
            "detect_total": 0.0,
            "landmarks_total": 0.0,
            "post_total": 0.0,
            "frame_count": 0,
        }

        env_models_dir = os.environ.get("STEP5_OPENSEEFACE_MODELS_DIR")
        models_dir_raw = models_dir or (env_models_dir.strip() if env_models_dir else "")
        if models_dir_raw:
            self._models_dir_explicit = True
            models_dir_candidate = Path(models_dir_raw)
            if models_dir_candidate.is_absolute() and models_dir_candidate.exists():
                self._models_dir = models_dir_candidate
            elif models_dir_candidate.exists():
                self._models_dir = models_dir_candidate.resolve()
            else:
                step5_dir = Path(__file__).resolve().parent
                project_root = step5_dir.parent.parent
                resolved_models_dir = None
                for c in [project_root / models_dir_candidate, step5_dir / models_dir_candidate]:
                    if c.exists():
                        resolved_models_dir = c
                        break
                self._models_dir = (resolved_models_dir or models_dir_candidate).resolve()
        else:
            self._models_dir_explicit = False
            step5_dir = Path(__file__).resolve().parent
            self._models_dir = step5_dir / "models" / "engines" / "openseeface"

        env_model_id = os.environ.get("STEP5_OPENSEEFACE_MODEL_ID")
        self._model_id = int(model_id if model_id is not None else (env_model_id or "1"))

        env_detect_every = os.environ.get("STEP5_OPENSEEFACE_DETECT_EVERY_N")
        env_blendshapes_throttle = os.environ.get("STEP5_BLENDSHAPES_THROTTLE_N")
        detect_every_raw = env_detect_every if (env_detect_every is not None and env_detect_every.strip()) else env_blendshapes_throttle
        self._detect_every_n = max(1, int(detect_every_raw or "1"))

        env_max_faces = os.environ.get("STEP5_OPENSEEFACE_MAX_FACES")
        self._max_faces = max(1, int(max_faces if max_faces is not None else (env_max_faces or "1")))

        env_detection_threshold = os.environ.get("STEP5_OPENSEEFACE_DETECTION_THRESHOLD")
        self._detection_threshold = float(
            detection_threshold if detection_threshold is not None else (env_detection_threshold or "0.6")
        )

        env_jaw_scale = os.environ.get("STEP5_OPENSEEFACE_JAWOPEN_SCALE")
        self._jaw_open_scale = float(env_jaw_scale or "1.0")

        try:
            cv2.setNumThreads(1)
        except Exception:
            pass

        env_openseeface_max_width = os.environ.get("STEP5_OPENSEEFACE_MAX_WIDTH")
        self._max_detection_width = max(
            1,
            int(
                (env_openseeface_max_width.strip() if env_openseeface_max_width else "")
                or os.environ.get("STEP5_YUNET_MAX_WIDTH", "640")
            ),
        )

        detection_override_path = os.environ.get("STEP5_OPENSEEFACE_DETECTION_MODEL_PATH")
        landmark_override_path = os.environ.get("STEP5_OPENSEEFACE_LANDMARK_MODEL_PATH")

        self._detection_model_path = self._resolve_model_path(
            detection_override_path,
            default_filename="mnv3_detection_opt.onnx",
        )
        self._landmark_model_path = self._resolve_model_path(
            landmark_override_path,
            default_filename=f"lm_model{self._model_id}_opt.onnx",
        )

        if not self._detection_model_path:
            raise RuntimeError(
                "OpenSeeFace detection model not found. Set STEP5_OPENSEEFACE_DETECTION_MODEL_PATH or "
                "STEP5_OPENSEEFACE_MODELS_DIR with mnv3_detection_opt.onnx."
            )
        if not self._landmark_model_path:
            raise RuntimeError(
                "OpenSeeFace landmark model not found. Set STEP5_OPENSEEFACE_LANDMARK_MODEL_PATH or "
                "STEP5_OPENSEEFACE_MODELS_DIR with lm_model{N}_opt.onnx."
            )

        intra_threads = int(os.environ.get("STEP5_ONNX_INTRA_OP_THREADS", "2"))
        inter_threads = int(os.environ.get("STEP5_ONNX_INTER_OP_THREADS", "1"))

        # Configure ONNX providers (GPU or CPU)
        available_providers = ort.get_available_providers()
        logger.info(f"[OpenSeeFace] Available ONNXRuntime providers: {available_providers}")

        if use_gpu:
            providers = ["CUDAExecutionProvider"]
            logger.info("[OpenSeeFace] Attempting to use CUDA provider for GPU inference (no CPU fallback)")
        else:
            providers = ["CPUExecutionProvider"]

        detection_sess_options = ort.SessionOptions()
        detection_sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
        if use_gpu:
            try:
                detection_sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_BASIC
                detection_sess_options.add_session_config_entry("session.disable_contrib_ops", "1")
                detection_sess_options.add_session_config_entry("session.disable_prepacking", "1")
                detection_sess_options.add_session_config_entry("session.disable_aot_function_inlining", "1")
                logger.info("[OpenSeeFace] Disabled contrib/fused ops to improve CUDA compatibility")
            except Exception as cfg_err:
                logger.warning("[OpenSeeFace] Failed to set session config entries: %s", cfg_err)
        detection_sess_options.intra_op_num_threads = max(1, int(intra_threads))
        detection_sess_options.inter_op_num_threads = max(1, int(inter_threads))
        detection_sess_options.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL

        def _create_session(model_path: Path, sess_options: "ort.SessionOptions", provider_list: list[str]):
            return ort.InferenceSession(
                str(model_path),
                sess_options=sess_options,
                providers=provider_list,
            )

        try:
            self._detection_session = _create_session(self._detection_model_path, detection_sess_options, providers)
        except Exception as session_error:
            logger.exception(
                "[OpenSeeFace] Failed to create detection session with providers %s: %s",
                providers,
                session_error,
            )
            if use_gpu:
                logger.warning("[OpenSeeFace] Falling back to CPUExecutionProvider due to CUDA initialization failure")
                providers = ["CPUExecutionProvider"]
                self._use_gpu = False
                self._detection_session = _create_session(self._detection_model_path, detection_sess_options, providers)
            else:
                raise
        self._detection_input_name = self._detection_session.get_inputs()[0].name
        
        # Log active provider
        active_provider = self._detection_session.get_providers()[0]
        logger.info(f"[OpenSeeFace] Detection session using provider: {active_provider}")
        if use_gpu and active_provider != "CUDAExecutionProvider":
            logger.error("[OpenSeeFace] CUDA provider requested but not active despite GPU mode (should have failed)")

        landmark_sess_options = ort.SessionOptions()
        landmark_sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
        landmark_sess_options.intra_op_num_threads = max(1, int(intra_threads))
        landmark_sess_options.inter_op_num_threads = max(1, int(inter_threads))
        landmark_sess_options.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL

        try:
            self._landmark_session = ort.InferenceSession(
                str(self._landmark_model_path),
                sess_options=landmark_sess_options,
                providers=providers,
            )
        except Exception as session_error:
            logger.exception(
                "[OpenSeeFace] Failed to create landmark session with providers %s: %s",
                providers,
                session_error,
            )
            raise
        self._landmark_input_name = self._landmark_session.get_inputs()[0].name
        
        active_provider_lm = self._landmark_session.get_providers()[0]
        logger.info(f"[OpenSeeFace] Landmark session using provider: {active_provider_lm}")

        self._mean = np.float32(np.array([0.485, 0.456, 0.406]))
        self._std = np.float32(np.array([0.229, 0.224, 0.225]))
        mean = (self._mean / self._std)
        std = (self._std * 255.0)
        self._mean = -mean
        self._std = 1.0 / std

        self._mean_224 = np.tile(self._mean, [224, 224, 1])
        self._std_224 = np.tile(self._std, [224, 224, 1])

        self._res = 224.0
        self._out_res = 27.0
        self._out_res_i = int(self._out_res) + 1
        self._logit_factor = 16.0

    def _resolve_model_path(self, override_path: Optional[str], default_filename: str) -> Optional[Path]:
        if override_path:
            candidate = Path(override_path)
            if candidate.is_absolute() and candidate.exists():
                return candidate
            if candidate.exists():
                return candidate.resolve()

            step5_dir = Path(__file__).resolve().parent
            project_root = step5_dir.parent.parent
            for c in [project_root / candidate, step5_dir / candidate]:
                if c.exists():
                    return c

        candidate = self._models_dir / default_filename
        if candidate.exists():
            return candidate

        if getattr(self, "_models_dir_explicit", False):
            return None

        step5_dir = Path(__file__).resolve().parent
        fallback_candidates = [
            step5_dir / "models" / "engines" / "openseeface" / default_filename,
            step5_dir / "models" / default_filename,
        ]
        for fallback in fallback_candidates:
            if fallback.exists():
                return fallback

        return None

    def _preprocess_detection(self, frame_bgr: np.ndarray) -> np.ndarray:
        resized = cv2.resize(frame_bgr, (224, 224), interpolation=cv2.INTER_LINEAR)
        rgb = resized[:, :, ::-1].astype(np.float32)
        im = rgb * self._std_224 + self._mean_224
        im = np.expand_dims(im, 0)
        im = np.transpose(im, (0, 3, 1, 2))
        return np.ascontiguousarray(im)

    def _detect_faces(self, frame_bgr: np.ndarray) -> list[tuple[float, float, float, float, float]]:
        im = self._preprocess_detection(frame_bgr)
        outputs = self._detection_session.run([], {self._detection_input_name: im})
        if not outputs or len(outputs) < 2:
            return []
        out_map = np.array(outputs[0])
        maxpool = np.array(outputs[1])
        if out_map.ndim != 4 or maxpool.ndim != 4:
            return []
        out_map[0, 0, out_map[0, 0] != maxpool[0, 0]] = 0

        detections = np.flip(np.argsort(out_map[0, 0].flatten()))
        results = []
        for det in detections[0 : self._max_faces]:
            y = int(det // 56)
            x = int(det % 56)
            conf = float(out_map[0, 0, y, x])
            radius = float(out_map[0, 1, y, x]) * 112.0
            x_px = float(x * 4)
            y_px = float(y * 4)
            if conf < self._detection_threshold:
                break
            results.append((x_px - radius, y_px - radius, 2 * radius, 2 * radius, conf))

        if not results:
            return []

        results_np = np.array([r[:4] for r in results], dtype=np.float32)
        results_np[:, [0, 2]] *= (frame_bgr.shape[1] / 224.0)
        results_np[:, [1, 3]] *= (frame_bgr.shape[0] / 224.0)
        scaled = []
        for i, (x, y, w, h) in enumerate(results_np.tolist()):
            scaled.append((x, y, w, h, float(results[i][4])))
        return scaled

    def _preprocess_landmarks(
        self,
        frame_bgr: np.ndarray,
        crop: tuple[int, int, int, int],
    ) -> tuple[np.ndarray, tuple[int, int, float, float]]:
        x1, y1, x2, y2 = crop
        im = frame_bgr[y1:y2, x1:x2, ::-1].astype(np.float32)
        im = cv2.resize(im, (int(self._res), int(self._res)), interpolation=cv2.INTER_LINEAR)
        im = im * np.tile(self._std, [int(self._res), int(self._res), 1]) + np.tile(
            self._mean, [int(self._res), int(self._res), 1]
        )
        im = np.expand_dims(im, 0)
        im = np.transpose(im, (0, 3, 1, 2))
        im = np.ascontiguousarray(im)

        scale_x = float(x2 - x1) / float(self._res)
        scale_y = float(y2 - y1) / float(self._res)
        return im, (x1, y1, scale_x, scale_y)

    def _decode_landmarks(self, tensor: np.ndarray, crop_info: tuple[int, int, float, float]) -> tuple[float, np.ndarray]:
        crop_x1, crop_y1, scale_x, scale_y = crop_info
        res = self._res - 1

        c0, c1, c2 = 66, 132, 198
        t_main = tensor[0:c0].reshape((c0, self._out_res_i * self._out_res_i))
        t_m = t_main.argmax(1)
        indices = np.expand_dims(t_m, 1)
        t_conf = np.take_along_axis(t_main, indices, 1).reshape((c0,))

        t_off_x = np.take_along_axis(
            tensor[c0:c1].reshape((c0, self._out_res_i * self._out_res_i)),
            indices,
            1,
        ).reshape((c0,))
        t_off_y = np.take_along_axis(
            tensor[c1:c2].reshape((c0, self._out_res_i * self._out_res_i)),
            indices,
            1,
        ).reshape((c0,))
        t_off_x = res * _openseeface_logit_arr(t_off_x, self._logit_factor)
        t_off_y = res * _openseeface_logit_arr(t_off_y, self._logit_factor)

        t_x = crop_y1 + scale_y * (res * np.floor(t_m / self._out_res_i) / self._out_res + t_off_x)
        t_y = crop_x1 + scale_x * (res * np.floor(np.mod(t_m, self._out_res_i)) / self._out_res + t_off_y)
        avg_conf = float(np.average(t_conf))
        lms_yx_conf = np.stack([t_x, t_y, t_conf], 1)
        if np.isnan(lms_yx_conf).any():
            lms_yx_conf[np.isnan(lms_yx_conf).any(axis=1)] = np.array([0.0, 0.0, 0.0], dtype=np.float32)
        return avg_conf, lms_yx_conf

    def _compute_jaw_open(self, landmarks_xy: np.ndarray) -> float:
        if landmarks_xy.shape[0] < 66:
            return 0.0

        norm_distance_y = float(
            np.mean(
                [
                    landmarks_xy[27, 1] - landmarks_xy[28, 1],
                    landmarks_xy[28, 1] - landmarks_xy[29, 1],
                    landmarks_xy[29, 1] - landmarks_xy[30, 1],
                ]
            )
        )
        denom = max(abs(norm_distance_y), 1e-6)

        upper = float(np.mean(landmarks_xy[[59, 60, 61], 1], axis=0))
        lower = float(np.mean(landmarks_xy[[63, 64, 65], 1], axis=0))
        mouth_open_ratio = abs(upper - lower) / denom
        return float(np.clip(mouth_open_ratio * self._jaw_open_scale, 0.0, 1.0))

    def _build_arkit_blendshapes(self, jaw_open: float) -> dict:
        out = {}
        for name in ARKIT_52_BLENDSHAPE_NAMES:
            out[name] = 0.0
        out["jawOpen"] = float(jaw_open)
        return out

    def detect(self, frame_bgr):
        self._frame_counter += 1
        if (self._frame_counter % self._detect_every_n) != 0:
            return list(self._last_detections)

        if frame_bgr is None or getattr(frame_bgr, "shape", None) is None:
            self._last_detections = []
            return []

        orig_h, orig_w = frame_bgr.shape[:2]

        t_resize_start = time.perf_counter() if self._enable_profiling else 0.0
        work_frame = frame_bgr
        scale_x = 1.0
        scale_y = 1.0

        if orig_w > self._max_detection_width:
            detect_w = self._max_detection_width
            scale_factor = detect_w / float(orig_w)
            detect_h = max(1, int(orig_h * scale_factor))
            work_frame = cv2.resize(frame_bgr, (detect_w, detect_h), interpolation=cv2.INTER_LINEAR)
            scale_x = orig_w / float(detect_w)
            scale_y = orig_h / float(detect_h)

        if self._enable_profiling:
            self._profiling_stats["resize_total"] += time.perf_counter() - t_resize_start

        work_h, work_w = work_frame.shape[:2]

        t_detect_start = time.perf_counter() if self._enable_profiling else 0.0
        faces = self._detect_faces(work_frame)
        if self._enable_profiling:
            self._profiling_stats["detect_total"] += time.perf_counter() - t_detect_start
        if not faces:
            self._last_detections = []
            return []

        detections = []
        for (x, y, bw, bh, conf) in faces:
            t_landmarks_start = time.perf_counter() if self._enable_profiling else 0.0
            x1 = int(max(0, x - 0.1 * bw))
            y1 = int(max(0, y - 0.125 * bh))
            x2 = int(min(work_w, x + bw + 0.1 * bw))
            y2 = int(min(work_h, y + bh + 0.125 * bh))
            if x2 - x1 < 4 or y2 - y1 < 4:
                continue

            with self._lock:
                input_tensor, crop_info = self._preprocess_landmarks(work_frame, (x1, y1, x2, y2))
                out = self._landmark_session.run([], {self._landmark_input_name: input_tensor})[0]

            if out is None:
                continue

            if out.ndim == 4:
                tensor = out[0]
            else:
                tensor = out

            avg_conf, lms_yx_conf = self._decode_landmarks(np.asarray(tensor), crop_info)

            if self._enable_profiling:
                self._profiling_stats["landmarks_total"] += time.perf_counter() - t_landmarks_start

            landmarks_xy = np.stack([lms_yx_conf[:, 1], lms_yx_conf[:, 0]], axis=1)

            t_post_start = time.perf_counter() if self._enable_profiling else 0.0
            if scale_x != 1.0 or scale_y != 1.0:
                landmarks_xy = landmarks_xy.astype(np.float32, copy=True)
                landmarks_xy[:, 0] *= float(scale_x)
                landmarks_xy[:, 1] *= float(scale_y)
                logger.debug(f"OpenSeeFace upscale: landmarks rescaled by x={scale_x:.2f}, y={scale_y:.2f}")

            x_min = int(max(0, np.min(landmarks_xy[:, 0])))
            y_min = int(max(0, np.min(landmarks_xy[:, 1])))
            x_max = int(min(orig_w - 1, np.max(landmarks_xy[:, 0])))
            y_max = int(min(orig_h - 1, np.max(landmarks_xy[:, 1])))
            bbox_w = max(0, x_max - x_min)
            bbox_h = max(0, y_max - y_min)

            centroid = (x_min + (bbox_w // 2), y_min + (bbox_h // 2))
            jaw_open = self._compute_jaw_open(landmarks_xy)

            landmarks_xyz = np.zeros((int(lms_yx_conf.shape[0]), 3), dtype=np.float32)
            landmarks_xyz[:, 0] = landmarks_xy[:, 0]
            landmarks_xyz[:, 1] = landmarks_xy[:, 1]

            detections.append(
                {
                    "bbox": (x_min, y_min, bbox_w, bbox_h),
                    "centroid": centroid,
                    "source_detector": "face_landmarker",
                    "label": "face",
                    "confidence": float(avg_conf if avg_conf > 0 else conf),
                    "landmarks": landmarks_xyz.tolist(),
                    "blendshapes": self._build_arkit_blendshapes(jaw_open),
                }
            )

            if self._enable_profiling:
                self._profiling_stats["post_total"] += time.perf_counter() - t_post_start

        self._last_detections = detections

        if self._enable_profiling:
            self._profiling_stats["frame_count"] += 1
            if (self._profiling_stats["frame_count"] % 20) == 0:
                self._log_profiling_stats()

        return detections

    def _log_profiling_stats(self) -> None:
        frame_count = int(self._profiling_stats.get("frame_count", 0) or 0)
        if frame_count <= 0:
            return

        resize_ms = (self._profiling_stats["resize_total"] / frame_count) * 1000.0
        detect_ms = (self._profiling_stats["detect_total"] / frame_count) * 1000.0
        landmarks_ms = (self._profiling_stats["landmarks_total"] / frame_count) * 1000.0
        post_ms = (self._profiling_stats["post_total"] / frame_count) * 1000.0

        logger.info(
            "[PROFILING] OpenSeeFace after %s frames: resize=%.2fms/frame, detect=%.2fms/frame, "
            "landmarks=%.2fms/frame, post=%.2fms/frame",
            frame_count,
            resize_ms,
            detect_ms,
            landmarks_ms,
            post_ms,
        )


class InsightFaceEngine:
    def __init__(
        self,
        model_name: Optional[str] = None,
        det_size: Optional[int] = None,
        use_gpu: bool = False,
    ):
        if not use_gpu:
            raise RuntimeError(
                "InsightFace engine is GPU-only. Set STEP5_ENABLE_GPU=1, include 'insightface' in STEP5_GPU_ENGINES, "
                "and run with use_gpu=True."
            )

        self._lock = threading.Lock()
        self._use_gpu = True

        try:
            import onnxruntime as ort
        except ImportError as e:
            raise RuntimeError(
                "InsightFace engine requires onnxruntime in insightface_env. Install it (and onnxruntime-gpu) in that venv."
            ) from e

        available_providers = ort.get_available_providers()
        logger.info(f"[InsightFace] Available ONNXRuntime providers: {available_providers}")
        if "CUDAExecutionProvider" not in available_providers:
            raise RuntimeError(
                "InsightFace engine requires CUDAExecutionProvider (onnxruntime-gpu). "
                "Install onnxruntime-gpu in insightface_env and ensure CUDA libs are available."
            )

        try:
            from insightface.app import FaceAnalysis
        except ImportError as e:
            raise RuntimeError(
                "InsightFace engine requires the 'insightface' Python package inside insightface_env."
            ) from e

        self._enable_profiling = os.environ.get("STEP5_ENABLE_PROFILING", "0").strip().lower() in {
            "1",
            "true",
            "yes",
        }
        self._profiling_stats = {
            "resize_total": 0.0,
            "detect_total": 0.0,
            "post_total": 0.0,
            "frame_count": 0,
        }

        env_max_faces = os.environ.get("STEP5_INSIGHTFACE_MAX_FACES")
        self._max_faces = _parse_optional_positive_int(env_max_faces)

        env_max_width = os.environ.get("STEP5_INSIGHTFACE_MAX_WIDTH")
        if env_max_width is None or env_max_width.strip() == "":
            env_max_width = os.environ.get("STEP5_YUNET_MAX_WIDTH", "1280")
        self._max_detection_width = max(1, int(str(env_max_width).strip() or "1280"))

        env_detect_every = os.environ.get("STEP5_INSIGHTFACE_DETECT_EVERY_N")
        if env_detect_every is None or env_detect_every.strip() == "":
            env_detect_every = os.environ.get("STEP5_BLENDSHAPES_THROTTLE_N", "1")
        self._detect_every_n = max(1, int(str(env_detect_every).strip() or "1"))

        self._jaw_open_scale = float(os.environ.get("STEP5_INSIGHTFACE_JAWOPEN_SCALE", "1.0"))

        env_model_name = os.environ.get("STEP5_INSIGHTFACE_MODEL_NAME")
        self._model_name = (model_name or (env_model_name.strip() if env_model_name else "") or "antelopev2")

        env_det_size = os.environ.get("STEP5_INSIGHTFACE_DET_SIZE")
        det_size_value = det_size
        if det_size_value is None:
            try:
                det_size_value = int(str(env_det_size).strip()) if env_det_size else 640
            except Exception:
                det_size_value = 640
        self._det_size = max(64, int(det_size_value))

        ctx_id_raw = os.environ.get("STEP5_INSIGHTFACE_CTX_ID")
        try:
            ctx_id = int(str(ctx_id_raw).strip()) if ctx_id_raw else 0
        except Exception:
            ctx_id = 0
        if ctx_id < 0:
            ctx_id = 0

        providers = ["CUDAExecutionProvider"]
        insightface_root_raw = os.environ.get("INSIGHTFACE_HOME", "").strip()
        insightface_root = str(Path(insightface_root_raw or "~/.insightface").expanduser())

        allowed_modules_env = os.environ.get("STEP5_INSIGHTFACE_ALLOWED_MODULES", "").strip()
        allowed_modules = None
        if allowed_modules_env:
            allowed_modules = [m.strip() for m in allowed_modules_env.split(",") if m.strip()]
            if "detection" not in allowed_modules:
                raise RuntimeError(
                    "STEP5_INSIGHTFACE_ALLOWED_MODULES must include 'detection' (required by FaceAnalysis)."
                )

        try:
            self._app = FaceAnalysis(
                name=self._model_name,
                root=insightface_root,
                providers=providers,
                allowed_modules=allowed_modules,
            )
        except FileExistsError as e:
            model_dir = Path(insightface_root) / "models" / self._model_name
            quarantine_suffix = f"corrupt_{int(time.time())}"
            quarantine_dir = model_dir.with_name(f"{model_dir.name}.{quarantine_suffix}")
            try:
                if model_dir.exists():
                    model_dir.rename(quarantine_dir)
                    logger.warning(
                        "[InsightFace] Model cache directory already exists but download attempted; "
                        "quarantined %s -> %s and retrying initialization.",
                        model_dir,
                        quarantine_dir,
                    )
            except Exception as rename_err:
                raise RuntimeError(
                    f"InsightFace model cache appears corrupted at {model_dir}. "
                    f"Automatic quarantine failed ({rename_err}). "
                    "Please remove or rename the directory and retry."
                ) from e

            self._app = FaceAnalysis(
                name=self._model_name,
                root=insightface_root,
                providers=providers,
                allowed_modules=allowed_modules,
            )
        self._app.prepare(ctx_id=ctx_id, det_size=(self._det_size, self._det_size))

        self._frame_counter = 0
        self._last_detections = []

        logger.info(
            f"[InsightFace] Initialized model_name={self._model_name}, det_size={self._det_size}, "
            f"max_width={self._max_detection_width}, max_faces={self._max_faces}, detect_every_n={self._detect_every_n}"
        )

    def _build_arkit_blendshapes(self, jaw_open: float) -> dict:
        out = {}
        for name in ARKIT_52_BLENDSHAPE_NAMES:
            out[name] = 0.0
        out["jawOpen"] = float(jaw_open)
        return out

    def _compute_jaw_open_from_dlib68(self, landmarks_68: np.ndarray) -> float:
        lm = np.asarray(landmarks_68, dtype=np.float32)
        if lm.ndim != 2 or lm.shape[0] < 68:
            return 0.0
        try:
            upper_inner = float(np.mean(lm[60:65, 1], axis=0))
            lower_inner = float(np.mean(lm[65:68, 1], axis=0))
            mouth_open = abs(lower_inner - upper_inner)

            nose_y = float(lm[33, 1])
            chin_y = float(lm[8, 1])
            denom = max(abs(chin_y - nose_y), 1e-6)
            ratio = (mouth_open / denom) * float(self._jaw_open_scale)
            return float(np.clip(ratio, 0.0, 1.0))
        except Exception:
            return 0.0

    def _get_face_landmarks_68(self, face_obj) -> Optional[np.ndarray]:
        candidates = [
            "landmark_3d_68",
            "landmark_2d_68",
        ]
        for attr in candidates:
            try:
                val = getattr(face_obj, attr, None)
            except Exception:
                val = None
            if val is None:
                try:
                    val = face_obj.get(attr) if hasattr(face_obj, "get") else None
                except Exception:
                    val = None
            if val is None:
                continue

            lm = np.asarray(val)
            if lm.ndim != 2 or lm.shape[0] < 68:
                continue
            if lm.shape[1] >= 2:
                return lm[:, :2].astype(np.float32)
        return None

    def detect(self, frame_bgr):
        self._frame_counter += 1
        if (self._frame_counter % self._detect_every_n) != 0:
            return list(self._last_detections)

        if frame_bgr is None or getattr(frame_bgr, "shape", None) is None:
            self._last_detections = []
            return []

        orig_h, orig_w = frame_bgr.shape[:2]
        work_frame = frame_bgr
        scale_x = 1.0
        scale_y = 1.0

        t_resize_start = time.perf_counter() if self._enable_profiling else 0.0
        if orig_w > self._max_detection_width:
            detect_w = self._max_detection_width
            scale_factor = detect_w / float(orig_w)
            detect_h = max(1, int(orig_h * scale_factor))
            work_frame = cv2.resize(frame_bgr, (detect_w, detect_h), interpolation=cv2.INTER_LINEAR)
            scale_x = orig_w / float(detect_w)
            scale_y = orig_h / float(detect_h)
        if self._enable_profiling:
            self._profiling_stats["resize_total"] += time.perf_counter() - t_resize_start

        t_detect_start = time.perf_counter() if self._enable_profiling else 0.0
        try:
            with self._lock:
                faces = self._app.get(work_frame)
        except Exception as e:
            logger.warning("[InsightFace] Detection failed: %s", e)
            self._last_detections = []
            return []
        if self._enable_profiling:
            self._profiling_stats["detect_total"] += time.perf_counter() - t_detect_start

        if not faces:
            self._last_detections = []
            return []

        if self._max_faces is not None:
            faces = faces[: self._max_faces]

        detections = []
        t_post_start = time.perf_counter() if self._enable_profiling else 0.0
        for face in faces:
            bbox = None
            try:
                bbox = getattr(face, "bbox", None)
            except Exception:
                bbox = None
            if bbox is None:
                try:
                    bbox = face.get("bbox") if hasattr(face, "get") else None
                except Exception:
                    bbox = None
            if bbox is None:
                continue

            bbox_arr = np.asarray(bbox, dtype=np.float32).reshape(-1)
            if bbox_arr.size < 4:
                continue

            x1, y1, x2, y2 = [float(v) for v in bbox_arr[:4]]
            if scale_x != 1.0 or scale_y != 1.0:
                x1 *= float(scale_x)
                x2 *= float(scale_x)
                y1 *= float(scale_y)
                y2 *= float(scale_y)

            x1_i = max(0, int(x1))
            y1_i = max(0, int(y1))
            x2_i = min(int(orig_w), int(x2))
            y2_i = min(int(orig_h), int(y2))
            if x2_i <= x1_i or y2_i <= y1_i:
                continue

            bbox_w = int(x2_i - x1_i)
            bbox_h = int(y2_i - y1_i)
            centroid = (x1_i + (bbox_w // 2), y1_i + (bbox_h // 2))

            try:
                score = float(getattr(face, "det_score", 1.0))
            except Exception:
                score = 1.0

            landmarks_68 = self._get_face_landmarks_68(face)
            if landmarks_68 is not None and (scale_x != 1.0 or scale_y != 1.0):
                landmarks_68 = landmarks_68.astype(np.float32, copy=True)
                landmarks_68[:, 0] *= float(scale_x)
                landmarks_68[:, 1] *= float(scale_y)

            jaw_open = self._compute_jaw_open_from_dlib68(landmarks_68) if landmarks_68 is not None else 0.0
            blendshapes = self._build_arkit_blendshapes(jaw_open)

            detections.append(
                {
                    "bbox": (x1_i, y1_i, bbox_w, bbox_h),
                    "centroid": centroid,
                    "source_detector": "face_landmarker",
                    "label": "face",
                    "confidence": score,
                    "landmarks": landmarks_68.tolist() if landmarks_68 is not None else [],
                    "blendshapes": blendshapes,
                }
            )

        if self._enable_profiling:
            self._profiling_stats["post_total"] += time.perf_counter() - t_post_start
            self._profiling_stats["frame_count"] += 1
            if (self._profiling_stats["frame_count"] % 20) == 0:
                frame_count = int(self._profiling_stats.get("frame_count", 0) or 0)
                if frame_count > 0:
                    resize_ms = (self._profiling_stats["resize_total"] / frame_count) * 1000.0
                    detect_ms = (self._profiling_stats["detect_total"] / frame_count) * 1000.0
                    post_ms = (self._profiling_stats["post_total"] / frame_count) * 1000.0
                    logger.info(
                        "[PROFILING] InsightFace after %s frames: resize=%.2fms/frame, detect=%.2fms/frame, post=%.2fms/frame",
                        frame_count,
                        resize_ms,
                        detect_ms,
                        post_ms,
                    )

        self._last_detections = detections
        return detections


class OpenCVHaarFaceEngine:
    def __init__(self, cascade_path: Optional[str] = None):
        self._lock = threading.Lock()
        self._cascade_path = cascade_path or os.environ.get("STEP5_OPENCV_HAAR_CASCADE_PATH")
        if not self._cascade_path:
            self._cascade_path = str(Path(cv2.data.haarcascades) / "haarcascade_frontalface_default.xml")

        self._max_faces = _parse_optional_positive_int(os.environ.get("STEP5_OPENCV_MAX_FACES"))

        self._classifier = cv2.CascadeClassifier(self._cascade_path)
        if self._classifier.empty():
            raise RuntimeError(f"Unable to load Haar cascade: {self._cascade_path}")

    def detect(self, frame_bgr):
        gray = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2GRAY)
        with self._lock:
            faces = self._classifier.detectMultiScale(
                gray,
                scaleFactor=1.1,
                minNeighbors=5,
                minSize=(30, 30),
            )

        if self._max_faces is not None:
            faces = faces[: self._max_faces]

        detections = []
        for (x, y, w, h) in faces:
            x_i = int(x)
            y_i = int(y)
            w_i = int(w)
            h_i = int(h)
            detections.append(
                {
                    "bbox": (x_i, y_i, w_i, h_i),
                    "centroid": (x_i + (w_i // 2), y_i + (h_i // 2)),
                    "source_detector": "face_landmarker",
                    "label": "face",
                    "confidence": 1.0,
                    "blendshapes": None,
                }
            )

        return detections


class OpenCVYuNetFaceEngine:
    def __init__(
        self,
        model_path: Optional[str] = None,
        score_threshold: float = 0.7,
        nms_threshold: float = 0.3,
        top_k: int = 5000,
        use_gpu: bool = False,
    ):
        self._lock = threading.Lock()
        self._use_gpu = use_gpu
        if use_gpu:
            logger.info(
                "YuNet detection currently repose sur cv2.FaceDetectorYN (CPU). "
                "Le flag GPU optimise uniquement les composants aval (FaceMesh/PyFeat)."
            )
        
        # Force OpenCV to use single thread to avoid contention with multiprocessing
        cv2.setNumThreads(1)
        
        # Configure downscaling for performance (coordinates will be rescaled to original)
        self._max_detection_width = int(os.environ.get("STEP5_YUNET_MAX_WIDTH", "640"))
        logger.info(f"YuNet downscale enabled: max_width={self._max_detection_width}px (coordinates auto-rescaled to original resolution)")

        self._max_faces = _parse_optional_positive_int(os.environ.get("STEP5_OPENCV_MAX_FACES"))
        
        self._model_path = model_path or os.environ.get("STEP5_YUNET_MODEL_PATH")
        if not self._model_path:
            raise RuntimeError("Missing STEP5_YUNET_MODEL_PATH (YuNet model .onnx)")

        self._model_path = self._resolve_model_path(self._model_path)
        if not Path(self._model_path).exists():
            raise RuntimeError(f"YuNet ONNX model not found: {self._model_path}")

        if not hasattr(cv2, "FaceDetectorYN"):
            raise RuntimeError("cv2.FaceDetectorYN is not available (opencv-contrib-python required)")

        self._score_threshold = float(score_threshold)
        self._nms_threshold = float(nms_threshold)
        self._top_k = int(top_k)

        self._detector = cv2.FaceDetectorYN.create(
            self._model_path,
            "",
            (320, 320),
            self._score_threshold,
            self._nms_threshold,
            self._top_k,
        )

    def _resolve_model_path(self, model_path: str) -> str:
        candidate = Path(model_path)
        if candidate.is_absolute():
            return str(candidate)

        if candidate.exists():
            return str(candidate)

        step5_dir = Path(__file__).resolve().parent
        project_root = step5_dir.parent.parent

        candidates = [
            project_root / candidate,
            step5_dir / candidate,
        ]
        for c in candidates:
            if c.exists():
                return str(c)

        return str(candidate)

    def detect(self, frame_bgr):
        orig_height, orig_width = frame_bgr.shape[:2]
        
        # Downscale for detection if needed
        if orig_width > self._max_detection_width:
            scale_factor = self._max_detection_width / orig_width
            detect_width = self._max_detection_width
            detect_height = int(orig_height * scale_factor)
            frame_resized = cv2.resize(frame_bgr, (detect_width, detect_height), interpolation=cv2.INTER_LINEAR)
        else:
            frame_resized = frame_bgr
            detect_width = orig_width
            detect_height = orig_height
            scale_factor = 1.0
        
        with self._lock:
            self._detector.setInputSize((detect_width, detect_height))
            _, faces = self._detector.detect(frame_resized)

        detections = []
        if faces is None:
            return detections

        # Rescale coordinates to original resolution
        rescale = orig_width / detect_width
        if scale_factor != 1.0:
            logger.debug(f"YuNet upscale: {detect_width}x{detect_height} -> {orig_width}x{orig_height} (rescale={rescale:.2f})")
        
        face_limit = self._max_faces
        for idx, row in enumerate(faces):
            if face_limit is not None and idx >= face_limit:
                break
            x, y, w, h = row[:4]
            score = float(row[-1]) if len(row) >= 15 else 0.0
            
            # Rescale to original coordinates
            x_orig = int(max(0, x * rescale))
            y_orig = int(max(0, y * rescale))
            w_orig = int(max(0, w * rescale))
            h_orig = int(max(0, h * rescale))
            
            detections.append(
                {
                    "bbox": (x_orig, y_orig, w_orig, h_orig),
                    "centroid": (x_orig + (w_orig // 2), y_orig + (h_orig // 2)),
                    "source_detector": "face_landmarker",
                    "label": "face",
                    "confidence": score,
                    "blendshapes": None,
                }
            )

        return detections


class OpenCVYuNetPyFeatEngine:
    def __init__(
        self,
        yunet_model_path: Optional[str] = None,
        facemesh_model_path: Optional[str] = None,
        pyfeat_model_path: Optional[str] = None,
        score_threshold: float = 0.7,
        nms_threshold: float = 0.3,
        top_k: int = 5000,
        use_gpu: bool = False,
    ):
        self._lock = threading.Lock()
        self._use_gpu = use_gpu
        
        # Debug: log environment variable values
        profiling_env = os.environ.get("STEP5_ENABLE_PROFILING", "NOT_SET")
        throttle_env = os.environ.get("STEP5_BLENDSHAPES_THROTTLE_N", "NOT_SET")
        logger.info(f"[DEBUG] STEP5_ENABLE_PROFILING={profiling_env}, STEP5_BLENDSHAPES_THROTTLE_N={throttle_env}")
        
        self._enable_profiling = os.environ.get("STEP5_ENABLE_PROFILING", "0").strip() in {"1", "true", "yes"}
        logger.info(f"[DEBUG] Profiling enabled: {self._enable_profiling}")
        
        self._profiling_stats = {
            "yunet_total": 0.0,
            "roi_extraction": 0.0,
            "facemesh_total": 0.0,
            "pyfeat_total": 0.0,
            "frame_count": 0
        }
        
        # Blendshapes throttling: compute every N frames to reduce CPU cost
        self._blendshapes_throttle_n = max(1, int(os.environ.get("STEP5_BLENDSHAPES_THROTTLE_N", "1")))
        logger.info(f"[DEBUG] Blendshapes throttle N: {self._blendshapes_throttle_n}")
        self._frame_counter = 0
        self._last_blendshapes_cache = {}  # object_id -> blendshapes dict

        self._jaw_open_scale = float(os.environ.get("STEP5_OPENCV_JAWOPEN_SCALE", "1.0"))
        
        self._yunet = OpenCVYuNetFaceEngine(
            model_path=yunet_model_path,
            score_threshold=score_threshold,
            nms_threshold=nms_threshold,
            top_k=top_k,
            use_gpu=use_gpu,
        )
        
        try:
            from onnx_facemesh_detector import ONNXFaceMeshDetector
            self._facemesh_detector = ONNXFaceMeshDetector(
                model_path=facemesh_model_path,
                use_gpu=use_gpu,
            )
        except Exception as e:
            raise RuntimeError(f"Failed to initialize FaceMesh ONNX detector: {e}")
        
        self._blendshape_extractor = None
        strict_pyfeat = os.environ.get("STEP5_PYFEAT_STRICT", "0").strip().lower() in {"1", "true", "yes"}
        try:
            from pyfeat_blendshape_extractor import PyFeatBlendshapeExtractor

            self._blendshape_extractor = PyFeatBlendshapeExtractor(
                model_path=pyfeat_model_path,
                use_gpu=use_gpu,
            )
        except Exception as e:
            if strict_pyfeat:
                raise RuntimeError(f"Failed to initialize py-feat blendshape extractor: {e}")

            logger.warning(
                "py-feat blendshape extractor disabled (continuing without blendshapes): %s",
                e,
            )

    def detect(self, frame_bgr):
        height, width = frame_bgr.shape[:2]
        self._frame_counter += 1
        should_compute_blendshapes = (self._frame_counter % self._blendshapes_throttle_n) == 0
        
        t_start_yunet = time.perf_counter() if self._enable_profiling else 0
        yunet_detections = self._yunet.detect(frame_bgr)
        if self._enable_profiling:
            self._profiling_stats["yunet_total"] += time.perf_counter() - t_start_yunet
        
        detections_with_blendshapes = []
        
        for det in yunet_detections:
            bbox = det["bbox"]
            x, y, w, h = bbox
            
            x_safe = max(0, x)
            y_safe = max(0, y)
            x2_safe = min(width, x + w)
            y2_safe = min(height, y + h)
            
            if x2_safe <= x_safe or y2_safe <= y_safe:
                detections_with_blendshapes.append(det)
                continue
            
            t_start_roi = time.perf_counter() if self._enable_profiling else 0
            face_roi = frame_bgr[y_safe:y2_safe, x_safe:x2_safe]
            if self._enable_profiling:
                self._profiling_stats["roi_extraction"] += time.perf_counter() - t_start_roi
            
            t_start_facemesh = time.perf_counter() if self._enable_profiling else 0
            landmarks_478 = self._facemesh_detector.detect_landmarks(
                face_roi,
                (x_safe, y_safe, x2_safe - x_safe, y2_safe - y_safe)
            )
            if self._enable_profiling:
                self._profiling_stats["facemesh_total"] += time.perf_counter() - t_start_facemesh
            
            blendshapes = None
            if landmarks_478 is not None and self._blendshape_extractor is not None:
                # Use simple bbox center as object identifier for caching
                object_id = f"{x_safe}_{y_safe}_{x2_safe}_{y2_safe}"
                
                if should_compute_blendshapes:
                    t_start_pyfeat = time.perf_counter() if self._enable_profiling else 0
                    blendshapes = self._blendshape_extractor.extract_blendshapes(
                        landmarks_478,
                        width,
                        height,
                    )
                    if self._enable_profiling:
                        self._profiling_stats["pyfeat_total"] += time.perf_counter() - t_start_pyfeat
                    
                    # Cache for next frames
                    if blendshapes is not None:
                        blendshapes_scaled = _apply_jawopen_scale(blendshapes, self._jaw_open_scale)
                        self._last_blendshapes_cache[object_id] = blendshapes_scaled
                        blendshapes = blendshapes_scaled
                else:
                    # Reuse cached blendshapes from previous computation
                    blendshapes = self._last_blendshapes_cache.get(object_id)
                    # If no cache available (first frames), compute anyway
                    if blendshapes is None:
                        t_start_pyfeat = time.perf_counter() if self._enable_profiling else 0
                        blendshapes = self._blendshape_extractor.extract_blendshapes(
                            landmarks_478,
                            width,
                            height,
                        )
                        if self._enable_profiling:
                            self._profiling_stats["pyfeat_total"] += time.perf_counter() - t_start_pyfeat
                        if blendshapes is not None:
                            blendshapes_scaled = _apply_jawopen_scale(blendshapes, self._jaw_open_scale)
                            self._last_blendshapes_cache[object_id] = blendshapes_scaled
                            blendshapes = blendshapes_scaled
            
            detection_with_bs = det.copy()
            detection_with_bs["bbox"] = (x_safe, y_safe, x2_safe - x_safe, y2_safe - y_safe)
            detection_with_bs["landmarks"] = landmarks_478.tolist() if landmarks_478 is not None else []
            detection_with_bs["blendshapes"] = blendshapes
            detections_with_blendshapes.append(detection_with_bs)
        
        if self._enable_profiling:
            self._profiling_stats["frame_count"] += 1
            # Log every 20 frames (compatible with multiprocessing chunk size)
            if self._profiling_stats["frame_count"] % 20 == 0:
                self._log_profiling_stats()
        
        return detections_with_blendshapes
    
    def _log_profiling_stats(self):
        """Log accumulated profiling statistics."""
        fc = self._profiling_stats["frame_count"]
        if fc == 0:
            return
        logger.info(
            f"[PROFILING] After {fc} frames: "
            f"YuNet={self._profiling_stats['yunet_total']/fc*1000:.2f}ms/frame, "
            f"ROI={self._profiling_stats['roi_extraction']/fc*1000:.2f}ms/frame, "
            f"FaceMesh={self._profiling_stats['facemesh_total']/fc*1000:.2f}ms/frame, "
            f"py-feat={self._profiling_stats['pyfeat_total']/fc*1000:.2f}ms/frame"
        )


class EosFaceEngine:
    def __init__(
        self,
        yunet_model_path: Optional[str] = None,
        facemesh_model_path: Optional[str] = None,
    ):
        try:
            import eos as eos_module
        except Exception as e:
            raise RuntimeError(
                "eos engine requires eos-py (install it inside eos_env)."
            ) from e

        self._eos = eos_module
        self._lock = threading.Lock()
        self._frame_counter = 0
        self._last_detections = []

        fit_every_raw = os.environ.get("STEP5_EOS_FIT_EVERY_N")
        if fit_every_raw is None or str(fit_every_raw).strip() == "":
            fit_every_raw = os.environ.get("STEP5_BLENDSHAPES_THROTTLE_N", "1")
        try:
            self._fit_every_n = max(1, int(str(fit_every_raw)))
        except Exception:
            self._fit_every_n = 1

        self._jaw_open_scale = float(os.environ.get("STEP5_EOS_JAWOPEN_SCALE", "1.0"))
        self._max_faces = (
            _parse_optional_positive_int(os.environ.get("STEP5_EOS_MAX_FACES"))
            or _parse_optional_positive_int(os.environ.get("STEP5_OPENCV_MAX_FACES"))
        )
        
        self._max_detection_width = int(os.environ.get("STEP5_EOS_MAX_WIDTH", "1280"))
        self._enable_profiling = os.environ.get("STEP5_ENABLE_PROFILING", "").strip().lower() in {"1", "true", "yes"}
        self._profiling_interval = 20

        self._yunet = OpenCVYuNetFaceEngine(model_path=yunet_model_path)

        try:
            from onnx_facemesh_detector import ONNXFaceMeshDetector
        except Exception as e:
            raise RuntimeError(
                "eos engine requires ONNXFaceMeshDetector dependencies (onnxruntime + FaceMesh model) inside eos_env."
            ) from e

        self._facemesh_detector = ONNXFaceMeshDetector(model_path=facemesh_model_path)

        self._models_dir = os.environ.get("STEP5_EOS_MODELS_DIR")
        self._sfm_model_path_raw = os.environ.get("STEP5_EOS_SFM_MODEL_PATH")
        self._expression_blendshapes_path_raw = os.environ.get("STEP5_EOS_EXPRESSION_BLENDSHAPES_PATH")
        self._landmark_mapper_path_raw = os.environ.get("STEP5_EOS_LANDMARK_MAPPER_PATH")
        self._edge_topology_path_raw = os.environ.get("STEP5_EOS_EDGE_TOPOLOGY_PATH")
        self._model_contour_path_raw = os.environ.get("STEP5_EOS_MODEL_CONTOUR_PATH")
        self._contour_landmarks_path_raw = os.environ.get("STEP5_EOS_CONTOUR_LANDMARKS_PATH")

        self._morphable_model = None
        self._landmark_mapper = None
        self._edge_topology = None
        self._contour_landmarks = None
        self._model_contour = None

        correspondence = [
            (127, 127),
            (234, 234),
            (93, 93),
            (132, 58),
            (58, 172),
            (136, 136),
            (150, 150),
            (176, 176),
            (152, 152),
            (400, 400),
            (379, 379),
            (365, 365),
            (397, 288),
            (361, 361),
            (323, 323),
            (454, 454),
            (356, 356),
            (70, 70),
            (63, 63),
            (105, 105),
            (66, 66),
            (107, 107),
            (336, 336),
            (296, 296),
            (334, 334),
            (293, 293),
            (300, 300),
            (168, 6),
            (197, 195),
            (5, 5),
            (4, 4),
            (75, 75),
            (97, 97),
            (2, 2),
            (326, 326),
            (305, 305),
            (33, 33),
            (160, 160),
            (158, 158),
            (133, 133),
            (153, 153),
            (144, 144),
            (362, 362),
            (385, 385),
            (387, 387),
            (263, 263),
            (373, 373),
            (380, 380),
            (61, 61),
            (39, 39),
            (37, 37),
            (0, 0),
            (267, 267),
            (269, 269),
            (291, 291),
            (321, 321),
            (314, 314),
            (17, 17),
            (84, 84),
            (91, 91),
            (78, 78),
            (82, 82),
            (13, 13),
            (312, 312),
            (308, 308),
            (317, 317),
            (14, 14),
            (87, 87),
        ]

        self._mp2dlib_pairs = np.asarray(correspondence, dtype=np.int32)

    def _resolve_model_path(self, model_path: str) -> str:
        candidate = Path(model_path)
        if candidate.is_absolute() and candidate.exists():
            return str(candidate)
        if candidate.exists():
            return str(candidate)

        step5_dir = Path(__file__).resolve().parent
        project_root = step5_dir.parent.parent

        candidates = [
            project_root / candidate,
            step5_dir / candidate,
        ]
        for c in candidates:
            if c.exists():
                return str(c)

        return str(candidate)

    def _get_models_base_dir(self) -> Path:
        if self._models_dir and str(self._models_dir).strip():
            return Path(self._resolve_model_path(self._models_dir)).resolve()

        step5_dir = Path(__file__).resolve().parent
        models_dir = step5_dir / "models" / "engines" / "eos"
        return models_dir.resolve()

    def _ensure_eos_assets_loaded(self) -> None:
        if self._morphable_model is not None:
            return

        models_base_dir = self._get_models_base_dir()
        sfm_model_path = self._sfm_model_path_raw or str(models_base_dir / "sfm_shape_3448.bin")
        expression_blendshapes_path = self._expression_blendshapes_path_raw or str(
            models_base_dir / "expression_blendshapes_3448.bin"
        )
        landmark_mapper_path = self._landmark_mapper_path_raw or str(models_base_dir / "ibug_to_sfm.txt")
        edge_topology_path = self._edge_topology_path_raw or str(models_base_dir / "sfm_3448_edge_topology.json")
        model_contour_path = self._model_contour_path_raw or str(models_base_dir / "sfm_model_contours.json")
        contour_landmarks_path = self._contour_landmarks_path_raw or landmark_mapper_path

        sfm_model_path = self._resolve_model_path(sfm_model_path)
        expression_blendshapes_path = self._resolve_model_path(expression_blendshapes_path)
        landmark_mapper_path = self._resolve_model_path(landmark_mapper_path)
        edge_topology_path = self._resolve_model_path(edge_topology_path)
        model_contour_path = self._resolve_model_path(model_contour_path)
        contour_landmarks_path = self._resolve_model_path(contour_landmarks_path)

        required_files = {
            "sfm_model": sfm_model_path,
            "expression_blendshapes": expression_blendshapes_path,
            "landmark_mapper": landmark_mapper_path,
            "edge_topology": edge_topology_path,
            "model_contour": model_contour_path,
            "contour_landmarks": contour_landmarks_path,
        }
        missing = [k for k, v in required_files.items() if not Path(v).exists()]
        if missing:
            raise RuntimeError(
                "Missing eos asset files: "
                + ", ".join(missing)
                + ". Configure STEP5_EOS_MODELS_DIR and/or STEP5_EOS_*_PATH variables."
            )

        model = self._eos.morphablemodel.load_model(sfm_model_path)
        blendshapes = self._eos.morphablemodel.load_blendshapes(expression_blendshapes_path)

        self._morphable_model = self._eos.morphablemodel.MorphableModel(
            model.get_shape_model(),
            blendshapes,
            color_model=self._eos.morphablemodel.PcaModel(),
            vertex_definitions=None,
            texture_coordinates=model.get_texture_coordinates(),
        )
        self._landmark_mapper = self._eos.core.LandmarkMapper(landmark_mapper_path)
        self._edge_topology = self._eos.morphablemodel.load_edge_topology(edge_topology_path)
        self._contour_landmarks = self._eos.fitting.ContourLandmarks.load(contour_landmarks_path)
        self._model_contour = self._eos.fitting.ModelContour.load(model_contour_path)

    def _convert_478_to_68(self, landmarks_478: np.ndarray) -> np.ndarray:
        lm = np.asarray(landmarks_478, dtype=np.float32)
        if lm.ndim != 2 or lm.shape[0] < 468:
            raise ValueError(f"Invalid FaceMesh landmarks shape: {lm.shape}")
        if lm.shape[0] < 478:
            last = lm[-1:, :]
            pad = np.repeat(last, 478 - lm.shape[0], axis=0)
            lm = np.concatenate([lm, pad], axis=0)
        return lm[self._mp2dlib_pairs].mean(axis=1)

    def _compute_jaw_open_from_dlib68(self, landmarks_68: np.ndarray) -> float:
        lm = np.asarray(landmarks_68, dtype=np.float32)
        if lm.ndim != 2 or lm.shape[0] < 68:
            return 0.0

        try:
            upper_inner = float(np.mean(lm[60:65, 1], axis=0))
            lower_inner = float(np.mean(lm[65:68, 1], axis=0))
            mouth_open = abs(lower_inner - upper_inner)

            nose_y = float(lm[33, 1])
            chin_y = float(lm[8, 1])
            denom = max(abs(chin_y - nose_y), 1e-6)
            ratio = (mouth_open / denom) * float(self._jaw_open_scale)
            return float(np.clip(ratio, 0.0, 1.0))
        except Exception:
            return 0.0

    def _build_arkit_blendshapes(self, jaw_open: float) -> dict:
        out = {}
        for name in ARKIT_52_BLENDSHAPE_NAMES:
            out[name] = 0.0
        out["jawOpen"] = float(jaw_open)
        return out

    def _safe_to_list(self, value) -> list:
        if value is None:
            return []
        try:
            return [float(x) for x in value]
        except Exception:
            try:
                return [float(x) for x in list(value)]
            except Exception:
                return []

    def detect(self, frame_bgr):
        import time
        
        self._frame_counter += 1
        
        if self._enable_profiling and (self._frame_counter % self._profiling_interval) == 0:
            frame_start_time = time.time()
        else:
            frame_start_time = None
        
        if (self._frame_counter % self._fit_every_n) != 0:
            return list(self._last_detections)

        if frame_bgr is None or getattr(frame_bgr, "shape", None) is None:
            self._last_detections = []
            return []

        orig_height, orig_width = frame_bgr.shape[:2]
        
        if orig_width > self._max_detection_width:
            scale_factor = self._max_detection_width / orig_width
            detect_width = self._max_detection_width
            detect_height = int(orig_height * scale_factor)
            frame_resized = cv2.resize(frame_bgr, (detect_width, detect_height), interpolation=cv2.INTER_LINEAR)
            height, width = detect_height, detect_width
        else:
            frame_resized = frame_bgr
            height, width = orig_height, orig_width
            scale_factor = 1.0

        try:
            self._ensure_eos_assets_loaded()
        except Exception as e:
            logger.error("EOS assets load failed: %s", e)
            self._last_detections = []
            return []
        
        if frame_start_time is not None:
            assets_time = time.time()

        yunet_detections = self._yunet.detect(frame_resized)
        if self._max_faces is not None:
            yunet_detections = yunet_detections[: self._max_faces]
        
        if frame_start_time is not None:
            yunet_time = time.time()

        detections = []
        for det in yunet_detections:
            bbox = det.get("bbox")
            if not bbox or len(bbox) < 4:
                continue

            x, y, w, h = bbox
            
            if scale_factor != 1.0:
                x = int(x / scale_factor)
                y = int(y / scale_factor)
                w = int(w / scale_factor)
                h = int(h / scale_factor)
                logger.debug(f"EOS upscale: bbox rescaled by 1/{scale_factor:.2f} to original resolution")
            
            x_safe = max(0, int(x))
            y_safe = max(0, int(y))
            x2_safe = min(int(orig_width), int(x + w))
            y2_safe = min(int(orig_height), int(y + h))
            if x2_safe <= x_safe or y2_safe <= y_safe:
                continue

            face_roi = frame_bgr[y_safe:y2_safe, x_safe:x2_safe]
            landmarks_478 = self._facemesh_detector.detect_landmarks(
                face_roi,
                (x_safe, y_safe, x2_safe - x_safe, y2_safe - y_safe),
            )
            if landmarks_478 is None:
                continue
            
            if frame_start_time is not None:
                facemesh_time = time.time()

            try:
                landmarks_68 = self._convert_478_to_68(landmarks_478)
            except Exception:
                continue

            jaw_open = self._compute_jaw_open_from_dlib68(landmarks_68)

            try:
                eos_landmarks = []
                for i in range(68):
                    eos_landmarks.append(
                        self._eos.core.Landmark(
                            str(i + 1),
                            [float(landmarks_68[i, 0]), float(landmarks_68[i, 1])],
                        )
                    )

                with self._lock:
                    (
                        _mesh,
                        _pose,
                        shape_coeffs,
                        blendshape_coeffs,
                    ) = self._eos.fitting.fit_shape_and_pose(
                        self._morphable_model,
                        eos_landmarks,
                        self._landmark_mapper,
                        int(orig_width),
                        int(orig_height),
                        self._edge_topology,
                        self._contour_landmarks,
                        self._model_contour,
                    )
            except Exception as e:
                logger.warning("EOS fitting failed: %s", e)
                continue
            
            if frame_start_time is not None:
                eos_fit_time = time.time()

            landmarks_xyz = np.zeros((68, 3), dtype=np.float32)
            landmarks_xyz[:, 0] = landmarks_68[:, 0]
            landmarks_xyz[:, 1] = landmarks_68[:, 1]

            eos_payload = {
                "shape_coeffs": self._safe_to_list(shape_coeffs),
                "expression_coeffs": self._safe_to_list(blendshape_coeffs),
            }

            out_det = dict(det)
            out_det["bbox"] = (x_safe, y_safe, x2_safe - x_safe, y2_safe - y_safe)
            out_det["centroid"] = (x_safe + ((x2_safe - x_safe) // 2), y_safe + ((y2_safe - y_safe) // 2))
            out_det["source_detector"] = "face_landmarker"
            out_det["label"] = "face"
            out_det["landmarks"] = landmarks_xyz.tolist()
            out_det["blendshapes"] = self._build_arkit_blendshapes(jaw_open)
            out_det["eos"] = eos_payload
            detections.append(out_det)

        self._last_detections = detections
        
        if frame_start_time is not None:
            total_time = time.time() - frame_start_time
            assets_duration = (assets_time - frame_start_time) * 1000 if 'assets_time' in locals() else 0
            yunet_duration = (yunet_time - assets_time) * 1000 if 'yunet_time' in locals() and 'assets_time' in locals() else 0
            facemesh_duration = (facemesh_time - yunet_time) * 1000 if 'facemesh_time' in locals() and 'yunet_time' in locals() else 0
            eos_duration = (eos_fit_time - facemesh_time) * 1000 if 'eos_fit_time' in locals() and 'facemesh_time' in locals() else 0
            logger.info(
                f"[PROFILING] Frame {self._frame_counter}: total={total_time*1000:.1f}ms "
                f"(assets={assets_duration:.1f}ms, yunet={yunet_duration:.1f}ms, "
                f"facemesh={facemesh_duration:.1f}ms, eos_fit={eos_duration:.1f}ms, "
                f"faces={len(detections)}, downscale={scale_factor:.2f})"
            )
        
        return detections


def create_face_engine(engine_name: str, use_gpu: bool = False):
    """
    Factory function to create face tracking engines.
    
    Args:
        engine_name: Name of the engine (mediapipe_landmarker, openseeface, etc.)
        use_gpu: If True, enable GPU acceleration for supported engines (v4.2+)
    
    Returns:
        Engine instance or None for MediaPipe (handled separately in workers)
    """
    normalized = (engine_name or "").strip().lower()
    if not normalized or normalized in {"mediapipe", "mediapipe_landmarker"}:
        return None
    if normalized == "openseeface":
        return OpenSeeFaceEngine(use_gpu=use_gpu)
    if normalized == "insightface":
        return InsightFaceEngine(use_gpu=use_gpu)
    if normalized == "eos":
        return EosFaceEngine()
    if normalized == "opencv_haar":
        return OpenCVHaarFaceEngine()
    if normalized == "opencv_yunet":
        return OpenCVYuNetFaceEngine(use_gpu=use_gpu)
    if normalized == "opencv_yunet_pyfeat":
        return OpenCVYuNetPyFeatEngine(use_gpu=use_gpu)
    raise ValueError(f"Unsupported tracking engine: {engine_name}")
```

## File: workflow_scripts/step5/object_detector_registry.py
```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Object Detector Model Registry for STEP5

Centralized configuration and resolution for object detection models
used as fallback when face detection fails in MediaPipe tracking engine.

Supports multiple model backends (TFLite, ONNX) with hardware-aware recommendations.
"""

import os
import logging
from pathlib import Path
from typing import Optional, Dict, Any
from dataclasses import dataclass

logger = logging.getLogger(__name__)


@dataclass
class ObjectDetectorModelSpec:
    """Specification for an object detection model."""
    
    name: str
    backend: str  # 'tflite' or 'onnx'
    filename: str
    description: str
    recommended_hardware: str  # 'edge_tpu', 'cpu_arm', 'cpu_desktop', 'gpu'
    map_coco: float  # mAP on COCO dataset
    latency_note: str


class ObjectDetectorRegistry:
    """
    Registry for object detection models used in STEP5 fallback detection.
    
    Provides centralized configuration, model resolution, and hardware-aware
    recommendations for object detection models.
    """
    
    # Model catalog with specifications from technical analysis
    MODELS = {
        'efficientdet_lite0': ObjectDetectorModelSpec(
            name='efficientdet_lite0',
            backend='tflite',
            filename='EfficientDet-Lite0.tflite',
            description='EfficientDet-Lite0 (320x320) - Fastest, Edge TPU compatible',
            recommended_hardware='edge_tpu',
            map_coco=25.69,
            latency_note='~37ms on Pixel 4 CPU, ~50% faster than Lite2'
        ),
        'efficientdet_lite1': ObjectDetectorModelSpec(
            name='efficientdet_lite1',
            backend='tflite',
            filename='EfficientDet-Lite1.tflite',
            description='EfficientDet-Lite1 (384x384) - Balanced, Edge TPU compatible',
            recommended_hardware='edge_tpu',
            map_coco=30.55,
            latency_note='~49ms on Pixel 4 CPU'
        ),
        'efficientdet_lite2': ObjectDetectorModelSpec(
            name='efficientdet_lite2',
            backend='tflite',
            filename='EfficientDet-Lite2-32.tflite',
            description='EfficientDet-Lite2 (448x448) - Current baseline',
            recommended_hardware='cpu_desktop',
            map_coco=33.97,
            latency_note='~69ms on Pixel 4 CPU'
        ),
        'ssd_mobilenet_v3': ObjectDetectorModelSpec(
            name='ssd_mobilenet_v3',
            backend='tflite',
            filename='ssd_mobilenet_v3.tflite',
            description='SSD-MobileNetV3 - Stable ecosystem, CPU optimized',
            recommended_hardware='cpu_arm',
            map_coco=28.0,
            latency_note='~40% faster than Lite2, high confidence stability'
        ),
        'yolo11n_onnx': ObjectDetectorModelSpec(
            name='yolo11n_onnx',
            backend='onnx',
            filename='yolo11n.onnx',
            description='YOLO11 Nano (ONNX) - Best CPU performance (experimental)',
            recommended_hardware='cpu_desktop',
            map_coco=39.5,
            latency_note='~56ms CPU ONNX, 20-30% faster with better precision'
        ),
        'nanodet_plus': ObjectDetectorModelSpec(
            name='nanodet_plus',
            backend='onnx',
            filename='nanodet-plus-m_416.onnx',
            description='NanoDet-Plus (416x416) - Ultra-lightweight (experimental)',
            recommended_hardware='cpu_arm',
            map_coco=34.1,
            latency_note='~25ms on ARM, 60-70% faster than Lite2'
        ),
    }
    
    # Default model (backward compatible with existing setup)
    DEFAULT_MODEL = 'efficientdet_lite2'

    @classmethod
    def _try_resolve_path(cls, raw_path: str, models_dir: Optional[Path]) -> Optional[Path]:
        if not raw_path:
            return None

        candidate = Path(raw_path)
        if candidate.exists():
            return candidate if candidate.is_absolute() else candidate.resolve()

        step5_dir = Path(__file__).resolve().parent
        project_root = step5_dir.parent.parent

        for base in [models_dir, project_root, step5_dir]:
            if base is None:
                continue
            if candidate.is_absolute():
                continue
            resolved = base / candidate
            if resolved.exists():
                return resolved.resolve()

        return None
    
    @classmethod
    def get_model_spec(cls, model_name: Optional[str] = None) -> ObjectDetectorModelSpec:
        """
        Get model specification by name.
        
        Args:
            model_name: Model identifier (e.g., 'efficientdet_lite0').
                       If None, returns default model.
        
        Returns:
            ObjectDetectorModelSpec for the requested model.
        
        Raises:
            ValueError: If model_name is not found in registry.
        """
        if not model_name:
            model_name = cls.DEFAULT_MODEL
        
        model_name_normalized = model_name.strip().lower()
        
        if model_name_normalized not in cls.MODELS:
            available = ', '.join(cls.MODELS.keys())
            raise ValueError(
                f"Unknown object detector model: '{model_name}'. "
                f"Available models: {available}"
            )
        
        return cls.MODELS[model_name_normalized]
    
    @classmethod
    def resolve_model_path(
        cls,
        model_name: Optional[str] = None,
        models_dir: Optional[Path] = None,
        override_path: Optional[str] = None
    ) -> Path:
        """
        Resolve the full path to an object detection model file.
        
        Priority order:
        1. override_path (if provided and exists)
        2. Environment variable STEP5_OBJECT_DETECTOR_MODEL_PATH
        3. models_dir / model_spec.filename
        4. Default models directory / model_spec.filename
        
        Args:
            model_name: Model identifier from registry.
            models_dir: Directory containing model files.
            override_path: Explicit path override.
        
        Returns:
            Resolved Path to model file.
        
        Raises:
            FileNotFoundError: If model file cannot be found.
            ValueError: If model_name is invalid.
        """
        # Priority 1: Explicit override path
        if override_path:
            override_path_obj = cls._try_resolve_path(override_path, models_dir=models_dir)
            if override_path_obj is not None:
                logger.info(f"Using override model path: {override_path_obj}")
                return override_path_obj
            logger.warning(f"Override path does not exist: {override_path}")

        # Priority 2: Environment variable
        env_path = os.environ.get('STEP5_OBJECT_DETECTOR_MODEL_PATH')
        if env_path:
            env_path_obj = cls._try_resolve_path(env_path, models_dir=models_dir)
            if env_path_obj is not None:
                logger.info(f"Using model path from STEP5_OBJECT_DETECTOR_MODEL_PATH: {env_path_obj}")
                return env_path_obj
            logger.warning(f"STEP5_OBJECT_DETECTOR_MODEL_PATH does not exist: {env_path}")
        
        # Get model spec
        spec = cls.get_model_spec(model_name)
        
        # Priority 3/4: Provided models_dir and default models directory
        search_base_dirs = []
        if models_dir is not None:
            search_base_dirs.append(models_dir)

        default_models_dir = Path(__file__).resolve().parent / "models"
        search_base_dirs.append(default_models_dir)

        backend_dir = "tflite" if spec.backend == "tflite" else "onnx"
        relative_candidates = [
            Path(spec.filename),
            Path("object_detectors") / backend_dir / spec.filename,
            Path("object_detectors") / spec.backend / spec.filename,
        ]

        for base_dir in search_base_dirs:
            for rel in relative_candidates:
                model_path = base_dir / rel
                if model_path.exists():
                    logger.info(f"Using object detector model: {model_path}")
                    return model_path

        searched_in = ", ".join(str(d) for d in search_base_dirs)
        raise FileNotFoundError(
            f"Object detector model not found: {spec.filename}\n"
            f"Model: {spec.name} ({spec.description})\n"
            f"Expected filename: {spec.filename}\n"
            f"Searched in: {searched_in}\n"
            f"To use this model, download it under workflow_scripts/step5/models/object_detectors/ "
            f"or set STEP5_OBJECT_DETECTOR_MODEL_PATH environment variable."
        )
    
    @classmethod
    def get_recommended_model(cls, hardware_target: str = 'cpu_desktop') -> str:
        """
        Get recommended model name for a specific hardware target.
        
        Args:
            hardware_target: One of 'edge_tpu', 'cpu_arm', 'cpu_desktop', 'gpu'
        
        Returns:
            Recommended model name for the hardware target.
        """
        recommendations = {
            'edge_tpu': 'efficientdet_lite0',
            'cpu_arm': 'efficientdet_lite0',  # or nanodet_plus if ONNX available
            'cpu_desktop': 'efficientdet_lite0',  # balance of speed and accuracy
            'gpu': 'efficientdet_lite2',  # can handle higher resolution
        }
        
        return recommendations.get(hardware_target, cls.DEFAULT_MODEL)
    
    @classmethod
    def list_available_models(cls) -> Dict[str, ObjectDetectorModelSpec]:
        """
        Get all registered models with their specifications.
        
        Returns:
            Dictionary of model_name -> ObjectDetectorModelSpec
        """
        return cls.MODELS.copy()
    
    @classmethod
    def validate_backend_support(cls, backend: str) -> bool:
        """
        Check if a model backend is supported in the current environment.
        
        Args:
            backend: 'tflite' or 'onnx'
        
        Returns:
            True if backend is supported, False otherwise.
        """
        if backend == 'tflite':
            # MediaPipe always supports TFLite
            return True
        elif backend == 'onnx':
            # ONNX support is experimental/future work
            try:
                import onnxruntime
                return True
            except ImportError:
                logger.warning("ONNX Runtime not available. ONNX models require: pip install onnxruntime")
                return False
        else:
            return False
```

## File: workflow_scripts/step5/onnx_facemesh_detector.py
```python
import logging
import os
from pathlib import Path
from typing import Optional, Tuple

import cv2
import numpy as np

logger = logging.getLogger(__name__)


class ONNXFaceMeshDetector:
    def __init__(self, model_path: Optional[str] = None, use_gpu: bool = False):
        self._model_path = model_path or os.environ.get("STEP5_FACEMESH_ONNX_PATH")
        self._model_path = self._resolve_model_path(self._model_path)
        
        if not self._model_path:
            self._model_path = self._find_mediapipe_facemesh_model()
        
        self._model_path = self._resolve_model_path(self._model_path)

        if not self._model_path or not Path(self._model_path).exists():
            raise RuntimeError(
                f"FaceMesh ONNX model not found. "
                f"Set STEP5_FACEMESH_ONNX_PATH or place face_landmark.onnx under models/face_landmarks/"
            )

        try:
            import onnxruntime as ort
            self._ort = ort
            
            sess_options = ort.SessionOptions()
            sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
            
            # Configure threading for optimal CPU performance
            intra_threads = int(os.environ.get("STEP5_ONNX_INTRA_OP_THREADS", "2"))
            inter_threads = int(os.environ.get("STEP5_ONNX_INTER_OP_THREADS", "1"))
            sess_options.intra_op_num_threads = intra_threads
            sess_options.inter_op_num_threads = inter_threads
            
            # Enable memory optimizations
            sess_options.enable_cpu_mem_arena = True
            sess_options.enable_mem_pattern = True
            
            providers = []
            if use_gpu:
                providers.append('CUDAExecutionProvider')
            providers.append('CPUExecutionProvider')
            
            self._session = ort.InferenceSession(
                self._model_path,
                sess_options=sess_options,
                providers=providers
            )
            active_providers = self._session.get_providers()
            
            logger.info(
                f"ONNX Runtime configured: intra_threads={intra_threads}, inter_threads={inter_threads}, "
                f"use_gpu={use_gpu}"
            )
            logger.info("FaceMesh ONNX providers active: %s", active_providers)
            
            self._input_name = self._session.get_inputs()[0].name
            self._input_shape = self._session.get_inputs()[0].shape
            self._output_names = [output.name for output in self._session.get_outputs()]
            
            logger.info(f"FaceMesh ONNX model loaded: {self._model_path}")
            logger.info(f"Input shape: {self._input_shape}, Outputs: {self._output_names}")
        except ImportError:
            raise RuntimeError(
                "onnxruntime is required for ONNX FaceMesh. "
                "Install with: pip install onnxruntime"
            )
        except Exception as e:
            logger.error(f"Failed to load ONNX FaceMesh model: {e}")
            raise

    def _find_mediapipe_facemesh_model(self) -> Optional[str]:
        search_paths = [
            Path(__file__).parent / "models" / "face_landmarks" / "opencv" / "face_landmark.onnx",
            Path(__file__).parent / "models" / "face_landmarks" / "opencv" / "face_landmarker.onnx",
            Path(__file__).parent / "models" / "face_landmarks" / "opencv" / "facemesh.onnx",
            Path(__file__).parent / "models" / "face_landmark.onnx",
            Path(__file__).parent / "models" / "face_landmarker.onnx",
            Path(__file__).parent / "models" / "facemesh.onnx",
        ]
        
        for path in search_paths:
            if path.exists():
                return str(path)
        
        return None

    def _resolve_model_path(self, model_path: Optional[str]) -> Optional[str]:
        if not model_path:
            return None

        candidate = Path(model_path)
        if candidate.is_absolute():
            return str(candidate)

        if candidate.exists():
            return str(candidate)

        step5_dir = Path(__file__).resolve().parent
        project_root = step5_dir.parent.parent

        candidates = [
            project_root / candidate,
            step5_dir / candidate,
        ]
        for c in candidates:
            if c.exists():
                return str(c)

        return str(candidate)

    def detect_landmarks(
        self,
        face_roi: np.ndarray,
        original_bbox: Tuple[int, int, int, int]
    ) -> Optional[np.ndarray]:
        try:
            input_size = 192
            if len(self._input_shape) >= 3 and self._input_shape[2] is not None:
                input_size = self._input_shape[2]
            
            # Optimize resize and preprocessing
            if face_roi.shape[:2] != (input_size, input_size):
                resized = cv2.resize(face_roi, (input_size, input_size), interpolation=cv2.INTER_LINEAR)
            else:
                resized = face_roi
            
            # Single operation: convert to float32, normalize, transpose
            input_tensor = np.ascontiguousarray(
                np.transpose(resized, (2, 0, 1))[np.newaxis, :, :, :].astype(np.float32) / 255.0
            )

            x, y, w, h = original_bbox
            feed_dict = {}
            for input_meta in self._session.get_inputs():
                input_name = input_meta.name
                if input_name == self._input_name:
                    feed_dict[input_name] = input_tensor
                elif input_name == "crop_x1":
                    feed_dict[input_name] = np.array([[int(x)]], dtype=np.int32)
                elif input_name == "crop_y1":
                    feed_dict[input_name] = np.array([[int(y)]], dtype=np.int32)
                elif input_name == "crop_width":
                    feed_dict[input_name] = np.array([[int(w)]], dtype=np.int32)
                elif input_name == "crop_height":
                    feed_dict[input_name] = np.array([[int(h)]], dtype=np.int32)

            missing_inputs = [i.name for i in self._session.get_inputs() if i.name not in feed_dict]
            if missing_inputs:
                raise RuntimeError(
                    f"FaceMesh ONNX model requires additional inputs not supported: {missing_inputs}"
                )

            outputs = self._session.run(self._output_names, feed_dict)
            
            landmarks_normalized = None
            for output in outputs:
                if output.shape[-1] == 3 and output.shape[-2] >= 468:
                    landmarks_normalized = output[0]
                    break
            
            if landmarks_normalized is None:
                logger.warning("Could not find landmarks in ONNX output")
                return None

            landmarks_normalized = landmarks_normalized[:478, :]

            if landmarks_normalized.shape[0] < 478:
                pad_count = 478 - int(landmarks_normalized.shape[0])
                if pad_count > 0 and landmarks_normalized.shape[0] > 0:
                    pad = np.repeat(landmarks_normalized[-1:, :], pad_count, axis=0)
                    landmarks_normalized = np.concatenate([landmarks_normalized, pad], axis=0)

            if np.issubdtype(landmarks_normalized.dtype, np.integer):
                return landmarks_normalized.astype(np.float32)

            max_abs = float(np.nanmax(np.abs(landmarks_normalized[:, :2]))) if landmarks_normalized.size else 0.0
            if max_abs > 2.0:
                return landmarks_normalized.astype(np.float32)

            landmarks_absolute = landmarks_normalized.copy()
            landmarks_absolute[:, 0] = landmarks_normalized[:, 0] * w + x
            landmarks_absolute[:, 1] = landmarks_normalized[:, 1] * h + y

            return landmarks_absolute.astype(np.float32)
        
        except Exception as e:
            logger.warning(f"Failed to detect landmarks with ONNX: {e}")
            return None
```

## File: workflow_scripts/step5/process_video_worker_multiprocessing.py
```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Enhanced CPU Worker with Multiprocessing Optimization
Combines proven techniques from backup implementations for maximum CPU performance.
"""

import os
import sys
import json
import argparse
import logging
import cv2
import numpy as np
try:
    import mediapipe as mp
except Exception:
    mp = None
import multiprocessing as mp_proc
import time
import math
from pathlib import Path
from functools import partial
from concurrent.futures import ProcessPoolExecutor, as_completed

# Load .env file before anything else (critical for multiprocessing workers)
try:
    from dotenv import load_dotenv
    env_path = Path(__file__).resolve().parent.parent.parent / '.env'
    if env_path.exists():
        load_dotenv(env_path)
except ImportError:
    pass  # dotenv not available, rely on system env vars

# Add project root to path
sys.path.append(str(Path(__file__).resolve().parent.parent.parent))
from utils.tracking_optimizations import apply_tracking_and_management
from utils.resource_manager import safe_video_processing, get_video_metadata, resource_tracker
from utils.enhanced_speaking_detection import EnhancedSpeakingDetector
from object_detector_registry import ObjectDetectorRegistry

# Configuration du logger
log_dir = Path(__file__).resolve().parent.parent.parent / "logs" / "step5"
log_dir.mkdir(parents=True, exist_ok=True)
worker_type_str = "CPU_MP" if "--mp_num_workers_internal" in sys.argv else "CPU"
video_name_for_log = Path(sys.argv[1]).stem if len(sys.argv) > 1 else "unknown"
log_file = log_dir / f"worker_{worker_type_str}_{video_name_for_log}_{os.getpid()}.log"
logging.basicConfig(level=logging.INFO, format='%(asctime)s - [%(levelname)s] - %(message)s',
                    handlers=[logging.FileHandler(log_file, encoding='utf-8'), logging.StreamHandler(sys.stdout)])


def _parse_optional_positive_int(raw):
    if raw is None:
        return None
    raw_str = str(raw).strip()
    if not raw_str:
        return None
    try:
        value = int(raw_str)
    except Exception:
        return None
    if value <= 0:
        return None
    return value


def _is_truthy_env(raw):
    if raw is None:
        return False
    return str(raw).strip().lower() in {"1", "true", "yes"}


def _apply_jawopen_scale(blendshapes, scale):
    if not blendshapes or not isinstance(blendshapes, dict):
        return blendshapes
    try:
        jaw_open_raw = blendshapes.get("jawOpen")
        if jaw_open_raw is None:
            return blendshapes
        jaw_open_scaled = float(jaw_open_raw) * float(scale)
        jaw_open_scaled = float(np.clip(jaw_open_scaled, 0.0, 1.0))
        out = dict(blendshapes)
        out["jawOpen"] = jaw_open_scaled
        return out
    except Exception:
        return blendshapes


# Global variables for worker processes
landmarker_global = None
object_detector_global = None
face_engine_global = None
tracking_engine_type = None


def init_worker_process(models_dir, args_dict):
    """
    Initialize tracking models once per worker process.
    Supports MediaPipe and OpenCV-based engines.
    """
    global landmarker_global, object_detector_global, face_engine_global, tracking_engine_type
    
    worker_pid = os.getpid()
    engine_name = args_dict.get('tracking_engine', '')
    engine_norm = (str(engine_name).strip().lower() if engine_name else "")
    
    logging.info(f"[WORKER-{worker_pid}] Initializing process with engine: {engine_norm or 'mediapipe'}")
    
    try:
        if engine_norm and engine_norm not in {"mediapipe", "mediapipe_landmarker"}:
            # Inject env vars for OpenCV engines (not inherited from parent process)
            if args_dict.get('yunet_model_path'):
                os.environ['STEP5_YUNET_MODEL_PATH'] = args_dict['yunet_model_path']
            if args_dict.get('eos_models_dir'):
                os.environ['STEP5_EOS_MODELS_DIR'] = str(args_dict['eos_models_dir'])
            if args_dict.get('eos_sfm_model_path'):
                os.environ['STEP5_EOS_SFM_MODEL_PATH'] = str(args_dict['eos_sfm_model_path'])
            if args_dict.get('eos_expression_blendshapes_path'):
                os.environ['STEP5_EOS_EXPRESSION_BLENDSHAPES_PATH'] = str(args_dict['eos_expression_blendshapes_path'])
            if args_dict.get('eos_landmark_mapper_path'):
                os.environ['STEP5_EOS_LANDMARK_MAPPER_PATH'] = str(args_dict['eos_landmark_mapper_path'])
            if args_dict.get('eos_edge_topology_path'):
                os.environ['STEP5_EOS_EDGE_TOPOLOGY_PATH'] = str(args_dict['eos_edge_topology_path'])
            if args_dict.get('eos_model_contour_path'):
                os.environ['STEP5_EOS_MODEL_CONTOUR_PATH'] = str(args_dict['eos_model_contour_path'])
            if args_dict.get('eos_contour_landmarks_path'):
                os.environ['STEP5_EOS_CONTOUR_LANDMARKS_PATH'] = str(args_dict['eos_contour_landmarks_path'])
            if args_dict.get('eos_fit_every_n') is not None:
                os.environ['STEP5_EOS_FIT_EVERY_N'] = str(args_dict['eos_fit_every_n'])
            if args_dict.get('eos_max_faces') is not None:
                os.environ['STEP5_EOS_MAX_FACES'] = str(args_dict['eos_max_faces'])
            if args_dict.get('eos_max_width') is not None:
                os.environ['STEP5_EOS_MAX_WIDTH'] = str(args_dict['eos_max_width'])
            if args_dict.get('eos_jawopen_scale') is not None:
                os.environ['STEP5_EOS_JAWOPEN_SCALE'] = str(args_dict['eos_jawopen_scale'])
            if args_dict.get('opencv_max_faces') is not None:
                os.environ['STEP5_OPENCV_MAX_FACES'] = str(args_dict['opencv_max_faces'])
            if args_dict.get('opencv_jawopen_scale') is not None:
                os.environ['STEP5_OPENCV_JAWOPEN_SCALE'] = str(args_dict['opencv_jawopen_scale'])
            if args_dict.get('facemesh_onnx_path'):
                os.environ['STEP5_FACEMESH_ONNX_PATH'] = args_dict['facemesh_onnx_path']
            if args_dict.get('pyfeat_model_path'):
                os.environ['STEP5_PYFEAT_MODEL_PATH'] = args_dict['pyfeat_model_path']
            if args_dict.get('step5_onnx_intra_op_threads'):
                os.environ['STEP5_ONNX_INTRA_OP_THREADS'] = str(args_dict['step5_onnx_intra_op_threads'])
            if args_dict.get('step5_onnx_inter_op_threads'):
                os.environ['STEP5_ONNX_INTER_OP_THREADS'] = str(args_dict['step5_onnx_inter_op_threads'])
            if args_dict.get('openseeface_models_dir'):
                os.environ['STEP5_OPENSEEFACE_MODELS_DIR'] = str(args_dict['openseeface_models_dir'])
            if args_dict.get('openseeface_model_id') is not None:
                os.environ['STEP5_OPENSEEFACE_MODEL_ID'] = str(args_dict['openseeface_model_id'])
            if args_dict.get('openseeface_detection_model_path'):
                os.environ['STEP5_OPENSEEFACE_DETECTION_MODEL_PATH'] = str(args_dict['openseeface_detection_model_path'])
            if args_dict.get('openseeface_landmark_model_path'):
                os.environ['STEP5_OPENSEEFACE_LANDMARK_MODEL_PATH'] = str(args_dict['openseeface_landmark_model_path'])
            if args_dict.get('openseeface_detect_every_n') is not None:
                os.environ['STEP5_OPENSEEFACE_DETECT_EVERY_N'] = str(args_dict['openseeface_detect_every_n'])
            if args_dict.get('openseeface_detection_threshold') is not None:
                os.environ['STEP5_OPENSEEFACE_DETECTION_THRESHOLD'] = str(args_dict['openseeface_detection_threshold'])
            if args_dict.get('openseeface_max_faces') is not None:
                os.environ['STEP5_OPENSEEFACE_MAX_FACES'] = str(args_dict['openseeface_max_faces'])
            if args_dict.get('openseeface_jawopen_scale') is not None:
                os.environ['STEP5_OPENSEEFACE_JAWOPEN_SCALE'] = str(args_dict['openseeface_jawopen_scale'])
            if args_dict.get('object_detector_model'):
                os.environ['STEP5_OBJECT_DETECTOR_MODEL'] = args_dict['object_detector_model']
            if args_dict.get('object_detector_model_path'):
                os.environ['STEP5_OBJECT_DETECTOR_MODEL_PATH'] = args_dict['object_detector_model_path']
            
            
            # Ensure profiling / throttling hints reach OpenCV-based engines
            profiling_enabled = bool(args_dict.get('enable_profiling'))
            os.environ['STEP5_ENABLE_PROFILING'] = "1" if profiling_enabled else "0"

            throttle_raw = args_dict.get('blendshapes_throttle_n', 1)
            try:
                throttle_value = str(max(1, int(throttle_raw or 1)))
            except Exception:
                throttle_value = "1"
            os.environ['STEP5_BLENDSHAPES_THROTTLE_N'] = throttle_value
            
            if engine_norm == "openseeface":
                logging.info(
                    f"[WORKER-{worker_pid}] OpenSeeFace config: "
                    f"model_id={os.environ.get('STEP5_OPENSEEFACE_MODEL_ID')} "
                    f"models_dir={os.environ.get('STEP5_OPENSEEFACE_MODELS_DIR')} "
                    f"detection_model_path={os.environ.get('STEP5_OPENSEEFACE_DETECTION_MODEL_PATH')} "
                    f"landmark_model_path={os.environ.get('STEP5_OPENSEEFACE_LANDMARK_MODEL_PATH')} "
                    f"detect_every_n={os.environ.get('STEP5_OPENSEEFACE_DETECT_EVERY_N')} "
                    f"detection_threshold={os.environ.get('STEP5_OPENSEEFACE_DETECTION_THRESHOLD')} "
                    f"max_faces={os.environ.get('STEP5_OPENSEEFACE_MAX_FACES')} "
                    f"jawopen_scale={os.environ.get('STEP5_OPENSEEFACE_JAWOPEN_SCALE')} "
                    f"max_width={os.environ.get('STEP5_OPENSEEFACE_MAX_WIDTH') or os.environ.get('STEP5_YUNET_MAX_WIDTH')}"
                )

            from face_engines import create_face_engine
            use_gpu = args_dict.get('use_gpu', False)
            face_engine_global = create_face_engine(engine_norm, use_gpu=use_gpu)

            if args_dict.get('enable_object_detection', False):
                try:
                    if mp is None:
                        raise RuntimeError("mediapipe is not available")

                    BaseOptions = mp.tasks.BaseOptions
                    VisionRunningMode = mp.tasks.vision.RunningMode
                    ObjectDetector = mp.tasks.vision.ObjectDetector
                    ObjectDetectorOptions = mp.tasks.vision.ObjectDetectorOptions

                    delegate = BaseOptions.Delegate.CPU

                    object_detector_model_name = args_dict.get('object_detector_model', 'efficientdet_lite2')
                    object_model_path = ObjectDetectorRegistry.resolve_model_path(
                        model_name=object_detector_model_name,
                        models_dir=models_dir,
                        override_path=args_dict.get('object_detector_model_path'),
                    )
                    logging.info(
                        f"[WORKER-{worker_pid}] Using object detector (face_engine mode): "
                        f"{object_detector_model_name} at {object_model_path}"
                    )

                    object_options = ObjectDetectorOptions(
                        base_options=BaseOptions(model_asset_path=str(object_model_path), delegate=delegate),
                        running_mode=VisionRunningMode.VIDEO,
                        max_results=args_dict.get('object_max_results', 5),
                        score_threshold=args_dict.get('object_score_threshold', 0.4),
                    )
                    object_detector_global = ObjectDetector.create_from_options(object_options)
                except Exception as e:
                    logging.error(
                        f"[WORKER-{worker_pid}] Failed to initialize object detector (face_engine mode): {e}"
                    )
                    object_detector_global = None

            tracking_engine_type = "face_engine"
            logging.info(f"[WORKER-{worker_pid}] Initialized {engine_norm} engine")
        else:
            if mp is None:
                raise RuntimeError(
                    "mediapipe is required for the default tracking engine but is not available in this environment"
                )

            BaseOptions = mp.tasks.BaseOptions
            VisionRunningMode = mp.tasks.vision.RunningMode
            FaceLandmarker = mp.tasks.vision.FaceLandmarker
            FaceLandmarkerOptions = mp.tasks.vision.FaceLandmarkerOptions
            ObjectDetector = mp.tasks.vision.ObjectDetector
            ObjectDetectorOptions = mp.tasks.vision.ObjectDetectorOptions
            
            # Configure delegate (CPU or GPU)
            use_gpu = args_dict.get('use_gpu', False)
            if use_gpu:
                try:
                    # Attempt to use GPU delegate for MediaPipe
                    delegate = BaseOptions.Delegate.GPU
                    logging.info(f"[WORKER-{worker_pid}] Attempting to use GPU delegate for MediaPipe")
                except AttributeError:
                    logging.warning(f"[WORKER-{worker_pid}] GPU delegate not available in MediaPipe, falling back to CPU")
                    delegate = BaseOptions.Delegate.CPU
            else:
                delegate = BaseOptions.Delegate.CPU
            face_model_candidates = [
                models_dir / "face_detectors" / "mediapipe" / "face_landmarker_v2_with_blendshapes.task",
                models_dir / "face_landmarker_v2_with_blendshapes.task",
            ]
            face_model_path = next((p for p in face_model_candidates if p.exists()), face_model_candidates[0])
            
            # Resolve object detector model using registry
            object_detector_model_name = args_dict.get('object_detector_model', 'efficientdet_lite2')
            try:
                object_model_path = ObjectDetectorRegistry.resolve_model_path(
                    model_name=object_detector_model_name,
                    models_dir=models_dir,
                    override_path=args_dict.get('object_detector_model_path')
                )
                logging.info(f"[WORKER-{worker_pid}] Using object detector: {object_detector_model_name} at {object_model_path}")
            except (ValueError, FileNotFoundError) as e:
                logging.error(f"[WORKER-{worker_pid}] Failed to resolve object detector model: {e}")
                raise
            
            env_max_faces = _parse_optional_positive_int(args_dict.get('mediapipe_max_faces'))
            num_faces = int(env_max_faces if env_max_faces is not None else (args_dict.get('mp_landmarker_num_faces', 5) or 5))

            face_options = FaceLandmarkerOptions(
                base_options=BaseOptions(model_asset_path=str(face_model_path), delegate=delegate),
                running_mode=VisionRunningMode.VIDEO,
                num_faces=num_faces,
                min_face_detection_confidence=args_dict.get('mp_landmarker_min_face_detection_confidence', 0.3),
                min_face_presence_confidence=args_dict.get('mp_landmarker_min_face_presence_confidence', 0.2),
                min_tracking_confidence=args_dict.get('mp_landmarker_min_tracking_confidence', 0.3),
                output_face_blendshapes=True
            )
            
            object_options = ObjectDetectorOptions(
                base_options=BaseOptions(model_asset_path=str(object_model_path), delegate=delegate),
                running_mode=VisionRunningMode.VIDEO,
                max_results=args_dict.get('object_max_results', 5),
                score_threshold=args_dict.get('object_score_threshold', 0.4)
            )
            
            landmarker_global = FaceLandmarker.create_from_options(face_options)
            object_detector_global = ObjectDetector.create_from_options(object_options)
            tracking_engine_type = "mediapipe"
            logging.info(f"[WORKER-{worker_pid}] Initialized MediaPipe engine")
        
        logging.info(f"[WORKER-{worker_pid}] Initialization complete")
        
    except Exception as e:
        logging.error(f"[WORKER-{worker_pid}] Initialization failed: {e}")
        landmarker_global = None
        object_detector_global = None
        face_engine_global = None


def process_frame_chunk(chunk_data):
    """
    Process a chunk of frames using the initialized models.
    Returns detection results for all frames in the chunk.
    """
    global landmarker_global, object_detector_global, face_engine_global, tracking_engine_type
    
    worker_pid = os.getpid()
    
    if tracking_engine_type == "face_engine":
        if not face_engine_global:
            logging.error(f"[WORKER-{worker_pid}] Face engine not properly initialized")
            return []
    elif tracking_engine_type == "mediapipe":
        if not landmarker_global:
            logging.error(f"[WORKER-{worker_pid}] MediaPipe not properly initialized")
            return []
    else:
        logging.error(f"[WORKER-{worker_pid}] No tracking engine initialized")
        return []
    
    chunk_start, chunk_end, video_path, args_dict = chunk_data
    results = []
    
    try:
        logging.info(f"[WORKER-{worker_pid}] Processing chunk [{chunk_start}, {chunk_end}] ({chunk_end - chunk_start + 1} frames)")

        enable_profiling = bool(args_dict.get('enable_profiling'))
        profiling_stats = {
            "to_rgb_total": 0.0,
            "detect_total": 0.0,
            "post_total": 0.0,
            "frame_count": 0,
        }
        blendshapes_throttle_n = max(1, int(args_dict.get('blendshapes_throttle_n', 1) or 1))
        jaw_open_scale = float(args_dict.get('mediapipe_jawopen_scale', 1.0) or 1.0)
        max_width = _parse_optional_positive_int(args_dict.get('mediapipe_max_width'))
        blendshapes_cache = {}
        
        # Open video capture for this chunk
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            logging.error(f"[WORKER-{worker_pid}] Failed to open video: {video_path}")
            return []
        
        cap.read()
        cap.set(cv2.CAP_PROP_POS_FRAMES, chunk_start)
        
        frames_processed = 0
        for frame_idx in range(chunk_start, chunk_end + 1):
            frames_processed += 1
            
            # Log progression every 10 frames
            if frames_processed % 10 == 0:
                logging.debug(f"[WORKER-{worker_pid}] Chunk [{chunk_start}, {chunk_end}]: processed {frames_processed}/{chunk_end - chunk_start + 1} frames")
            if enable_profiling:
                profiling_stats["frame_count"] += 1
            ret, frame = cap.read()
            if not ret:
                try:
                    cap.release()
                except Exception:
                    pass

                cap = cv2.VideoCapture(video_path)
                if cap.isOpened():
                    cap.read()
                    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
                    ret, frame = cap.read()

                    if (not ret) and frame_idx > 0:
                        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx - 1)
                        cap.read()
                        ret, frame = cap.read()

                    if (not ret) and args_dict.get('fps'):
                        try:
                            cap.set(
                                cv2.CAP_PROP_POS_MSEC,
                                int(frame_idx * 1000 / float(args_dict.get('fps', 25))),
                            )
                            ret, frame = cap.read()
                        except Exception:
                            pass

                if not ret:
                    # Tolerate last frame read failures (common with certain codecs)
                    if frame_idx >= args_dict.get('total_frames', 0) - 1:
                        logging.debug(
                            f"Skipping unreadable frame {frame_idx} (near end of video) "
                            f"for chunk [{chunk_start}, {chunk_end}] in {Path(video_path).name}."
                        )
                    else:
                        logging.warning(
                            f"Failed to read frame {frame_idx} for chunk [{chunk_start}, {chunk_end}] "
                            f"in {Path(video_path).name}."
                        )
                    results.append({
                        'frame_idx': frame_idx,
                        'detections': [],
                        'face_detected': False
                    })

                    next_frame_idx = frame_idx + 1
                    if next_frame_idx <= chunk_end:
                        try:
                            cap.release()
                        except Exception:
                            pass
                        cap = cv2.VideoCapture(video_path)
                        if cap.isOpened():
                            cap.read()
                            cap.set(cv2.CAP_PROP_POS_FRAMES, next_frame_idx)
                    continue
            
            current_detections = []
            face_detected = False
            
            if tracking_engine_type == "face_engine":
                try:
                    detections = face_engine_global.detect(frame)
                    if detections:
                        face_detected = True
                        current_detections.extend(detections)
                except Exception as e:
                    logging.warning(f"Face engine detection failed for frame {frame_idx}: {e}")
            else:
                orig_h, orig_w = frame.shape[:2]
                work_frame = frame
                scale_to_original = 1.0
                if max_width is not None and orig_w > max_width:
                    scale_factor = float(max_width) / float(orig_w)
                    work_h = max(1, int(orig_h * scale_factor))
                    work_frame = cv2.resize(frame, (int(max_width), int(work_h)), interpolation=cv2.INTER_LINEAR)
                    scale_to_original = float(orig_w) / float(work_frame.shape[1])

                t_to_rgb = time.perf_counter() if enable_profiling else 0.0
                mp_image = mp.Image(
                    image_format=mp.ImageFormat.SRGB,
                    data=cv2.cvtColor(work_frame, cv2.COLOR_BGR2RGB)
                )
                if enable_profiling:
                    profiling_stats["to_rgb_total"] += time.perf_counter() - t_to_rgb

                timestamp_ms = int(frame_idx * 1000 / args_dict.get('fps', 25))
                
                try:
                    t_detect = time.perf_counter() if enable_profiling else 0.0
                    face_result = landmarker_global.detect_for_video(mp_image, timestamp_ms)
                    if enable_profiling:
                        profiling_stats["detect_total"] += time.perf_counter() - t_detect
                    
                    if face_result.face_landmarks:
                        face_detected = True
                        
                        for i, landmarks in enumerate(face_result.face_landmarks):
                            t_post = time.perf_counter() if enable_profiling else 0.0
                            x_coords = [lm.x * orig_w for lm in landmarks]
                            y_coords = [lm.y * orig_h for lm in landmarks]
                            bbox = (
                                int(min(x_coords)), int(min(y_coords)),
                                int(max(x_coords) - min(x_coords)),
                                int(max(y_coords) - min(y_coords))
                            )
                            centroid = (int(np.mean(x_coords)), int(np.mean(y_coords)))
                            
                            det = {
                                "bbox": bbox,
                                "centroid": centroid,
                                "source_detector": "face_landmarker",
                                "label": "face"
                            }
                            
                            if face_result.face_blendshapes:
                                raw_blendshapes = {
                                    cat.category_name: cat.score
                                    for cat in face_result.face_blendshapes[i]
                                }
                                scaled_blendshapes = _apply_jawopen_scale(raw_blendshapes, jaw_open_scale)

                                current_frame_num = frame_idx + 1
                                should_update_blendshapes = (blendshapes_throttle_n <= 1) or ((current_frame_num % blendshapes_throttle_n) == 0)
                                object_id = f"{bbox[0]}_{bbox[1]}_{bbox[2]}_{bbox[3]}"
                                if should_update_blendshapes or object_id not in blendshapes_cache:
                                    blendshapes_cache[object_id] = scaled_blendshapes
                                    det["blendshapes"] = scaled_blendshapes
                                else:
                                    det["blendshapes"] = blendshapes_cache.get(object_id)
                            
                            current_detections.append(det)

                            if enable_profiling:
                                profiling_stats["post_total"] += time.perf_counter() - t_post
                
                except Exception as e:
                    logging.warning(f"Face detection failed for frame {frame_idx}: {e}")

                if enable_profiling:
                    fc = int(profiling_stats.get("frame_count", 0) or 0)
                    if fc > 0 and (fc % 20) == 0:
                        to_rgb_ms = (profiling_stats["to_rgb_total"] / fc) * 1000.0
                        detect_ms = (profiling_stats["detect_total"] / fc) * 1000.0
                        post_ms = (profiling_stats["post_total"] / fc) * 1000.0
                        logging.info(
                            "[PROFILING] MediaPipe after %s frames: to_rgb=%.2fms/frame, detect=%.2fms/frame, post=%.2fms/frame",
                            fc,
                            to_rgb_ms,
                            detect_ms,
                            post_ms,
                        )
            
            # Object detection fallback if no faces detected
            if not face_detected and args_dict.get('enable_object_detection', False) and object_detector_global:
                try:
                    if tracking_engine_type == "face_engine":
                        mp_image = mp.Image(
                            image_format=mp.ImageFormat.SRGB,
                            data=cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                        )
                        timestamp_ms = int(frame_idx * 1000 / args_dict.get('fps', 25))
                    object_result = object_detector_global.detect_for_video(mp_image, timestamp_ms)
                    
                    if object_result.detections:
                        for detection in object_result.detections:
                            bbox = detection.bounding_box
                            x_min = int(bbox.origin_x)
                            y_min = int(bbox.origin_y)
                            width = int(bbox.width)
                            height = int(bbox.height)

                            if tracking_engine_type != "face_engine":
                                x_min = int(max(0, x_min * scale_to_original))
                                y_min = int(max(0, y_min * scale_to_original))
                                width = int(max(0, width * scale_to_original))
                                height = int(max(0, height * scale_to_original))
                            
                            centroid = (x_min + width // 2, y_min + height // 2)
                            
                            best_category = detection.categories[0] if detection.categories else None
                            label = best_category.category_name if best_category else "object"
                            confidence = best_category.score if best_category else 0.0
                            
                            det = {
                                "bbox": (x_min, y_min, width, height),
                                "centroid": centroid,
                                "source_detector": "object_detector",
                                "label": label,
                                "confidence": confidence,
                                "blendshapes": None,
                                "is_speaking": None
                            }
                            current_detections.append(det)
                            
                except Exception as e:
                    logging.warning(f"Object detection failed for frame {frame_idx}: {e}")
            
            results.append({
                'frame_idx': frame_idx,
                'detections': current_detections,
                'face_detected': face_detected
            })
        
        cap.release()
        logging.info(f"[WORKER-{worker_pid}] Chunk [{chunk_start}, {chunk_end}] completed: {len(results)} frames processed")
        return results
        
    except Exception as e:
        logging.error(f"[WORKER-{worker_pid}] CRITICAL ERROR in chunk [{chunk_start}, {chunk_end}]: {type(e).__name__}: {e}")
        import traceback
        logging.error(f"[WORKER-{worker_pid}] Traceback: {traceback.format_exc()}")
        try:
            cap.release()
        except:
            pass
        return []


def process_video_multiprocessing(args, video_capture, total_frames):
    """
    Process video using multiprocessing with frame chunking.
    """
    logging.info(f"Starting multiprocessing with {args.mp_num_workers_internal} workers")
    
    # Get video metadata
    fps = video_capture.get(cv2.CAP_PROP_FPS)
    
    # Prepare arguments for workers
    models_dir = Path(args.models_dir)
    args_dict = {
        'tracking_engine': getattr(args, 'tracking_engine', None),
        'mp_landmarker_num_faces': args.mp_landmarker_num_faces,
        'mp_landmarker_min_face_detection_confidence': args.mp_landmarker_min_face_detection_confidence,
        'mp_landmarker_min_face_presence_confidence': args.mp_landmarker_min_face_presence_confidence,
        'mp_landmarker_min_tracking_confidence': args.mp_landmarker_min_tracking_confidence,
        'object_max_results': getattr(args, 'object_max_results', 5),
        'object_score_threshold': getattr(args, 'object_score_threshold', 0.4),
        'enable_object_detection': getattr(args, 'enable_object_detection', False),
        'fps': fps,
        'enable_profiling': _is_truthy_env(os.environ.get('STEP5_ENABLE_PROFILING', '0')),
        'blendshapes_throttle_n': max(1, int(os.environ.get('STEP5_BLENDSHAPES_THROTTLE_N', '1'))),
        'mediapipe_max_width': os.environ.get('STEP5_MEDIAPIPE_MAX_WIDTH'),
        'mediapipe_max_faces': os.environ.get('STEP5_MEDIAPIPE_MAX_FACES'),
        'mediapipe_jawopen_scale': os.environ.get('STEP5_MEDIAPIPE_JAWOPEN_SCALE', '1.0'),
        # OpenCV engine paths (env vars not inherited by ProcessPoolExecutor)
        'yunet_model_path': os.environ.get('STEP5_YUNET_MODEL_PATH'),
        'opencv_max_faces': os.environ.get('STEP5_OPENCV_MAX_FACES'),
        'opencv_jawopen_scale': os.environ.get('STEP5_OPENCV_JAWOPEN_SCALE'),
        'facemesh_onnx_path': os.environ.get('STEP5_FACEMESH_ONNX_PATH'),
        'pyfeat_model_path': os.environ.get('STEP5_PYFEAT_MODEL_PATH'),
        # ONNX Runtime performance tuning (used by several engines)
        'step5_onnx_intra_op_threads': os.environ.get('STEP5_ONNX_INTRA_OP_THREADS'),
        'step5_onnx_inter_op_threads': os.environ.get('STEP5_ONNX_INTER_OP_THREADS'),
        # OpenSeeFace engine configuration
        'openseeface_models_dir': os.environ.get('STEP5_OPENSEEFACE_MODELS_DIR'),
        'openseeface_model_id': os.environ.get('STEP5_OPENSEEFACE_MODEL_ID'),
        'openseeface_detection_model_path': os.environ.get('STEP5_OPENSEEFACE_DETECTION_MODEL_PATH'),
        'openseeface_landmark_model_path': os.environ.get('STEP5_OPENSEEFACE_LANDMARK_MODEL_PATH'),
        'openseeface_detect_every_n': os.environ.get('STEP5_OPENSEEFACE_DETECT_EVERY_N'),
        'openseeface_detection_threshold': os.environ.get('STEP5_OPENSEEFACE_DETECTION_THRESHOLD'),
        'openseeface_max_faces': os.environ.get('STEP5_OPENSEEFACE_MAX_FACES'),
        'openseeface_jawopen_scale': os.environ.get('STEP5_OPENSEEFACE_JAWOPEN_SCALE'),
        'eos_models_dir': os.environ.get('STEP5_EOS_MODELS_DIR'),
        'eos_sfm_model_path': os.environ.get('STEP5_EOS_SFM_MODEL_PATH'),
        'eos_expression_blendshapes_path': os.environ.get('STEP5_EOS_EXPRESSION_BLENDSHAPES_PATH'),
        'eos_landmark_mapper_path': os.environ.get('STEP5_EOS_LANDMARK_MAPPER_PATH'),
        'eos_edge_topology_path': os.environ.get('STEP5_EOS_EDGE_TOPOLOGY_PATH'),
        'eos_model_contour_path': os.environ.get('STEP5_EOS_MODEL_CONTOUR_PATH'),
        'eos_contour_landmarks_path': os.environ.get('STEP5_EOS_CONTOUR_LANDMARKS_PATH'),
        'eos_fit_every_n': os.environ.get('STEP5_EOS_FIT_EVERY_N'),
        'eos_max_faces': os.environ.get('STEP5_EOS_MAX_FACES'),
        'eos_max_width': os.environ.get('STEP5_EOS_MAX_WIDTH'),
        'eos_jawopen_scale': os.environ.get('STEP5_EOS_JAWOPEN_SCALE'),
        'object_detector_model': getattr(args, 'object_detector_model', None) or os.environ.get('STEP5_OBJECT_DETECTOR_MODEL'),
        'object_detector_model_path': getattr(args, 'object_detector_model_path', None) or os.environ.get('STEP5_OBJECT_DETECTOR_MODEL_PATH'),
        'total_frames': total_frames,
    }
    
    # Create frame chunks (adaptive chunk size to keep CPU workers saturated)
    # If chunk_size is provided (>0), honor it; otherwise, derive adaptively.
    provided_chunk_size = int(getattr(args, 'chunk_size', 400))
    if provided_chunk_size <= 0:
        # Target ~5 chunks per worker, minimum 20 chunks overall.
        mp_workers = max(1, int(getattr(args, 'mp_num_workers_internal', 1)))
        target_chunks = max(mp_workers * 5, 20)
        # Compute chunk size from total frames and clamp to avoid too fine granularity.
        adaptive_chunk = int(math.ceil(total_frames / max(1, target_chunks)))
        # Allow small chunks to reach ~5x workers (e.g., ~75 chunks for ~1600 frames & 15 workers)
        # Bounds can be overridden via args.chunk_min/chunk_max if provided.
        min_bound = int(getattr(args, 'chunk_min', 20) or 20)
        max_bound = int(getattr(args, 'chunk_max', 400) or 400)
        if min_bound > max_bound:
            min_bound, max_bound = max_bound, min_bound
        chunk_size = max(min_bound, min(max_bound, adaptive_chunk))
        logging.info(
            f"Adaptive chunking enabled: total_frames={total_frames}, workers={mp_workers}, "
            f"target_chunks={target_chunks}, bounds=[{min_bound},{max_bound}], selected_chunk_size={chunk_size}"
        )
    else:
        chunk_size = provided_chunk_size
        logging.info(f"Using provided chunk_size={chunk_size}")

    chunks = []
    
    for start_frame in range(0, total_frames, chunk_size):
        end_frame = min(start_frame + chunk_size - 1, total_frames - 1)
        chunks.append((start_frame, end_frame, args.video_file_path, args_dict))
    
    logging.info(f"Created {len(chunks)} chunks for processing (chunk_size={chunk_size})")
    
    # Process chunks using multiprocessing
    all_results = {}
    face_detection_count = 0
    processing_start_time = time.time()
    
    # Use ProcessPoolExecutor for better resource management
    with ProcessPoolExecutor(
        max_workers=args.mp_num_workers_internal,
        initializer=init_worker_process,
        initargs=(models_dir, args_dict)
    ) as executor:
        
        # Submit all chunks
        future_to_chunk = {executor.submit(process_frame_chunk, chunk): chunk for chunk in chunks}
        
        completed_chunks = 0
        # Throttle progress logs to ~10% intervals
        progress_every = max(1, len(chunks) // 10)
        for future in as_completed(future_to_chunk):
            chunk = future_to_chunk[future]
            try:
                chunk_results = future.result()
                
                # Store results by frame index
                for result in chunk_results:
                    all_results[result['frame_idx']] = result
                    if result['face_detected']:
                        face_detection_count += 1
                
                completed_chunks += 1
                
                # Progress logging (throttled)
                if completed_chunks % progress_every == 0:
                    progress_percent = (completed_chunks / len(chunks)) * 100
                    elapsed_time = time.time() - processing_start_time
                    fps_rate = (completed_chunks * chunk_size) / elapsed_time if elapsed_time > 0 else 0
                    print(f"[Progression]|{int(progress_percent)}|{min(completed_chunks * chunk_size, total_frames)}|{total_frames}")
                    logging.info(f"Multiprocessing: {completed_chunks}/{len(chunks)} chunks ({progress_percent:.1f}%) - {fps_rate:.1f} fps (chunk_size={chunk_size})")
                    
            except Exception as e:
                logging.error(f"Chunk processing failed: {e}")
    
    logging.info("Multiprocessing complete, applying sequential tracking...")
    
    # Apply tracking sequentially (must be sequential for consistency)
    tracked_objects = {}
    next_id_counter = {'value': 0}
    final_output = {
        "metadata": {
            "video_path": args.video_file_path,
            "total_frames": total_frames,
            "fps": fps
        },
        "frames": []
    }
    
    # Initialize enhanced speaking detector
    enhanced_speaking_detector = None
    try:
        enhanced_speaking_detector = EnhancedSpeakingDetector(
            video_path=args.video_file_path,
            jaw_threshold=args.speaking_detection_jaw_open_threshold
        )
        logging.info("Enhanced speaking detection initialized")
    except Exception as e:
        logging.warning(f"Failed to initialize enhanced speaking detector: {e}")
    
    missing_frame_count = 0
    first_missing_frame = None
    for frame_idx in range(total_frames):
        result = all_results.get(frame_idx)
        if result is None:
            missing_frame_count += 1
            if first_missing_frame is None:
                first_missing_frame = frame_idx
            detections = []
        else:
            detections = result.get('detections', [])
        
        tracked_for_frame = apply_tracking_and_management(
            tracked_objects, detections, next_id_counter,
            args.mp_max_distance_tracking, args.mp_frames_unseen_deregister,
            args.speaking_detection_jaw_open_threshold,
            enhanced_speaking_detector=enhanced_speaking_detector,
            current_frame_num=frame_idx + 1
        )
        
        final_output["frames"].append({
            "frame": frame_idx + 1,
            "tracked_objects": tracked_for_frame if tracked_for_frame else []
        })

    if missing_frame_count > 0:
        logging.warning(
            f"Missing detection results for {missing_frame_count}/{total_frames} frames "
            f"(first missing frame index: {first_missing_frame}). Output remains dense with empty tracked_objects." 
        )
    
    # Calculate final statistics
    face_detection_rate = (face_detection_count / total_frames * 100) if total_frames > 0 else 0
    processing_time = time.time() - processing_start_time
    
    logging.info("Multiprocessing summary:")
    logging.info(f"  Total frames expected: {total_frames}")
    logging.info(f"  Total frames with detection results: {len(all_results)}")
    logging.info(f"  Frames with faces: {face_detection_count}")
    logging.info(f"  Face detection success rate: {face_detection_rate:.2f}%")
    logging.info(f"  Processing time: {processing_time:.2f} seconds")
    logging.info(f"  Average FPS: {total_frames / processing_time:.2f}")
    logging.info(f"  Total frames exported: {len(final_output['frames'])}")
    
    return final_output


def main(args):
    """Main processing function with multiprocessing support."""
    engine_norm = (str(getattr(args, 'tracking_engine', '') or '').strip().lower())
    
    worker_type = "CPU_MULTIPROCESSING" if getattr(args, 'mp_num_workers_internal', 1) > 1 else "CPU"
    logging.info(f"Starting {worker_type} processing for: {Path(args.video_file_path).name}")
    
    # Use safe video processing with automatic resource cleanup
    try:
        with safe_video_processing(args.video_file_path) as (video_capture, temp_manager):
            # Get video metadata safely
            fps = video_capture.get(cv2.CAP_PROP_FPS)
            total_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))
            
            logging.info(f"Video metadata - FPS: {fps}, Total frames: {total_frames}")
            
            # Check if multiprocessing should be used
            use_multiprocessing = (not getattr(args, 'use_gpu', False) and 
                                 getattr(args, 'mp_num_workers_internal', 1) > 1)
            
            if use_multiprocessing:
                final_output = process_video_multiprocessing(args, video_capture, total_frames)
            else:
                # Fallback to sequential processing (existing implementation)
                logging.info("Using sequential processing (fallback)")
                # ... existing sequential logic would go here
                return False
                
    except Exception as e:
        logging.error(f"Error processing video {args.video_file_path}: {e}")
        raise
    
    # Save output
    output_path = Path(args.video_file_path).with_suffix('.json')
    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(final_output, f, indent=2, ensure_ascii=False)
        logging.info(f"Processing complete. JSON saved: {output_path.name}")
        print(f"[Progression]|100|{total_frames}|{total_frames}", flush=True)
    except Exception as e:
        logging.error(f"Failed to save output file {output_path}: {e}")
        raise


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Enhanced CPU Worker with Multiprocessing")
    parser.add_argument("video_file_path")
    parser.add_argument("--models_dir", required=True)
    parser.add_argument("--use_gpu", action="store_true")
    parser.add_argument("--tracking_engine", default=None, help="Tracking engine: mediapipe_landmarker (default), opencv_haar, opencv_yunet, opencv_yunet_pyfeat, openseeface")
    parser.add_argument("--mp_landmarker_num_faces", type=int, default=5)
    parser.add_argument("--mp_landmarker_min_face_detection_confidence", type=float, default=0.3)
    parser.add_argument("--mp_landmarker_min_face_presence_confidence", type=float, default=0.2)
    parser.add_argument("--mp_landmarker_min_tracking_confidence", type=float, default=0.3)
    parser.add_argument("--mp_max_distance_tracking", type=int, default=80)
    parser.add_argument("--mp_frames_unseen_deregister", type=int, default=7)
    parser.add_argument("--speaking_detection_jaw_open_threshold", type=float, default=0.08)
    parser.add_argument("--enable_object_detection", action="store_true")
    parser.add_argument("--object_score_threshold", type=float, default=0.4)
    parser.add_argument("--object_max_results", type=int, default=5)
    parser.add_argument("--mp_num_workers_internal", type=int, default=1)
    parser.add_argument("--chunk_size", type=int, default=0, help="Chunk size (frames) for multiprocessing splitting; 0=adaptive")
    parser.add_argument("--chunk_min", type=int, default=None, help="Minimum chunk size when adaptive is used")
    parser.add_argument("--chunk_max", type=int, default=None, help="Maximum chunk size when adaptive is used")
    
    args, _ = parser.parse_known_args()
    
    try:
        mp_proc.freeze_support()  # Required for Windows compatibility
        main(args)
        sys.exit(0)
    except Exception as e:
        logging.error(f"Critical error in worker: {e}", exc_info=True)
        sys.exit(1)
```

## File: workflow_scripts/step5/process_video_worker.py
```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import os, sys, json, argparse, logging, importlib, cv2, numpy as np
import threading, queue, time
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path

sys.path.append(str(Path(__file__).resolve().parent.parent.parent))
from utils.tracking_optimizations import apply_tracking_and_management
from utils.resource_manager import safe_video_processing, get_video_metadata, resource_tracker
from utils.enhanced_speaking_detection import EnhancedSpeakingDetector

from face_engines import create_face_engine
from object_detector_registry import ObjectDetectorRegistry

mp = None


def _ensure_mediapipe_loaded(required: bool = False):
    """Lazy import Mediapipe to avoid TensorFlow dependency unless needed."""
    global mp
    if mp is not None:
        return mp
    try:
        if not hasattr(np, "complex_"):
            np.complex_ = np.complex128
        mp = importlib.import_module("mediapipe")
        return mp
    except Exception as exc:
        msg = f"MediaPipe import failed: {exc}"
        if required:
            logging.error(msg)
            raise
        logging.warning(msg)
        return None

# Configuration du logger
log_dir = Path(__file__).resolve().parent.parent.parent / "logs" / "step5"
log_dir.mkdir(parents=True, exist_ok=True)
worker_type_str = "GPU" if "--use_gpu" in sys.argv else "CPU"
video_name_for_log = Path(sys.argv[1]).stem if len(sys.argv) > 1 else "unknown"
log_file = log_dir / f"worker_{worker_type_str}_{video_name_for_log}_{os.getpid()}.log"
logging.basicConfig(level=logging.INFO, format='%(asctime)s - [%(levelname)s] - %(message)s',
                    handlers=[logging.FileHandler(log_file, encoding='utf-8'), logging.StreamHandler(sys.stdout)])

# Limiter le threading interne d'OpenCV pour éviter la contention avec nos propres pools
try:
    cv2.setNumThreads(1)
    cv2.ocl.setUseOpenCL(False)
except Exception:
    # Sécurité: ignorer si non supporté sur la plateforme
    pass


def _parse_optional_positive_int(raw):
    if raw is None:
        return None
    raw_str = str(raw).strip()
    if not raw_str:
        return None
    try:
        value = int(raw_str)
    except Exception:
        return None
    if value <= 0:
        return None
    return value


def _is_truthy_env(raw):
    if raw is None:
        return False
    return str(raw).strip().lower() in {"1", "true", "yes"}


def _apply_jawopen_scale(blendshapes, scale):
    if not blendshapes or not isinstance(blendshapes, dict):
        return blendshapes
    try:
        jaw_open_raw = blendshapes.get("jawOpen")
        if jaw_open_raw is None:
            return blendshapes
        jaw_open_scaled = float(jaw_open_raw) * float(scale)
        jaw_open_scaled = float(np.clip(jaw_open_scaled, 0.0, 1.0))
        out = dict(blendshapes)
        out["jawOpen"] = jaw_open_scaled
        return out
    except Exception:
        return blendshapes


class FrameProcessor:
    """Thread-safe frame processor for multi-threaded CPU processing."""

    def __init__(self, landmarker, object_detector, args, enhanced_speaking_detector=None):
        self.landmarker = landmarker
        self.object_detector = object_detector
        self.args = args
        self.enhanced_speaking_detector = enhanced_speaking_detector
        self.lock = threading.Lock()

        self._enable_profiling = _is_truthy_env(os.environ.get("STEP5_ENABLE_PROFILING", "0"))
        self._profiling_stats = {
            "to_rgb_total": 0.0,
            "detect_total": 0.0,
            "post_total": 0.0,
            "frame_count": 0,
        }
        self._blendshapes_throttle_n = max(1, int(os.environ.get("STEP5_BLENDSHAPES_THROTTLE_N", "1")))
        self._jaw_open_scale = float(os.environ.get("STEP5_MEDIAPIPE_JAWOPEN_SCALE", "1.0"))
        self._max_width = _parse_optional_positive_int(os.environ.get("STEP5_MEDIAPIPE_MAX_WIDTH"))
        self._blendshapes_cache = {}

    def process_frame(self, frame_data):
        """Process a single frame and return detection results."""
        frame, frame_idx, timestamp_ms = frame_data

        try:
            orig_h, orig_w = frame.shape[:2]
            work_frame = frame
            scale_to_original = 1.0
            if self._max_width is not None and orig_w > self._max_width:
                scale_factor = float(self._max_width) / float(orig_w)
                work_h = max(1, int(orig_h * scale_factor))
                work_frame = cv2.resize(frame, (int(self._max_width), int(work_h)), interpolation=cv2.INTER_LINEAR)
                scale_to_original = float(orig_w) / float(work_frame.shape[1])

            if self._enable_profiling:
                with self.lock:
                    self._profiling_stats["frame_count"] += 1

            t_to_rgb = time.perf_counter() if self._enable_profiling else 0.0
            mp_image = mp.Image(
                image_format=mp.ImageFormat.SRGB,
                data=cv2.cvtColor(work_frame, cv2.COLOR_BGR2RGB)
            )
            if self._enable_profiling:
                with self.lock:
                    self._profiling_stats["to_rgb_total"] += time.perf_counter() - t_to_rgb

            current_detections = []
            face_detected = False

            # Try face detection first
            t_detect = time.perf_counter() if self._enable_profiling else 0.0
            face_result = self.landmarker.detect_for_video(mp_image, timestamp_ms)
            if self._enable_profiling:
                with self.lock:
                    self._profiling_stats["detect_total"] += time.perf_counter() - t_detect

            if face_result.face_landmarks:
                face_detected = True

                for i, landmarks in enumerate(face_result.face_landmarks):
                    t_post = time.perf_counter() if self._enable_profiling else 0.0
                    x_coords = [lm.x * orig_w for lm in landmarks]
                    y_coords = [lm.y * orig_h for lm in landmarks]
                    bbox = (
                        int(min(x_coords)), int(min(y_coords)),
                        int(max(x_coords) - min(x_coords)),
                        int(max(y_coords) - min(y_coords))
                    )
                    centroid = (int(np.mean(x_coords)), int(np.mean(y_coords)))
                    det = {
                        "bbox": bbox,
                        "centroid": centroid,
                        "source_detector": "face_landmarker",
                        "label": "face"
                    }
                    if face_result.face_blendshapes:
                        raw_blendshapes = {
                            cat.category_name: cat.score
                            for cat in face_result.face_blendshapes[i]
                        }
                        scaled_blendshapes = _apply_jawopen_scale(raw_blendshapes, self._jaw_open_scale)

                        current_frame_num = frame_idx + 1
                        should_update_blendshapes = (self._blendshapes_throttle_n <= 1) or ((current_frame_num % self._blendshapes_throttle_n) == 0)
                        object_id = f"{bbox[0]}_{bbox[1]}_{bbox[2]}_{bbox[3]}"
                        with self.lock:
                            if should_update_blendshapes or object_id not in self._blendshapes_cache:
                                self._blendshapes_cache[object_id] = scaled_blendshapes
                                det["blendshapes"] = scaled_blendshapes
                            else:
                                det["blendshapes"] = self._blendshapes_cache.get(object_id)
                    current_detections.append(det)

                    if self._enable_profiling:
                        with self.lock:
                            self._profiling_stats["post_total"] += time.perf_counter() - t_post

            if self._enable_profiling:
                with self.lock:
                    fc = int(self._profiling_stats.get("frame_count", 0) or 0)
                    if fc > 0 and (fc % 20) == 0:
                        to_rgb_ms = (self._profiling_stats["to_rgb_total"] / fc) * 1000.0
                        detect_ms = (self._profiling_stats["detect_total"] / fc) * 1000.0
                        post_ms = (self._profiling_stats["post_total"] / fc) * 1000.0
                        logging.info(
                            "[PROFILING] MediaPipe after %s frames: to_rgb=%.2fms/frame, detect=%.2fms/frame, post=%.2fms/frame",
                            fc,
                            to_rgb_ms,
                            detect_ms,
                            post_ms,
                        )

            # Use object detection fallback if no faces detected
            if not face_detected and getattr(self.args, 'enable_object_detection', False):
                try:
                    object_result = self.object_detector.detect_for_video(mp_image, timestamp_ms)

                    if object_result.detections:
                        for detection in object_result.detections:
                            bbox = detection.bounding_box
                            x_min = int(bbox.origin_x)
                            y_min = int(bbox.origin_y)
                            width = int(bbox.width)
                            height = int(bbox.height)

                            if scale_to_original != 1.0:
                                x_min = int(max(0, x_min * scale_to_original))
                                y_min = int(max(0, y_min * scale_to_original))
                                width = int(max(0, width * scale_to_original))
                                height = int(max(0, height * scale_to_original))

                            centroid = (x_min + width // 2, y_min + height // 2)

                            best_category = detection.categories[0] if detection.categories else None
                            label = best_category.category_name if best_category else "object"
                            confidence = best_category.score if best_category else 0.0

                            det = {
                                "bbox": (x_min, y_min, width, height),
                                "centroid": centroid,
                                "source_detector": "object_detector",
                                "label": label,
                                "confidence": confidence,
                                "blendshapes": None,
                                "is_speaking": None
                            }
                            current_detections.append(det)
                except Exception as e:
                    logging.warning(f"Object detection failed for frame {frame_idx + 1}: {e}")

            return {
                'frame_idx': frame_idx,
                'detections': current_detections,
                'face_detected': face_detected,
                'timestamp_ms': timestamp_ms
            }

        except Exception as e:
            logging.error(f"Error processing frame {frame_idx + 1}: {e}")
            return {
                'frame_idx': frame_idx,
                'detections': [],
                'face_detected': False,
                'timestamp_ms': timestamp_ms,
                'error': str(e)
            }


def process_video_multithreaded(args, video_capture, landmarker, object_detector, enhanced_speaking_detector, total_frames):
    """Process video using multiple threads for CPU workers."""
    logging.info(f"Starting multi-threaded processing with {args.mp_num_workers_internal} workers (bounded queue)")

    # Initialize tracking variables
    tracked_objects = {}
    next_id_counter = {'value': 0}

    # Get video metadata
    fps = video_capture.get(cv2.CAP_PROP_FPS)

    final_output = {
        "metadata": {
            "video_path": args.video_file_path,
            "total_frames": total_frames,
            "fps": fps
        },
        "frames": []
    }

    # Performance tracking
    face_detection_success_count = 0
    frames_processed = 0
    processing_start_time = time.time()

    # Create frame processor
    frame_processor = FrameProcessor(landmarker, object_detector, args, enhanced_speaking_detector)

    # Bounded queue pipeline: un thread lecteur + N threads workers
    q = queue.Queue(maxsize=64)
    results = {}
    results_lock = threading.Lock()

    def worker_loop(worker_id: int):
        nonlocal frames_processed
        completed_local = 0
        while True:
            item = q.get()
            if item is None:
                q.task_done()
                break
            frame, idx, ts_ms = item
            try:
                result = frame_processor.process_frame((frame, idx, ts_ms))
                with results_lock:
                    results[idx] = result
                completed_local += 1
                frames_processed += 1
                if frames_processed % 50 == 0:
                    progress_percent = (frames_processed / total_frames) * 100 if total_frames else 0
                    elapsed_time = time.time() - processing_start_time
                    fps_now = frames_processed / elapsed_time if elapsed_time > 0 else 0
                    print(f"[Progression]|{int(progress_percent)}|{frames_processed}|{total_frames}", flush=True)
                    logging.info(f"MT workers: {frames_processed}/{total_frames} ({progress_percent:.1f}%) - {fps_now:.1f} fps")
            except Exception as e:
                logging.error(f"Frame {idx} processing failed: {e}")
                with results_lock:
                    results[idx] = {
                        'frame_idx': idx,
                        'detections': [],
                        'face_detected': False,
                        'error': str(e)
                    }
            finally:
                # Libérer la référence à la frame pour GC
                del frame
                q.task_done()

    # Lancer les workers
    workers = [threading.Thread(target=worker_loop, args=(i,), daemon=True)
               for i in range(int(args.mp_num_workers_internal))]
    for t in workers:
        t.start()

    # Thread lecteur: lit et pousse les frames dans la queue (bornée)
    frame_read = 0
    logging.info("Reading frames and feeding bounded queue...")
    while video_capture.isOpened():
        ret, frame = video_capture.read()
        if not ret:
            break
        ts_ms = int(video_capture.get(cv2.CAP_PROP_POS_MSEC))
        q.put((frame, frame_read, ts_ms))
        frame_read += 1
        if frame_read % 100 == 0:
            logging.info(f"Read {frame_read} frames...")

    # Envoyer les sentinelles d'arrêt aux workers
    for _ in workers:
        q.put(None)

    # Attendre la fin du traitement
    q.join()
    for t in workers:
        t.join(timeout=1)

    logging.info("Parallel processing complete, applying tracking...")

    # Apply tracking in sequential order (must be sequential for consistency)
    for frame_idx in sorted(results.keys()):
        result = results[frame_idx]

        if result.get('face_detected', False):
            face_detection_success_count += 1

        # Apply tracking and management
        tracked_for_frame = apply_tracking_and_management(
            tracked_objects, result['detections'], next_id_counter,
            args.mp_max_distance_tracking, args.mp_frames_unseen_deregister,
            args.speaking_detection_jaw_open_threshold,
            enhanced_speaking_detector=enhanced_speaking_detector,
            current_frame_num=frame_idx + 1
        )

        # Add to final output
        final_output["frames"].append({
            "frame": frame_idx + 1,
            "tracked_objects": tracked_for_frame if tracked_for_frame else []
        })

    # Calculate final statistics
    face_detection_rate = (face_detection_success_count / frames_processed * 100) if frames_processed > 0 else 0
    processing_time = time.time() - processing_start_time

    logging.info("Multi-threaded processing summary:")
    logging.info(f"  Total frames processed: {frames_processed}")
    logging.info(f"  Frames with faces: {face_detection_success_count}")
    logging.info(f"  Face detection success rate: {face_detection_rate:.2f}%")
    logging.info(f"  Processing time: {processing_time:.2f} seconds")
    logging.info(f"  Average FPS: {frames_processed / processing_time:.2f}")
    logging.info(f"  Object detection fallback used: {getattr(args, 'enable_object_detection', False)}")
    logging.info(f"  Total frames exported: {len(final_output['frames'])}")

    return final_output


def main(args):
    """Traite une vidéo de manière séquentielle (un seul thread)."""
    worker_type = "GPU" if args.use_gpu else "CPU"
    logging.info(f"Démarrage du traitement séquentiel {worker_type} pour: {Path(args.video_file_path).name}")
    logging.info(f"LD_LIBRARY_PATH (worker) = {os.environ.get('LD_LIBRARY_PATH', '')}")
    logging.info(f"INSIGHTFACE_HOME (worker) = {os.environ.get('INSIGHTFACE_HOME', '')}")

    engine_name = getattr(args, 'tracking_engine', None)
    use_gpu_flag = bool(getattr(args, 'use_gpu', False))
    face_engine = None
    if engine_name:
        try:
            face_engine = create_face_engine(engine_name, use_gpu=use_gpu_flag)
        except Exception as e:
            logging.exception(f"Failed to initialize tracking engine '{engine_name}': {e}")
            sys.exit(1)

    final_output = None
    total_frames = 0

    if face_engine is not None:
        logging.info(f"Using OpenCV tracking engine: {engine_name}")
        try:
            with safe_video_processing(args.video_file_path) as (video_capture, temp_manager):
                fps = video_capture.get(cv2.CAP_PROP_FPS)
                total_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))

                object_detector = None
                object_options = None
                mp_module = None
                ObjectDetector = None
                object_detection_workers = max(1, int(getattr(args, 'mp_num_workers_internal', 1) or 1))
                if args.use_gpu and getattr(args, 'enable_object_detection', False):
                    logging.info(
                        "Object detection fallback workers (GPU face engine mode): %s",
                        object_detection_workers,
                    )
                if getattr(args, 'enable_object_detection', False):
                    try:
                        mp_module = _ensure_mediapipe_loaded(required=False)
                        if mp_module is None:
                            raise RuntimeError("mediapipe is not available (lazy import failed)")

                        BaseOptions = mp_module.tasks.BaseOptions
                        VisionRunningMode = mp_module.tasks.vision.RunningMode
                        ObjectDetector = mp_module.tasks.vision.ObjectDetector
                        ObjectDetectorOptions = mp_module.tasks.vision.ObjectDetectorOptions

                        delegate = BaseOptions.Delegate.CPU
                        models_dir = Path(args.models_dir)
                        object_detector_model_name = (
                            getattr(args, 'object_detector_model', None)
                            or os.environ.get('STEP5_OBJECT_DETECTOR_MODEL', 'efficientdet_lite2')
                        )
                        object_detector_model_path = (
                            getattr(args, 'object_detector_model_path', None)
                            or os.environ.get('STEP5_OBJECT_DETECTOR_MODEL_PATH')
                        )
                        object_model_path = ObjectDetectorRegistry.resolve_model_path(
                            model_name=object_detector_model_name,
                            models_dir=models_dir,
                            override_path=object_detector_model_path,
                        )
                        logging.info(
                            f"Using object detector (face_engine mode): {object_detector_model_name} at {object_model_path}"
                        )
                        object_options = ObjectDetectorOptions(
                            base_options=BaseOptions(model_asset_path=str(object_model_path), delegate=delegate),
                            running_mode=VisionRunningMode.IMAGE,
                            max_results=getattr(args, 'object_max_results', 5),
                            score_threshold=getattr(args, 'object_score_threshold', 0.4),
                        )
                        object_detector = ObjectDetector.create_from_options(object_options)
                        if object_detection_workers > 1:
                            logging.info(
                                "Object detection fallback: using one MediaPipe ObjectDetector instance per thread (IMAGE mode)."
                            )
                    except Exception as e:
                        logging.warning(
                            f"Failed to initialize object detector (face_engine mode); continuing without it: {e}"
                        )
                        object_detector = None
                        object_options = None
                        mp_module = None
                        ObjectDetector = None

                final_output = {
                    "metadata": {
                        "video_path": args.video_file_path,
                        "total_frames": total_frames,
                        "fps": fps,
                        "tracking_engine": engine_name,
                    },
                    "frames": [],
                }

                tracked_objects, next_id_counter = {}, {'value': 0}

                resource_id = resource_tracker.register_resource(
                    video_capture, 'video_capture', f"Processing {Path(args.video_file_path).name}"
                )

                try:
                    enhanced_speaking_detector = None
                    if getattr(args, 'speaking_detection_enabled', True):
                        try:
                            enhanced_speaking_detector = EnhancedSpeakingDetector(
                                video_path=args.video_file_path,
                                jaw_threshold=args.speaking_detection_jaw_open_threshold,
                            )
                            logging.info("Enhanced speaking detection initialized successfully")
                        except Exception as e:
                            logging.warning(f"Failed to initialize enhanced speaking detector: {e}")
                            enhanced_speaking_detector = None

                    object_tasks = None
                    object_results = {}
                    object_results_lock = threading.Lock()
                    object_worker_threads = []

                    def _object_worker_loop(worker_id: int):
                        thread_object_detector = None
                        if object_options is not None and ObjectDetector is not None:
                            try:
                                thread_object_detector = ObjectDetector.create_from_options(object_options)
                            except Exception as e:
                                logging.warning(
                                    "Object detection failed to initialize detector for worker %s (face_engine mode): %s",
                                    worker_id,
                                    e,
                                )
                                thread_object_detector = None
                        while True:
                            task = object_tasks.get() if object_tasks is not None else None
                            if task is None:
                                try:
                                    if thread_object_detector is not None and hasattr(thread_object_detector, 'close'):
                                        thread_object_detector.close()
                                except Exception:
                                    pass
                                if object_tasks is not None:
                                    object_tasks.task_done()
                                break
                            frame_local, frame_idx_local, timestamp_ms_local = task
                            try:
                                if mp_module is None or thread_object_detector is None:
                                    raise RuntimeError("object detector is not available for object detection fallback")
                                mp_image = mp_module.Image(
                                    image_format=mp_module.ImageFormat.SRGB,
                                    data=cv2.cvtColor(frame_local, cv2.COLOR_BGR2RGB)
                                )
                                object_result = thread_object_detector.detect(mp_image)
                                detections_out = []
                                if object_result.detections:
                                    for detection in object_result.detections:
                                        bbox = detection.bounding_box
                                        x_min = int(bbox.origin_x)
                                        y_min = int(bbox.origin_y)
                                        width = int(bbox.width)
                                        height = int(bbox.height)
                                        centroid = (x_min + width // 2, y_min + height // 2)
                                        best_category = detection.categories[0] if detection.categories else None
                                        label = best_category.category_name if best_category else "object"
                                        confidence = best_category.score if best_category else 0.0
                                        detections_out.append(
                                            {
                                                "bbox": (x_min, y_min, width, height),
                                                "centroid": centroid,
                                                "source_detector": "object_detector",
                                                "label": label,
                                                "confidence": confidence,
                                                "blendshapes": None,
                                                "is_speaking": None,
                                            }
                                        )
                                with object_results_lock:
                                    object_results[frame_idx_local] = detections_out
                            except Exception as e:
                                logging.warning(
                                    f"Object detection failed for frame {frame_idx_local + 1} (face_engine mode): {e}"
                                )
                                with object_results_lock:
                                    object_results[frame_idx_local] = []
                            finally:
                                del frame_local
                                object_tasks.task_done()

                    enable_object_fallback = bool(getattr(args, 'enable_object_detection', False))
                    enable_object_threads = bool(object_detector is not None and object_detection_workers > 1 and enable_object_fallback)
                    if enable_object_threads:
                        object_tasks = queue.Queue(maxsize=max(1, object_detection_workers) * 2)
                        for i in range(int(object_detection_workers)):
                            t = threading.Thread(target=_object_worker_loop, args=(i,), daemon=True)
                            object_worker_threads.append(t)
                            t.start()

                    frame_idx = 0
                    frame_detections = {}
                    while video_capture.isOpened():
                        ret, frame = video_capture.read()
                        if not ret:
                            break

                        current_detections = face_engine.detect(frame)
                        frame_idx += 1
                        if frame_idx % 50 == 0:
                            progress_percent = int((frame_idx / total_frames) * 90) if total_frames else 0
                            print(f"[Progression]|{progress_percent}|{frame_idx}|{total_frames}", flush=True)

                        if (not current_detections) and object_detector is not None and enable_object_fallback:
                            if enable_object_threads and object_tasks is not None:
                                object_tasks.put((frame, frame_idx - 1, None))
                            else:
                                try:
                                    if mp_module is None:
                                        raise RuntimeError("mediapipe is not available for object detection fallback")
                                    mp_image = mp_module.Image(
                                        image_format=mp_module.ImageFormat.SRGB,
                                        data=cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                                    )
                                    object_result = object_detector.detect(mp_image)
                                    if object_result.detections:
                                        for detection in object_result.detections:
                                            bbox = detection.bounding_box
                                            x_min = int(bbox.origin_x)
                                            y_min = int(bbox.origin_y)
                                            width = int(bbox.width)
                                            height = int(bbox.height)
                                            centroid = (x_min + width // 2, y_min + height // 2)
                                            best_category = detection.categories[0] if detection.categories else None
                                            label = best_category.category_name if best_category else "object"
                                            confidence = best_category.score if best_category else 0.0
                                            current_detections.append(
                                                {
                                                    "bbox": (x_min, y_min, width, height),
                                                    "centroid": centroid,
                                                    "source_detector": "object_detector",
                                                    "label": label,
                                                    "confidence": confidence,
                                                    "blendshapes": None,
                                                    "is_speaking": None,
                                                }
                                            )
                                except Exception as e:
                                    logging.warning(
                                        f"Object detection failed for frame {frame_idx} (face_engine mode): {e}"
                                    )

                        frame_detections[frame_idx - 1] = list(current_detections)
                        del frame

                    if enable_object_threads and object_tasks is not None:
                        object_tasks.join()
                        for _ in object_worker_threads:
                            object_tasks.put(None)
                        object_tasks.join()
                        for t in object_worker_threads:
                            t.join(timeout=1)

                    for frame_idx_out in range(total_frames):
                        current_detections = frame_detections.get(frame_idx_out, [])
                        if (not current_detections) and object_detector is not None and enable_object_fallback:
                            with object_results_lock:
                                current_detections = list(object_results.get(frame_idx_out, []))

                        tracked_for_frame = apply_tracking_and_management(
                            tracked_objects,
                            current_detections,
                            next_id_counter,
                            args.mp_max_distance_tracking,
                            args.mp_frames_unseen_deregister,
                            args.speaking_detection_jaw_open_threshold,
                            enhanced_speaking_detector=enhanced_speaking_detector,
                            current_frame_num=frame_idx_out + 1,
                        )
                        final_output["frames"].append(
                            {
                                "frame": frame_idx_out + 1,
                                "tracked_objects": tracked_for_frame if tracked_for_frame else [],
                            }
                        )
                        if (frame_idx_out + 1) % 50 == 0:
                            progress_percent = 90 + int(((frame_idx_out + 1) / total_frames) * 10) if total_frames else 100
                            print(f"[Progression]|{progress_percent}|{frame_idx_out + 1}|{total_frames}", flush=True)

                    frame_idx = total_frames

                    if total_frames and frame_idx < total_frames:
                        for fill_idx in range(frame_idx, total_frames):
                            tracked_for_frame = apply_tracking_and_management(
                                tracked_objects,
                                [],
                                next_id_counter,
                                args.mp_max_distance_tracking,
                                args.mp_frames_unseen_deregister,
                                args.speaking_detection_jaw_open_threshold,
                                enhanced_speaking_detector=enhanced_speaking_detector,
                                current_frame_num=fill_idx + 1,
                            )
                            final_output["frames"].append(
                                {
                                    "frame": fill_idx + 1,
                                    "tracked_objects": tracked_for_frame if tracked_for_frame else [],
                                }
                            )
                finally:
                    try:
                        if object_detector is not None and hasattr(object_detector, 'close'):
                            object_detector.close()
                    except Exception:
                        pass
                    resource_tracker.unregister_resource(resource_id)

        except Exception as e:
            logging.error(f"Error processing video {args.video_file_path} with OpenCV engine: {e}")
            raise

        output_path = Path(args.video_file_path).with_suffix('.json')
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(final_output, f, indent=2, ensure_ascii=False)
            logging.info(f"Traitement terminé. JSON sauvegardé: {output_path.name}")
            print(f"[Progression]|100|{total_frames}|{total_frames}", flush=True)
        except Exception as e:
            logging.error(f"Failed to save output file {output_path}: {e}")
            raise
        return

    mp_module = _ensure_mediapipe_loaded(required=True)
    BaseOptions = mp_module.tasks.BaseOptions
    VisionRunningMode = mp_module.tasks.vision.RunningMode
    FaceLandmarker = mp_module.tasks.vision.FaceLandmarker
    FaceLandmarkerOptions = mp_module.tasks.vision.FaceLandmarkerOptions
    ObjectDetector = mp_module.tasks.vision.ObjectDetector
    ObjectDetectorOptions = mp_module.tasks.vision.ObjectDetectorOptions

    delegate = BaseOptions.Delegate.GPU if args.use_gpu else BaseOptions.Delegate.CPU
    models_dir = Path(args.models_dir)
    face_model_candidates = [
        models_dir / "face_detectors" / "mediapipe" / "face_landmarker_v2_with_blendshapes.task",
        models_dir / "face_landmarker_v2_with_blendshapes.task",
    ]
    face_model_path = next((p for p in face_model_candidates if p.exists()), face_model_candidates[0])
    
    if not face_model_path.exists():
        logging.error(f"Modèle FaceLandmarker non trouvé: {face_model_path}");
        sys.exit(1)
    
    # Resolve object detector model using registry
    object_detector_model_name = getattr(args, 'object_detector_model', 'efficientdet_lite2')
    try:
        object_model_path = ObjectDetectorRegistry.resolve_model_path(
            model_name=object_detector_model_name,
            models_dir=models_dir,
            override_path=getattr(args, 'object_detector_model_path', None)
        )
        logging.info(f"Using object detector: {object_detector_model_name} at {object_model_path}")
    except (ValueError, FileNotFoundError) as e:
        logging.error(f"Failed to resolve object detector model: {e}")
        sys.exit(1)

    face_options = FaceLandmarkerOptions(
        base_options=BaseOptions(model_asset_path=str(face_model_path), delegate=delegate),
        running_mode=VisionRunningMode.VIDEO,
        num_faces=int(_parse_optional_positive_int(os.environ.get("STEP5_MEDIAPIPE_MAX_FACES")) or args.mp_landmarker_num_faces),
        min_face_detection_confidence=args.mp_landmarker_min_face_detection_confidence,
        min_face_presence_confidence=args.mp_landmarker_min_face_presence_confidence,
        min_tracking_confidence=args.mp_landmarker_min_tracking_confidence,
        output_face_blendshapes=True
    )

    object_options = ObjectDetectorOptions(
        base_options=BaseOptions(model_asset_path=str(object_model_path), delegate=delegate),
        running_mode=VisionRunningMode.VIDEO,
        max_results=getattr(args, 'object_max_results', 5),
        score_threshold=getattr(args, 'object_score_threshold', 0.5)
    )

    # Use safe video processing with automatic resource cleanup
    try:
        with safe_video_processing(args.video_file_path) as (video_capture, temp_manager):
            # Get video metadata safely
            fps = video_capture.get(cv2.CAP_PROP_FPS)
            total_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))

            logging.info(f"Video metadata - FPS: {fps}, Total frames: {total_frames}")

            # Register video capture for tracking
            resource_id = resource_tracker.register_resource(
                video_capture, 'video_capture', f"Processing {Path(args.video_file_path).name}"
            )

            try:
                with FaceLandmarker.create_from_options(face_options) as landmarker, \
                     ObjectDetector.create_from_options(object_options) as object_detector:

                    final_output = {
                        "metadata": {
                            "video_path": args.video_file_path,
                            "total_frames": total_frames,
                            "fps": fps
                        },
                        "frames": []
                    }
                    tracked_objects, next_id_counter = {}, {'value': 0}

                    # Fallback tracking variables
                    face_detection_success_count = 0
                    frames_processed = 0
                    use_object_detection_fallback = getattr(args, 'enable_object_detection', False)
                    fallback_threshold = 50  # Check every 50 frames for fallback decision

                    # Initialize enhanced speaking detector
                    enhanced_speaking_detector = None
                    # Speaking detector activé par défaut
                    if getattr(args, 'speaking_detection_enabled', True):
                        try:
                            enhanced_speaking_detector = EnhancedSpeakingDetector(
                                video_path=args.video_file_path,
                                jaw_threshold=args.speaking_detection_jaw_open_threshold
                            )
                            logging.info("Enhanced speaking detection initialized successfully")
                            logging.info(f"Detection stats: {enhanced_speaking_detector.get_detection_stats()}")
                        except Exception as e:
                            logging.warning(f"Failed to initialize enhanced speaking detector: {e}")
                            enhanced_speaking_detector = None

                    # Check if multi-threading should be used (CPU workers with > 1 internal workers)
                    use_multithreading = (not args.use_gpu and
                                        getattr(args, 'mp_num_workers_internal', 1) > 1)

                    if use_multithreading:
                        logging.info(f"Using multi-threaded processing with {args.mp_num_workers_internal} workers")
                        final_output = process_video_multithreaded(
                            args, video_capture, landmarker, object_detector,
                            enhanced_speaking_detector, total_frames
                        )
                    else:
                        logging.info("Using sequential processing")
                        # Sequential processing (original logic)
                        frame_idx = 0
                        while video_capture.isOpened():
                            ret, frame = video_capture.read()
                            if not ret:
                                break

                            mp_image = mp.Image(
                                image_format=mp.ImageFormat.SRGB,
                                data=cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                            )
                            timestamp_ms = int(video_capture.get(cv2.CAP_PROP_POS_MSEC))

                            # Try face detection first
                            face_result = landmarker.detect_for_video(mp_image, timestamp_ms)
                            frames_processed += 1

                            current_detections = []
                            face_detected = False

                            if face_result.face_landmarks:
                                face_detected = True
                                face_detection_success_count += 1

                                for i, landmarks in enumerate(face_result.face_landmarks):
                                    h, w, _ = frame.shape
                                    x_coords = [lm.x * w for lm in landmarks]
                                    y_coords = [lm.y * h for lm in landmarks]
                                    bbox = (
                                        int(min(x_coords)), int(min(y_coords)),
                                        int(max(x_coords) - min(x_coords)),
                                        int(max(y_coords) - min(y_coords))
                                    )
                                    centroid = (int(np.mean(x_coords)), int(np.mean(y_coords)))
                                    det = {
                                        "bbox": bbox,
                                        "centroid": centroid,
                                        "source_detector": "face_landmarker",
                                        "label": "face"
                                    }
                                    if face_result.face_blendshapes:
                                        det["blendshapes"] = {
                                            cat.category_name: cat.score
                                            for cat in face_result.face_blendshapes[i]
                                        }
                                    current_detections.append(det)

                            # Check if we should enable object detection fallback
                            if frames_processed % fallback_threshold == 0:
                                face_success_rate = face_detection_success_count / frames_processed
                                if face_success_rate < 0.1:  # Less than 10% face detection success
                                    if not use_object_detection_fallback:
                                        logging.info(f"Low face detection rate ({face_success_rate:.2%}). Enabling object detection fallback.")
                                        use_object_detection_fallback = True

                            # Use object detection fallback if no faces detected and fallback is enabled
                            if not face_detected and use_object_detection_fallback:
                                try:
                                    object_result = object_detector.detect_for_video(mp_image, timestamp_ms)

                                    if object_result.detections:
                                        for detection in object_result.detections:
                                            bbox = detection.bounding_box
                                            # MediaPipe object detection returns coordinates already in pixel format
                                            # No conversion needed - use values directly
                                            x_min = int(bbox.origin_x)
                                            y_min = int(bbox.origin_y)
                                            width = int(bbox.width)
                                            height = int(bbox.height)

                                            centroid = (x_min + width // 2, y_min + height // 2)

                                            # Get the best category
                                            best_category = detection.categories[0] if detection.categories else None
                                            label = best_category.category_name if best_category else "object"
                                            confidence = best_category.score if best_category else 0.0

                                            det = {
                                                "bbox": (x_min, y_min, width, height),
                                                "centroid": centroid,
                                                "source_detector": "object_detector",
                                                "label": label,
                                                "confidence": confidence,
                                                "blendshapes": None,  # Not applicable for objects
                                                "is_speaking": None   # Not applicable for objects
                                            }
                                            current_detections.append(det)
                                except Exception as e:
                                    logging.warning(f"Object detection failed for frame {frame_idx + 1}: {e}")

                            tracked_for_frame = apply_tracking_and_management(
                                tracked_objects, current_detections, next_id_counter,
                                args.mp_max_distance_tracking, args.mp_frames_unseen_deregister,
                                args.speaking_detection_jaw_open_threshold,
                                enhanced_speaking_detector=enhanced_speaking_detector,
                                current_frame_num=frame_idx + 1
                            )

                            # Always export frame data to maintain consistent structure
                            # Even if no objects are tracked, we include the frame with empty tracked_objects
                            final_output["frames"].append({
                                "frame": frame_idx + 1,
                                "tracked_objects": tracked_for_frame if tracked_for_frame else []
                            })

                            frame_idx += 1
                            if frame_idx % 50 == 0:
                                progress_percent = int((frame_idx / total_frames) * 100)
                                print(f"[Progression]|{progress_percent}|{frame_idx}|{total_frames}", flush=True)

                        # Log processing summary for sequential processing
                        face_success_rate = face_detection_success_count / frames_processed if frames_processed > 0 else 0
                        logging.info(f"Processing summary:")
                        logging.info(f"  Total frames processed: {frames_processed}")
                        logging.info(f"  Frames with faces: {face_detection_success_count}")
                        logging.info(f"  Face detection success rate: {face_success_rate:.2%}")
                        logging.info(f"  Object detection fallback used: {use_object_detection_fallback}")
                        logging.info(f"  Total frames exported: {len(final_output['frames'])}")

            finally:
                # Unregister the video capture resource
                resource_tracker.unregister_resource(resource_id)

    except Exception as e:
        logging.error(f"Error processing video {args.video_file_path}: {e}")
        raise

    # Save output with proper error handling
    output_path = Path(args.video_file_path).with_suffix('.json')
    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(final_output, f, indent=2, ensure_ascii=False)
        logging.info(f"Traitement terminé. JSON sauvegardé: {output_path.name}")
        print(f"[Progression]|100|{total_frames}|{total_frames}", flush=True)
    except Exception as e:
        logging.error(f"Failed to save output file {output_path}: {e}")
        raise


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Worker séquentiel pour MediaPipe.")
    parser.add_argument("video_file_path")
    parser.add_argument("--models_dir", required=True)
    parser.add_argument("--use_gpu", action="store_true")
    parser.add_argument("--tracking_engine", default=None, help="Tracking engine: mediapipe_landmarker (default), opencv_haar, opencv_yunet")
    # Conserver les arguments pour la compatibilité avec le manager, même s'ils ne sont pas tous utilisés
    parser.add_argument("--mp_landmarker_num_faces", type=int, default=1)
    parser.add_argument("--mp_landmarker_min_face_detection_confidence", type=float, default=0.5)
    parser.add_argument("--mp_landmarker_min_face_presence_confidence", type=float, default=0.3)
    parser.add_argument("--mp_landmarker_min_tracking_confidence", type=float, default=0.5)
    parser.add_argument("--mp_landmarker_output_blendshapes", action="store_true")
    parser.add_argument("--mp_max_distance_tracking", type=int, default=70)
    parser.add_argument("--mp_frames_unseen_deregister", type=int, default=7)
    parser.add_argument("--speaking_detection_jaw_open_threshold", type=float, default=0.08)
    parser.add_argument("--speaking_detection_enabled", action="store_true", default=True, help="Enable enhanced speaking detection module (enabled by default)")
    # Object detection parameters
    parser.add_argument("--enable_object_detection", action="store_true", help="Enable object detection fallback")
    parser.add_argument("--object_score_threshold", type=float, default=0.5, help="Object detection confidence threshold")
    parser.add_argument("--object_max_results", type=int, default=5, help="Maximum number of objects to detect")
    # Multi-threading parameter for CPU workers
    parser.add_argument("--mp_num_workers_internal", type=int, default=1, help="Number of internal worker threads for CPU processing")

    args, _ = parser.parse_known_args()

    try:
        main(args)
        sys.exit(0)
    except Exception as e:
        logging.error(f"Erreur critique dans le worker: {e}", exc_info=True)
        sys.exit(1)
```

## File: workflow_scripts/step5/pyfeat_blendshape_extractor.py
```python
import logging
import os
from pathlib import Path
from typing import Optional, Dict, Any, List

import numpy as np

logger = logging.getLogger(__name__)


class PyFeatBlendshapeExtractor:
    def __init__(
        self,
        model_path: Optional[str] = None,
        device: Optional[str] = None,
        use_gpu: bool = False,
    ):
        self._model_path = model_path or os.environ.get("STEP5_PYFEAT_MODEL_PATH")
        self._model = None
        self._blendshape_names = None
        self._use_gpu = use_gpu
        
        try:
            import torch
            self._torch = torch
        except ImportError:
            raise RuntimeError(
                "PyTorch is required for py-feat blendshape extraction. "
                "Install with: pip install torch"
            )

        if device:
            self._device = device
        else:
            if use_gpu and torch.cuda.is_available():
                self._device = "cuda"
            else:
                if use_gpu and not torch.cuda.is_available():
                    logger.warning("CUDA requested for py-feat but no GPU detected; falling back to CPU mode")
                self._device = "cpu"

        self._initialize_model()

    def _initialize_model(self):
        if not self._model_path:
            self._download_model()
        
        if not Path(self._model_path).exists():
            raise RuntimeError(f"py-feat model not found at: {self._model_path}")

        try:
            checkpoint = self._torch.load(
                self._model_path,
                map_location=self._device,
                weights_only=True,
            )

            if isinstance(checkpoint, dict) and "net" in checkpoint:
                state_dict = checkpoint["net"]
            else:
                state_dict = checkpoint

            self._model = self._create_model_architecture(state_dict)
            self._model.load_state_dict(state_dict)
            self._model.eval()
            self._model.to(self._device)
            
            self._blendshape_names = self._get_blendshape_names()
            
            logger.info(f"py-feat blendshape model loaded: {self._model_path}")
        except Exception as e:
            logger.error(f"Failed to load py-feat model: {e}")
            raise

    def _download_model(self):
        try:
            from huggingface_hub import hf_hub_download
            
            preferred_cache_dir = (
                Path(__file__).parent
                / "models"
                / "blendshapes"
                / "opencv"
                / "pyfeat_models"
            )
            legacy_cache_dir = Path(__file__).parent / "models" / "pyfeat"

            cache_dir = legacy_cache_dir if legacy_cache_dir.exists() else preferred_cache_dir
            cache_dir.mkdir(parents=True, exist_ok=True)
            
            self._model_path = hf_hub_download(
                repo_id="py-feat/mp_blendshapes",
                filename="face_blendshapes.pth",
                cache_dir=str(cache_dir),
            )
            logger.info(f"Downloaded py-feat model to: {self._model_path}")
        except Exception as e:
            raise RuntimeError(
                f"Failed to download py-feat model from HuggingFace: {e}. "
                "Set STEP5_PYFEAT_MODEL_PATH manually or install: pip install huggingface_hub"
            )

    def _create_model_architecture(self, state_dict: Dict[str, Any]):
        torch = self._torch

        if isinstance(state_dict, dict) and any(k.startswith("mlpmixer_blocks.") for k in state_dict.keys()):

            class _ChannelLayerNormNoBias(torch.nn.Module):
                def __init__(self, channels: int, eps: float = 1e-5):
                    super().__init__()
                    self.weight = torch.nn.Parameter(torch.ones(channels))
                    self.eps = eps

                def forward(self, x):
                    mean = x.mean(dim=1, keepdim=True)
                    var = (x - mean).pow(2).mean(dim=1, keepdim=True)
                    x = (x - mean) / torch.sqrt(var + self.eps)
                    return x * self.weight.view(1, -1, 1, 1)

            class _MLPMixerBlock(torch.nn.Module):
                def __init__(
                    self,
                    tokens: int,
                    channels: int,
                    token_mlp_dim: int,
                    channel_mlp_dim: int,
                ):
                    super().__init__()
                    self.norm1 = _ChannelLayerNormNoBias(channels)
                    self.mlp_token_mixing = torch.nn.Sequential(
                        torch.nn.Conv2d(tokens, token_mlp_dim, kernel_size=1, bias=True),
                        torch.nn.GELU(),
                        torch.nn.Identity(),
                        torch.nn.Conv2d(token_mlp_dim, tokens, kernel_size=1, bias=True),
                    )
                    self.norm2 = _ChannelLayerNormNoBias(channels)
                    self.mlp_channel_mixing = torch.nn.Sequential(
                        torch.nn.Conv2d(channels, channel_mlp_dim, kernel_size=1, bias=True),
                        torch.nn.GELU(),
                        torch.nn.Identity(),
                        torch.nn.Conv2d(channel_mlp_dim, channels, kernel_size=1, bias=True),
                    )

                def forward(self, x):
                    y = self.norm1(x)
                    y = y.permute(0, 2, 1, 3)
                    y = self.mlp_token_mixing(y)
                    y = y.permute(0, 2, 1, 3)
                    x = x + y

                    y = self.norm2(x)
                    y = self.mlp_channel_mixing(y)
                    x = x + y
                    return x

            class MediaPipeBlendshapesMLPMixer(torch.nn.Module):
                def __init__(self, sd: Dict[str, Any]):
                    super().__init__()

                    conv1_w = sd["conv1.weight"]
                    conv2_w = sd["conv2.weight"]
                    out_w = sd["output_mlp.weight"]

                    tokens_without_extra = int(conv1_w.shape[0])
                    num_landmarks = int(conv1_w.shape[1])
                    num_blendshapes = int(out_w.shape[0])

                    tokens = tokens_without_extra + 1
                    channels = int(conv2_w.shape[0])
                    token_mlp_dim = int(sd["mlpmixer_blocks.0.mlp_token_mixing.0.weight"].shape[0])
                    channel_mlp_dim = int(sd["mlpmixer_blocks.0.mlp_channel_mixing.0.weight"].shape[0])
                    num_blocks = len({k.split(".")[1] for k in sd.keys() if k.startswith("mlpmixer_blocks.")})

                    self.num_landmarks = num_landmarks
                    self.num_blendshapes = num_blendshapes

                    self.conv1 = torch.nn.Conv2d(num_landmarks, tokens_without_extra, kernel_size=1, bias=True)
                    self.conv2 = torch.nn.Conv2d(2, channels, kernel_size=1, bias=True)
                    self.extra_token = torch.nn.Parameter(torch.zeros(1, channels, 1, 1))

                    self.mlpmixer_blocks = torch.nn.ModuleList(
                        [
                            _MLPMixerBlock(
                                tokens=tokens,
                                channels=channels,
                                token_mlp_dim=token_mlp_dim,
                                channel_mlp_dim=channel_mlp_dim,
                            )
                            for _ in range(int(num_blocks))
                        ]
                    )

                    self.output_mlp = torch.nn.Conv2d(channels, num_blendshapes, kernel_size=1, bias=True)
                    self.sigmoid = torch.nn.Sigmoid()

                def forward(self, landmarks_xy):
                    if landmarks_xy.ndim != 3:
                        raise ValueError(f"Expected landmarks_xy to have shape [B, N, 2], got {tuple(landmarks_xy.shape)}")

                    x = landmarks_xy.unsqueeze(-1)
                    x = self.conv1(x)
                    x = x.permute(0, 2, 1, 3)
                    x = self.conv2(x)

                    extra = self.extra_token.expand(x.shape[0], -1, -1, -1)
                    x = torch.cat([x, extra], dim=2)

                    for block in self.mlpmixer_blocks:
                        x = block(x)

                    x = self.output_mlp(x)
                    x = x[:, :, -1, 0]
                    x = self.sigmoid(x)
                    return x

            return MediaPipeBlendshapesMLPMixer(state_dict)

        class MediaPipeBlendshapesSimpleMLP(torch.nn.Module):
            def __init__(self):
                super().__init__()
                self.num_landmarks = 146
                self.num_blendshapes = 52

                self.fc1 = torch.nn.Linear(self.num_landmarks * 2, 256)
                self.bn1 = torch.nn.BatchNorm1d(256)
                self.fc2 = torch.nn.Linear(256, 256)
                self.bn2 = torch.nn.BatchNorm1d(256)
                self.fc3 = torch.nn.Linear(256, 128)
                self.bn3 = torch.nn.BatchNorm1d(128)
                self.fc4 = torch.nn.Linear(128, self.num_blendshapes)
                self.relu = torch.nn.ReLU()
                self.sigmoid = torch.nn.Sigmoid()

            def forward(self, x):
                x = x.view(x.size(0), -1)
                x = self.relu(self.bn1(self.fc1(x)))
                x = self.relu(self.bn2(self.fc2(x)))
                x = self.relu(self.bn3(self.fc3(x)))
                x = self.sigmoid(self.fc4(x))
                return x

        return MediaPipeBlendshapesSimpleMLP()

    def _get_blendshape_names(self) -> List[str]:
        return [
            "browDownLeft", "browDownRight", "browInnerUp", "browOuterUpLeft",
            "browOuterUpRight", "cheekPuff", "cheekSquintLeft", "cheekSquintRight",
            "eyeBlinkLeft", "eyeBlinkRight", "eyeLookDownLeft", "eyeLookDownRight",
            "eyeLookInLeft", "eyeLookInRight", "eyeLookOutLeft", "eyeLookOutRight",
            "eyeLookUpLeft", "eyeLookUpRight", "eyeSquintLeft", "eyeSquintRight",
            "eyeWideLeft", "eyeWideRight", "jawForward", "jawLeft", "jawOpen",
            "jawRight", "mouthClose", "mouthDimpleLeft", "mouthDimpleRight",
            "mouthFrownLeft", "mouthFrownRight", "mouthFunnel", "mouthLeft",
            "mouthLowerDownLeft", "mouthLowerDownRight", "mouthPressLeft",
            "mouthPressRight", "mouthPucker", "mouthRight", "mouthRollLower",
            "mouthRollUpper", "mouthShrugLower", "mouthShrugUpper", "mouthSmileLeft",
            "mouthSmileRight", "mouthStretchLeft", "mouthStretchRight",
            "mouthUpperUpLeft", "mouthUpperUpRight", "noseSneerLeft", "noseSneerRight",
            "tongueOut"
        ]

    def _select_landmark_subset(self, landmarks_478: np.ndarray) -> np.ndarray:
        landmark_indices = [
            0, 1, 4, 5, 6, 7, 8, 10, 13, 14, 17, 21, 33, 37, 39, 40, 46, 52, 53, 54,
            55, 58, 61, 63, 65, 66, 67, 68, 69, 70, 78, 80, 81, 82, 84, 87, 88, 91,
            93, 95, 103, 105, 107, 109, 127, 132, 133, 136, 144, 145, 146, 148, 149,
            150, 152, 153, 154, 155, 157, 158, 159, 160, 161, 162, 163, 168, 172,
            173, 176, 178, 181, 185, 191, 195, 197, 234, 246, 249, 251, 263, 267,
            269, 270, 276, 282, 283, 284, 285, 288, 291, 293, 295, 296, 297, 298,
            299, 300, 308, 310, 311, 312, 314, 317, 318, 321, 323, 324, 332, 334,
            336, 338, 356, 361, 362, 365, 373, 374, 375, 377, 378, 379, 380, 381,
            382, 384, 385, 386, 387, 388, 389, 390, 397, 398, 402, 405, 415, 466,
            468, 469, 470, 471, 472, 473, 474, 475, 476
        ]
        expected = getattr(self._model, "num_landmarks", None)
        if isinstance(expected, int) and expected > 0 and len(landmark_indices) != expected:
            logger.warning(
                "py-feat landmark subset size mismatch (got %s, expected %s). Truncating indices.",
                len(landmark_indices),
                expected,
            )
            landmark_indices = landmark_indices[:expected]
        return landmarks_478[landmark_indices, :2]

    def extract_blendshapes(
        self,
        landmarks_478: np.ndarray,
        image_width: int,
        image_height: int
    ) -> Optional[Dict[str, float]]:
        if landmarks_478 is None or len(landmarks_478) < 478:
            return None

        try:
            landmarks_subset = self._select_landmark_subset(landmarks_478)
            
            # Optimize: check normalization before tensor creation
            max_coord = float(np.max(np.abs(landmarks_subset))) if landmarks_subset.size > 0 else 0.0
            if max_coord <= 2.0:
                # Apply scaling in numpy (faster than tensor ops)
                landmarks_subset = landmarks_subset * np.array([image_width, image_height], dtype=np.float32)
            
            # Single tensor creation with optimal dtype
            landmarks_tensor = self._torch.from_numpy(landmarks_subset.copy()).float().unsqueeze(0)
            landmarks_tensor = landmarks_tensor.to(self._device)

            with self._torch.no_grad():
                blendshapes_tensor = self._model(landmarks_tensor)

            blendshapes_array = blendshapes_tensor.squeeze(0).detach().cpu().numpy()
            
            return {
                name: float(value)
                for name, value in zip(self._blendshape_names, blendshapes_array)
            }
        except Exception as e:
            logger.warning(f"Failed to extract blendshapes: {e}")
            return None
```

## File: workflow_scripts/step6/json_reducer.py
```python
import os
import sys
import json
import argparse
import logging
from datetime import datetime

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)


def setup_logging(log_dir: str):
    os.makedirs(log_dir, exist_ok=True)
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_path = os.path.join(log_dir, f"json_reducer_{timestamp}.log")

    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')

    fh = logging.FileHandler(log_path, encoding='utf-8')
    fh.setLevel(logging.INFO)
    fh.setFormatter(formatter)

    ch = logging.StreamHandler(sys.stdout)
    ch.setLevel(logging.INFO)
    ch.setFormatter(formatter)

    logger.handlers.clear()
    logger.addHandler(fh)
    logger.addHandler(ch)
    logger.propagate = False

    logger.info(f"Log file initialized: {log_path}")
    return log_path


def reduce_video_json(data):
    """
    Réduit un objet JSON de données vidéo pour ne conserver que les clés
    utiles au script After Effects.
    """
    if "frames" not in data:
        return {"frames_analysis": []}  # Retourne une structure vide si le format est inattendu

    new_frames_data = []
    for frame in data["frames"]:
        new_tracked_objects = []
        if "tracked_objects" in frame and frame["tracked_objects"] is not None:
            for obj in frame["tracked_objects"]:
                # Initialisation de l'objet simplifié
                new_obj = {
                    "id": obj.get("id"),
                    "centroid_x": obj.get("centroid_x"),
                    "source": obj.get("source"),
                    "label": obj.get("label"),
                    "active_speakers": []  # Valeur par défaut
                }

                # Inclure la taille du bbox si disponible (ajout depuis l'étape 5)
                bbox_w = obj.get("bbox_width")
                bbox_h = obj.get("bbox_height")
                if bbox_w is not None and bbox_h is not None:
                    new_obj["bbox_width"] = bbox_w
                    new_obj["bbox_height"] = bbox_h

                # Extraction sécurisée de active_speakers
                if (obj.get("speaking_sources") and
                        isinstance(obj["speaking_sources"], dict) and
                        obj["speaking_sources"].get("audio") and
                        isinstance(obj["speaking_sources"]["audio"], dict)):
                    new_obj["active_speakers"] = obj["speaking_sources"]["audio"].get("active_speakers", [])

                new_tracked_objects.append(new_obj)

        new_frames_data.append({
            "frame": frame.get("frame"),
            "tracked_objects": new_tracked_objects
        })

    # La structure finale utilise "frames_analysis" pour correspondre au script AE
    return {"frames_analysis": new_frames_data}


def reduce_audio_json(data):
    """
    Réduit un objet JSON de données audio pour ne conserver que les clés
    utiles au script After Effects.
    """
    if "frames_analysis" not in data:
        return {"frames_analysis": []}  # Retourne une structure vide si le format est inattendu

    new_frames_analysis = []
    for frame_data in data["frames_analysis"]:
        if "audio_info" in frame_data and frame_data["audio_info"] is not None:
            new_audio_info = {
                "is_speech_present": frame_data["audio_info"].get("is_speech_present", False),
                "active_speaker_labels": frame_data["audio_info"].get("active_speaker_labels", [])
            }
            new_frames_analysis.append({
                "frame": frame_data.get("frame"),
                "audio_info": new_audio_info
            })

    return {"frames_analysis": new_frames_analysis}


def process_directory(base_path, keyword="Camille"):
    """
    Analyse les dossiers dans le chemin de base, recherche le mot-clé,
    et traite les paires de fichiers JSON trouvées dans les sous-dossiers "docs".
    """
    logger.info(f"Démarrage du scan dans : {base_path}")
    if not os.path.isdir(base_path):
        logger.error(f"Erreur : Le répertoire de base '{base_path}' n'existe pas.")
        return

    # 1. Lister les dossiers de projet
    project_folders = [d for d in os.listdir(base_path)
                       if os.path.isdir(os.path.join(base_path, d)) and keyword in d]

    if not project_folders:
        print(f"Aucun dossier contenant le mot-clé '{keyword}' n'a été trouvé.")
        return

    logger.info(f"Dossiers de projet trouvés : {len(project_folders)}")

    for folder in project_folders:
        docs_path = os.path.join(base_path, folder, "docs")

        if not os.path.isdir(docs_path):
            logger.warning(f"-> Avertissement : Le dossier 'docs' est manquant dans '{folder}'.")
            continue

        logger.info(f"\n--- Traitement du dossier : {docs_path} ---")

        # 2. Identifier les paires de fichiers JSON
        all_files = os.listdir(docs_path)
        video_json_files = [f for f in all_files if f.endswith(".json") and not f.endswith("_audio.json")]

        if not video_json_files:
            logger.info("Aucun fichier JSON vidéo principal trouvé.")
            continue

        for video_file in video_json_files:
            base_name = video_file.rsplit('.', 1)[0]
            audio_file = f"{base_name}_audio.json"

            video_path = os.path.join(docs_path, video_file)
            audio_path = os.path.join(docs_path, audio_file)

            if audio_file not in all_files:
                logger.warning(f"  - Fichier audio '{audio_file}' manquant pour '{video_file}'. Ignoré.")
                continue

            logger.info(f"  - Paire trouvée : \n    - {video_file}\n    - {audio_file}")

            try:
                # 3. Traiter le JSON vidéo
                with open(video_path, 'r', encoding='utf-8') as f:
                    video_data = json.load(f)

                reduced_video_data = reduce_video_json(video_data)

                with open(video_path, 'w', encoding='utf-8') as f:
                    json.dump(reduced_video_data, f, indent=2)

                logger.info("    - Fichier vidéo réduit avec succès.")

                # 4. Traiter le JSON audio
                with open(audio_path, 'r', encoding='utf-8') as f:
                    audio_data = json.load(f)

                reduced_audio_data = reduce_audio_json(audio_data)

                with open(audio_path, 'w', encoding='utf-8') as f:
                    json.dump(reduced_audio_data, f, indent=2)

                logger.info("    - Fichier audio réduit avec succès.")

            except json.JSONDecodeError as e:
                logger.error(f"    - ERREUR : Impossible de lire un fichier JSON. Erreur : {e}")
            except Exception as e:
                logger.error(f"    - ERREUR : Une erreur inattendue est survenue. Erreur : {e}")

    logger.info("\n--- Traitement terminé ! ---")


def main():
    parser = argparse.ArgumentParser(description="Étape 6 - Réduction JSON (vidéo + audio)")
    parser.add_argument('--base_dir', type=str, default=os.environ.get('BASE_PATH_SCRIPTS', ''), help='Chemin base du projet (contenant projets_extraits)')
    parser.add_argument('--work_dir', type=str, default=None, help='Chemin explicite vers projets_extraits')
    parser.add_argument('--keyword', type=str, default=os.environ.get('FOLDER_KEYWORD', 'Camille'), help='Mot-clé pour filtrer les dossiers projet')
    parser.add_argument('--log_dir', type=str, default=str(os.path.join(os.path.dirname(__file__), '..', '..', '..', 'logs', 'step6')),
                        help='Répertoire pour les logs (par défaut logs/step6)')

    args = parser.parse_args()

    # Resolve working directory
    if args.work_dir:
        work_dir = args.work_dir
    else:
        base_dir = args.base_dir if args.base_dir else os.getcwd()
        work_dir = os.path.join(base_dir, 'projets_extraits')

    # Setup logging
    setup_logging(args.log_dir)

    # Progress total: count candidate projects
    try:
        if not os.path.isdir(work_dir):
            logger.warning(f"Répertoire de travail introuvable: {work_dir}")
            print(f"TOTAL_JSON_TO_REDUCE: 0")
            sys.exit(0)
        projects = [d for d in os.listdir(work_dir) if os.path.isdir(os.path.join(work_dir, d)) and args.keyword in d]
        print(f"TOTAL_JSON_TO_REDUCE: {len(projects)}")
    except Exception:
        print("TOTAL_JSON_TO_REDUCE: 0")

    # Run processing
    process_directory(work_dir, keyword=args.keyword)


if __name__ == "__main__":
    main()
```

## File: workflow_scripts/step7/finalize_and_copy.py
```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Script de finalisation et copie des projets vers la destination finale.
Étape 7 (Ubuntu)
\- Archive d'abord les artefacts d'analyse (scènes/tracking/audio) avant suppression
\- Copie ensuite le dossier du projet vers la destination finale
"""

import os
import errno
import sys
import json
import shutil
import logging
import subprocess
import tempfile
from pathlib import Path
from datetime import datetime

ROOT_DIR = Path(__file__).resolve().parents[2]
if str(ROOT_DIR) not in sys.path:
    sys.path.insert(0, str(ROOT_DIR))

from config.settings import config
from services.results_archiver import ResultsArchiver, SCENES_SUFFIX, AUDIO_SUFFIX, TRACKING_SUFFIX, VIDEO_METADATA_NAME

# --- Configuration ---
WORK_DIR = Path(os.getcwd())
# --- MODIFICATION: Changer la destination ---
OUTPUT_DIR = Path(os.environ.get("OUTPUT_DIR", "/mnt/cache"))
# --- FIN DE LA MODIFICATION ---
FINALIZE_MODE = os.environ.get("FINALIZE_MODE", "lenient").lower()
BASE_DIR = ROOT_DIR
LOG_DIR = BASE_DIR / "logs" / "step7"

# --- Configuration du Logger ---
LOG_DIR.mkdir(parents=True, exist_ok=True)
log_file = LOG_DIR / f"finalize_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file, encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)


def find_projects_to_finalize():
    """Trouve tous les projets dans WORK_DIR qui sont prêts à être finalisés."""
    projects = []
    logging.info(f"Recherche de projets à finaliser dans: {WORK_DIR}")

    for project_dir in WORK_DIR.iterdir():
        if not project_dir.is_dir() or project_dir.name.startswith("_temp_"):
            continue

        is_ready = False
        found_reason = ""
        for video_file in project_dir.rglob("*.mp4"):
            stem = video_file.stem
            scenes_csv = next(project_dir.rglob(f"{stem}{SCENES_SUFFIX}"), None)
            tracking_json = next(project_dir.rglob(f"{stem}{TRACKING_SUFFIX}"), None)
            audio_json = next(project_dir.rglob(f"{stem}{AUDIO_SUFFIX}"), None)
            if FINALIZE_MODE == "strict":
                if scenes_csv and scenes_csv.exists() and tracking_json and tracking_json.exists():
                    found_reason = f"artefacts scènes+tracking pour '{video_file.name}'"
                    is_ready = True
                    break
            elif FINALIZE_MODE == "videos":
                found_reason = f"vidéo présente '{video_file.name}'"
                is_ready = True
                break
            else:
                if (scenes_csv and scenes_csv.exists()) or (tracking_json and tracking_json.exists()) or (audio_json and audio_json.exists()):
                    found_reason = f"au moins un artefact pour '{video_file.name}'"
                    is_ready = True
                    break

        if is_ready:
            logging.info(f"Projet '{project_dir.name}' est prêt ({found_reason}). Mode: {FINALIZE_MODE}")
            projects.append(project_dir)
        else:
            logging.info(f"Projet '{project_dir.name}' ignoré (mode={FINALIZE_MODE}). Aucune vidéo/artefact requis trouvés.")

    return projects


def _is_dir_writable(path: Path) -> bool:
    """Teste la capacité d'écriture/suppression dans un répertoire cible.

    Plus fiable que os.access() sur des montages FUSE/NTFS.
    """
    try:
        path.mkdir(parents=True, exist_ok=True)
        with tempfile.NamedTemporaryFile(dir=str(path), delete=True) as tmp:
            tmp.write(b"writable-check")
            tmp.flush()
        return True
    except Exception:
        return False


def _select_output_dir(preferred: Path, base_dir: Path) -> Path:
    """Sélectionne une destination inscriptible, avec repli si nécessaire.

    - Utilise `preferred` si inscriptible.
    - Sinon, utilise `FALLBACK_OUTPUT_DIR` si défini et inscriptible.
    - Sinon, utilise `base_dir / "_finalized_output"`.
    """
    if _is_dir_writable(preferred):
        logging.info(f"Destination vérifiée: '{preferred}' est inscriptible")
        return preferred

    logging.warning(
        f"La destination préférée '{preferred}' n'est pas inscriptible (montage RO ? permissions ?). "
        "Activation d'une destination de repli."
    )

    env_fallback = os.environ.get("FALLBACK_OUTPUT_DIR")
    if env_fallback:
        fb = Path(env_fallback)
        if _is_dir_writable(fb):
            logging.warning(f"Bascule vers FALLBACK_OUTPUT_DIR: '{fb}'")
            return fb
        else:
            logging.warning(f"FALLBACK_OUTPUT_DIR défini mais non inscriptible: '{fb}'")

    local_fb = base_dir / "_finalized_output"
    local_fb.mkdir(parents=True, exist_ok=True)
    logging.warning(f"Bascule vers le répertoire de repli local: '{local_fb}'")
    return local_fb


def _safe_rmtree(path: Path) -> None:
    """Supprime un dossier en consignant proprement les erreurs de permissions."""
    def _onerror(func, p, exc_info):
        logging.error(f"Suppression échouée sur '{p}': {exc_info[1]}")
    shutil.rmtree(path, onerror=_onerror)


def _destination_supports_chmod(dest_dir: Path) -> bool:
    """Détecte si le FS destination supporte chmod (NTFS typiquement renvoie EPERM).
    
    Si on ne peut pas créer de fichier, la question du chmod est secondaire; laisser True.
    """
    try:
        dest_dir.mkdir(parents=True, exist_ok=True)
        with tempfile.NamedTemporaryFile(dir=str(dest_dir), delete=True) as tmp:
            try:
                os.chmod(tmp.name, 0o664)
            except PermissionError:
                return False
            except OSError as e:
                err = getattr(e, 'errno', None)
                if err in (errno.EPERM, errno.EACCES, getattr(errno, 'EOPNOTSUPP', None)):
                    return False
                raise
    except Exception:
        return True


def _copy_project_tree(src: Path, dst: Path) -> None:
    """Copie le projet sans préserver les permissions sur FS type NTFS.

    Stratégie:
    - Si chmod supporté: utiliser shutil.copytree (comportement standard).
    - Sinon: tenter rsync --no-perms/owner/group; à défaut cp --no-preserve; à défaut copie Python manuelle.
    """
    dst.parent.mkdir(parents=True, exist_ok=True)
    supports_chmod = _destination_supports_chmod(dst)
    if supports_chmod:
        shutil.copytree(src, dst, dirs_exist_ok=True)
        return

    logging.info("Destination ne supporte pas chmod — copie sans préservation des permissions.")
    # 1) rsync si disponible
    try:
        subprocess.run([
            "rsync", "-a", "--no-perms", "--no-owner", "--no-group", "--no-times",
            f"{str(src)}/", f"{str(dst)}/"
        ], check=True)
        return
    except FileNotFoundError:
        logging.info("rsync non disponible, tentative via cp --no-preserve")
    except subprocess.CalledProcessError as e:
        logging.warning(f"rsync a échoué: {e}")

    # 2) cp --no-preserve si disponible (fusionner le contenu dans dst)
    try:
        dst.mkdir(parents=True, exist_ok=True)
        subprocess.run([
            "bash", "-lc",
            f"cp -r --no-preserve=mode,ownership '{str(src)}/.' '{str(dst)}/'"
        ], check=True)
        return
    except subprocess.CalledProcessError as e:
        logging.warning(f"cp --no-preserve a échoué: {e}")

    # 3) Fallback Python: os.walk et shutil.copyfile sans copystat
    for root, dirs, files in os.walk(src):
        rel = os.path.relpath(root, src)
        target_dir = dst / rel if rel != "." else dst
        target_dir.mkdir(parents=True, exist_ok=True)
        for d in dirs:
            (target_dir / d).mkdir(parents=True, exist_ok=True)
        for f in files:
            s = Path(root) / f
            t = target_dir / f
            shutil.copyfile(s, t)


def _compute_alternative_output_dir(existing_dst: Path) -> Path:
    """Calcule un répertoire de sortie alternatif si `existing_dst` ne peut pas être supprimé.

    Ex.: /mnt/cache/33 Camille -> /mnt/cache/33 Camille__finalized_YYYYmmdd_HHMMSS[_n]
    """
    base = existing_dst.parent
    name = existing_dst.name
    ts = datetime.now().strftime('%Y%m%d_%H%M%S')
    candidate = base / f"{name}__finalized_{ts}"
    i = 1
    while candidate.exists():
        candidate = base / f"{name}__finalized_{ts}_{i}"
        i += 1
    return candidate


def _normalize_project_docs_structure(dst_project_dir: Path) -> None:
    """Ensure destination project has a docs/ subfolder containing media and related files.

    If files are currently at the project root (common when previous steps didn't use a docs/
    directory), move recognized media and analysis files into docs/.
    """
    docs_dir = dst_project_dir / "docs"
    docs_dir.mkdir(parents=True, exist_ok=True)

    media_exts = {".mp4", ".mov", ".avi", ".mkv", ".webm", ".png", ".jpg", ".jpeg"}
    analysis_suffixes = {SCENES_SUFFIX, AUDIO_SUFFIX, TRACKING_SUFFIX}

    video_exts = {".mp4", ".mov", ".avi", ".mkv", ".webm"}
    video_stems: set[str] = set()
    try:
        for p in dst_project_dir.iterdir():
            if p.is_file() and p.suffix.lower() in video_exts:
                video_stems.add(p.stem)
        if docs_dir.exists():
            for p in docs_dir.rglob("*"):
                if p.is_file() and p.suffix.lower() in video_exts:
                    video_stems.add(p.stem)
    except Exception:
        pass

    for entry in dst_project_dir.iterdir():
        if entry.name == "docs":
            continue
        if entry.is_file():
            ext = entry.suffix.lower()
            stem = entry.stem
            move = False
            if ext in media_exts:
                move = True
            else:
                for suf in analysis_suffixes:
                    if entry.name.endswith(suf) or entry.name == f"{stem}.csv":
                        move = True
                        break
                if not move and ext == ".json" and stem in video_stems:
                    move = True
            if move:
                target = docs_dir / entry.name
                try:
                    if target.exists():
                        target.unlink()
                    shutil.move(str(entry), str(target))
                except Exception as e:
                    logging.warning(f"Impossible de déplacer '{entry}' vers '{target}': {e}")


def restore_archived_analysis(project_name: str, output_project_dir: Path) -> None:
    """Restore archived analysis artifacts into the destination project docs/ folder.

    For each detected video file in docs/, attempt to retrieve archived artifacts
    (scenes CSV, audio JSON, tracking JSON) using ResultsArchiver and copy them next to the video.
    """
    try:
        docs_dir = output_project_dir / "docs"
        docs_dir.mkdir(parents=True, exist_ok=True)

        video_exts = (".mp4", ".mov", ".avi", ".mkv", ".webm")
        videos = []
        if docs_dir.exists():
            videos.extend([p for p in docs_dir.rglob("*") if p.is_file() and p.suffix.lower() in video_exts])
        videos.extend([p for p in output_project_dir.iterdir() if p.is_file() and p.suffix.lower() in video_exts])

        for v in videos:
            target_dir = docs_dir if v.parent != docs_dir else v.parent
            stem = v.stem

            scenes_path = ResultsArchiver.find_analysis_file(project_name, v, SCENES_SUFFIX)
            if scenes_path:
                dst = target_dir / f"{stem}{SCENES_SUFFIX}"
                try:
                    shutil.copy2(scenes_path, dst)
                except Exception as e:
                    logging.warning(f"Restauration scenes échouée {scenes_path} -> {dst}: {e}")

            audio_path = ResultsArchiver.find_analysis_file(project_name, v, AUDIO_SUFFIX)
            if audio_path:
                dst = target_dir / f"{stem}{AUDIO_SUFFIX}"
                try:
                    shutil.copy2(audio_path, dst)
                except Exception as e:
                    logging.warning(f"Restauration audio échouée {audio_path} -> {dst}: {e}")

            tracking_path = ResultsArchiver.find_analysis_file(project_name, v, TRACKING_SUFFIX)
            if tracking_path:
                dst = target_dir / f"{stem}{TRACKING_SUFFIX}"
                try:
                    shutil.copy2(tracking_path, dst)
                except Exception as e:
                    logging.warning(f"Restauration tracking échouée {tracking_path} -> {dst}: {e}")
    except Exception as e:
        logging.warning(f"Restauration des analyses archivées échouée (projet={project_name}): {e}")

def finalize_project(project_dir):
    """Copie un projet vers la destination finale et supprime la source."""
    try:
        project_name = project_dir.name
        logging.info(f"Finalisation du projet: {project_name}")
        print(f"Finalisation en cours pour '{project_name}'...")

        # 1) Archiver d'abord tous les artefacts d'analyse disponibles
        try:
            arch_summary = ResultsArchiver.archive_project_analysis(project_name)
            if arch_summary and not arch_summary.get("error"):
                logging.info(
                    "Archivage des analyses terminé: %s",
                    json.dumps({k: arch_summary.get(k) for k in ("processed", "copied")}, ensure_ascii=False)
                )
            else:
                logging.warning(f"Archivage des analyses non effectué ou en erreur: {arch_summary}")
        except Exception as e:
            logging.warning(f"Erreur lors de l'archivage des analyses du projet '{project_name}': {e}")

        output_project_dir = OUTPUT_DIR / project_name

        if output_project_dir.exists():
            logging.warning(
                f"Le dossier de destination '{output_project_dir}' existe déjà. Tentative de suppression avant copie."
            )
            _safe_rmtree(output_project_dir)
            if output_project_dir.exists():
                alt_dir = _compute_alternative_output_dir(output_project_dir)
                logging.warning(
                    f"Impossible de supprimer la destination existante. Bascule vers: '{alt_dir}'"
                )
                output_project_dir = alt_dir

        try:
            _copy_project_tree(project_dir, output_project_dir)
        except Exception as e:
            logging.error("Erreur lors de la copie du projet: %s", e, exc_info=True)
            raise
        logging.info(f"Projet '{project_name}' copié avec succès vers '{output_project_dir}'")

        _normalize_project_docs_structure(output_project_dir)

        if os.environ.get("RESTORE_ARCHIVES_TO_OUTPUT", "0") in ("1", "true", "True"):
            try:
                restore_archived_analysis(project_name, output_project_dir)
            except Exception as e:
                logging.warning(f"Restauration des analyses archivées échouée: {e}")

        # --- Suppression du dossier source (après archivage) ---
        try:
            project_dir.resolve().relative_to(config.ARCHIVES_DIR.resolve())
            logging.error(f"Refus de suppression: '{project_dir}' est sous ARCHIVES_DIR")
            return False
        except Exception:
            pass
        _safe_rmtree(project_dir)
        logging.info(f"Dossier source '{project_dir}' supprimé avec succès.")
        # --- FIN suppression ---

        # --- MODIFICATION: Suppression de la création du fichier metadata_final.json ---
        # --- FIN DE LA MODIFICATION ---

        logging.info(f"Finalisation terminée pour '{project_name}'")
        print(f"Finalisation terminée pour '{project_name}'")
        return True

    except Exception as e:
        logging.error(f"Erreur lors de la finalisation du projet {project_dir.name}: {e}", exc_info=True)
        return False


def main():
    """Fonction principale du script."""
    logging.info(f"--- Démarrage du script de Finalisation et Nettoyage ---")

    global OUTPUT_DIR
    try:
        OUTPUT_DIR = _select_output_dir(OUTPUT_DIR, BASE_DIR)
        logging.info(f"Le répertoire de destination est: {OUTPUT_DIR.resolve()}")
    except Exception as e:
        logging.critical(
            f"Impossible de préparer un répertoire de destination inscriptible. Erreur: {e}"
        )
        sys.exit(1)

    projects = find_projects_to_finalize()
    total_projects = len(projects)
    logging.info(f"{total_projects} projet(s) à finaliser")
    print(f"{total_projects} projet(s) à finaliser")

    if total_projects == 0:
        logging.info("Aucun projet à finaliser. Fin du script.")
        return

    successful_count = 0
    for project in projects:
        if finalize_project(project):
            successful_count += 1

    logging.info(f"--- Finalisation terminée ---")
    logging.info(f"Résumé: {successful_count}/{total_projects} projet(s) finalisé(s) et déplacé(s) avec succès.")

    if successful_count < total_projects:
        sys.exit(1)


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        logging.critical(f"Erreur critique non gérée dans le script de finalisation: {e}", exc_info=True)
        sys.exit(1)
```

## File: services/cache_service.py
```python
"""
Cache Service
Centralized caching service for improved performance.
"""

import logging
import json
import time
import re
from functools import lru_cache, wraps
from typing import Dict, Any, Optional, Callable
from pathlib import Path
from flask_caching import Cache

from config.settings import config
from services.workflow_state import get_workflow_state

logger = logging.getLogger(__name__)

_SAFE_STEP_KEY_PATTERN = re.compile(r'^[A-Za-z0-9_-]+$')

# Global cache instance (will be initialized by Flask app)
cache_instance: Optional[Cache] = None

# Cache statistics
cache_stats = {
    "hits": 0,
    "misses": 0,
    "errors": 0,
    "last_reset": time.time()
}


class CacheService:
    """
    Centralized caching service with performance tracking.
    Provides intelligent caching for expensive operations.
    """
    
    @staticmethod
    def initialize(cache: Cache) -> None:
        """
        Initialize the cache service with Flask-Caching instance.
        
        Args:
            cache: Flask-Caching instance
        """
        global cache_instance
        cache_instance = cache
        logger.info("Cache service initialized")
    
    @staticmethod
    def get_cache_stats() -> Dict[str, Any]:
        """
        Get cache performance statistics.
        
        Returns:
            Cache statistics dictionary
        """
        total_requests = cache_stats["hits"] + cache_stats["misses"]
        hit_rate = (cache_stats["hits"] / total_requests * 100) if total_requests > 0 else 0
        
        return {
            "hits": cache_stats["hits"],
            "misses": cache_stats["misses"],
            "errors": cache_stats["errors"],
            "hit_rate_percent": round(hit_rate, 2),
            "total_requests": total_requests,
            "uptime_seconds": round(time.time() - cache_stats["last_reset"], 1)
        }
    
    @staticmethod
    def clear_cache() -> None:
        """Clear all cached data."""
        if cache_instance:
            cache_instance.clear()
            logger.info("Cache cleared")
    
    @staticmethod
    def reset_stats() -> None:
        """Reset cache statistics."""
        global cache_stats
        cache_stats = {
            "hits": 0,
            "misses": 0,
            "errors": 0,
            "last_reset": time.time()
        }
        logger.info("Cache statistics reset")
    
    @staticmethod
    def cached_with_stats(timeout: int = 300, key_prefix: str = ""):
        """
        Decorator for caching with statistics tracking.
        
        Args:
            timeout: Cache timeout in seconds
            key_prefix: Prefix for cache keys
            
        Returns:
            Decorator function
        """
        def decorator(func: Callable) -> Callable:
            @wraps(func)
            def wrapper(*args, **kwargs):
                if not cache_instance:
                    cache_stats["errors"] += 1
                    return func(*args, **kwargs)
                
                # Generate cache key
                cache_key = f"{key_prefix}:{func.__name__}:{hash(str(args) + str(kwargs))}"
                
                try:
                    # Try to get from cache
                    result = cache_instance.get(cache_key)
                    if result is not None:
                        cache_stats["hits"] += 1
                        return result
                    
                    # Cache miss - execute function
                    cache_stats["misses"] += 1
                    result = func(*args, **kwargs)
                    
                    # Store in cache
                    cache_instance.set(cache_key, result, timeout=timeout)
                    return result
                    
                except Exception as e:
                    cache_stats["errors"] += 1
                    logger.error(f"Cache error for {func.__name__}: {e}")
                    return func(*args, **kwargs)
            
            return wrapper
        return decorator
    
    @staticmethod
    @lru_cache(maxsize=128)
    def get_video_metadata(video_path: str) -> Dict[str, Any]:
        """
        Get video metadata with caching to avoid repeated file reads.
        
        Args:
            video_path: Path to video file
            
        Returns:
            Video metadata dictionary
        """
        try:
            import cv2
            cap = cv2.VideoCapture(video_path)
            try:
                if not cap.isOpened():
                    raise ValueError(f"Cannot open video: {video_path}")
                
                metadata = {
                    'frame_count': int(cap.get(cv2.CAP_PROP_FRAME_COUNT)),
                    'fps': cap.get(cv2.CAP_PROP_FPS),
                    'width': int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),
                    'height': int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)),
                    'duration_seconds': None
                }
                
                # Calculate duration
                if metadata['fps'] > 0 and metadata['frame_count'] > 0:
                    metadata['duration_seconds'] = metadata['frame_count'] / metadata['fps']
                
                return metadata
            finally:
                cap.release()
                
        except Exception as e:
            logger.error(f"Video metadata error for {video_path}: {e}")
            return {
                'frame_count': 0,
                'fps': 0,
                'width': 0,
                'height': 0,
                'duration_seconds': None,
                'error': str(e)
            }
    
    @staticmethod
    def get_cached_frontend_config() -> Dict[str, Any]:
        """
        Get cached frontend configuration.

        Returns:
            Frontend-safe configuration dictionary
        """
        try:
            # Try to use cache if available
            if cache_instance:
                try:
                    cached_result = cache_instance.get("frontend_config")
                    if cached_result is not None:
                        cache_stats["hits"] += 1
                        logger.debug("Frontend config cache hit")
                        return cached_result
                except Exception as cache_error:
                    logger.warning(f"Cache access failed, generating fresh config: {cache_error}")

            # Generate fresh config
            from config.workflow_commands import WorkflowCommandsConfig

            commands_config = WorkflowCommandsConfig().get_config()
            if not commands_config:
                logger.error("WorkflowCommandsConfig is empty or not available")
                cache_stats["errors"] += 1
                return {}

            result: Dict[str, Any] = {}
            for step_key, step_data_orig in commands_config.items():
                if not isinstance(step_key, str) or not _SAFE_STEP_KEY_PATTERN.match(step_key):
                    logger.error(
                        "Unsafe step_key detected in WorkflowCommandsConfig; skipping for frontend DOM safety: %r",
                        step_key,
                    )
                    continue
                frontend_step_data: Dict[str, Any] = {}
                for key, value in step_data_orig.items():
                    if key == "progress_patterns":
                        continue
                    if isinstance(value, Path):
                        frontend_step_data[key] = str(value)
                    elif key == "cmd" and isinstance(value, list):
                        frontend_step_data[key] = [str(item) for item in value]
                    elif key == "specific_logs" and isinstance(value, list):
                        safe_logs = []
                        for log_entry in value:
                            if not isinstance(log_entry, dict):
                                continue
                            safe_entry = log_entry.copy()
                            if 'path' in safe_entry and isinstance(safe_entry['path'], Path):
                                safe_entry['path'] = str(safe_entry['path'])
                            safe_logs.append(safe_entry)
                        frontend_step_data[key] = safe_logs
                    else:
                        frontend_step_data[key] = value
                result[step_key] = frontend_step_data
            logger.debug(f"Generated fresh frontend config with {len(result)} steps")

            # Cache the result if cache is available
            if cache_instance:
                try:
                    cache_instance.set("frontend_config", result, timeout=300)
                    cache_stats["misses"] += 1
                    logger.debug("Frontend config cached successfully")
                except Exception as cache_error:
                    logger.warning(f"Failed to cache frontend config: {cache_error}")

            return result
        except Exception as e:
            logger.error(f"Frontend config cache error: {e}")
            cache_stats["errors"] += 1
            return {}
    
    @staticmethod
    def get_cached_log_content(step_key: str, log_index: int) -> Dict[str, Any]:
        """
        Get cached log file content.

        Args:
            step_key: Step identifier
            log_index: Log file index

        Returns:
            Log content dictionary

        Raises:
            ValueError: If step or log not found
        """
        try:
            # Try to use cache if available
            cache_key = f"log_content:{step_key}:{log_index}"
            if cache_instance:
                cached_result = cache_instance.get(cache_key)
                if cached_result is not None:
                    cache_stats["hits"] += 1
                    return cached_result

            from services.workflow_service import WorkflowService

            result = WorkflowService.get_step_log_file(step_key, log_index)

            # Cache the result if cache is available
            if cache_instance:
                cache_instance.set(cache_key, result, timeout=60)
                cache_stats["misses"] += 1

            return result
            
        except Exception as e:
            logger.error(f"Log content cache error for {step_key}/{log_index}: {e}")
            raise
    
    @staticmethod
    def get_cached_step_status(step_key: str) -> Dict[str, Any]:
        """
        Get cached step status to reduce computation.
        
        Args:
            step_key: Step identifier
            
        Returns:
            Step status dictionary
        """
        try:
            step_info = get_workflow_state().get_step_info(step_key)

            if not step_info:
                raise ValueError(f"Step '{step_key}' not found")

            return step_info.copy()
            
        except Exception as e:
            logger.error(f"Step status cache error for {step_key}: {e}")
            raise
    
    @staticmethod
    def invalidate_step_cache(step_key: str) -> None:
        """
        Invalidate cache entries for a specific step.
        
        Args:
            step_key: Step identifier
        """
        if not cache_instance:
            return
        
        try:
            # Clear step-specific cache entries
            cache_keys_to_clear = [
                f"step_status:{step_key}",
                f"log_content:{step_key}"
            ]
            
            for key in cache_keys_to_clear:
                cache_instance.delete(key)
            
            logger.debug(f"Invalidated cache for step {step_key}")
            
        except Exception as e:
            logger.error(f"Cache invalidation error for {step_key}: {e}")
    
    @staticmethod
    def warm_cache() -> None:
        """
        Pre-populate cache with commonly accessed data.
        """
        try:
            logger.info("Starting cache warm-up")
            
            # Warm up frontend config
            CacheService.get_cached_frontend_config()
            
            # Warm up step statuses
            workflow_state = get_workflow_state()
            step_keys = list((workflow_state.get_all_steps_info() or {}).keys())
            if not step_keys:
                from config.workflow_commands import WorkflowCommandsConfig
                step_keys = WorkflowCommandsConfig().get_all_step_keys()

            for step_key in step_keys:
                try:
                    CacheService.get_cached_step_status(step_key)
                except Exception:
                    pass  # Skip errors during warm-up
            
            logger.info("Cache warm-up completed")
            
        except Exception as e:
            logger.error(f"Cache warm-up error: {e}")
    
    @staticmethod
    def get_cache_size_estimate() -> Dict[str, Any]:
        """
        Get an estimate of cache memory usage.
        
        Returns:
            Cache size information
        """
        try:
            # This is a rough estimate since Flask-Caching doesn't provide direct size info
            stats = CacheService.get_cache_stats()
            
            # Estimate based on number of cached items
            estimated_items = stats["hits"] + stats["misses"]
            estimated_size_mb = estimated_items * 0.001  # Rough estimate: 1KB per item
            
            return {
                "estimated_items": estimated_items,
                "estimated_size_mb": round(estimated_size_mb, 2),
                "cache_type": "SimpleCache",
                "note": "Size estimates are approximate"
            }
            
        except Exception as e:
            logger.error(f"Cache size estimation error: {e}")
            return {
                "estimated_items": 0,
                "estimated_size_mb": 0,
                "error": str(e)
            }
```

## File: services/csv_service.py
```python
"""
CSV Service
Centralized service for CSV monitoring functionality.
"""

import logging
import os
import json
import re
from datetime import datetime, timezone
from typing import Dict, Any, Set, Optional, List
import shutil
import urllib.parse
import html
from config.settings import config
from services.download_history_repository import download_history_repository

logger = logging.getLogger(__name__)

# Import WebhookService for external JSON source
try:
    from services.webhook_service import fetch_records as webhook_fetch_records, get_service_status as webhook_status
    WEBHOOK_SERVICE_AVAILABLE = True
except Exception:
    WEBHOOK_SERVICE_AVAILABLE = False
    def webhook_status():
        return {"available": False, "error": "WebhookService not available"}

# Note: CSV monitoring state and downloads tracking are now managed in app_new.py
# This service acts as an interface to those global variables

LEGACY_DOWNLOAD_HISTORY_FILE = config.BASE_PATH_SCRIPTS / "download_history.json"
_SHARED_GROUP = config.DOWNLOAD_HISTORY_SHARED_GROUP
_SHARED_FILE_MODE = 0o664

# Optional in-memory cache for last known good history set to avoid bursts on transient read errors
_LAST_KNOWN_HISTORY_SET: Set[str] = set()


def _is_dropbox_url(url: str) -> bool:
    """Return True if the URL belongs to Dropbox domains.

    Args:
        url: URL string

    Returns:
        bool: True if hostname matches known Dropbox hostnames
    """
    try:
        from urllib.parse import urlparse
        parsed = urlparse((url or "").strip())
        host = (parsed.hostname or "").lower()
        # Known Dropbox hostnames
        return host in {"dropbox.com", "www.dropbox.com", "dl.dropboxusercontent.com"}
    except Exception:
        return False


def _is_dropbox_proxy_url(url: str) -> bool:
    """Return True if the URL looks like a worker/R2 proxy for Dropbox downloads."""
    try:
        u = (url or "").strip().lower()
        return "/dropbox/" in u and ("workers.dev" in u or "worker" in u)
    except Exception:
        return False


def _looks_like_archive_download(url: Optional[str], original_filename: Optional[str]) -> bool:
    """Heuristic to avoid auto-downloading non-archive Dropbox links (e.g. png previews)."""
    u = (url or "").strip().lower()
    fn = (original_filename or "").strip().lower()
    if fn.endswith('.zip'):
        return True
    if '/scl/fo/' in u:
        return True
    if u.endswith('.zip') or '.zip?' in u:
        return True
    return False


class CSVService:
    """
    Centralized service for CSV monitoring functionality.
    Handles CSV file monitoring and download tracking.
    """
    
    _initialized = False

    @staticmethod
    def initialize() -> None:
        """Initialize the CSV service."""
        if not CSVService._initialized:
            CSVService._initialized = True
            logger.info("CSV service initialized")
            try:
                download_history_repository.initialize()
            except Exception as e:
                logger.warning(f"History DB initialization failed: {e}")

            try:
                CSVService._migrate_legacy_history_json_to_sqlite_if_needed()
            except Exception as e:
                logger.warning(f"Legacy history migration skipped due to error: {e}")

    @staticmethod
    def _migrate_legacy_history_json_to_sqlite_if_needed() -> Dict[str, Any]:
        try:
            if not LEGACY_DOWNLOAD_HISTORY_FILE.exists():
                return {"status": "noop", "reason": "legacy_missing"}
            if download_history_repository.count() > 0:
                return {"status": "noop", "reason": "db_not_empty"}

            items = CSVService._load_structured_history()
            if not items:
                return {"status": "noop", "reason": "legacy_empty"}

            def _normalize_ts_for_db(ts: str) -> str:
                if not ts:
                    return ""
                raw = str(ts).strip()
                if not raw:
                    return ""
                try:
                    dt = datetime.fromisoformat(raw.replace('Z', '+00:00'))
                    if dt.tzinfo:
                        return dt.astimezone().strftime('%Y-%m-%d %H:%M:%S')
                except Exception:
                    pass
                return raw

            entries = []
            for item in items:
                url = item.get('url')
                ts = item.get('timestamp') or ''
                if not url:
                    continue
                entries.append((url, _normalize_ts_for_db(ts)))

            download_history_repository.upsert_many(entries)
            return {"status": "success", "total": len(entries)}
        except Exception as e:
            logger.warning(f"Legacy history migration failed: {e}")
            return {"status": "error", "message": str(e)}
    
    @staticmethod
    def get_monitor_status() -> Dict[str, Any]:
        """
        Get monitoring service status (CSV or Airtable).

        Returns:
            Monitor status dictionary with both CSV and Airtable information
        """
        # Import WorkflowState singleton
        from services.workflow_state import get_workflow_state
        
        workflow_state = get_workflow_state()
        monitor_status = workflow_state.get_csv_monitor_status()

        # Webhook is the single data source
        status = {
            "csv_monitor": monitor_status,
            "data_source": "webhook",
            "monitor_interval": config.WEBHOOK_MONITOR_INTERVAL,
            "webhook": webhook_status()
        }

        return status
    

    
    @staticmethod
    def get_download_history() -> Set[str]:
        """
        Load download history from file as a set of URLs (backward compatible).

        The underlying file may be:
        - A list of strings: ["url1", "url2", ...]
        - A list of objects: [{"url": "...", "timestamp": "YYYY-MM-DD HH:MM:SS"}, ...]

        Returns:
            Set[str]: URLs present in history
        """
        try:
            CSVService.initialize()
            global _LAST_KNOWN_HISTORY_SET
            urls = download_history_repository.get_urls()
            _LAST_KNOWN_HISTORY_SET = set(urls)
            return set(urls)
        except Exception as e:
            logger.error(f"Error loading download history: {e}")
            return set(_LAST_KNOWN_HISTORY_SET)

    @staticmethod
    def _parse_history_to_set(data: Any) -> Set[str]:
        """Convert history JSON content to a set of URLs supporting both formats."""
        if not isinstance(data, list):
            return set()
        if data and isinstance(data[0], dict):
            return {CSVService._normalize_url(str(item.get('url'))) for item in data if isinstance(item, dict) and item.get('url')}
        return {CSVService._normalize_url(str(u)) for u in data if isinstance(u, str)}

    @staticmethod
    def _normalize_url(url: str) -> str:
        """Normalize URLs to prevent duplicates due to minor variations.

        Rules:
        - Trim whitespace
        - Unescape HTML entities (e.g., '&amp;' -> '&')
        - Recursively decode double-encoded sequences (e.g., amp%3Bdl=0 -> &dl=0)
        - Lowercase scheme and hostname
        - Remove default ports (80 for http, 443 for https)
        - Sort query parameters by key and value
        - Remove empty query parameters
        - For Dropbox links, ensure a single dl=1 param and collapse duplicates
        - Remove trailing slashes for non-root paths
        - Percent-decode path and then re-encode safely
        """
        if not url:
            return ""
        try:
            raw = url.strip()
            # First, unescape HTML entities that may come from CSV/HTML sources
            # Example: '...&amp;dl=0' -> '...&dl=0'
            try:
                raw = html.unescape(raw)
            except Exception:
                pass
            
            # Pre-process: recursively decode double-encoded sequences that may cause parsing issues
            # Example: "amp%3Bdl=0&dl=1" becomes "&dl=0&dl=1", which is then properly parsed
            prev_url = None
            max_decode_iterations = 3
            iteration = 0
            while prev_url != raw and iteration < max_decode_iterations:
                prev_url = raw
                # Decode common double-encoded patterns (HTML entity codes)
                if 'amp%3B' in raw or 'amp%3b' in raw:
                    raw = raw.replace('amp%3B', '&').replace('amp%3b', '&')
                # Detect and clean malformed ampersands that appear before valid params
                # Pattern: "?amp%3Bdl=0&dl=1" -> "?dl=1"
                if '%3B' in raw or '%3b' in raw:
                    # Try URL decode once to catch other double-encoded params
                    try:
                        decoded = urllib.parse.unquote(raw)
                        # Only accept if it still looks like a valid URL
                        if '://' in decoded:
                            raw = decoded
                    except Exception:
                        pass
                iteration += 1
            
            parsed = urllib.parse.urlsplit(raw)
            scheme = (parsed.scheme or '').lower()
            netloc = (parsed.hostname or '').lower()
            port = parsed.port
            # Preserve username/password if any
            if parsed.username or parsed.password:
                auth = ''
                if parsed.username:
                    auth += urllib.parse.quote(parsed.username)
                if parsed.password:
                    auth += f":{urllib.parse.quote(parsed.password)}"
                netloc = f"{auth}@{netloc}" if auth else netloc

            # Drop default ports
            if port and not (
                (scheme == 'http' and port == 80) or (scheme == 'https' and port == 443)
            ):
                netloc = f"{netloc}:{port}"

            # Normalize path: decode, strip, remove duplicate slashes
            path = urllib.parse.unquote(parsed.path or '')
            if '//' in path:
                while '//' in path:
                    path = path.replace('//', '/')
            # Remove trailing slash unless root
            if path.endswith('/') and path != '/':
                path = path[:-1]
            # Re-encode path safely
            path = urllib.parse.quote(path, safe='/-._~')

            # Normalize query params
            q = urllib.parse.parse_qsl(parsed.query or '', keep_blank_values=False)
            # Special handling for Dropbox: force dl=1
            host_lower = (parsed.hostname or '').lower() if parsed.hostname else ''
            is_dropbox = host_lower.endswith('dropbox.com') or host_lower == 'dl.dropboxusercontent.com'
            filtered = []
            seen = set()
            for k, v in q:
                key = k.strip()
                val = v.strip()
                # Skip completely empty params
                if not key:
                    continue
                # Skip params with empty values (except legitimate ones like rlkey)
                if not val and key.lower() not in ('rlkey',):
                    # For Dropbox dl param, empty value is invalid
                    if is_dropbox and key.lower() == 'dl':
                        continue
                # collapse duplicates
                tup = (key, val)
                if tup in seen:
                    continue
                seen.add(tup)
                # We will handle dl param below for Dropbox
                if is_dropbox and key.lower() == 'dl':
                    continue
                filtered.append((key, val))

            if is_dropbox:
                filtered.append(('dl', '1'))

            # Sort for determinism
            filtered.sort(key=lambda kv: (kv[0].lower(), kv[1]))
            query = urllib.parse.urlencode(filtered, doseq=True)

            # Fragment is not relevant for downloads; drop it
            normalized = urllib.parse.urlunsplit((scheme, netloc, path, query, ''))
            return normalized
        except Exception:
            return url.strip()

    @staticmethod
    def _load_structured_history() -> List[Dict[str, str]]:
        """Load the history as a list of {url, timestamp} objects (best-effort).

        Returns:
            List[Dict[str, str]]: Each item contains 'url' and 'timestamp' (string)
        """
        try:
            if not LEGACY_DOWNLOAD_HISTORY_FILE.exists():
                return []
            with open(LEGACY_DOWNLOAD_HISTORY_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
            if not isinstance(data, list):
                return []
            # If already structured
            if data and isinstance(data[0], dict):
                result: List[Dict[str, str]] = []
                for item in data:
                    if isinstance(item, dict) and item.get('url'):
                        ts = str(item.get('timestamp')) if item.get('timestamp') else None
                        result.append({
                            'url': CSVService._normalize_url(str(item.get('url'))),
                            'timestamp': ts if ts else ''
                        })
                return result
            # Flat list of URLs -> convert to objects with empty timestamp
            result = []
            for u in data:
                if isinstance(u, str) and u:
                    result.append({'url': CSVService._normalize_url(u), 'timestamp': ''})
            return result
        except Exception as e:
            logger.error(f"Error loading structured download history: {e}")
            return []

    @staticmethod
    def _now_ts_str() -> str:
        """Return current timestamp as 'YYYY-MM-DD HH:MM:SS' in LOCAL time."""
        # Use system local timezone
        return datetime.now().astimezone().strftime('%Y-%m-%d %H:%M:%S')

    @staticmethod
    def migrate_history_to_local_time() -> Dict[str, Any]:
        """
        Convert existing download_history.json timestamps from UTC to local time.

        Assumes stored timestamps are in 'YYYY-MM-DD HH:MM:SS' and represent UTC.
        For each, convert to local timezone and re-save chronologically.

        Returns:
            dict with migration summary
        """
        try:
            items = CSVService._load_structured_history()
            if not items:
                return {"status": "noop", "updated": 0}

            updated = 0
            converted: List[Dict[str, str]] = []
            for item in items:
                url = item.get('url')
                ts = item.get('timestamp') or ''
                new_ts = ts
                if ts:
                    try:
                        # Parse as UTC naive and set tzinfo=UTC, then convert to local
                        dt_utc = datetime.strptime(ts, '%Y-%m-%d %H:%M:%S').replace(tzinfo=timezone.utc)
                        dt_local = dt_utc.astimezone()  # system local tz
                        new_ts = dt_local.strftime('%Y-%m-%d %H:%M:%S')
                        if new_ts != ts:
                            updated += 1
                    except Exception:
                        # Best-effort: try ISO parse
                        try:
                            dt_any = datetime.fromisoformat(ts.replace('Z', '+00:00'))
                            dt_local = (dt_any if dt_any.tzinfo else dt_any.replace(tzinfo=timezone.utc)).astimezone()
                            new_ts = dt_local.strftime('%Y-%m-%d %H:%M:%S')
                            if new_ts != ts:
                                updated += 1
                        except Exception:
                            # Leave as is if unparseable
                            new_ts = ts
                converted.append({'url': url, 'timestamp': new_ts})

            # Sort chronologically, then by URL
            def _sort_key(item: Dict[str, str]):
                ts = item.get('timestamp') or '9999-12-31 23:59:59'
                return (ts, item.get('url') or '')

            converted.sort(key=_sort_key)

            with open(LEGACY_DOWNLOAD_HISTORY_FILE, 'w', encoding='utf-8') as f:
                json.dump(converted, f, indent=2, ensure_ascii=False)

            return {"status": "success", "updated": updated, "total": len(converted)}
        except Exception as e:
            logger.error(f"Error migrating history to local time: {e}")
            return {"status": "error", "message": str(e)}
    
    @staticmethod
    def _ensure_shared_permissions(target):
        """
        Apply shared group/permission settings to the target file if configured.
        """
        if not target or not target.exists():
            return
        if _SHARED_GROUP:
            try:
                shutil.chown(target, group=_SHARED_GROUP)
            except Exception as chown_err:
                logger.warning(f"Unable to assign shared group '{_SHARED_GROUP}' to {target}: {chown_err}")
        try:
            os.chmod(target, _SHARED_FILE_MODE)
        except Exception as chmod_err:
            logger.warning(f"Unable to set shared permissions on {target}: {chmod_err}")

    @staticmethod
    def save_download_history(history_set: Set[str]) -> None:
        """
        Save download history to file, as a chronologically sorted list of
        objects {url, timestamp}. Backward compatible with callers passing a set.

        Rules:
        - Preserve existing timestamps for URLs already present.
        - Assign current timestamp to new URLs.
        - Only persist URLs present in history_set (reflects clear/overwrite).
        - Sort by timestamp ASC, then URL for deterministic order.
        """
        try:
            ts_by_url = download_history_repository.get_ts_by_url()
            normalized_set = {CSVService._normalize_url(u) for u in history_set}

            entries = []
            for url in sorted(normalized_set):
                ts = ts_by_url.get(url) or CSVService._now_ts_str()
                entries.append((url, ts))

            def _sort_key(item):
                ts = item[1] or '9999-12-31 23:59:59'
                return (ts, item[0] or '')

            entries.sort(key=_sort_key)
            download_history_repository.replace_all(entries)

            global _LAST_KNOWN_HISTORY_SET
            _LAST_KNOWN_HISTORY_SET = set(normalized_set)
        except Exception as e:
            logger.error(f"Error saving download history: {e}")
    
    @staticmethod
    def add_to_download_history(url: str) -> bool:
        """
        Add URL to download history.
        
        Args:
            url: URL to add to history
            
        Returns:
            True if successful, False otherwise
        """
        return CSVService.add_to_download_history_with_timestamp(url, None)

    @staticmethod
    def add_to_download_history_with_timestamp(url: str, timestamp_str: Optional[str]) -> bool:
        """
        Add URL to download history with a provided timestamp (if any).

        Preserves the earliest timestamp seen for a URL. If no valid timestamp is
        provided, uses current time.

        Args:
            url: URL to add
            timestamp_str: Optional timestamp string 'YYYY-MM-DD HH:MM:SS' or ISO; best-effort parsed

        Returns:
            True if persisted successfully, else False
        """
        try:
            ts_norm = None
            if timestamp_str:
                try:
                    dt = datetime.fromisoformat(str(timestamp_str).replace('Z', '+00:00'))
                    ts_norm = dt.strftime('%Y-%m-%d %H:%M:%S')
                except Exception:
                    try:
                        dt = datetime.strptime(str(timestamp_str), '%Y-%m-%d %H:%M:%S')
                        ts_norm = dt.strftime('%Y-%m-%d %H:%M:%S')
                    except Exception:
                        ts_norm = str(timestamp_str)
            if not ts_norm:
                ts_norm = CSVService._now_ts_str()

            norm_url = CSVService._normalize_url(url)
            download_history_repository.upsert(norm_url, ts_norm)

            global _LAST_KNOWN_HISTORY_SET
            _LAST_KNOWN_HISTORY_SET.add(norm_url)
            return True
        except Exception as e:
            logger.error(f"Error adding to download history with timestamp: {e}")
            return False
    
    @staticmethod
    def is_url_downloaded(url: str) -> bool:
        """
        Check if URL has been downloaded before.
        
        Args:
            url: URL to check
            
        Returns:
            True if URL was previously downloaded
        """
        history = CSVService.get_download_history()
        return CSVService._normalize_url(url) in history
    
    @staticmethod
    def get_csv_downloads_status() -> Dict[str, Any]:
        """
        Get status of CSV downloads.

        Returns:
            CSV downloads status dictionary
        """
        # Import WorkflowState singleton
        from services.workflow_state import get_workflow_state
        
        workflow_state = get_workflow_state()
        active_downloads = workflow_state.get_active_csv_downloads_dict()
        recent_statuses = workflow_state.get_kept_csv_downloads_list()

        return {
            "active_downloads": active_downloads,
            "recent_statuses": recent_statuses,
            "total_active": len(active_downloads),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
    
    @staticmethod
    def add_csv_download(download_id: str, download_info: Dict[str, Any]) -> None:
        """
        Add a CSV download to tracking.

        Args:
            download_id: Unique download identifier
            download_info: Download information dictionary
        """
        # Import WorkflowState singleton
        from services.workflow_state import get_workflow_state
        
        workflow_state = get_workflow_state()
        download_data = {
            **download_info,
            "start_time": datetime.now(timezone.utc).isoformat()
        }
        workflow_state.add_csv_download(download_id, download_data)

        logger.info(f"CSV download added: {download_id}")
    
    @staticmethod
    def update_csv_download(download_id: str, status: str, **kwargs) -> None:
        """
        Update CSV download status.

        Args:
            download_id: Download identifier
            status: New status
            **kwargs: Additional fields to update
        """
        # Import WorkflowState singleton
        from services.workflow_state import get_workflow_state
        
        workflow_state = get_workflow_state()
        
        # Update download status with individual parameters
        progress = kwargs.get('progress')
        message = kwargs.get('message')
        filename = kwargs.get('filename')
        
        workflow_state.update_csv_download(
            download_id=download_id,
            status=status,
            progress=progress,
            message=message,
            filename=filename
        )

        # If download completed, move to history
        if status in ["completed", "failed", "cancelled", "unknown_error"]:
            workflow_state.move_csv_download_to_history(download_id)

        logger.debug(f"CSV download updated: {download_id} -> {status}")
    
    @staticmethod
    def remove_csv_download(download_id: str) -> None:
        """
        Remove CSV download from tracking.

        Args:
            download_id: Download identifier
        """
        # Import WorkflowState singleton
        from services.workflow_state import get_workflow_state
        
        workflow_state = get_workflow_state()
        workflow_state.remove_csv_download(download_id)

        logger.info(f"CSV download removed: {download_id}")
    
    @staticmethod
    def rewrite_dropbox_to_dl_host(url_str: str) -> str:
        """Rewrite a www.dropbox.com URL to dl.dropboxusercontent.com preserving path/query.

        Example:
        https://www.dropbox.com/scl/fo/<id>?rlkey=...&dl=1 ->
        https://dl.dropboxusercontent.com/scl/fo/<id>?rlkey=...&dl=1

        Args:
            url_str: Original Dropbox URL

        Returns:
            Rewritten URL string
        """
        try:
            parsed = urllib.parse.urlsplit(url_str)
            host = parsed.hostname or ""
            # Do NOT rewrite folder share links ('/scl/fo/') which are not served by dl.dropboxusercontent.com
            if (host.endswith("dropbox.com") and host != "dl.dropboxusercontent.com"
                    and not (parsed.path or '').lower().startswith('/scl/fo/')):
                new_netloc = "dl.dropboxusercontent.com"
                # preserve port if any (rare)
                if parsed.port and parsed.port not in (80, 443):
                    new_netloc = f"{new_netloc}:{parsed.port}"
                return urllib.parse.urlunsplit((parsed.scheme or "https", new_netloc, parsed.path, parsed.query, ""))
            return url_str
        except Exception:
            return url_str

    @staticmethod
    def _check_csv_for_downloads() -> None:
        """
        Check for new downloads using Webhook as the single data source.
        """
        try:
            # Import here to avoid circular imports
            from app_new import execute_csv_download_worker
            from services.workflow_state import get_workflow_state
            import threading
            import os

            # Fetch data from Webhook (single data source)
            if not WEBHOOK_SERVICE_AVAILABLE:
                logger.error("WebhookService not available - monitoring disabled")
                return

            logger.debug("Fetching data from Webhook")
            data_rows = webhook_fetch_records()
            source_type = "WEBHOOK"

            if data_rows is None:
                logger.warning(f"Could not fetch data from {source_type}")
                return

            # Get current download history (normalized URLs)
            download_history = CSVService.get_download_history()

            workflow_state = get_workflow_state()
            active_downloads = workflow_state.get_active_csv_downloads_dict()
            kept_downloads = workflow_state.get_kept_csv_downloads_list()

            tracked_urls: Set[str] = set()
            try:
                candidates = list(active_downloads.values()) + list(kept_downloads)
                for download in candidates:
                    if not isinstance(download, dict):
                        continue
                    status = str(download.get('status') or '').strip().lower()
                    if status in ('failed', 'cancelled', 'unknown_error'):
                        continue
                    raw_url = (download.get('original_url') or download.get('url') or '').strip()
                    if not raw_url:
                        continue
                    norm_existing = CSVService._normalize_url(raw_url)
                    if norm_existing:
                        tracked_urls.add(norm_existing)
            except Exception:
                tracked_urls = set()

            def _is_url_already_tracked(norm_primary: Optional[str], norm_fallback: Optional[str]) -> bool:
                if norm_primary and norm_primary in tracked_urls:
                    return True
                if norm_fallback and norm_fallback in tracked_urls:
                    return True
                return False

            total_rows = len(data_rows)
            skipped_missing_url = 0
            skipped_already_in_history = 0
            skipped_already_tracked = 0

            # Check for new URLs
            new_downloads = 0
            # Optional dry-run to avoid real downloads (useful for tests/CI):
            dry_run = os.environ.get('DRY_RUN_DOWNLOADS', 'false').lower() in ('true', '1')

            handled_in_this_pass: Set[str] = set()

            for row in data_rows:
                url = row.get('url')
                fallback_url = row.get('fallback_url')
                original_filename = row.get('original_filename')
                provider = row.get('provider')
                timestamp_str = row.get('timestamp')

                norm_url = CSVService._normalize_url(url) if url else None
                norm_fallback_url = CSVService._normalize_url(fallback_url) if fallback_url else None

                if norm_url and norm_url in handled_in_this_pass:
                    continue
                if norm_fallback_url and norm_fallback_url in handled_in_this_pass:
                    continue

                already_in_history = (
                    (norm_url and norm_url in download_history)
                    or (norm_fallback_url and norm_fallback_url in download_history)
                )
                if not norm_url:
                    skipped_missing_url += 1
                    continue
                if _is_url_already_tracked(norm_url, norm_fallback_url):
                    skipped_already_tracked += 1
                    handled_in_this_pass.add(norm_url)
                    if norm_fallback_url:
                        handled_in_this_pass.add(norm_fallback_url)
                    continue
                if already_in_history:
                    skipped_already_in_history += 1
                    # Common case: preferred URL removed from history, but fallback URL still present.
                    # This is expected behavior to prevent re-downloads across URL variants.
                    if (
                        norm_fallback_url
                        and norm_fallback_url in download_history
                        and norm_url not in download_history
                    ):
                        logger.debug(
                            f"{source_type} MONITOR: Skipping URL because fallback is already in download history "
                            f"(preferred={norm_url}, fallback={norm_fallback_url})"
                        )
                    continue

                try:
                    parsed_primary = urllib.parse.urlsplit(url or '')
                    scheme_primary = (parsed_primary.scheme or '').lower()
                    if scheme_primary and scheme_primary not in ('http', 'https'):
                        logger.debug(
                            f"{source_type} MONITOR: Ignoring unsupported URL scheme '{scheme_primary}': {url}"
                        )
                        handled_in_this_pass.add(norm_url)
                        if norm_fallback_url:
                            handled_in_this_pass.add(norm_fallback_url)
                        continue
                except Exception:
                    # If URL parsing fails, treat it as non-eligible.
                    logger.debug(
                        f"{source_type} MONITOR: Ignoring invalid URL (parse error): {url}"
                    )
                    handled_in_this_pass.add(norm_url)
                    if norm_fallback_url:
                        handled_in_this_pass.add(norm_fallback_url)
                    continue

                url_lower = (url or '').lower()
                provider_lower = str(provider or '').strip().lower()

                # Determine URL type for UI hints / routing
                url_type = (
                    str(row.get('url_type') or '').strip().lower()
                    or (
                        'fromsmash' if 'fromsmash.com' in url_lower else (
                            'swisstransfer' if 'swisstransfer.com' in url_lower else (
                                'dropbox' if (_is_dropbox_url(url) or _is_dropbox_proxy_url(url) or provider_lower == 'dropbox') else 'external'
                            )
                        )
                    )
                )

                is_dropbox_like = (
                    url_type == 'dropbox'
                    or provider_lower == 'dropbox'
                    or _is_dropbox_url(url)
                    or _is_dropbox_proxy_url(url)
                )

                # Auto-download is intentionally restricted:
                # - Prevents large backlog downloads from legacy entries.
                # - Focuses on the new webhook schema (original_filename / fallback_url) and R2 proxy URLs.
                has_new_schema_hints = bool(
                    (original_filename and str(original_filename).strip())
                    or (fallback_url and str(fallback_url).strip())
                    or _is_dropbox_proxy_url(url)
                )
                auto_download_allowed = (
                    is_dropbox_like
                    and _looks_like_archive_download(url, original_filename)
                    and has_new_schema_hints
                )

                if auto_download_allowed:
                    logger.info(
                        f"{source_type} MONITOR: New eligible URL detected: {url} (timestamp: {timestamp_str}) [type={url_type}]"
                    )
                    # Dropbox-like: proceed with auto-download (unless dry-run)
                    if dry_run:
                        logger.info(
                            f"[DRY RUN] Would start Dropbox download for URL: {url} (timestamp: {timestamp_str})"
                        )
                        CSVService.add_to_download_history_with_timestamp(norm_url, timestamp_str)
                        download_history.add(norm_url)
                        handled_in_this_pass.add(norm_url)
                        if norm_fallback_url:
                            CSVService.add_to_download_history_with_timestamp(norm_fallback_url, timestamp_str)
                            download_history.add(norm_fallback_url)
                            handled_in_this_pass.add(norm_fallback_url)
                        new_downloads += 1
                    else:
                        download_thread = threading.Thread(
                            target=execute_csv_download_worker,
                            args=(url, timestamp_str, fallback_url, original_filename),
                            name=f"Download-{str(timestamp_str).replace('/', '').replace(' ', '_').replace(':', '')}"
                        )
                        download_thread.daemon = True
                        download_thread.start()
                        handled_in_this_pass.add(norm_url)
                        if norm_fallback_url:
                            handled_in_this_pass.add(norm_fallback_url)
                        new_downloads += 1
                else:
                    # Non-eligible link: ignore it (no UI entry, no history write) to keep auto-download Dropbox-only.
                    logger.debug(
                        f"{source_type} MONITOR: Ignoring non-eligible URL (auto-download disabled): {url} "
                        f"(timestamp: {timestamp_str}) [type={url_type}]"
                    )
                    handled_in_this_pass.add(norm_url)
                    if norm_fallback_url:
                        handled_in_this_pass.add(norm_fallback_url)

            if new_downloads > 0:
                logger.info(f"{source_type} MONITOR: {new_downloads} new download(s) started")
            if new_downloads == 0:
                logger.debug(
                    f"{source_type} MONITOR: No new items (rows={total_rows}, "
                    f"skipped_in_history={skipped_already_in_history}, skipped_tracked={skipped_already_tracked}, "
                    f"skipped_missing_url={skipped_missing_url})"
                )

        except Exception as e:
            logger.error(f"Error checking for downloads: {e}", exc_info=True)

    @staticmethod
    def _normalize_and_deduplicate_history() -> None:
        """Normalize and deduplicate the persisted history file.

        Preserves earliest timestamp per logical URL.
        """
        try:
            existing = CSVService._load_structured_history()
            if not existing:
                return
            # Build minimal map url->timestamp keeping earliest timestamp (lexicographically smaller)
            ts_by_url: Dict[str, str] = {}
            for item in existing:
                url = CSVService._normalize_url(item.get('url') or '')
                ts = item.get('timestamp') or ''
                if not url:
                    continue
                if url in ts_by_url:
                    prev = ts_by_url[url]
                    ts_by_url[url] = min(prev or ts, ts or prev)
                else:
                    ts_by_url[url] = ts
            CSVService.save_download_history(set(ts_by_url.keys()))
        except Exception as e:
            logger.warning(f"Failed to normalize/deduplicate history: {e}")

    @staticmethod
    def clear_download_history() -> Dict[str, str]:
        """
        Clear download history.
        
        Returns:
            Result dictionary with status and message
        """
        try:
            CSVService.initialize()
            download_history_repository.delete_all()
            global _LAST_KNOWN_HISTORY_SET
            _LAST_KNOWN_HISTORY_SET = set()
            return {
                "status": "success",
                "message": "Download history cleared"
            }
        except Exception as e:
            logger.error(f"Error clearing download history: {e}")
            return {
                "status": "error",
                "message": f"Failed to clear download history: {str(e)}"
            }
    
    @staticmethod
    def get_statistics() -> Dict[str, Any]:
        """
        Get CSV service statistics.
        
        Returns:
            Statistics dictionary
        """
        history = CSVService.get_download_history()
        downloads_status = CSVService.get_csv_downloads_status()
        monitor_status = CSVService.get_monitor_status()

        return {
            "download_history_count": len(history),
            "active_downloads": downloads_status["total_active"],
            "monitor_status": monitor_status["csv_monitor"]["status"],
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
```

## File: services/filesystem_service.py
```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
FilesystemService

Service layer responsible for safe filesystem-related operations required by the
frontend (searching cache folders and opening them locally on the host).

All business logic is centralized here per project standards.
"""
from __future__ import annotations

import logging
import os
import re
import shlex
import subprocess
from dataclasses import dataclass
from pathlib import Path
from typing import List, Optional, Tuple, Dict
from datetime import datetime, date

from config.settings import config

logger = logging.getLogger(__name__)

CACHE_ROOT = Path(config.CACHE_ROOT_DIR)


@dataclass
class CacheSearchResult:
    """Result for a cache folder search.

    Attributes:
        number: The numeric identifier searched (as string to preserve formatting)
        matches: List of absolute paths to matching folders
        best_match: The best match path if determinable, else None
    """
    number: str
    matches: List[str]
    best_match: Optional[str]


class FilesystemService:
    """Service providing safe filesystem utilities for the app.

    This class intentionally restricts operations to known safe directories and
    validates inputs to prevent path traversal or unintended access.
    """

    @staticmethod
    def _ensure_cache_root() -> None:
        """Ensure the cache root exists, log a warning if not."""
        if not CACHE_ROOT.exists():
            logger.warning("CACHE_ROOT does not exist: %s", CACHE_ROOT)

    @staticmethod
    def find_cache_folder_by_number(number: str) -> CacheSearchResult:
        """Search for folders inside /mnt/cache that start with the given number.

        The expected folder format is for example: "115 Camille". We will match
        case-insensitively any directory whose basename starts with "{number}" and
        is followed by optional separators/spaces and text.

        Args:
            number: Numeric string provided by the user (e.g., "115"). Non-digits are ignored.

        Returns:
            CacheSearchResult with all matches and the best_match if exactly one clear match exists.
        """
        FilesystemService._ensure_cache_root()

        sanitized = re.sub(r"\D+", "", number or "")
        if not sanitized:
            logger.debug("Invalid number provided for cache search: %r", number)
            return CacheSearchResult(number=number or "", matches=[], best_match=None)

        if not CACHE_ROOT.exists() or not CACHE_ROOT.is_dir():
            return CacheSearchResult(number=sanitized, matches=[], best_match=None)

        regex = re.compile(rf"^{re.escape(sanitized)}[\s_-]?.*", re.IGNORECASE)

        matches: List[str] = []
        try:
            for entry in CACHE_ROOT.iterdir():
                try:
                    if entry.is_dir() and regex.match(entry.name):
                        matches.append(str(entry.resolve()))
                except PermissionError:
                    continue
        except Exception as e:
            logger.error("Error while scanning cache root: %s", e)
            return CacheSearchResult(number=sanitized, matches=[], best_match=None)

        best: Optional[str] = None
        if len(matches) == 1:
            best = matches[0]
        else:
            exact_regex = re.compile(rf"^{re.escape(sanitized)}\b.*", re.IGNORECASE)
            exact_matches = [m for m in matches if exact_regex.search(Path(m).name)]
            if len(exact_matches) >= 1:
                exact_matches.sort(key=lambda p: (len(Path(p).name), Path(p).name.lower()))
                best = exact_matches[0]

        return CacheSearchResult(number=sanitized, matches=matches, best_match=best)

    @staticmethod
    def open_path_in_explorer(abs_path: str, select_parent: bool = False) -> Tuple[bool, str]:
        """Open a given absolute path in the system's file explorer (server-side).

        IMPORTANT: This runs on the server where Flask is hosted. It assumes a desktop
        environment is available (e.g., the app is used locally). On headless servers,
        this will likely fail; we handle and return a clear error message.

        Args:
            abs_path: Absolute path to open.
            select_parent: If True, open the parent directory and try to preselect the target
                folder in the file manager when supported (nautilus/dolphin/thunar/nemo). If not
                supported, fall back to opening the parent directory.

        Returns:
            (success, message) tuple.
        """
        if config.DISABLE_EXPLORER_OPEN:
            logger.info("Explorer opening is disabled by configuration")
            return False, "Ouverture explorateur désactivée par configuration."
        if not config.DEBUG and not config.ENABLE_EXPLORER_OPEN:
            logger.info("Explorer opening is disabled in production/headless mode")
            return False, "Ouverture explorateur désactivée en production/headless."

        is_headless = not (os.environ.get("DISPLAY") or os.environ.get("WAYLAND_DISPLAY"))
        if is_headless and not config.ENABLE_EXPLORER_OPEN:
            logger.info("Explorer opening is disabled in headless mode")
            return False, "Ouverture explorateur désactivée en mode headless."
        try:
            path = Path(abs_path).resolve()
        except Exception:
            return False, "Chemin invalide."

        try:
            path.relative_to(CACHE_ROOT)
        except ValueError:
            return False, f"Chemin en dehors de {str(CACHE_ROOT)} non autorisé."

        if not path.exists():
            return False, "Le dossier n'existe pas."
        if not path.is_dir():
            return False, "Le chemin n'est pas un dossier."

        candidates: List[List[str]] = []

        if select_parent:
            parent = path.parent if path.parent.exists() else CACHE_ROOT
            candidates.extend([
                ["nautilus", "--no-desktop", "--browser", "--select", str(path)],
                ["nemo", "--no-desktop", "--browser", "--select", str(path)],
                ["dolphin", "--select", str(path)],
                ["thunar", "--select", str(path)],
                ["xdg-open", str(parent)],
                ["gio", "open", str(parent)],
                ["nautilus", str(parent)],
                ["nemo", str(parent)],
                ["thunar", str(parent)],
                ["dolphin", str(parent)],
            ])
        else:
            candidates.extend([
                ["xdg-open", str(path)],
                ["gio", "open", str(path)],
                ["nautilus", str(path)],
                ["nemo", str(path)],
                ["thunar", str(path)],
                ["dolphin", str(path)],
            ])

        last_error: Optional[str] = None
        for cmd in candidates:
            try:
                subprocess.Popen(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                logger.info(
                    "Opened path in explorer using %s (select_parent=%s): %s",
                    cmd[0], select_parent, str(path)
                )
                return True, "Dossier ouvert dans l'explorateur."
            except FileNotFoundError:
                last_error = f"Commande introuvable: {cmd[0]}"
                continue
            except Exception as e:
                last_error = str(e)
                continue

        return False, last_error or "Impossible d'ouvrir l'explorateur sur ce système."

    @staticmethod
    def list_today_cache_folders() -> List[Dict[str, Optional[str]]]:
        """List folders created/modified today under CACHE_ROOT with extracted numeric prefix.

        We use the folder's modification time (mtime) to approximate creation date across platforms,
        and filter only directories whose mtime date equals today's local date. For each folder, we
        extract a leading numeric sequence if present (e.g., "115 Camille" -> "115").

        Returns:
            A list of dicts with keys: {"path": str, "name": str, "number": Optional[str], "mtime": str}
            where mtime is ISO 8601 string for display/logging purposes.
        """
        FilesystemService._ensure_cache_root()

        results: List[Dict[str, Optional[str]]] = []
        if not CACHE_ROOT.exists() or not CACHE_ROOT.is_dir():
            return results

        today = date.today()
        number_regex = re.compile(r"^(\d+)")

        try:
            for entry in CACHE_ROOT.iterdir():
                try:
                    if not entry.is_dir():
                        continue
                    stat = entry.stat()
                    mtime_dt = datetime.fromtimestamp(stat.st_mtime)
                    if mtime_dt.date() != today:
                        continue
                    name = entry.name
                    m = number_regex.match(name)
                    number = m.group(1) if m else None
                    if number:
                        results.append({
                            "path": str(entry.resolve()),
                            "name": name,
                            "number": number,
                            "mtime": mtime_dt.isoformat()
                        })
                except PermissionError:
                    continue
                except Exception as e:
                    logger.warning("Skipping entry due to error: %s", e)
                    continue
        except Exception as e:
            logger.error("Error while listing today's cache folders: %s", e)
            return []

        results.sort(key=lambda x: x.get("mtime", ""), reverse=True)
        return results

    @staticmethod
    def sanitize_filename(filename_str: Optional[str], max_length: int = 230) -> str:
        """Sanitize filename for safe filesystem use.

        Args:
            filename_str: Original filename string (can be None)
            max_length: Maximum filename length (default: 230)

        Returns:
            Sanitized filename string
        """
        if filename_str is None:
            logger.warning("sanitize_filename: filename_str was None, using default 'fichier_nom_absent'.")
            filename_str = "fichier_nom_absent"
        
        s = str(filename_str).strip().replace(' ', '_')
        s = re.sub(r'(?u)[^-\w.]', '', s)
        
        if not s:
            s = "fichier_sans_nom_valide"
            logger.warning(f"sanitize_filename: Sanitized filename was empty for input '{filename_str}', using '{s}'.")
        
        if len(s) > max_length:
            original_full_name_for_log = s
            name_part, ext_part = os.path.splitext(s)
            max_name_len = max_length - len(ext_part) - (1 if ext_part else 0)
            if max_name_len < 1:
                s = s[:max_length]
            else:
                s = name_part[:max_name_len] + ext_part
            logger.info(f"sanitize_filename: Filename '{original_full_name_for_log}' truncated to '{s}' (max_length: {max_length}).")
        
        return s

    @staticmethod
    def format_bytes_human(n_bytes: int) -> str:
        """Return a human readable size string for bytes.

        Args:
            n_bytes: Number of bytes

        Returns:
            Formatted size string (e.g., '213.0KB', '151.5MB')
        """
        try:
            if n_bytes is None or n_bytes < 0:
                return "0B"
            if n_bytes < 1024:
                return f"{n_bytes}B"
            kb = n_bytes / 1024.0
            if kb < 1024.0:
                return f"{kb:.1f}KB"
            mb = kb / 1024.0
            return f"{mb:.1f}MB"
        except Exception:
            return f"{n_bytes}B"

    @staticmethod
    def find_videos_for_tracking(base_path: Path, keyword: str = None, subdir: str = None) -> List[str]:
        """Find video files that don't have corresponding JSON tracking results.

        Robust scan under 'projets_extraits/' for common video extensions.
        A video is considered already processed ONLY if an exact sibling JSON
        with the same stem exists (e.g., 'video.mp4' -> 'video.json').
        Files like '*_audio.json' are ignored for this check.

        Args:
            base_path: Base path to search under (typically BASE_PATH_SCRIPTS / 'projets_extraits')
            keyword: Optional keyword filter (informational, not enforced)
            subdir: Optional subdirectory filter (informational, not enforced)

        Returns:
            List of absolute paths to video files needing tracking
        """
        search_base = Path(base_path) / "projets_extraits" if "projets_extraits" not in str(base_path) else Path(base_path)
        logger.info(
            f"Searching for videos for tracking in {search_base} (filter info: keyword='{keyword}', subdir='{subdir}')"
        )
        
        videos_to_process = []
        video_extensions = (".mp4", ".avi", ".mov", ".mkv", ".webm")

        found_videos = 0
        already_processed = 0
        
        try:
            for video_file in search_base.rglob("*"):
                if not video_file.is_file():
                    continue
                if video_file.suffix.lower() not in video_extensions:
                    continue
                
                found_videos += 1
                json_file = video_file.with_suffix('.json')
                
                if json_file.exists():
                    already_processed += 1
                    continue
                
                videos_to_process.append(str(video_file.resolve()))
                logger.debug(f"Video to process found: {video_file}")

            logger.info(
                f"Videos detected: {found_videos}, already processed (exact JSON sibling): {already_processed}, to process: {len(videos_to_process)}"
            )
            
            if len(videos_to_process) == 0 and found_videos > 0 and already_processed == 0:
                logger.warning(
                    "No <stem>.json found but videos exist. Check write permissions and Step5 output paths."
                )
        except Exception as e:
            logger.error(f"Error scanning for videos: {e}")
        
        return videos_to_process
```

## File: static/css/components/controls.css
```css
/* ===== UNIFIED CONTROLS SECTION ===== */
.unified-controls-section {
    background: linear-gradient(135deg, var(--bg-card) 0%, rgba(121, 134, 203, 0.02) 100%);
    border: 1px solid var(--border-color);
    border-radius: 16px;
    padding: 30px;
    margin: 30px auto 40px auto;
    max-width: 900px;
    box-shadow: 0 6px 20px rgba(0, 0, 0, 0.08);
    display: flex;
    flex-direction: column;
    gap: 25px;
    align-items: center;
    position: relative;
    overflow: hidden;
}

.unified-controls-section::before {
    content: '';
    position: absolute;
    top: 0;
    left: 0;
    right: 0;
    height: 3px;
    background: linear-gradient(90deg, var(--accent-primary), var(--accent-secondary), var(--orange));
    border-radius: 16px 16px 0 0;
}

.topbar-affix {
    position: fixed;
    top: 0;
    left: 0;
    right: 0;
    z-index: 40;
    display: flex;
    flex-direction: column;
    align-items: center;
}

/* Sticky Topbar variant */
.unified-controls--topbar {
    position: relative;
    z-index: 30; /* above logs panel (20) */
    border-radius: 0 0 16px 16px;
    margin: 0 0 16px 0;
    padding: 12px 16px;
    max-width: unset;
    width: 100%;
    backdrop-filter: saturate(120%) blur(4px);
}

.unified-controls--topbar::before {
    height: 2px;
    border-radius: 0;
}

.unified-controls--topbar .workflow-controls,
.unified-controls--topbar .utility-widgets {
    width: 100%;
    display: flex;
    align-items: center;
    gap: 10px;
    flex-wrap: wrap;
}

.unified-controls--topbar .workflow-controls {
    justify-content: flex-start;
}

.unified-controls--topbar .utility-widgets {
    justify-content: flex-end;
}

/* Sequence controls group */
.sequence-controls { display: inline-flex; align-items: center; gap: 10px; flex-wrap: wrap; }

.control-group {
    display: inline-flex;
    flex-wrap: wrap;
    gap: 8px;
    align-items: center;
}

.control-group--primary {
    padding: 4px 12px;
    background: var(--bg-tertiary);
    border-radius: 10px;
    box-shadow: 0 6px 20px rgba(0, 0, 0, 0.18);
}

.control-group--secondary {
    gap: 6px;
    padding: 4px 10px;
    background: color-mix(in oklab, var(--bg-card) 80%, rgba(0, 0, 0, 0.6) 20%);
    border-radius: 10px;
    border: 1px solid var(--border-color);
}

#run-all-steps-button {
    background: linear-gradient(135deg, var(--accent-primary), var(--accent-secondary));
    font-size: 1.05em;
    padding: 14px 24px;
    min-height: var(--touch-target-min);
    border-radius: 999px;
    color: var(--bg-dark);
    font-weight: 600;
    box-shadow: 0 4px 12px rgba(94, 114, 228, 0.4);
    border: none;
}

#run-all-steps-button:hover {
    box-shadow: 0 6px 14px rgba(94, 114, 228, 0.55);
    transform: translateY(-1px);
}

.control-group--secondary button {
    background: transparent;
    border: 1px solid var(--border-bright);
    color: var(--text-primary);
    min-height: var(--touch-target-min);
    border-radius: 999px;
    padding: 10px 18px;
    font-weight: 500;
}

.control-group--secondary button:hover {
    border-color: var(--accent-primary);
    color: var(--accent-primary);
}

/* Settings toggle button */
.settings-toggle {
  background: var(--bg-secondary);
  border: 1px solid var(--border-color);
  color: var(--text-primary);
  padding: 8px 12px;
  border-radius: 8px;
  cursor: pointer;
  font-size: 14px;
  margin-left: auto; /* push to the right */
}

/* Collapsible settings panel */
.settings-panel {
  display: flex;
  flex-direction: column;
  gap: 16px;
  overflow: hidden;
  max-height: 0;
  opacity: 0;
  transition: max-height 0.25s ease, opacity 0.25s ease, padding-top 0.25s ease;
  padding-top: 0;
}

.settings-panel.open {
  max-height: 500px;
  opacity: 1;
  padding-top: 8px;
}

/* --- Harmonized styles for settings actions --- */
.settings-panel button,
.settings-panel .compact-toggle label,
.settings-panel .btn-like-switch {
  background: var(--bg-secondary);
  border: 1px solid var(--border-color);
  color: var(--text-primary);
  padding: 8px 12px;
  border-radius: 8px;
  cursor: pointer;
  font-size: 14px;
  display: inline-flex;
  align-items: center;
  gap: 8px;
  transition: background-color 0.2s ease, border-color 0.2s ease, color 0.2s ease, box-shadow 0.2s ease, transform 0.15s ease;
  position: relative; /* for ripple */
  overflow: hidden;   /* clip ripple */
}

.settings-panel button:disabled {
  opacity: 0.6;
  cursor: not-allowed;
}

.settings-panel .compact-toggle input[type="checkbox"],
.settings-panel .btn-like-switch input[type="checkbox"] {
  margin-left: 6px;
}

/* Ensure upload button matches harmonized style even if other rules apply */
.settings-panel #upload-button {
  background: var(--bg-secondary);
  border: 1px solid var(--border-color);
  color: var(--text-primary);
  padding: 8px 12px;
  border-radius: 8px;
  font-size: 14px;
}

/* Hover and focus styles (align with Stats/Theme/Cinematic interactions) */
.settings-panel button:hover,
.settings-panel .compact-toggle label:hover,
.settings-panel .btn-like-switch:hover,
.settings-panel .settings-action:hover,
.settings-panel a.settings-action:hover {
  background: var(--bg-card);
  border-color: var(--accent-primary);
  color: var(--accent-primary);
  box-shadow: 0 2px 8px rgba(0,0,0,0.12);
  transform: translateY(-1px);
}

.settings-row {
  display: flex;
  flex-wrap: wrap;
  gap: 12px;
  align-items: center;
}

.settings-row--stacked {
  gap: 18px;
  align-items: stretch;
}

.settings-block {
  background: color-mix(in oklab, var(--bg-card) 90%, transparent);
  border: 1px solid var(--border-color);
  border-radius: 10px;
  padding: 12px 14px;
  flex: 1 1 220px;
  min-width: 200px;
  transition: border-color 0.2s ease, box-shadow 0.2s ease;
}

.settings-block:focus-within,
.settings-block:hover {
  border-color: var(--accent-primary);
  box-shadow: 0 6px 18px rgba(0,0,0,0.12);
}

.settings-section {
  width: 100%;
  border-bottom: 1px solid color-mix(in oklab, var(--border-color) 80%, transparent);
  padding-bottom: 12px;
}

.settings-section:last-child {
  border-bottom: none;
  padding-bottom: 4px;
}

.settings-title {
  font-size: 0.85em;
  font-weight: 600;
  letter-spacing: 0.05em;
  text-transform: uppercase;
  color: var(--text-secondary);
  margin: 0 0 10px 0;
}

.settings-panel button:focus-visible,
.settings-panel .compact-toggle label:focus-visible,
.settings-panel .btn-like-switch:focus-visible,
.settings-panel .settings-action:focus-visible,
.settings-panel a.settings-action:focus-visible {
  outline: none;
  box-shadow: 0 0 0 3px color-mix(in oklab, var(--accent-primary) 35%, transparent);
  border-color: var(--accent-primary);
}

/* Ripple feedback on click (settings panel scope) */
.settings-panel button::after,
.settings-panel .btn-like-switch::after,
.settings-panel .compact-toggle label::after {
  content: '';
  position: absolute;
  top: 50%;
  left: 50%;
  width: 0;
  height: 0;
  border-radius: 50%;
  background: rgba(255,255,255,0.25);
  transform: translate(-50%, -50%);
  opacity: 0;
  transition: width 0.6s, height 0.6s, opacity 0.6s;
}

.settings-panel button:active::after,
.settings-panel .btn-like-switch:active::after,
.settings-panel .compact-toggle label:active::after {
  width: 220px;
  height: 220px;
  opacity: 1;
  transition: 0s;
}

/* Utility: settings-action base style (for future controls) */
.settings-panel .settings-action,
.settings-panel a.settings-action {
  background: var(--bg-secondary);
  border: 1px solid var(--border-color);
  color: var(--text-primary);
  padding: 8px 12px;
  border-radius: 8px;
  cursor: pointer;
  font-size: 14px;
  display: inline-flex;
  align-items: center;
  gap: 8px;
  text-decoration: none;
  transition: background-color 0.2s ease, border-color 0.2s ease, color 0.2s ease, box-shadow 0.2s ease, transform 0.15s ease;
}

/* Compact pill styles for widgets inside topbar */
.utility-widgets .auto-scroll-widget,
.utility-widgets .sound-control-widget,
.utility-widgets #upload-widget {
    display: inline-flex;
    align-items: center;
    gap: 8px;
    padding: 8px 10px;
    border: 1px solid var(--border-color);
    border-radius: 10px;
    background: var(--bg-elevated, var(--bg-card));
}

.auto-scroll-header, .sound-control-header { display: inline-flex; align-items: center; gap: 6px; }
.auto-scroll-title, .sound-control-title { font-size: 12px; color: var(--text-secondary); }
.auto-scroll-status, .sound-control-status { font-size: 12px; color: var(--text-secondary); }

.auto-scroll-switch, .sound-control-switch { position: relative; display: inline-flex; align-items: center; }
.auto-scroll-switch input, .sound-control-switch input { cursor: pointer; }

.upload-button { 
    background: var(--bg-secondary);
    border: 1px solid var(--border-color);
    color: var(--text-primary);
    padding: 6px 12px;
    border-radius: 8px;
    cursor: pointer;
    font-size: 13px;
}

/* Inline button-like switches used in topbar (for auto-scroll and sound) */
.btn-like-switch {
  display: inline-flex;
  align-items: center;
  gap: 8px;
  background: var(--bg-secondary);
  border: 1px solid var(--border-color);
  color: var(--text-primary);
  padding: 8px 12px;
  border-radius: 8px;
  cursor: pointer;
  font-size: 14px;
  user-select: none;
}

.btn-like-switch input[type="checkbox"] {
  position: static;
  width: auto;
  height: auto;
  opacity: 1;
  margin-left: 6px;
}

/* Align upload widget container with button baseline */
.upload-widget-inline { display: inline-flex; align-items: center; }

/* Responsive tweaks */
@media (max-width: 900px) {
  .unified-controls--topbar .workflow-controls { order: 1; }
  .unified-controls--topbar .utility-widgets { order: 2; justify-content: flex-start; }
}

@media (max-width: 720px) {
  .settings-row {
    flex-direction: column;
    align-items: stretch;
  }
  .settings-block {
    width: 100%;
  }
  .advanced-controls .advanced-row {
    flex-direction: column;
    align-items: stretch;
  }
}
```

## File: static/css/components/logs.css
```css
.logs-column {
    position: relative;
    background: color-mix(in oklab, var(--bg-card) 92%, transparent);
    border: 1px solid color-mix(in oklab, var(--border-color) 75%, transparent);
    border-radius: var(--pipeline-card-radius);
    box-shadow:
        0 10px 26px rgba(0,0,0,0.22),
        0 0 0 1px rgb(var(--accent-primary-rgb) / 0.12);
}

.logs-column > * {
    position: relative;
    z-index: 1;
}

.logs-column::before {
    content: '';
    position: absolute;
    left: 12px;
    top: 0;
    bottom: 0;
    width: var(--pipeline-axis-width);
    background: color-mix(in oklab, var(--accent-primary) 45%, transparent);
    border-radius: 999px;
    opacity: 0.9;
    pointer-events: none;
}

.logs-column::after {
    content: '';
    position: absolute;
    left: 0;
    top: 0;
    bottom: 0;
    width: 36px;
    background: radial-gradient(
        circle at 12px 30%,
        rgb(var(--accent-primary-rgb) / 0.22) 0%,
        rgb(var(--accent-primary-rgb) / 0.10) 40%,
        rgb(var(--accent-primary-rgb) / 0.00) 70%
    );
    pointer-events: none;
}

.log-panel-header {
    display: flex;
    flex-direction: column;
    align-items: stretch;
    gap: 10px;
    padding-bottom: 10px;
    margin-bottom: 15px;
    border-bottom: 1px solid color-mix(in oklab, var(--border-color) 80%, transparent);
    color: var(--accent-primary);
    font-size: 1.3em;
    font-weight: 500;
    flex-shrink: 0;
    transition: color var(--motion-duration-medium) var(--motion-ease-standard);
}

.log-panel-header-main {
    display: flex;
    justify-content: space-between;
    align-items: center;
    gap: 12px;
}

.log-panel-subheader {
    display: flex;
    flex-wrap: wrap;
    align-items: baseline;
    gap: 10px;
    font-size: 0.85em;
    color: var(--text-secondary);
}

#log-panel-context-step {
    color: var(--text-primary);
    font-weight: 500;
}

.log-panel-specific-buttons {
    display: flex;
    flex-wrap: wrap;
    gap: 10px;
}

.log-panel-specific-buttons .specific-log-button {
    display: inline-flex;
    align-items: center;
    justify-content: center;
    gap: 8px;
    border-radius: 999px;
    padding: 10px 18px;
    background: color-mix(in oklab, var(--bg-card) 80%, transparent);
    border: 1px solid color-mix(in oklab, var(--border-color) 80%, transparent);
    color: var(--text-secondary);
    cursor: pointer;
    transition:
        color var(--motion-duration-fast) var(--motion-ease-standard),
        background var(--motion-duration-fast) var(--motion-ease-standard),
        border-color var(--motion-duration-fast) var(--motion-ease-standard),
        transform var(--motion-duration-fast) var(--motion-ease-standard);
}

.log-panel-specific-buttons .specific-log-button:hover {
    color: var(--text-primary);
    background: color-mix(in oklab, var(--bg-card) 70%, var(--accent-primary) 10%);
    border-color: rgb(var(--accent-primary-rgb) / 0.35);
}

.log-panel-specific-buttons .specific-log-button:active {
    transform: scale(0.98);
}

#close-log-panel {
    display: inline-flex;
    align-items: center;
    justify-content: center;
    width: 44px;
    height: 44px;
    background: color-mix(in oklab, var(--bg-card) 80%, transparent);
    border: 1px solid color-mix(in oklab, var(--border-color) 80%, transparent);
    border-radius: 999px;
    color: var(--text-secondary);
    font-size: 1.45em;
    cursor: pointer;
    transition:
        color var(--motion-duration-fast) var(--motion-ease-standard),
        background var(--motion-duration-fast) var(--motion-ease-standard),
        border-color var(--motion-duration-fast) var(--motion-ease-standard),
        transform var(--motion-duration-fast) var(--motion-ease-standard);
}

#close-log-panel:hover {
    color: var(--text-primary);
    background: color-mix(in oklab, var(--bg-card) 70%, var(--accent-primary) 10%);
    border-color: rgb(var(--accent-primary-rgb) / 0.35);
}

#close-log-panel:active {
    transform: scale(0.96);
}

.log-container { border: 1px solid color-mix(in oklab, var(--border-color) 80%, transparent); border-radius: calc(var(--pipeline-card-radius) - 8px); margin-bottom: 20px; flex-shrink: 0; display: flex; flex-direction: column; background: color-mix(in oklab, var(--bg-card) 94%, transparent);}
.log-header { background-color: color-mix(in oklab, var(--bg-card) 78%, black 10%); padding: 10px 15px; font-weight: 600; border-bottom: 1px solid color-mix(in oklab, var(--border-color) 80%, transparent); border-radius: calc(var(--pipeline-card-radius) - 8px) calc(var(--pipeline-card-radius) - 8px) 0 0; color: var(--text-secondary); }

.log-output,
.specific-log-output {
    background-color: var(--log-bg);
    color: var(--text-bright);
    padding: 15px;
    font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace;
    font-size: 1em;
    line-height: 1.45;
    max-height: 300px;
    overflow-y: auto;
    white-space: pre-wrap;
    word-break: break-all;
    border-radius: 0 0 8px 8px;
    flex-grow: 1;
}

/* Enhanced Log Styling - Different log types */
.log-line {
    display: block;
    margin: 2px 0;
    padding: 2px 6px;
    border-radius: 3px;
    position: relative;
}

.log-line.log-success {
    background-color: rgb(var(--status-success-rgb) / 0.12);
    border-left: 3px solid var(--status-success);
    color: color-mix(in oklab, var(--status-success) 55%, var(--text-primary));
}

.log-line.log-warning {
    background-color: rgb(var(--status-warning-rgb) / 0.12);
    border-left: 3px solid var(--status-warning);
    color: color-mix(in oklab, var(--status-warning) 55%, var(--text-primary));
}

.log-line.log-error {
    background-color: rgb(var(--status-error-rgb) / 0.12);
    border-left: 3px solid var(--status-error);
    color: color-mix(in oklab, var(--status-error) 55%, var(--text-primary));
    font-weight: 500;
}

.log-line.log-info {
    background-color: rgb(var(--status-running-rgb) / 0.12);
    border-left: 3px solid var(--status-running);
    color: color-mix(in oklab, var(--status-running) 55%, var(--text-primary));
}

.log-line.log-debug {
    background-color: rgba(158, 158, 158, 0.1);
    border-left: 3px solid #9e9e9e;
    color: #616161;
    font-size: 0.85em;
}

.log-line.log-command {
    background-color: rgba(156, 39, 176, 0.1);
    border-left: 3px solid #9c27b0;
    color: #7b1fa2;
    font-weight: 500;
}

.log-line.log-progress {
    background-color: rgb(var(--accent-primary-rgb) / 0.10);
    border-left: 3px solid var(--accent-primary);
    color: color-mix(in oklab, var(--accent-primary) 70%, var(--text-primary));
}

/* Log icons for better visual distinction */
.log-line::before {
    content: '';
    display: inline-block;
    width: 12px;
    height: 12px;
    margin-right: 8px;
    border-radius: 50%;
    vertical-align: middle;
}

.log-line.log-success::before {
    background-color: var(--status-success);
    content: '✓';
    color: white;
    font-size: 8px;
    text-align: center;
    line-height: 12px;
    font-weight: bold;
}

.log-line.log-warning::before {
    background-color: var(--status-warning);
    content: '⚠';
    color: white;
    font-size: 8px;
    text-align: center;
    line-height: 12px;
}

.log-line.log-error::before {
    background-color: var(--status-error);
    content: '✕';
    color: white;
    font-size: 8px;
    text-align: center;
    line-height: 12px;
    font-weight: bold;
}

.log-line.log-info::before {
    background-color: var(--status-running);
    content: 'ℹ';
    color: white;
    font-size: 8px;
    text-align: center;
    line-height: 12px;
}

.log-line.log-debug::before {
    background-color: #9e9e9e;
    content: '•';
    color: white;
    font-size: 10px;
    text-align: center;
    line-height: 12px;
}

.log-line.log-command::before {
    background-color: #9c27b0;
    content: '$';
    color: white;
    font-size: 8px;
    text-align: center;
    line-height: 12px;
    font-weight: bold;
}

.log-line.log-progress::before {
    background-color: var(--accent-primary);
    content: '⟳';
    color: white;
    font-size: 8px;
    text-align: center;
    line-height: 12px;
}

/* Dark mode adjustments for log styling */
@media (prefers-color-scheme: dark) {
    .log-line.log-success {
        background-color: rgb(var(--status-success-rgb) / 0.15);
        color: color-mix(in oklab, var(--status-success) 60%, var(--text-primary));
    }

    .log-line.log-warning {
        background-color: rgb(var(--status-warning-rgb) / 0.15);
        color: color-mix(in oklab, var(--status-warning) 60%, var(--text-primary));
    }

    .log-line.log-error {
        background-color: rgb(var(--status-error-rgb) / 0.15);
        color: color-mix(in oklab, var(--status-error) 60%, var(--text-primary));
    }

    .log-line.log-info {
        background-color: rgb(var(--status-running-rgb) / 0.15);
        color: color-mix(in oklab, var(--status-running) 60%, var(--text-primary));
    }

    .log-line.log-debug {
        background-color: rgba(158, 158, 158, 0.15);
        color: #bdbdbd;
    }

    .log-line.log-command {
        background-color: rgba(156, 39, 176, 0.15);
        color: #ba68c8;
    }

    .log-line.log-progress {
        background-color: rgb(var(--accent-primary-rgb) / 0.15);
        color: color-mix(in oklab, var(--accent-primary) 70%, var(--text-primary));
    }
}

/* Hover effects for better interactivity */
.log-line:hover {
    background-color: color-mix(in oklab, rgb(var(--accent-primary-rgb) / 0.10) 40%, transparent);
    transition: background-color var(--motion-duration-fast) var(--motion-ease-standard);
}

/* Improved spacing and readability */
.log-output .log-line:first-child {
    margin-top: 0;
}

.log-output .log-line:last-child {
    margin-bottom: 0;
}
.log-output:empty:before {
    content: "En attente de logs...";
    color: var(--text-secondary);
    font-style: italic;
}
.specific-log-output:empty:before {
    content: "Aucun log spécifique chargé.";
    color: var(--text-secondary);
    font-style: italic;
}

.specific-log-controls-wrapper { margin-top: 15px; }
.specific-log-controls-wrapper h4 { font-weight: 500; color: var(--text-secondary); margin-bottom: 10px; font-size: 1em;}

.specific-log-path { font-size: 0.8em; color: var(--text-muted); margin-bottom: 8px; word-break: break-all; padding: 5px 15px;}

.log-table { width: 100%; border-collapse: collapse; font-size: 0.9em; margin-top: 5px; }
.log-table th, .log-table td { border: 1px solid var(--border-color); padding: 8px; text-align: left; }
.log-table th { background-color: color-mix(in oklab, var(--bg-card) 88%, black 8%); color: var(--text-primary); }
.log-table tr:nth-child(even) { background-color: color-mix(in oklab, var(--bg-card) 85%, black 10%); }
```

## File: static/css/components/steps.css
```css
.step {
    background-color: var(--bg-card);
    border: 1px solid var(--border-color);
    border-left: 3px solid var(--border-color);
    padding: 25px;
    margin-bottom: 25px;
    border-radius: 12px;
    box-shadow: 0 5px 15px rgba(0,0,0,0.2);
    width: 100%;
    max-width: 700px;
    transition:
        transform 0.4s ease-out,
        opacity 0.4s ease-in-out,
        border-color 0.3s ease,
        border-left-color 0.3s ease,
        margin-bottom 0.4s ease-in-out,
        padding 0.4s ease-in-out,
        box-shadow 0.3s ease;
    scroll-margin-top: 0; /* Désactivé pour permettre un centrage parfait */
}

.step[data-status="running"],
.step[data-status="starting"],
.step[data-status="initiated"] {
    border-left-color: var(--status-running);
}

.step[data-status="completed"],
.step[data-status="success"] {
    border-left-color: var(--status-success);
}

.step[data-status="failed"],
.step[data-status="error"],
.step[data-status="cancelled"] {
    border-left-color: var(--status-error);
}

.step[data-status="warning"],
.step[data-status="paused"] {
    border-left-color: var(--status-warning);
}

/* Compact mode reduces padding/margins and typography to fit viewport */
.workflow-wrapper.compact-mode:not(.logs-active) .step {
    max-width: 100%;
    /* Reduce bottom padding to limit empty space below last controls/log buttons */
    padding: 14px 14px 10px; /* top right bottom left */
    margin-bottom: 0; /* handled by grid gap */
    box-shadow: 0 2px 6px rgba(0,0,0,0.12); /* softer shadow to reduce visual noise */
    align-self: start; /* avoid vertical stretch inside grid on desktop */
}

/* Smooth reappearance after closing logs panel - now only affects opacity/transform */
.workflow-wrapper.logs-leaving .steps-column .step {
    will-change: opacity, transform;
}

/* Utility class applied via JS for deterministic transitions */
.step.steps-hidden {
    opacity: 0 !important;
    transform: translateY(8px) scale(0.97) !important;
    pointer-events: none;
}

/* Staggered delays in compact mode - no longer needed since layout is stable */
.workflow-wrapper.compact-mode.logs-leaving .steps-column .step {
    transition-timing-function: cubic-bezier(0.2, 0.6, 0.2, 1);
    transition-duration: 0.4s;
}
.step:hover:not(.active-for-log-panel) {
     transform: translateY(-4px);
     box-shadow: 0 8px 18px rgba(0,0,0,0.28);
}
 .step.active-for-log-panel {
    border: 2px solid var(--accent-primary);
    box-shadow: 0 8px 25px rgba(121, 134, 203, 0.3), 0 0 0 1px var(--accent-primary);
    transform: translateY(-2px);
}
.step.custom-sequence-selected {
    border: 2px solid var(--orange);
    box-shadow: 0 0 15px var(--orange);
}

.step-header-content {
    display: flex;
    justify-content: space-between;
    align-items: center;
    border-bottom: 1px solid var(--border-color);
    padding-bottom: 10px;
    margin-bottom: 20px;
}

.step-title-group {
    display: flex;
    align-items: center;
    gap: 12px;
    flex-wrap: wrap;
}

.step h2 {
    margin-top: 0;
    font-size: 1.5em;
    color: var(--accent-primary);
    display: flex;
    align-items: center;
    font-weight: 500;
    border-bottom: none;
    padding-bottom: 0;
    margin-bottom: 0;
}
.step h2 .step-icon { margin-right: 12px; font-size: 1.6em; }

/* Icon accent on hover for better affordance */
.step:hover h2 .step-icon {
    color: var(--accent-primary);
    transform: translateY(-1px);
    transition: color 0.2s ease, transform 0.2s ease;
}

.workflow-wrapper.compact-mode:not(.logs-active) .step h2 {
    font-size: 1.05em; /* slightly smaller titles in compact */
}
.workflow-wrapper.compact-mode:not(.logs-active) .step h2 .step-icon {
    font-size: 1.1em;
    margin-right: 6px;
}

.step-state-chip {
    display: inline-flex;
    align-items: center;
    gap: 6px;
    padding: 4px 12px;
    border-radius: 999px;
    font-size: 0.78em;
    font-weight: 600;
    text-transform: capitalize;
    border: 1px solid transparent;
    transition: background-color 0.2s ease, color 0.2s ease, border-color 0.2s ease, box-shadow 0.2s ease;
    letter-spacing: 0.02em;
}

.step-state-chip span {
    font-size: 0.9em;
}

.state-running {
    background: color-mix(in oklab, var(--status-running) 18%, transparent);
    color: var(--status-running);
    border-color: color-mix(in oklab, var(--status-running) 35%, transparent);
    box-shadow: 0 0 12px color-mix(in oklab, var(--status-running) 16%, transparent);
}

.state-success {
    background: color-mix(in oklab, var(--status-success) 18%, transparent);
    color: var(--status-success);
    border-color: color-mix(in oklab, var(--status-success) 35%, transparent);
}

.state-error {
    background: color-mix(in oklab, var(--status-error) 18%, transparent);
    color: var(--status-error);
    border-color: color-mix(in oklab, var(--status-error) 35%, transparent);
}

.state-warning {
    background: color-mix(in oklab, var(--status-warning) 18%, transparent);
    color: var(--status-warning);
    border-color: color-mix(in oklab, var(--status-warning) 35%, transparent);
}

.state-idle {
    background: color-mix(in oklab, var(--status-idle) 18%, transparent);
    color: var(--status-idle);
    border-color: color-mix(in oklab, var(--status-idle) 35%, transparent);
}

/* Hover tuning for processing/active steps: keep subtle, avoid large motion */
.steps-column .step[data-status="running"]:hover,
.steps-column .step[data-status="starting"]:hover,
.steps-column .step[data-status="initiated"]:hover,
.steps-column .step.active-for-log-panel:hover {
    transform: translateY(-2px) scale(1.005);
    box-shadow: 0 10px 24px rgba(121, 134, 203, 0.22), 0 2px 8px rgba(0,0,0,0.2);
}

/* Respect reduced motion for hover effects */
@media (prefers-reduced-motion: reduce) {
    .step:hover:not(.active-for-log-panel),
    .steps-column .step[data-status="running"]:hover,
    .steps-column .step[data-status="starting"]:hover,
    .steps-column .step[data-status="initiated"]:hover,
    .steps-column .step.active-for-log-panel:hover {
        transform: none !important;
        box-shadow: 0 6px 16px rgba(0,0,0,0.18);
    }
}

.step-selection-control {
    display: flex;
    align-items: center;
}
.step-selection-control input[type="checkbox"] {
    width: 20px;
    height: 20px;
    margin-right: 8px;
    cursor: pointer;
}
.step-selection-order-number {
    font-size: 1em;
    font-weight: bold;
    color: var(--orange);
    min-width: 20px;
    text-align: center;
}

.step-controls button,
.specific-log-button {
    padding: 12px 22px;
    margin-right: 12px;
    margin-bottom: 12px;
    font-size: 0.95em;
    cursor: pointer;
    border: none;
    border-radius: 25px;
    color: var(--bg-dark);
    transition: background-color 0.2s ease, transform 0.1s ease, box-shadow 0.2s ease;
    font-weight: 600;
    box-shadow: 0 2px 5px rgba(0,0,0,0.15);
    min-height: var(--touch-target-min);
}
.workflow-wrapper.compact-mode:not(.logs-active) .step-controls button,
.workflow-wrapper.compact-mode:not(.logs-active) .specific-log-button {
    padding: var(--button-compact-padding);
    font-size: 0.86em;
    margin-right: 6px;
    /* Reduce vertical gap below the last row of buttons */
    margin-bottom: 3px;
    border-radius: 16px;
}

/* Remove extra bottom gap under the final log button in compact mode */
.workflow-wrapper.compact-mode:not(.logs-active) .specific-log-button:last-child {
    margin-bottom: 0;
}
.step-controls button:hover, .specific-log-button:hover {
     box-shadow: 0 4px 8px rgba(0,0,0,0.2);
}
.step-controls button:active, .specific-log-button:active { transform: scale(0.97); }

.run-button { background-color: var(--green); color: white; }
.run-button:hover { background-color: #5cb85c; }
.run-button:disabled,
.cancel-button:disabled,
.specific-log-button:disabled {
    background: color-mix(in oklab, var(--bg-tertiary) 80%, black 8%);
    color: var(--text-muted);
    border-color: var(--border-color);
}

.cancel-button { background-color: var(--red); color: white;}
.cancel-button:hover { background-color: #d9534f; }
/* Universal focus-visible for action buttons (keyboard accessibility) */
.run-button:focus-visible,
.cancel-button:focus-visible,
.specific-log-button:focus-visible {
    outline: none;
    box-shadow: 0 0 0 3px color-mix(in oklab, var(--accent-primary) 35%, transparent);
    border-color: var(--accent-primary);
}

.specific-log-button { background-color: var(--yellow); color: #333; }
.specific-log-button:hover { background-color: #ffeb3b; }

.status-line { margin-top: 15px; margin-bottom:10px; font-weight: bold; font-size: 1.05em; }
.workflow-wrapper.compact-mode:not(.logs-active) .status-line {
    margin-top: 6px;
    margin-bottom: 4px;
    font-size: 0.9em;
}
.status-line {
    display: flex;
    align-items: center;
    gap: 10px;
    flex-wrap: wrap;
}
.status-badge {
    display: inline-flex;
    align-items: center;
    padding: 4px 10px;
    border-radius: 999px;
    font-weight: 600;
    font-size: 0.85em;
    letter-spacing: 0.01em;
    text-transform: capitalize;
    background: color-mix(in oklab, var(--status-idle) 18%, transparent);
    color: var(--status-idle);
    border: 1px solid color-mix(in oklab, var(--status-idle) 35%, transparent);
}
.status-running,
.status-starting,
.status-initiated {
    background: color-mix(in oklab, var(--status-running) 18%, transparent);
    color: var(--status-running);
    border-color: color-mix(in oklab, var(--status-running) 35%, transparent);
    animation: pulseStatus 1.8s infinite ease-in-out;
}
.status-success,
.status-completed {
    background: color-mix(in oklab, var(--status-success) 18%, transparent);
    color: var(--status-success);
    border-color: color-mix(in oklab, var(--status-success) 35%, transparent);
}
.status-error,
.status-failed,
.status-cancelled {
    background: color-mix(in oklab, var(--status-error) 18%, transparent);
    color: var(--status-error);
    border-color: color-mix(in oklab, var(--status-error) 35%, transparent);
}
.status-warning,
.status-paused {
    background: color-mix(in oklab, var(--status-warning) 18%, transparent);
    color: var(--status-warning);
    border-color: color-mix(in oklab, var(--status-warning) 35%, transparent);
}
.status-idle {
    background: color-mix(in oklab, var(--status-idle) 18%, transparent);
    color: var(--status-idle);
}
.status-line span.timer {
    font-weight: normal;
    font-size: 0.9em;
    color: var(--text-secondary);
    margin-left: 10px;
}

.step-progress-container {
    width: 100%;
    margin-top:10px;
}
.workflow-wrapper.compact-mode:not(.logs-active) .step-progress-container { margin-top: 6px; }
.progress-bar-wrapper {
    background-color: var(--border-color);
    border-radius: 8px;
    padding: 3px;
}
.progress-bar-step {
    width: 0%;
    height: 18px;
    background-color: var(--blue);
    border-radius: 5px;
    text-align: center;
    line-height: 18px;
    color: white;
    font-size: 0.85em;
    font-weight: bold;
    /* Smoother width transition */
    transition: width 0.4s cubic-bezier(0.4, 0, 0.2, 1), background-color 0.3s ease;
    position: relative;
    overflow: hidden; /* for shine effect */
}
.workflow-wrapper.compact-mode:not(.logs-active) .progress-bar-step {
    height: 12px;
    line-height: 12px;
    font-size: 0.76em;
}
.progress-bar-step[data-active="true"]::after {
    content: '';
    position: absolute;
    top: 0;
    left: -100%;
    width: 100%;
    height: 100%;
    background: linear-gradient(90deg, transparent, rgba(255,255,255,0.22), transparent);
    animation: progressShine 2.0s infinite;
}
.progress-text-step {
    text-align: center;
    margin-top: 4px;
    font-size: 0.85em;
    color: var(--text-secondary);
    white-space: pre-line;
    overflow: hidden;
    text-overflow: ellipsis;
    max-height: 3.6em;
    line-height: 1.2em;
}
.workflow-wrapper.compact-mode:not(.logs-active) .progress-text-step {
    font-size: 0.76em;
}

/* Subtle pulse for current filename while processing */
.progress-text-step[data-processing="true"] {
    animation: textPulse 1.5s ease-in-out infinite;
}

/* Reduce extra spacing around specific log controls to avoid empty space under buttons */
.workflow-wrapper.compact-mode:not(.logs-active) .step .specific-log-controls-wrapper {
    margin-top: 8px; /* was 15px in logs.css; tighter in compact mode */
    margin-bottom: 0;
}
.workflow-wrapper.compact-mode:not(.logs-active) .step .specific-log-controls-wrapper h4 {
    margin-bottom: 6px; /* override logs.css (10px) to reduce vertical gap */
}
.workflow-wrapper.compact-mode:not(.logs-active) .step .specific-log-controls-wrapper > div {
    /* the immediate container of buttons; ensure no unintended bottom margin */
    margin-bottom: 0;
}

/* ========== Micro-interactions for steps ========== */
/* Non-active steps get a very subtle halo when any step is running */
.workflow-wrapper.any-step-running .steps-column .step:not(.active-for-log-panel)[data-status="idle"],
.workflow-wrapper.any-step-running .steps-column .step:not(.active-for-log-panel)[data-status="completed"],
.workflow-wrapper.any-step-running .steps-column .step:not(.active-for-log-panel)[data-status="failed"] {
    position: relative;
}

.workflow-wrapper.any-step-running .steps-column .step:not(.active-for-log-panel)[data-status="idle"]::after,
.workflow-wrapper.any-step-running .steps-column .step:not(.active-for-log-panel)[data-status="completed"]::after,
.workflow-wrapper.any-step-running .steps-column .step:not(.active-for-log-panel)[data-status="failed"]::after {
    content: '';
    position: absolute;
    inset: 0;
    border-radius: 12px;
    pointer-events: none;
    box-shadow: 0 0 0 2px rgba(121, 134, 203, 0.35);
    opacity: 0.22;
    animation: subtleOpacityPulse 2.8s ease-in-out infinite;
}

/* Active/processing step: breathing effect to indicate ongoing work */
.steps-column .step.active-for-log-panel,
.steps-column .step[data-status="running"],
.steps-column .step[data-status="starting"],
.steps-column .step[data-status="initiated"] {
    will-change: transform;
    animation: cardBreath 3.2s ease-in-out infinite;
}

/* Respect reduced motion preferences */
@media (prefers-reduced-motion: reduce) {
    .workflow-wrapper.any-step-running .steps-column .step:not(.active-for-log-panel)[data-status]::after,
    .steps-column .step.active-for-log-panel,
    .steps-column .step[data-status="running"],
    .steps-column .step[data-status="starting"],
    .steps-column .step[data-status="initiated"] {
        animation: none !important;
    }
}

/* Override transform reset from layout.css to allow hover lift on idle steps */
.workflow-wrapper:not(.logs-active) .steps-column .step:hover:not(.active-for-log-panel) {
    transform: translateY(-4px);
    box-shadow: 0 8px 18px rgba(0,0,0,0.28);
}

.workflow-wrapper.compact-mode .steps-column {
    display: flex;
    flex-direction: column;
    align-items: center;
    gap: var(--pipeline-gap);
}

.workflow-pipeline {
    width: 100%;
}

.pipeline-timeline {
    position: relative;
    width: 100%;
    max-width: var(--pipeline-card-max-width);
    margin: 0 auto;
    display: flex;
    flex-direction: column;
    gap: var(--pipeline-gap);
}

.timeline-scroll-spacer {
    width: 100%;
    height: calc(100vh - var(--topbar-height));
    min-height: 520px;
}

.timeline-row {
    display: grid;
    grid-template-columns: var(--pipeline-rail-column-width) minmax(0, 1fr);
    column-gap: var(--pipeline-rail-gap);
    align-items: stretch;
}

.timeline-rail-column {
    position: relative;
    display: flex;
    align-items: center;
    justify-content: center;
    padding: var(--pipeline-card-padding) 0;
}

.timeline-cards-column {
    display: flex;
    flex-direction: column;
}

.timeline-axis {
    position: absolute;
    left: calc((var(--pipeline-rail-column-width) / 2) - (var(--pipeline-axis-width) / 2));
    top: 0;
    bottom: 0;
    width: var(--pipeline-axis-width);
    background: color-mix(in oklab, var(--accent-primary) 45%, transparent);
    border-radius: 999px;
    opacity: 0.9;
    pointer-events: none;
    z-index: 0;
}

.timeline-step {
    position: relative;
    z-index: 2;
    width: 100%;
    max-width: var(--pipeline-card-max-width);
    margin: 0;
    border-left: none;
    padding: 0;
    border-radius: var(--pipeline-card-radius);
    background: color-mix(in oklab, var(--bg-card) 92%, transparent);
    border: 1px solid color-mix(in oklab, var(--border-color) 75%, transparent);
    box-shadow: 0 10px 26px rgba(0,0,0,0.22);
    scroll-margin-top: 0;
    transition:
        transform var(--motion-duration-slow) var(--motion-ease-standard),
        box-shadow var(--motion-duration-medium) var(--motion-ease-standard),
        background var(--motion-duration-medium) var(--motion-ease-standard),
        border-color var(--motion-duration-medium) var(--motion-ease-standard);
}

.timeline-step:hover:not(.active-for-log-panel) {
    transform: translateY(-6px) scale(1.008);
    box-shadow:
        0 20px 40px rgba(0,0,0,0.25),
        0 0 0 1px rgba(var(--accent-primary-rgb), 0.2);
}

.timeline-step:focus-within:not(.active-for-log-panel) {
    box-shadow:
        0 16px 34px rgba(0,0,0,0.24),
        0 0 0 2px rgba(var(--accent-primary-rgb), 0.26);
    transform: translateY(-2px);
}

.timeline-step[data-status="running"],
.timeline-step[data-status="starting"],
.timeline-step[data-status="initiated"] {
    background: color-mix(in oklab, var(--status-running) 12%, var(--bg-card));
    border-color: color-mix(in oklab, var(--status-running) 65%, var(--border-color));
    box-shadow: 0 10px 30px rgba(var(--status-running-rgb), 0.25);
}

.timeline-step[data-status="failed"],
.timeline-step[data-status="error"],
.timeline-step[data-status="cancelled"] {
    background: color-mix(in oklab, var(--status-error) 10%, var(--bg-card));
    border-color: color-mix(in oklab, var(--status-error) 55%, var(--border-color));
}

.timeline-step[data-status="completed"],
.timeline-step[data-status="success"] {
    background: color-mix(in oklab, var(--status-success) 10%, var(--bg-card));
    border-color: color-mix(in oklab, var(--status-success) 55%, var(--border-color));
}

.timeline-step[data-status="warning"],
.timeline-step[data-status="paused"],
.timeline-step[data-status="pending"] {
    background: color-mix(in oklab, var(--status-warning) 10%, var(--bg-card));
    border-color: color-mix(in oklab, var(--status-warning) 55%, var(--border-color));
}

.timeline-node {
    position: relative;
    z-index: 2;
    width: var(--pipeline-node-dot-size);
    height: var(--pipeline-node-dot-size);
    border-radius: 50%;
    background: color-mix(in oklab, var(--bg-dark) 72%, transparent);
    border: var(--pipeline-node-border-width) solid var(--pipeline-color-idle);
    box-shadow: 0 0 12px rgba(0,0,0,0.25);
    transition:
        transform var(--motion-duration-fast) var(--motion-ease-standard),
        background var(--motion-duration-medium) var(--motion-ease-standard),
        border-color var(--motion-duration-medium) var(--motion-ease-standard),
        box-shadow var(--motion-duration-medium) var(--motion-ease-standard);
}

.timeline-step:hover:not(.active-for-log-panel) .timeline-node {
    transform: scale(1.08);
}

.timeline-step.is-selected {
    border-color: color-mix(in oklab, var(--accent-primary) 65%, var(--border-color));
    box-shadow:
        0 16px 34px rgba(0,0,0,0.24),
        0 0 0 2px rgba(var(--accent-primary-rgb), 0.26);
}

.timeline-step[data-status="running"] .timeline-node,
.timeline-step[data-status="starting"] .timeline-node,
.timeline-step[data-status="initiated"] .timeline-node {
    border-color: var(--pipeline-color-running);
    box-shadow: 0 0 16px color-mix(in oklab, var(--pipeline-color-running) 45%, transparent);
}

.timeline-step[data-status="completed"] .timeline-node,
.timeline-step[data-status="success"] .timeline-node {
    border-color: var(--pipeline-color-success);
    box-shadow: 0 0 16px color-mix(in oklab, var(--pipeline-color-success) 45%, transparent);
}

.timeline-step[data-status="failed"] .timeline-node,
.timeline-step[data-status="error"] .timeline-node,
.timeline-step[data-status="cancelled"] .timeline-node {
    border-color: var(--pipeline-color-error);
    box-shadow: 0 0 16px color-mix(in oklab, var(--pipeline-color-error) 45%, transparent);
}

.timeline-content {
    display: grid;
    gap: 0.85rem;
    padding: var(--pipeline-card-padding);
}

.timeline-head {
    display: flex;
    justify-content: space-between;
    align-items: center;
    gap: 12px;
    padding-bottom: 10px;
    border-bottom: 1px solid color-mix(in oklab, var(--border-color) 80%, transparent);
}

.node-actions {
    display: flex;
    flex-wrap: wrap;
    align-items: center;
    gap: 10px;
}

.timeline-step .step-controls button,
.timeline-step .specific-log-button {
    border-radius: 999px;
    padding: 10px 18px;
    margin: 0;
}

.timeline-step .specific-log-controls-wrapper {
    margin-top: 4px;
}

.steps-column .step[data-status="initiated"] {
    animation: none !important;
}

@media (prefers-reduced-motion: reduce) {
    .timeline-step {
        transition: none !important;
    }
    .timeline-step:hover:not(.active-for-log-panel) {
        transform: none;
    }
    .timeline-axis {
        transition: none !important;
    }
    .timeline-node {
        transition: none !important;
    }
}

@media (max-width: 860px) {
    .workflow-pipeline {
        --pipeline-node-size: 68px;
        --pipeline-card-padding: 1.25rem;
        --pipeline-gap: 1.6rem;
        --pipeline-rail-column-width: 84px;
    }

    .timeline-step:hover:not(.active-for-log-panel) {
        transform: translateY(-4px) scale(1.006);
    }
}

@media (max-width: 720px) {
    .workflow-pipeline {
        --pipeline-rail-column-width: 72px;
        --pipeline-rail-gap: 0.85rem;
    }
}

@media (max-width: 620px) {
    .timeline-axis {
        display: none;
    }

    .timeline-step {
        grid-template-columns: 1fr;
    }

    .timeline-rail {
        flex-direction: row;
        justify-content: flex-start;
        margin-bottom: 10px;
        gap: 12px;
    }

    .timeline-head {
        flex-direction: column;
        align-items: flex-start;
    }

    .step-selection-control {
        align-self: flex-end;
    }

    .node-actions {
        gap: 8px;
        align-items: stretch;
    }

    .timeline-step .step-controls button,
    .timeline-step .specific-log-button {
        width: 100%;
        justify-content: center;
    }
}

@media (max-width: 520px) {
    .workflow-pipeline {
        --pipeline-node-size: 60px;
        --pipeline-card-padding: 1rem;
        --pipeline-gap: 1.25rem;
    }
}

.step-details-panel {
    background: color-mix(in oklab, var(--bg-card) 92%, transparent);
    border: 1px solid color-mix(in oklab, var(--border-color) 75%, transparent);
    border-radius: var(--pipeline-card-radius);
    box-shadow: -5px 0 18px rgba(0,0,0,0.25);
    overflow: hidden;
}

.step-details-header {
    display: flex;
    align-items: center;
    justify-content: space-between;
    gap: 12px;
    padding: 14px 14px 10px;
    border-bottom: 1px solid color-mix(in oklab, var(--border-color) 80%, transparent);
}

.step-details-title {
    font-weight: 700;
    letter-spacing: 0.01em;
    color: var(--text-primary);
}

.step-details-close {
    width: 38px;
    height: 38px;
    border-radius: 10px;
    border: 1px solid color-mix(in oklab, var(--border-color) 80%, transparent);
    background: color-mix(in oklab, var(--bg-tertiary) 40%, transparent);
    color: var(--text-primary);
    cursor: pointer;
    transition: background var(--motion-duration-medium) var(--motion-ease-standard);
}

.step-details-close:hover {
    background: color-mix(in oklab, var(--accent-primary) 18%, var(--bg-tertiary));
}

.step-details-body {
    padding: 14px;
    display: grid;
    gap: 12px;
}

.step-details-meta {
    display: flex;
    align-items: center;
    flex-wrap: wrap;
    gap: 10px;
}

.step-details-progress {
    background: color-mix(in oklab, var(--bg-dark) 45%, transparent);
    border: 1px solid color-mix(in oklab, var(--border-color) 70%, transparent);
    border-radius: 12px;
    padding: 10px;
}

.step-details-actions {
    display: flex;
    flex-direction: column;
    gap: 10px;
}

.step-details-actions .run-button,
.step-details-actions .cancel-button,
.step-details-actions .step-details-open-logs {
    width: 100%;
    justify-content: center;
    margin: 0;
}

.step-details-open-logs {
    background-color: color-mix(in oklab, var(--accent-primary) 60%, var(--bg-tertiary));
    color: white;
}

.step-details-open-logs:hover {
    background-color: color-mix(in oklab, var(--accent-primary) 72%, var(--bg-tertiary));
}
```

## File: static/css/components/workflow-buttons.css
```css
/* Workflow controls within unified section */
.workflow-controls {
    display: flex;
    justify-content: center;
    gap: 15px;
    flex-wrap: wrap;
    width: 100%;
    padding-top: 10px;
    border-top: 1px solid rgba(121, 134, 203, 0.1);
}

/* ===== WORKFLOW CONTROL BUTTONS STYLING ===== */
#run-all-steps-button, #run-custom-sequence-button, #clear-custom-sequence-button {
    background-color: var(--accent-secondary);
    color: white;
    padding: 15px 30px;
    font-size: 1.1em;
    font-weight: 500;
    border-radius: 30px;
    border: none;
    cursor: pointer;
    transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
    box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
    position: relative;
    overflow: hidden;
    min-width: 200px;
    white-space: nowrap;
}

/* Add subtle shine effect on hover */
#run-all-steps-button::before, #run-custom-sequence-button::before, #clear-custom-sequence-button::before {
    content: '';
    position: absolute;
    top: 0;
    left: -100%;
    width: 100%;
    height: 100%;
    background: linear-gradient(90deg, transparent, rgba(255, 255, 255, 0.2), transparent);
    transition: left 0.5s ease;
}

#run-all-steps-button:hover::before, #run-custom-sequence-button:hover::before, #clear-custom-sequence-button:hover::before {
    left: 100%;
}
#run-all-steps-button:hover, #run-custom-sequence-button:hover, #clear-custom-sequence-button:hover {
    transform: translateY(-3px);
    box-shadow: 0 6px 20px rgba(0, 0, 0, 0.25);
    filter: brightness(1.1);
}

#run-all-steps-button:active, #run-custom-sequence-button:active, #clear-custom-sequence-button:active {
    transform: translateY(-1px);
    box-shadow: 0 2px 8px rgba(0, 0, 0, 0.2);
    transition: all 0.1s ease;
}
/* Individual button color schemes */
#run-all-steps-button {
    background: linear-gradient(135deg, var(--accent-secondary) 0%, rgba(121, 134, 203, 0.9) 100%);
    border: 1px solid rgba(121, 134, 203, 0.3);
}

#run-custom-sequence-button {
    background: linear-gradient(135deg, var(--orange) 0%, rgba(255, 152, 0, 0.9) 100%);
    border: 1px solid rgba(255, 152, 0, 0.3);
}

#clear-custom-sequence-button {
    background: linear-gradient(135deg, var(--text-secondary) 0%, rgba(108, 117, 125, 0.9) 100%);
    border: 1px solid rgba(108, 117, 125, 0.3);
}

#run-all-steps-button:disabled, #run-custom-sequence-button:disabled, #clear-custom-sequence-button:disabled {
    background: linear-gradient(135deg, #555 0%, #444 100%) !important;
    color: #999 !important;
    cursor: not-allowed !important;
    box-shadow: none !important;
    transform: none !important;
    filter: none !important;
    border: 1px solid #666 !important;
    opacity: 0.6;
}

.global-progress-affix {
    width: min(720px, 100%);
    padding: 12px 18px 10px 18px;
    border-radius: 14px;
    backdrop-filter: saturate(120%) blur(6px);
    background: color-mix(in oklab, var(--bg-card) 88%, rgba(255,255,255,0.08));
    box-shadow: 0 12px 30px rgba(0, 0, 0, 0.35);
    border: 1px solid color-mix(in oklab, var(--border-color) 70%, transparent);
    display: none;
    flex-direction: column;
    gap: 6px;
    margin: 12px auto 0 auto;
}

.global-progress-container {
    width: 100%;
    background-color: color-mix(in oklab, var(--border-color) 80%, rgba(0,0,0,0.15));
    border-radius: 999px;
    padding: 4px;
    display: none;
    box-shadow: inset 0 2px 6px rgba(0,0,0,0.25);
}

#global-progress-bar {
    width: 0%;
    height: 18px;
    background: linear-gradient(90deg, var(--accent-primary), var(--accent-secondary));
    border-radius: 999px;
    text-align: center;
    line-height: 18px;
    color: white;
    font-weight: 600;
    font-size: 0.85em;
    transition: width 0.3s var(--motion-ease-standard), background 0.3s ease, color 0.3s ease;
}

#global-progress-text {
    text-align: center;
    font-size: 0.85em;
    color: var(--text-secondary);
    display: none;
}

@media (max-width: 768px) {
    .global-progress-affix {
        width: 100%;
        padding: 12px 14px 10px 14px;
        margin: 10px 0 0 0;
    }
}
```

## File: static/css/base.css
```css
html {
    scroll-behavior: smooth;
}

@media (prefers-reduced-motion: reduce) {
    html {
        scroll-behavior: auto;
    }

    *,
    *::before,
    *::after {
        transition: none !important;
        animation: none !important;
    }
}

body {
    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    margin: 0;
    background-color: var(--bg-dark);
    color: var(--text-primary);
    line-height: 1.6;
    display: flex;
    flex-direction: column;
    align-items: center;
    padding-top: calc(var(--topbar-height) + 20px);
    padding-left: 20px;
    min-height: 100vh;
    overflow-x: hidden;
}

:where(a, button, input, select, textarea, summary):focus-visible {
    outline: 2px solid color-mix(in oklab, var(--accent-primary) 45%, transparent);
    outline-offset: 2px;
}

button:disabled,
input[type="button"]:disabled,
input[type="submit"]:disabled,
input[type="checkbox"]:disabled,
input[type="radio"]:disabled {
    opacity: 0.5;
    cursor: not-allowed;
    box-shadow: none !important;
    filter: saturate(85%);
}
```

## File: static/css/variables.css
```css
:root {
    /* Backgrounds */
    --bg-dark: #1e1e2f;
    --bg-card: #2c2c3e;
    --bg-secondary: #2c2c3e; /* Secondary background for widgets */
    --bg-tertiary: #3a3a4e; /* Tertiary background for disabled elements */
    --bg-hover: rgba(121, 134, 203, 0.1);
    --log-bg: #161625;

    /* Text */
    --text-primary: #e0e0e0;
    --text-secondary: #a0a0b0;
    --text-muted: #707080; /* Muted text for disabled states */
    --text-bright: #f0f0f0;
    --log-text: #c0c0d0;
    --font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;

    /* UI Elements */
    --border-color: #39394d;
    --border-color-translucent: #39394d88;
    --border-bright: #4a4a5e;
    --touch-target-min: 44px;
    --button-compact-padding: 12px;
    --topbar-height: 68px;

    --motion-duration-fast: 120ms;
    --motion-duration-medium: 220ms;
    --motion-duration-slow: 420ms;
    --motion-ease-standard: cubic-bezier(0.4, 0, 0.2, 1);
    --motion-ease-emphasized: cubic-bezier(0.2, 0.8, 0.2, 1);

    /* Standard Colors */
    --accent-primary: #7986cb;
    --accent-secondary: #ff8a65;
    --green: #66bb6a;
    --red: #ef5350;
    --yellow: #ffee58;
    --blue: #42a5f5;
    --orange: #ffb74d;

    /* Status Colors (Hex) */
    --status-running: #4dabf7;
    --status-success: #4caf50;
    --status-error: #e53935;
    --status-warning: #ff9800;
    --status-idle: #9e9e9e;

    /* RGB Values (for rgba usage) */
    --accent-primary-rgb: 121 134 203;
    --accent-secondary-rgb: 255 138 101;
    --status-running-rgb: 77 171 247;
    --status-success-rgb: 76 175 80;
    --status-error-rgb: 229 57 53;
    --status-warning-rgb: 255 152 0;
    --status-idle-rgb: 158 158 158;

    /* Pipeline Visualization Variables */
    --pipeline-node-size: 80px;
    --pipeline-connector-width: 3px;
    --pipeline-gap: 2rem;
    --pipeline-card-radius: 20px;
    --pipeline-card-padding: 1.5rem;
    --pipeline-card-max-width: 980px;
    --pipeline-node-dot-size: 18px;
    --pipeline-node-border-width: 3px;
    --pipeline-rail-column-width: 110px;
    --pipeline-rail-gap: 1.25rem;
    --pipeline-axis-width: 4px;

    /* Pipeline Status Colors mapped to Status Colors */
    --pipeline-color-idle: var(--status-idle);
    --pipeline-color-running: var(--status-running);
    --pipeline-color-success: var(--status-success);
    --pipeline-color-error: var(--status-error);
}
```

## File: static/state/AppState.js
```javascript
class AppState {
    constructor() {
        this.state = {
            pollingIntervals: {},
            
            activeStepKeyForLogsPanel: null,
            isAnySequenceRunning: false,
            focusedElementBeforePopup: null,
            ui: {
                compactMode: false
            },
            
            stepTimers: {},
            selectedStepsOrder: [],
            
            processInfo: {},
            
            performanceMetrics: {
                apiResponseTimes: [],
                errorCounts: {},
                lastUpdate: null
            },
            
            cacheStats: {
                hits: 0,
                misses: 0,
                hitRate: 0
            }
        };
        
        this.listeners = new Set();
        this.isDestroyed = false;
        
        this.stateChangeCount = 0;
        this.lastStateChange = Date.now();
        
        console.debug('[AppState] Initialized with immutable state management');
    }
    
    getState() {
        if (this.isDestroyed) {
            console.warn('[AppState] Attempted to access destroyed state');
            return {};
        }
        return this._deepClone(this.state);
    }
    
    getStateProperty(path) {
        if (this.isDestroyed) return undefined;
        
        return path.split('.').reduce((obj, key) => {
            return obj && obj[key] !== undefined ? obj[key] : undefined;
        }, this.state);
    }
    
    setState(updates, source = 'unknown') {
        if (this.isDestroyed) {
            console.warn('[AppState] Attempted to update destroyed state');
            return;
        }
        
        const oldState = this._deepClone(this.state);
        const newState = this._mergeDeep(this.state, updates);
        
        if (this._stateChanged(oldState, newState)) {
            this.state = newState;
            this.stateChangeCount++;
            this.lastStateChange = Date.now();
            
            console.debug(`[AppState] State updated from ${source}:`, updates);
            
            this._notifyListeners(newState, oldState, source);
        }
    }
    
    subscribe(listener) {
        if (typeof listener !== 'function') {
            throw new Error('[AppState] Listener must be a function');
        }
        
        this.listeners.add(listener);
        
        return () => {
            this.listeners.delete(listener);
        };
    }
    
    /**
     * Subscribe to specific state property changes.
     * @param {string} path - Dot-notation path to property
     * @param {Function} listener - Callback function (newValue, oldValue) => void
     * @returns {Function} Unsubscribe function
     */
    subscribeToProperty(path, listener) {
        const propertyListener = (newState, oldState) => {
            const newValue = this._getPropertyByPath(newState, path);
            const oldValue = this._getPropertyByPath(oldState, path);
            
            if (newValue !== oldValue) {
                listener(newValue, oldValue);
            }
        };
        
        return this.subscribe(propertyListener);
    }
    
    batchUpdate(updateFn, source = 'batch') {
        const originalNotify = this._notifyListeners;
        const updates = [];
        
        this._notifyListeners = (newState, oldState, updateSource) => {
            updates.push({ newState, oldState, source: updateSource });
        };
        
        try {
            updateFn();
        } finally {
            this._notifyListeners = originalNotify;
            
            if (updates.length > 0) {
                const finalUpdate = updates[updates.length - 1];
                this._notifyListeners(finalUpdate.newState, updates[0].oldState, source);
            }
        }
    }
    
    reset() {
        const initialState = {
            pollingIntervals: {},
            activeStepKeyForLogsPanel: null,
            isAnySequenceRunning: false,
            focusedElementBeforePopup: null,
            ui: {
                compactMode: false
            },
            stepTimers: {},
            selectedStepsOrder: [],

            processInfo: {},
            performanceMetrics: {
                apiResponseTimes: [],
                errorCounts: {},
                lastUpdate: null
            },
            cacheStats: {
                hits: 0,
                misses: 0,
                hitRate: 0
            }
        };
        
        this.setState(initialState, 'reset');
        console.info('[AppState] State reset to initial values');
    }
    
    getStats() {
        return {
            listenerCount: this.listeners.size,
            stateChangeCount: this.stateChangeCount,
            lastStateChange: this.lastStateChange,
            isDestroyed: this.isDestroyed,
            stateSize: JSON.stringify(this.state).length
        };
    }
    
    destroy() {
        console.info('[AppState] Destroying state manager');
        
        this.listeners.clear();
        this.state = {};
        this.isDestroyed = true;
    }
    
    _deepClone(obj) {
        if (typeof structuredClone === 'function') {
            try {
                return structuredClone(obj);
            } catch (error) {
                console.warn('[AppState] structuredClone failed, falling back to manual clone:', error);
            }
        }

        if (obj === null || typeof obj !== 'object') {
            return obj;
        }

        if (obj instanceof Date) {
            return new Date(obj.getTime());
        }

        if (Array.isArray(obj)) {
            return obj.map(item => this._deepClone(item));
        }

        const cloned = {};
        for (const key in obj) {
            if (Object.prototype.hasOwnProperty.call(obj, key)) {
                cloned[key] = this._deepClone(obj[key]);
            }
        }
        return cloned;
    }
    
    _mergeDeep(target, source) {
        const result = this._deepClone(target);
        
        for (const key in source) {
            if (source.hasOwnProperty(key)) {
                if (source[key] && typeof source[key] === 'object' && !Array.isArray(source[key])) {
                    result[key] = this._mergeDeep(result[key] || {}, source[key]);
                } else {
                    result[key] = source[key];
                }
            }
        }
        
        return result;
    }
    
    _stateChanged(oldState, newState) {
        return !this._areValuesEqual(oldState, newState);
    }

    _areValuesEqual(a, b, visited = new WeakMap()) {
        if (Object.is(a, b)) {
            return true;
        }

        if (typeof a !== typeof b) {
            return false;
        }

        if (a === null || b === null) {
            return false;
        }

        if (typeof a !== 'object') {
            return false;
        }

        if (visited.has(a) && visited.get(a) === b) {
            return true;
        }
        visited.set(a, b);

        const aKeys = Object.keys(a);
        const bKeys = Object.keys(b);
        if (aKeys.length !== bKeys.length) {
            return false;
        }

        for (const key of aKeys) {
            if (!Object.prototype.hasOwnProperty.call(b, key)) {
                return false;
            }
            if (!this._areValuesEqual(a[key], b[key], visited)) {
                return false;
            }
        }

        return true;
    }
    
    _getPropertyByPath(obj, path) {
        return path.split('.').reduce((current, key) => {
            return current && current[key] !== undefined ? current[key] : undefined;
        }, obj);
    }
    
    _notifyListeners(newState, oldState, source) {
        this.listeners.forEach(listener => {
            try {
                listener(newState, oldState, source);
            } catch (error) {
                console.error('[AppState] Listener error:', error);
            }
        });
    }
}

export const appState = new AppState();

window.addEventListener('beforeunload', () => {
    appState.destroy();
});

if (window.location.hostname === 'localhost' || window.location.hostname === '127.0.0.1') {
    window.appState = appState;
    
    appState.subscribe((newState, oldState, source) => {
        console.debug(`[AppState] Change from ${source}:`, {
            newState: newState,
            oldState: oldState
        });
    });
}

export default appState;
```

## File: static/utils/PollingManager.js
```javascript
/**
 * Centralized polling management utility for workflow_mediapipe frontend.
 * 
 * This module provides safe interval management with automatic cleanup
 * to prevent memory leaks and ensure proper resource management.
 */

class PollingManager {
    /**
     * Initialize the polling manager.
     */
    constructor() {
        this.intervals = new Map();
        this.timeouts = new Map();
        this.isDestroyed = false;
        this.pendingResumes = new Map();
        this.errorCounts = new Map();
        this.maxErrorCount = 5;
        
        this._bindCleanupEvents();
        
        console.debug('PollingManager initialized');
    }

    /**
     * Start polling with a given callback function.
     * 
     * @param {string} name - Unique name for this polling operation
     * @param {Function} callback - Function to call on each interval
     * @param {number} interval - Interval in milliseconds
     * @param {Object} options - Additional options
     * @param {boolean} options.immediate - Whether to call callback immediately
     * @param {number} options.maxErrors - Maximum consecutive errors before stopping
     * @returns {string|null} - Polling ID or null if manager is destroyed
     */
    startPolling(name, callback, interval, options = {}) {
        if (this.isDestroyed) {
            console.warn('PollingManager: Cannot start polling, manager is destroyed');
            return null;
        }

        this.stopPolling(name);

        const {
            immediate = false,
            maxErrors = this.maxErrorCount
        } = options;

        this.errorCounts.set(name, 0);

        const wrappedCallback = async () => {
            if (this.isDestroyed) {
                return;
            }

            try {
                const result = await callback();
                this.errorCounts.set(name, 0);

                if (typeof result === 'number' && result > 0) {
                    const existing = this.intervals.get(name);
                    if (existing) {
                        clearInterval(existing.id);
                        this.intervals.delete(name);
                    }
                    if (this.pendingResumes.has(name)) {
                        clearTimeout(this.pendingResumes.get(name));
                    }
                    const resumeId = setTimeout(() => {
                        if (!this.isDestroyed && !this.intervals.has(name)) {
                            const newIntervalId = setInterval(wrappedCallback, interval);
                            this.intervals.set(name, {
                                id: newIntervalId,
                                callback: wrappedCallback,
                                interval: interval,
                                startTime: Date.now()
                            });
                            this.pendingResumes.delete(name);
                            console.debug(`Resumed polling: ${name} after ${result}ms backoff`);
                        }
                    }, result);
                    this.pendingResumes.set(name, resumeId);
                    return;
                }
            } catch (error) {
                const errorCount = (this.errorCounts.get(name) || 0) + 1;
                this.errorCounts.set(name, errorCount);
                
                console.error(`Polling error in ${name} (attempt ${errorCount}):`, error);
                
                if (errorCount >= maxErrors) {
                    console.error(`Stopping polling ${name} due to ${errorCount} consecutive errors`);
                    this.stopPolling(name);
                    
                    this._dispatchPollingError(name, error, errorCount);
                }
            }
        };

        if (immediate) {
            wrappedCallback();
        }

        const intervalId = setInterval(wrappedCallback, interval);
        this.intervals.set(name, {
            id: intervalId,
            callback: wrappedCallback,
            interval: interval,
            startTime: Date.now()
        });

        console.debug(`Started polling: ${name} (interval: ${interval}ms)`);
        return name;
    }

    /**
     * Stop a specific polling operation.
     * 
     * @param {string} name - Name of the polling operation to stop
     * @returns {boolean} - True if polling was stopped, false if not found
     */
    stopPolling(name) {
        const pollingInfo = this.intervals.get(name);
        if (pollingInfo) {
            clearInterval(pollingInfo.id);
            this.intervals.delete(name);
            this.errorCounts.delete(name);
            if (this.pendingResumes.has(name)) {
                clearTimeout(this.pendingResumes.get(name));
                this.pendingResumes.delete(name);
            }
            
            const duration = Date.now() - pollingInfo.startTime;
            console.debug(`Stopped polling: ${name} (ran for ${duration}ms)`);
            return true;
        }
        return false;
    }

    /**
     * Schedule a one-time delayed execution.
     * 
     * @param {string} name - Unique name for this timeout
     * @param {Function} callback - Function to call after delay
     * @param {number} delay - Delay in milliseconds
     * @returns {string|null} - Timeout ID or null if manager is destroyed
     */
    setTimeout(name, callback, delay) {
        if (this.isDestroyed) {
            console.warn('PollingManager: Cannot set timeout, manager is destroyed');
            return null;
        }

        this.clearTimeout(name);

        const wrappedCallback = () => {
            if (this.isDestroyed) {
                return;
            }

            try {
                callback();
            } catch (error) {
                console.error(`Timeout callback error in ${name}:`, error);
            } finally {
                this.timeouts.delete(name);
            }
        };

        const timeoutId = setTimeout(wrappedCallback, delay);
        this.timeouts.set(name, {
            id: timeoutId,
            callback: wrappedCallback,
            delay: delay,
            startTime: Date.now()
        });

        console.debug(`Set timeout: ${name} (delay: ${delay}ms)`);
        return name;
    }

    /**
     * Clear a specific timeout.
     * 
     * @param {string} name - Name of the timeout to clear
     * @returns {boolean} - True if timeout was cleared, false if not found
     */
    clearTimeout(name) {
        const timeoutInfo = this.timeouts.get(name);
        if (timeoutInfo) {
            clearTimeout(timeoutInfo.id);
            this.timeouts.delete(name);
            
            console.debug(`Cleared timeout: ${name}`);
            return true;
        }
        return false;
    }

    /**
     * Get information about active polling operations.
     * 
     * @returns {Object} - Information about active operations
     */
    getActiveOperations() {
        const now = Date.now();
        
        return {
            intervals: Array.from(this.intervals.entries()).map(([name, info]) => ({
                name,
                interval: info.interval,
                runningTime: now - info.startTime,
                errorCount: this.errorCounts.get(name) || 0
            })),
            timeouts: Array.from(this.timeouts.entries()).map(([name, info]) => ({
                name,
                delay: info.delay,
                timeRemaining: Math.max(0, (info.startTime + info.delay) - now)
            })),
            totalIntervals: this.intervals.size,
            totalTimeouts: this.timeouts.size
        };
    }

    /**
     * Get summarized statistics for monitoring consumers (PerformanceMonitor).
     *
     * @returns {Object}
     */
    getStats() {
        const ops = this.getActiveOperations();
        return {
            totalIntervals: ops.totalIntervals,
            totalTimeouts: ops.totalTimeouts,
            intervals: ops.intervals,
            timeouts: ops.timeouts
        };
    }

    /**
     * Destroy the polling manager and clean up all resources.
     */
    destroy() {
        if (this.isDestroyed) {
            return;
        }

        console.debug('Destroying PollingManager...');

        this.intervals.forEach((info, name) => {
            clearInterval(info.id);
            console.debug(`Cleaned up interval: ${name}`);
        });
        this.intervals.clear();

        this.timeouts.forEach((info, name) => {
            clearTimeout(info.id);
            console.debug(`Cleaned up timeout: ${name}`);
        });
        this.timeouts.clear();

        this.errorCounts.clear();

        this.pendingResumes.forEach((timeoutId, name) => {
            clearTimeout(timeoutId);
            console.debug(`Cleared pending resume: ${name}`);
        });
        this.pendingResumes.clear();

        this.isDestroyed = true;
        console.debug('PollingManager destroyed');
    }

    /**
     * Bind cleanup events to prevent memory leaks.
     * @private
     */
    _bindCleanupEvents() {
        window.addEventListener('beforeunload', () => {
            this.destroy();
        });

        window.addEventListener('pagehide', () => {
            this.destroy();
        });

        document.addEventListener('visibilitychange', () => {
            if (document.hidden) {
                console.debug('Page hidden, polling continues in background');
            }
        });
    }

    /**
     * Dispatch a custom event for polling errors.
     * @private
     */
    _dispatchPollingError(name, error, errorCount) {
        const event = new CustomEvent('pollingError', {
            detail: {
                name,
                error,
                errorCount,
                timestamp: new Date().toISOString()
            }
        });
        
        window.dispatchEvent(event);
    }
}

// Create and export global polling manager instance
const pollingManager = new PollingManager();

// Export for use in other modules
export { PollingManager, pollingManager };

window.pollingManager = pollingManager;
```

## File: static/scrollManager.js
```javascript
/**
 * Scroll Manager Module
 * Handles automatic scrolling to active workflow steps with smooth animations
 * and intelligent viewport positioning.
 */

import * as dom from './domElements.js';


const SCROLL_CONFIG = {
    behavior: 'smooth',
    block: 'center',
    inline: 'nearest',
    topOffset: 100,
    minScrollDistance: 50,
    scrollDelay: 150,
    topbarHeight: 68, // Hauteur de la topbar depuis variables.css
    bottomMargin: 40   // Marge inférieure pour éviter le débordement
};

/**
 * Checks if an element is currently visible in the viewport
 * @param {HTMLElement} element - The element to check
 * @returns {boolean} True if element is fully or partially visible
 */
function isElementInViewport(element) {
    if (!element) return false;
    
    const rect = element.getBoundingClientRect();
    const windowHeight = window.innerHeight || document.documentElement.clientHeight;
    const windowWidth = window.innerWidth || document.documentElement.clientWidth;
    
    return (
        rect.top >= 0 &&
        rect.left >= 0 &&
        rect.bottom <= windowHeight &&
        rect.right <= windowWidth
    );
}

/**
 * Checks if an element is partially visible in the viewport
 * @param {HTMLElement} element - The element to check
 * @returns {boolean} True if element is at least partially visible
 */
function isElementPartiallyVisible(element) {
    if (!element) return false;
    
    const rect = element.getBoundingClientRect();
    const windowHeight = window.innerHeight || document.documentElement.clientHeight;
    const windowWidth = window.innerWidth || document.documentElement.clientWidth;
    
    return (
        rect.bottom > 0 &&
        rect.right > 0 &&
        rect.top < windowHeight &&
        rect.left < windowWidth
    );
}

/**
 * Calculates the optimal scroll position for an element with perfect centering
 * @param {HTMLElement} element - The target element
 * @returns {number} The optimal scroll top position
 */
function calculateOptimalScrollPosition(element) {
    if (!element) return 0;
    
    const rect = element.getBoundingClientRect();
    const currentScrollTop = window.pageYOffset || document.documentElement.scrollTop;
    const windowHeight = window.innerHeight || document.documentElement.clientHeight;
    
    // Zone visible effective : topbar + margin inférieure
    const effectiveViewportHeight = windowHeight - SCROLL_CONFIG.topbarHeight - SCROLL_CONFIG.bottomMargin;
    const elementTop = rect.top + currentScrollTop;
    const elementHeight = rect.height;
    
    // Centrage agressif dans la zone effective (pas de contraintes min/max)
    const viewportCenter = SCROLL_CONFIG.topbarHeight + (effectiveViewportHeight / 2);
    const targetScrollTop = elementTop + (elementHeight / 2) - viewportCenter;
    
    // Contrainte simple : ne pas aller en négatif
    const finalScrollTop = Math.max(0, targetScrollTop);
    
    console.log('[SCROLL] Position calculation:', {
        elementTop,
        elementHeight,
        windowHeight,
        effectiveViewportHeight,
        viewportCenter,
        targetScrollTop,
        finalScrollTop
    });
    
    return finalScrollTop;
}

/**
 * Determines if scrolling is necessary based on current element visibility
 * @param {HTMLElement} element - The target element
 * @returns {boolean} True if scrolling should be performed
 */
function shouldScroll(element) {
    if (!element) return false;
    
    // Pour les séquences, toujours autoriser le scroll pour garantir le repositionnement
    console.log('[SCROLL] shouldScroll: allowing scroll for sequence positioning');
    return true;
}

/**
 * Smoothly scrolls to bring the target element into optimal view
 * @param {HTMLElement} element - The element to scroll to
 * @param {Object} options - Additional scroll options
 */
function scrollToElement(element, options = {}) {
    if (!element) {
        console.warn('[SCROLL] No element provided for scrolling');
        return;
    }
    
    const config = { ...SCROLL_CONFIG, ...options };
    
    if (!shouldScroll(element)) {
        return;
    }
    
    console.log(`[SCROLL] Scrolling to element: ${element.id || element.className}`);

    const behavior = config.behavior === 'smooth' ? 'smooth' : 'auto';
    const targetScrollTop = calculateOptimalScrollPosition(element);
    window.scrollTo({
        top: targetScrollTop,
        behavior
    });
}

/**
 * Scrolls to the active workflow step with a delay to allow UI transitions
 * @param {string} stepKey - The key of the step to scroll to
 * @param {Object} options - Additional options for scrolling
 */
export function scrollToActiveStep(stepKey, options = {}) {
    if (!stepKey) {
        console.warn('[SCROLL] No stepKey provided for scrollToActiveStep');
        return;
    }
    
    const stepElement = document.getElementById(`step-${stepKey}`);
    if (!stepElement) {
        console.warn(`[SCROLL] Step element not found: step-${stepKey}`);
        return;
    }
    
    const config = { ...SCROLL_CONFIG, ...options };
    
    setTimeout(() => {
        scrollToElement(stepElement, config);
    }, config.scrollDelay);
}

/**
 * Scrolls to a step immediately without delay (for manual triggers)
 * @param {string} stepKey - The key of the step to scroll to
 * @param {Object} options - Additional options for scrolling
 */
export function scrollToStepImmediate(stepKey, options = {}) {
    if (!stepKey) return;
    
    const stepElement = document.getElementById(`step-${stepKey}`);
    if (!stepElement) return;
    
    scrollToElement(stepElement, { ...SCROLL_CONFIG, ...options });
}

/**
 * Scrolls to a step with forced repositioning for sequences (ignores current position)
 * @param {string} stepKey - The key of the step to scroll to
 * @param {Object} options - Additional options for scrolling
 */
export function scrollToStepForced(stepKey, options = {}) {
    if (!stepKey) {
        console.warn('[SCROLL] No stepKey provided for scrollToStepForced');
        return;
    }
    
    const stepElement = document.getElementById(`step-${stepKey}`);
    if (!stepElement) {
        console.warn(`[SCROLL] Step element not found: step-${stepKey}`);
        return;
    }
    
    const config = { ...SCROLL_CONFIG, ...options };
    
    // Forcer le scroll avec scrollIntoView direct (plus efficace pour les grids)
    console.log(`[SCROLL] Forced scrolling to step: ${stepKey}`);
    
    try {
        // Utiliser scrollIntoView avec block 'center' pour forcer le positionnement
        stepElement.scrollIntoView({
            behavior: config.behavior,
            block: 'center',
            inline: 'nearest'
        });
        
        console.log(`[SCROLL] Applied scrollIntoView with block: center`);
        
        // Backup : forcer avec window.scrollTo si scrollIntoView ne fonctionne pas
        setTimeout(() => {
            const rect = stepElement.getBoundingClientRect();
            const currentScrollTop = window.pageYOffset || document.documentElement.scrollTop;
            const windowHeight = window.innerHeight || document.documentElement.clientHeight;
            
            // Calcul simple : centrer l'élément dans le viewport
            const elementCenter = rect.top + currentScrollTop + (rect.height / 2);
            const viewportCenter = windowHeight / 2;
            const targetScrollTop = elementCenter - viewportCenter;
            
            console.log(`[SCROLL] Backup scroll calculation:`, {
                rectTop: rect.top,
                currentScrollTop,
                windowHeight,
                elementCenter,
                viewportCenter,
                targetScrollTop
            });
            
            window.scrollTo({
                top: Math.max(0, targetScrollTop),
                behavior: 'instant' // instant pour le backup
            });
        }, 50);
        
    } catch (error) {
        console.warn('[SCROLL] scrollIntoView failed, using manual calculation:', error);
        
        // Fallback manuel
        const optimalScrollTop = calculateOptimalScrollPosition(stepElement);
        window.scrollTo({
            top: optimalScrollTop,
            behavior: config.behavior
        });
    }
}

/**
 * Checks if auto-scroll should be enabled based on user preferences and context
 * @returns {boolean} True if auto-scroll should be active
 */
export function isAutoScrollEnabled() {
    const userPreference = localStorage.getItem('workflow-auto-scroll');
    if (userPreference === 'disabled') {
        return false;
    }

    if (userPreference === 'enabled') {
        return true;
    }

    const isLogsActive = dom.workflowWrapper && dom.workflowWrapper.classList.contains('logs-active');
    return isLogsActive;
}

/**
 * Enables or disables auto-scroll functionality
 * @param {boolean} enabled - Whether to enable auto-scroll
 */
export function setAutoScrollEnabled(enabled) {
    localStorage.setItem('workflow-auto-scroll', enabled ? 'enabled' : 'disabled');
    console.log(`[SCROLL] Auto-scroll ${enabled ? 'enabled' : 'disabled'}`);
}

/**
 * Checks if auto-scroll for sequences should be enabled based on user preferences
 * @returns {boolean} True if sequence auto-scroll should be active
 */
export function isSequenceAutoScrollEnabled() {
    const sequencePreference = localStorage.getItem('workflow-sequence-auto-scroll');
    if (sequencePreference === 'disabled') {
        return false;
    }
    
    if (sequencePreference === 'enabled') {
        return true;
    }
    
    // Par défaut, activer l'auto-scroll pour les séquences
    return true;
}

/**
 * Enables or disables auto-scroll for sequences specifically
 * @param {boolean} enabled - Whether to enable sequence auto-scroll
 */
export function setSequenceAutoScrollEnabled(enabled) {
    localStorage.setItem('workflow-sequence-auto-scroll', enabled ? 'enabled' : 'disabled');
    console.log(`[SCROLL] Sequence auto-scroll ${enabled ? 'enabled' : 'disabled'}`);
}

/**
 * Scrolls to a step with ultra-aggressive repositioning (instant scroll)
 * @param {string} stepKey - The key of the step to scroll to
 * @param {Object} options - Additional options for scrolling
 */
export function scrollToStepUltraAggressive(stepKey, options = {}) {
    if (!stepKey) {
        console.warn('[SCROLL] No stepKey provided for scrollToStepUltraAggressive');
        return;
    }
    
    const stepElement = document.getElementById(`step-${stepKey}`);
    if (!stepElement) {
        console.warn(`[SCROLL] Step element not found: step-${stepKey}`);
        return;
    }
    
    console.log(`[SCROLL] Ultra-aggressive scrolling to step: ${stepKey}`);
    
    // Scroll instantané avec scrollIntoView
    stepElement.scrollIntoView({
        behavior: 'instant',
        block: 'center',
        inline: 'nearest'
    });
    
    // Forcer un second scroll immédiat après
    setTimeout(() => {
        const rect = stepElement.getBoundingClientRect();
        const windowHeight = window.innerHeight || document.documentElement.clientHeight;
        const currentScrollTop = window.pageYOffset || document.documentElement.scrollTop;
        
        // Calcul ultra-simple : centrer parfaitement
        const elementCenter = rect.top + currentScrollTop + (rect.height / 2);
        const viewportCenter = (windowHeight / 2);
        const targetScrollTop = elementCenter - viewportCenter;
        
        console.log(`[SCROLL] Ultra-aggressive calculation:`, {
            rectTop: rect.top,
            rectHeight: rect.height,
            elementCenter,
            viewportCenter,
            targetScrollTop
        });
        
        window.scrollTo({
            top: Math.max(0, targetScrollTop),
            behavior: 'instant'
        });
    }, 10);
}

/**
 * Scrolls to a step with absolute forced repositioning (ignores all CSS and layout factors)
 * @param {string} stepKey - The key of the step to scroll to
 * @param {Object} options - Additional options for scrolling
 */
export function scrollToStepAbsolute(stepKey, options = {}) {
    if (!stepKey) {
        console.warn('[SCROLL] No stepKey provided for scrollToStepAbsolute');
        return;
    }
    
    const stepElement = document.getElementById(`step-${stepKey}`);
    if (!stepElement) {
        console.warn(`[SCROLL] Step element not found: step-${stepKey}`);
        return;
    }
    
    console.log(`[SCROLL] Absolute forced scrolling to step: ${stepKey}`);
    
    // Forcer un scroll absolu en calculant la position exacte
    const rect = stepElement.getBoundingClientRect();
    const currentScrollTop = window.pageYOffset || document.documentElement.scrollTop;
    const windowHeight = window.innerHeight || document.documentElement.clientHeight;
    
    // Calcul absolu : centrer l'élément parfaitement au milieu du viewport
    const elementAbsoluteTop = rect.top + currentScrollTop;
    const elementCenter = elementAbsoluteTop + (rect.height / 2);
    const viewportCenter = windowHeight / 2;
    const absoluteScrollTop = elementCenter - viewportCenter;
    
    console.log(`[SCROLL] Absolute calculation:`, {
        rectTop: rect.top,
        rectHeight: rect.height,
        currentScrollTop,
        elementAbsoluteTop,
        elementCenter,
        viewportCenter,
        absoluteScrollTop
    });
    
    // Appliquer le scroll absolu instantané
    window.scrollTo({
        top: Math.max(0, absoluteScrollTop),
        behavior: 'instant'
    });
    
    // Forcer un second scroll après un court delay pour contrer toute animation CSS
    setTimeout(() => {
        window.scrollTo({
            top: Math.max(0, absoluteScrollTop),
            behavior: 'instant'
        });
        console.log(`[SCROLL] Applied absolute scroll to: ${Math.max(0, absoluteScrollTop)}px`);
    }, 5);
}

export { SCROLL_CONFIG };
```

## File: static/state.js
```javascript
// Import new immutable state management
import { appState } from './state/AppState.js';

export const PROCESS_INFO_CLIENT = new Proxy({}, {
    get(_target, prop) {
        if (typeof prop !== 'string') return undefined;
        return appState.getStateProperty(`processInfo.${prop}`);
    },
    set(_target, prop, value) {
        if (typeof prop !== 'string') return false;
        appState.setState({ processInfo: { [prop]: value } }, 'process_info_update');
        return true;
    },
    ownKeys() {
        const root = appState.getStateProperty('processInfo') || {};
        return Reflect.ownKeys(root);
    },
    getOwnPropertyDescriptor(_target, prop) {
        const root = appState.getStateProperty('processInfo') || {};
        if (Object.prototype.hasOwnProperty.call(root, prop)) {
            return { enumerable: true, configurable: true };
        }
        return undefined;
    }
});

// Legacy exports for backward compatibility (deprecated - use appState instead)
export let pollingIntervals = {};
export let activeStepKeyForLogsPanel = null;
export let stepTimers = {};
export let selectedStepsOrder = [];
export let isAnySequenceRunning = false;
export let focusedElementBeforePopup = null;


// --- MODIFICATION: La liste des étapes est mise à jour pour correspondre au backend ---
export const REMOTE_SEQUENCE_STEP_KEYS = [
    "STEP1",
    "STEP2",
    "STEP3",
    "STEP4",
    "STEP5",
    "STEP6",
    "STEP7"
];

// Modern state management functions using AppState
export function setActiveStepKeyForLogs(key) {
    activeStepKeyForLogsPanel = key; // Legacy
    appState.setState({ activeStepKeyForLogsPanel: key }, 'setActiveStepKeyForLogs');
}
export function getActiveStepKeyForLogs() {
    return appState.getStateProperty('activeStepKeyForLogsPanel') || activeStepKeyForLogsPanel;
}

export function addStepTimer(stepKey, timerData) {
    stepTimers[stepKey] = timerData; // Legacy
    appState.setState({
        stepTimers: { ...appState.getStateProperty('stepTimers'), [stepKey]: timerData }
    }, 'addStepTimer');
}
export function getStepTimer(stepKey) {
    return appState.getStateProperty(`stepTimers.${stepKey}`) || stepTimers[stepKey];
}
export function clearStepTimerInterval(stepKey) {
    const timer = getStepTimer(stepKey);
    if (timer && timer.intervalId) {
        clearInterval(timer.intervalId);
        const updatedTimer = { ...timer, intervalId: null };
        addStepTimer(stepKey, updatedTimer);
    }
}
export function deleteStepTimer(stepKey) {
    if (getStepTimer(stepKey)) {
        clearStepTimerInterval(stepKey);
        delete stepTimers[stepKey]; // Legacy
        const currentTimers = appState.getStateProperty('stepTimers') || {};
        const { [stepKey]: removed, ...remainingTimers } = currentTimers;
        appState.setState({ stepTimers: remainingTimers }, 'deleteStepTimer');
    }
}

export function setSelectedStepsOrder(order) {
    selectedStepsOrder = order; // Legacy
    appState.setState({ selectedStepsOrder: order }, 'setSelectedStepsOrder');
}
export function getSelectedStepsOrder() {
    return appState.getStateProperty('selectedStepsOrder') || selectedStepsOrder;
}

export function setIsAnySequenceRunning(running) {
    isAnySequenceRunning = running; // Legacy
    appState.setState({ isAnySequenceRunning: running }, 'setIsAnySequenceRunning');
}
export function getIsAnySequenceRunning() {
    return appState.getStateProperty('isAnySequenceRunning') || isAnySequenceRunning;
}

export function setFocusedElementBeforePopup(element) {
    focusedElementBeforePopup = element; // Legacy
    appState.setState({ focusedElementBeforePopup: element }, 'setFocusedElementBeforePopup');
}
export function getFocusedElementBeforePopup() {
    return appState.getStateProperty('focusedElementBeforePopup') || focusedElementBeforePopup;
}

export function setAutoModeLogPanelOpened(opened) {
    appState.setState({ ui: { autoModeLogPanelOpened: !!opened } }, 'setAutoModeLogPanelOpened');
}

export function getAutoModeLogPanelOpened() {
    return !!appState.getStateProperty('ui.autoModeLogPanelOpened');
}

export function addPollingInterval(stepKey, id) {
    pollingIntervals[stepKey] = id; // Legacy
    appState.setState({
        pollingIntervals: { ...appState.getStateProperty('pollingIntervals'), [stepKey]: id }
    }, 'addPollingInterval');
}
export function clearPollingInterval(stepKey) {
    if (pollingIntervals[stepKey]) {
        clearInterval(pollingIntervals[stepKey]);
        delete pollingIntervals[stepKey]; // Legacy
    }
    const currentIntervals = appState.getStateProperty('pollingIntervals') || {};
    const { [stepKey]: removed, ...remainingIntervals } = currentIntervals;
    appState.setState({ pollingIntervals: remainingIntervals }, 'clearPollingInterval');
}
export function getPollingInterval(stepKey) {
    return appState.getStateProperty(`pollingIntervals.${stepKey}`) || pollingIntervals[stepKey];
}



// Export the appState for direct access to modern state management
export { appState };
```

## File: static/utils.js
```javascript
import { getNotificationsArea } from './domElements.js';

export function formatElapsedTime(startTime) {
    if (!startTime) return "";
    const now = new Date();
    let seconds = Math.floor((now - startTime) / 1000);
    let minutes = Math.floor(seconds / 60);
    let hours = Math.floor(minutes / 60);
    seconds %= 60;
    minutes %= 60;
    let timeStr = "";
    if (hours > 0) timeStr += `${hours}h `;
    if (minutes > 0 || hours > 0) timeStr += `${minutes}m `;
    timeStr += `${seconds}s`;
    return timeStr;
}

export function showNotification(message, type = 'info') { // type can be 'info', 'success', 'error', 'warning'
    const notificationsArea = getNotificationsArea();
    if (!notificationsArea) return;
    const notif = document.createElement('div');
    notif.className = `notification ${type}`;
    notif.textContent = message;
    notificationsArea.appendChild(notif);
    setTimeout(() => {
        notif.remove();
    }, 5000);
}

// Web Notifications API helpers
export async function ensureBrowserNotificationsPermission() {
    if (!('Notification' in window)) {
        console.warn('[Notifications] Browser does not support Notification API');
        return false;
    }
    if (Notification.permission === 'granted') return true;
    if (Notification.permission === 'denied') return false;
    try {
        const perm = await Notification.requestPermission();
        return perm === 'granted';
    } catch (e) {
        console.warn('[Notifications] Permission request failed:', e);
        return false;
    }
}

export async function sendBrowserNotification(title, body, options = {}) {
    try {
        const ok = await ensureBrowserNotificationsPermission();
        if (!ok) return false;
        const notif = new Notification(title || 'Notification', { body: body || '', ...options });
        setTimeout(() => notif && notif.close && notif.close(), 8000);
        return true;
    } catch (e) {
        console.debug('[Notifications] Fallback to UI banner due to error:', e);
        showNotification(`${title || 'Notification'}: ${body || ''}`, 'info');
        return false;
    }
}
```

## File: workflow_scripts/step5/run_tracking_manager.py
```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import os, sys, json, argparse, subprocess, threading, time, logging
from pathlib import Path
from collections import OrderedDict, deque
from datetime import datetime
from typing import Mapping, Optional

try:
    from dotenv import load_dotenv
except ImportError:
    load_dotenv = None


class _EnvConfig:
    def __init__(self, environ: Mapping[str, str]):
        self._environ = environ

    def get_optional_str(self, key: str) -> Optional[str]:
        raw = self._environ.get(key)
        if raw is None:
            return None
        value = str(raw).strip()
        return value if value else None

    def get_str(self, key: str, default: str = "") -> str:
        value = self.get_optional_str(key)
        return value if value is not None else default

    def get_int(self, key: str, default: int) -> int:
        raw = self._environ.get(key)
        if raw is None:
            return default
        try:
            return int(str(raw).strip())
        except Exception:
            return default

    def get_bool(self, key: str, default: bool) -> bool:
        raw = self._environ.get(key)
        if raw is None:
            return default

        normalized = str(raw).strip().lower()
        if normalized in {"1", "true", "yes", "y", "on"}:
            return True
        if normalized in {"0", "false", "no", "n", "off"}:
            return False
        return default

    def get_csv_list(self, key: str) -> list[str]:
        raw = self.get_str(key, default="").strip().lower()
        return [p.strip() for p in raw.split(",") if p.strip()]

    def snapshot(self, keys: list[str]) -> dict[str, Optional[str]]:
        return {k: self.get_optional_str(k) for k in keys}


def _load_env_file():
    """Load the project .env even if python-dotenv is unavailable."""
    env_path = Path(__file__).resolve().parent.parent.parent / ".env"
    if not env_path.exists():
        return
    if load_dotenv:
        load_dotenv(env_path)
        return

    for raw_line in env_path.read_text(encoding="utf-8").splitlines():
        line = raw_line.strip()
        if not line or line.startswith("#") or "=" not in line:
            continue
        key, value = line.split("=", 1)
        key = key.strip()
        if not key:
            continue
        cleaned = value.strip().strip('"').strip("'")
        os.environ.setdefault(key, cleaned)


_load_env_file()


ENV = _EnvConfig(os.environ)
step5_enable_object_detection = ENV.get_bool("STEP5_ENABLE_OBJECT_DETECTION", default=True)

def _log_env_snapshot():
    relevant_keys = [
        "STEP5_ENABLE_GPU",
        "STEP5_GPU_ENGINES",
        "STEP5_TRACKING_ENGINE",
        "TRACKING_DISABLE_GPU",
        "TRACKING_CPU_WORKERS",
        "STEP5_GPU_FALLBACK_AUTO",
        "STEP5_ENABLE_OBJECT_DETECTION",
        "INSIGHTFACE_HOME",
    ]
    snapshot = ENV.snapshot(relevant_keys)
    logging.info(f"[EnvSnapshot] {snapshot}")


# --- CONFIGURATIONS GLOBALES ---
BASE_DIR = Path(__file__).resolve().parent.parent.parent

try:
    sys.path.insert(0, str(BASE_DIR))
    from config.settings import config as app_config
    TRACKING_ENV_PYTHON = app_config.get_venv_python("tracking_env")
    EOS_ENV_PYTHON = app_config.get_venv_python("eos_env")
    INSIGHTFACE_ENV_PYTHON = app_config.get_venv_python("insightface_env")
except (ImportError, AttributeError):
    TRACKING_ENV_PYTHON = BASE_DIR / "tracking_env" / "bin" / "python"
    EOS_ENV_PYTHON = BASE_DIR / "eos_env" / "bin" / "python"
    INSIGHTFACE_ENV_PYTHON = BASE_DIR / "insightface_env" / "bin" / "python"

TRACKING_ENV_PYTHON = Path(TRACKING_ENV_PYTHON)
EOS_ENV_PYTHON = Path(EOS_ENV_PYTHON)
INSIGHTFACE_ENV_PYTHON = Path(INSIGHTFACE_ENV_PYTHON)

WORKER_SCRIPT = Path(__file__).parent / "process_video_worker.py"

TRACKING_ENGINE = None

TF_GPU_ENV_PYTHON = ENV.get_str("STEP5_TF_GPU_ENV_PYTHON", "").strip()
if TF_GPU_ENV_PYTHON:
    TF_GPU_ENV_PYTHON = Path(TF_GPU_ENV_PYTHON)

CUDA_LIB_SUBDIRS = [
    "cublas/lib",
    "cuda_runtime/lib",
    "cuda_nvrtc/lib",
    "cufft/lib",
    "curand/lib",
    "cusolver/lib",
    "cusparse/lib",
    "cudnn/lib",
    "nvjitlink/lib",
]

SYSTEM_CUDA_DEFAULTS = [
    "/usr/local/cuda-12.4",
    "/usr/local/cuda-12",
    "/usr/local/cuda-11.8",
    "/usr/local/cuda",
]


def _build_venv_cuda_paths(python_exe: Path) -> list[str]:
    """Return CUDA library directories bundled in a venv (for ONNX Runtime CUDA provider)."""
    try:
        venv_dir = Path(python_exe).expanduser().parent.parent
    except Exception:
        return []

    python_version = f"python{sys.version_info.major}.{sys.version_info.minor}"
    site_packages = venv_dir / "lib" / python_version / "site-packages"
    nvidia_dir = site_packages / "nvidia"
    if not nvidia_dir.exists():
        return []

    paths = []
    for sub in CUDA_LIB_SUBDIRS:
        candidate = nvidia_dir / sub
        if candidate.exists():
            paths.append(str(candidate))
    return paths


def _discover_system_cuda_lib_paths(env: _EnvConfig) -> list[str]:
    """
    Detect CUDA libraries available on the host for engines that bundle their own interpreter (InsightFace).
    Priority order:
      1. Explicit STEP5_CUDA_LIB_PATH (colon-separated).
      2. STEP5_CUDA_HOME / CUDA_HOME lib64 folders.
      3. Common /usr/local/cuda-* lib64 folders.
      4. /usr/lib/x86_64-linux-gnu in case distro ships the libs there.
    """
    explicit_paths = env.get_str("STEP5_CUDA_LIB_PATH", "").strip()
    if explicit_paths:
        resolved = [
            str(Path(p.strip()).expanduser())
            for p in explicit_paths.split(":")
            if p.strip() and Path(p.strip()).expanduser().exists()
        ]
        if resolved:
            return resolved

    candidates: list[Path] = []
    for env_var in ("STEP5_CUDA_HOME", "CUDA_HOME"):
        value = env.get_str(env_var, "").strip()
        if value:
            candidates.append(Path(value).expanduser())
    for default_path in SYSTEM_CUDA_DEFAULTS:
        candidates.append(Path(default_path))

    discovered: list[str] = []
    for base_dir in candidates:
        if not base_dir.exists():
            continue
        for sub in ("lib64", "targets/x86_64-linux/lib"):
            candidate = base_dir / sub
            if candidate.exists():
                discovered.append(str(candidate))
    debian_cuda = Path("/usr/lib/x86_64-linux-gnu")
    if debian_cuda.exists():
        expected = ["libcufft.so.11", "libcublas.so.12"]
        if all((debian_cuda / lib_name).exists() for lib_name in expected):
            discovered.append(str(debian_cuda))
    return discovered


# --- CONFIGURATION DU LOGGER ---
LOG_DIR = BASE_DIR / "logs" / "step5"
LOG_DIR.mkdir(parents=True, exist_ok=True)
log_file = LOG_DIR / f"manager_tracking_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
logging.basicConfig(level=logging.INFO, format='%(asctime)s - [MANAGER] - %(message)s',
                    handlers=[logging.FileHandler(log_file, 'w', 'utf-8'), logging.StreamHandler(sys.stdout)])

WORKER_CONFIG_TEMPLATE = {
    "mp_landmarker_num_faces": 5, "mp_landmarker_min_face_detection_confidence": 0.5,
    "mp_landmarker_min_face_presence_confidence": 0.3, "mp_landmarker_min_tracking_confidence": 0.5,
    "mp_landmarker_output_blendshapes": True, "enable_object_detection": step5_enable_object_detection,
    "object_score_threshold": 0.5, "object_max_results": 5, "mp_max_distance_tracking": 70,
    "mp_frames_unseen_deregister": 7, "speaking_detection_jaw_open_threshold": 0.08,
    "object_detector_model": ENV.get_str('STEP5_OBJECT_DETECTOR_MODEL', 'efficientdet_lite2'),
    "object_detector_model_path": ENV.get_optional_str('STEP5_OBJECT_DETECTOR_MODEL_PATH'),
}

CPU_OPTIMIZED_CONFIG = {
    "mp_landmarker_min_face_detection_confidence": 0.3,
    "mp_landmarker_min_face_presence_confidence": 0.2,
    "mp_landmarker_min_tracking_confidence": 0.3,
    "object_score_threshold": 0.4,
    "mp_max_distance_tracking": 80,
}


_DEFAULT_SUBPROCESS_ENV: dict[str, str] = {
    'OMP_NUM_THREADS': '1',
    'OPENBLAS_NUM_THREADS': '1',
    'MKL_NUM_THREADS': '1',
    'NUMEXPR_NUM_THREADS': '1',
    'TF_CPP_MIN_LOG_LEVEL': '2',
    'GLOG_minloglevel': '2',
    'ABSL_LOGGING_MIN_LOG_LEVEL': '2',
}


def _collect_cuda_lib_paths(worker_python: Path, engine_norm: str) -> list[str]:
    cuda_paths: list[str] = []
    cuda_paths_worker = _build_venv_cuda_paths(worker_python)
    cuda_paths_tracking = _build_venv_cuda_paths(TRACKING_ENV_PYTHON)
    if cuda_paths_worker:
        cuda_paths.extend(cuda_paths_worker)
    elif cuda_paths_tracking:
        cuda_paths.extend(cuda_paths_tracking)

    if engine_norm == "insightface":
        system_cuda_paths = _discover_system_cuda_lib_paths(ENV)
        for path in system_cuda_paths:
            if path not in cuda_paths:
                cuda_paths.append(path)
    return cuda_paths


def _apply_ld_library_path(env: dict[str, str], extra_paths: list[str]) -> None:
    if not extra_paths:
        return
    extra_ld_path = ":".join(extra_paths)
    existing_ld_path = env.get("LD_LIBRARY_PATH", "")
    env["LD_LIBRARY_PATH"] = (
        f"{extra_ld_path}:{existing_ld_path}" if existing_ld_path else extra_ld_path
    )


def _build_subprocess_env(worker_python: Path, engine_norm: str) -> dict[str, str]:
    env = os.environ.copy()
    cuda_paths = _collect_cuda_lib_paths(worker_python, engine_norm)
    if cuda_paths:
        _apply_ld_library_path(env, cuda_paths)
        logging.info("[MANAGER] Injected CUDA library paths for ONNX Runtime")

    for key, value in _DEFAULT_SUBPROCESS_ENV.items():
        env.setdefault(key, value)
    return env


def log_reader_thread(process, video_name, progress_map, lock):
    if process.stdout:
        for line in iter(process.stdout.readline, ''):
            line = line.strip()
            logging.info(f"[{video_name}] {line}")
            if line.startswith("[Progression]|"):
                try:
                    _, percent, _, _ = line.split('|')
                    with lock:
                        progress_map[video_name] = f"{int(percent)}%"
                    try:
                        print(f"{video_name}: {int(percent)}%", flush=True)
                    except Exception:
                        pass
                except:
                    pass
        process.stdout.close()


def monitor_progress(processes, progress_map, lock, total_jobs_to_run):
    while len(processes) < total_jobs_to_run or any(p.poll() is None for p in processes.values()):
        time.sleep(1)
        with lock:
            progress_copy = progress_map.copy()
        progress_parts = [f"{name}: {status}" for name, status in sorted(progress_copy.items())]
        if progress_parts: print(f"[Progression-MultiLine]{' || '.join(progress_parts)}", flush=True)
    time.sleep(1)


def launch_worker_process(video_path, use_gpu, internal_workers=1, tracking_engine=None):
    video_name = Path(video_path).name
    worker_type_log = "GPU" if use_gpu else f"CPU (x{internal_workers})"
    logging.info(f"Préparation du job {worker_type_log} pour: {video_name}")

    config = WORKER_CONFIG_TEMPLATE.copy()

    engine_norm = (str(tracking_engine).strip().lower() if tracking_engine is not None else "")
    if engine_norm in {"mediapipe", "mediapipe_landmarker"}:
        engine_norm = ""

    gpu_capable_engines = {"openseeface", "opencv_yunet_pyfeat", "insightface"}
    
    if engine_norm and engine_norm not in gpu_capable_engines:
        if use_gpu:
            logging.info(f"Engine {engine_norm} not GPU-capable, forcing CPU mode")
        use_gpu = False

    if not use_gpu:
        config.update(CPU_OPTIMIZED_CONFIG)
        config["mp_num_workers_internal"] = internal_workers

        if internal_workers > 1:
            worker_script = "process_video_worker_multiprocessing.py"
            logging.info(f"Using multiprocessing worker with {internal_workers} processes")
        else:
            worker_script = "process_video_worker.py"
            logging.info("Using single-threaded CPU worker")

        logging.info(f"Applied CPU optimizations: lower confidence thresholds for better detection rate")
    else:
        worker_script = "process_video_worker.py"
        logging.info("Using GPU worker with sequential processing")
        if step5_enable_object_detection and internal_workers > 1:
            config["mp_num_workers_internal"] = internal_workers

    models_dir_path = Path(__file__).resolve().parent / "models"
    worker_script_path = Path(__file__).resolve().parent / worker_script

    worker_python_override_eos = ENV.get_optional_str("STEP5_EOS_ENV_PYTHON")
    worker_python_override_insightface = ENV.get_optional_str("STEP5_INSIGHTFACE_ENV_PYTHON")
    worker_python = TRACKING_ENV_PYTHON
    
    if engine_norm == "eos":
        worker_python = Path(worker_python_override_eos) if worker_python_override_eos else EOS_ENV_PYTHON
        if not worker_python.exists():
            raise RuntimeError(
                f"EOS engine selected but python interpreter not found: {worker_python}. "
                "Create eos_env or set STEP5_EOS_ENV_PYTHON to the eos_env python path."
            )
    elif engine_norm == "insightface":
        if not use_gpu:
            raise RuntimeError(
                "InsightFace engine is GPU-only. Enable STEP5_ENABLE_GPU=1 and include 'insightface' in STEP5_GPU_ENGINES."
            )

        worker_python = (
            Path(worker_python_override_insightface)
            if worker_python_override_insightface
            else INSIGHTFACE_ENV_PYTHON
        )
        if not worker_python.exists():
            raise RuntimeError(
                f"InsightFace engine selected but python interpreter not found: {worker_python}. "
                "Create insightface_env or set STEP5_INSIGHTFACE_ENV_PYTHON to the insightface_env python path."
            )
    elif use_gpu and not engine_norm:
        if TF_GPU_ENV_PYTHON and isinstance(TF_GPU_ENV_PYTHON, Path):
            if TF_GPU_ENV_PYTHON.exists():
                worker_python = TF_GPU_ENV_PYTHON
                logging.info(f"Using TensorFlow GPU interpreter for MediaPipe: {worker_python}")
            else:
                logging.warning(
                    "STEP5_TF_GPU_ENV_PYTHON is set but the interpreter was not found "
                    f"({TF_GPU_ENV_PYTHON}). Falling back to tracking_env."
                )

    command_args = [str(worker_python), str(worker_script_path), video_path, "--models_dir", str(models_dir_path)]
    if use_gpu: command_args.append("--use_gpu")

    if engine_norm:
        command_args.extend(["--tracking_engine", engine_norm])

    for key, value in config.items():
        if key == "mp_num_workers_internal" and use_gpu and not (step5_enable_object_detection and internal_workers > 1):
            continue
        if isinstance(value, bool):
            if value:
                command_args.append(f"--{key}")
            continue
        if value is not None:
            command_args.extend([f"--{key}", str(value)])

    if (not use_gpu) and internal_workers > 1:
        command_args.extend(["--chunk_size", "0"])  # 0 = adaptive in worker

    try:
        env = _build_subprocess_env(Path(worker_python), engine_norm)

        p = subprocess.Popen(
            command_args,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            encoding='utf-8',
            errors='replace',
            env=env,
        )
        return p
    except Exception as e:
        logging.error(f"ERREUR LANCEMENT de {video_name}: {e}")
        return None


def run_job_and_monitor(job_info, processes, progress_map, lock):
    video_path = job_info['path']
    video_name = Path(video_path).name
    internal_workers = job_info.get('cpu_internal_workers', 15) if (not job_info['use_gpu'] or step5_enable_object_detection) else 1
    tracking_engine = job_info.get('tracking_engine')
    p = launch_worker_process(video_path, job_info['use_gpu'], internal_workers, tracking_engine=tracking_engine)

    if p:
        with lock:
            processes[video_name] = p
            progress_map[video_name] = "Démarrage..."

        reader_thread = threading.Thread(target=log_reader_thread, args=(p, video_name, progress_map, lock),
                                         daemon=True)
        reader_thread.start()
        p.wait()
        reader_thread.join(timeout=1)

        try:
            if p.returncode == 0:
                print(f"[Gestionnaire] Succès pour {video_name}", flush=True)
            else:
                print(f"[Gestionnaire] Échec pour {video_name}", flush=True)
        except Exception:
            pass



def resource_worker_loop(resource_name, use_gpu, videos_deque, deque_lock, processes, progress_map, lock):
    """Continuously pull videos from the shared deque and process them on the given resource.

    Args:
        resource_name (str): Human-readable resource label (e.g., 'GPU' or 'CPU').
        use_gpu (bool): Whether to use GPU for the worker process.
        videos_deque (collections.deque): Shared queue of video paths to process.
        deque_lock (threading.Lock): Lock protecting access to the shared deque.
        processes (OrderedDict): Shared mapping of video_name -> subprocess.Popen.
        progress_map (OrderedDict): Shared mapping of video_name -> progress string.
        lock (threading.Lock): Lock protecting shared maps.
    """
    while True:
        with deque_lock:
            if videos_deque:
                video_path = videos_deque.popleft()
            else:
                break
        video_name = Path(video_path).name
        logging.info(f"[Scheduler] Assigning {video_name} to {resource_name}")
        run_job_and_monitor(
            {
                'path': video_path,
                'use_gpu': use_gpu,
                'cpu_internal_workers': CPU_INTERNAL_WORKERS,
                'tracking_engine': TRACKING_ENGINE,
            },
            processes,
            progress_map,
            lock,
        )


def main():
    parser = argparse.ArgumentParser(description="Gestionnaire de tracking parallèle intelligent.")
    parser.add_argument("--videos_json_path", required=True, help="Chemin JSON des vidéos.")
    parser.add_argument("--disable_gpu", action="store_true", help="Désactiver le worker GPU et utiliser uniquement le CPU")
    parser.add_argument("--cpu_internal_workers", type=int, default=15, help="Nombre de workers internes CPU par vidéo")
    parser.add_argument(
        "--tracking_engine",
        default=None,
        help="Moteur de tracking: mediapipe_landmarker (défaut), opencv_haar, opencv_yunet, opencv_yunet_pyfeat, openseeface, eos, insightface",
    )
    args = parser.parse_args()
    try:
        if ENV.get_bool('TRACKING_DISABLE_GPU', default=False):
            args.disable_gpu = True

        args.cpu_internal_workers = ENV.get_int('TRACKING_CPU_WORKERS', args.cpu_internal_workers)

        if args.tracking_engine is None:
            raw_engine = ENV.get_optional_str('STEP5_TRACKING_ENGINE')
            if raw_engine:
                args.tracking_engine = raw_engine
    except Exception:
        pass

    _engine_norm = (str(args.tracking_engine).strip().lower() if args.tracking_engine is not None else "")
    
    gpu_enabled_global = ENV.get_str('STEP5_ENABLE_GPU', '0').strip() == '1'
    gpu_engines_str = ENV.get_str('STEP5_GPU_ENGINES', '').strip().lower()
    gpu_engines = [e.strip() for e in gpu_engines_str.split(',') if e.strip()]
    _log_env_snapshot()
    
    engine_normalized = _engine_norm if _engine_norm else "mediapipe_landmarker"
    if engine_normalized in {"mediapipe", "mediapipe_landmarker"}:
        engine_normalized = "mediapipe_landmarker"
    
    engine_supports_gpu = False
    if gpu_enabled_global and not args.disable_gpu:
        if engine_normalized == "insightface":
            if engine_normalized in gpu_engines or 'all' in gpu_engines:
                engine_supports_gpu = True
                logging.info(f"GPU mode requested for engine: {engine_normalized}")
                
                try:
                    sys.path.insert(0, str(Path(__file__).parent.parent.parent))
                    from config.settings import Config
                    gpu_status = Config.check_gpu_availability()
                    
                    if not gpu_status['available']:
                        logging.warning(f"GPU requested but unavailable: {gpu_status['reason']}")
                        fallback_auto = ENV.get_str('STEP5_GPU_FALLBACK_AUTO', '1').strip() == '1'
                        if fallback_auto:
                            logging.info("Auto-fallback to CPU enabled")
                            args.disable_gpu = True
                            engine_supports_gpu = False
                        else:
                            logging.error("GPU fallback disabled, aborting")
                            raise RuntimeError(f"GPU unavailable: {gpu_status['reason']}")
                    else:
                        logging.info(f"GPU validation passed: VRAM {gpu_status['vram_free_gb']:.1f} Go free, CUDA {gpu_status.get('cuda_version', 'N/A')}")
                except ImportError as e:
                    logging.error(f"Failed to import config.settings: {e}")
                    args.disable_gpu = True
                    engine_supports_gpu = False
                except Exception as e:
                    logging.error(f"GPU validation failed: {e}")
                    args.disable_gpu = True
                    engine_supports_gpu = False
            else:
                logging.warning(f"InsightFace not listed in STEP5_GPU_ENGINES ({gpu_engines_str}), forcing CPU-only mode")
                args.disable_gpu = True
        else:
            logging.info(f"GPU mode is reserved for InsightFace only. Engine '{engine_normalized}' will run in CPU-only mode.")
            args.disable_gpu = True
    
    if _engine_norm in {"opencv_haar", "opencv_yunet", "eos"}:
        if not args.disable_gpu:
            args.disable_gpu = True
            logging.info(f"Engine {_engine_norm} does not support GPU, forcing CPU-only mode")
    
    if not args.disable_gpu and engine_supports_gpu:
        logging.info(f"✓ GPU mode ENABLED for {_engine_norm or 'mediapipe_landmarker'}")
    else:
        logging.info(f"CPU-only mode (GPU disabled or not supported for this engine)")

    if _engine_norm == "insightface" and (args.disable_gpu or not engine_supports_gpu):
        logging.error(
            "InsightFace engine is GPU-only, but GPU mode is disabled or not authorized. "
            "Set STEP5_ENABLE_GPU=1 and include 'insightface' in STEP5_GPU_ENGINES."
        )
        sys.exit(1)

    with open(args.videos_json_path, 'r') as f:
        data = json.load(f)
    
    if isinstance(data, list):
        videos_list = data
    elif isinstance(data, dict) and 'videos' in data:
        logging.info("Legacy videos JSON schema detected; using 'videos' key.")
        videos_list = data['videos']
    else:
        logging.error(f"Invalid JSON format in {args.videos_json_path}. Expected a list or object with 'videos' key.")
        sys.exit(1)
    
    videos_to_process = deque(videos_list)
    if not videos_to_process: logging.info("Aucune vidéo à traiter."); return

    logging.info(f"--- DÉMARRAGE DU GESTIONNAIRE DE TRACKING (Planificateur Dynamique GPU/CPU) ---")
    total_jobs = len(videos_to_process)
    logging.info(f"Vidéos à traiter: {total_jobs}")
    global CPU_INTERNAL_WORKERS
    CPU_INTERNAL_WORKERS = max(1, int(args.cpu_internal_workers))

    global TRACKING_ENGINE
    TRACKING_ENGINE = args.tracking_engine
    if args.disable_gpu:
        logging.info("Mode FULL CPU activé: le worker GPU est désactivé")

    processes, video_progress_map, progress_lock = OrderedDict(), OrderedDict(), threading.Lock()

    monitor_thread = threading.Thread(target=monitor_progress,
                                      args=(processes, video_progress_map, progress_lock, total_jobs), daemon=True)
    monitor_thread.start()

    deque_lock = threading.Lock()

    threads = []
    if not args.disable_gpu:
        gpu_thread = threading.Thread(
            target=resource_worker_loop,
            args=("GPU", True, videos_to_process, deque_lock, processes, video_progress_map, progress_lock),
            daemon=True,
        )
        threads.append(gpu_thread)
    if _engine_norm != "insightface":
        cpu_thread = threading.Thread(
            target=resource_worker_loop,
            args=("CPU", False, videos_to_process, deque_lock, processes, video_progress_map, progress_lock),
            daemon=True,
        )
        threads.append(cpu_thread)
    else:
        logging.info("InsightFace is GPU-only: CPU worker thread disabled")

    if args.disable_gpu:
        workers_label = "CPU seul"
    else:
        workers_label = "GPU seul" if _engine_norm == "insightface" else "GPU et CPU"
    logging.info("Lancement des workers: " + workers_label)
    for t in threads:
        t.start()

    for t in threads:
        t.join()

    monitor_thread.join(timeout=2)

    success_count = sum(1 for p in processes.values() if p.returncode == 0)
    logging.info(f"--- FIN DU GESTIONNAIRE ---")
    logging.info(f"Traitement terminé. {success_count}/{total_jobs} jobs réussis.")
    if success_count < total_jobs: sys.exit(1)


if __name__ == "__main__":
    main()
```

## File: app_new.py
```python
import csv
import html
import json
import logging
import os
import re
import subprocess
import sys
import threading
import time
import atexit
from collections import deque
from pathlib import Path
from datetime import datetime, timezone, timedelta
import uuid

import psutil
import requests
from flask import Flask, render_template, jsonify, request, send_from_directory
from flask_caching import Cache

try:
    from dotenv import load_dotenv
    load_dotenv()
    print("✅ Dotenv loaded successfully")
except ImportError:
    print("⚠️ Dotenv not available - using environment variables directly")
    pass

from config.settings import config
from config.security import SecurityConfig, require_internal_worker_token, require_render_register_token
from config.workflow_commands import WorkflowCommandsConfig

from routes.api_routes import api_bp
from routes.workflow_routes import workflow_bp
from services.monitoring_service import MonitoringService
from services.csv_service import CSVService
from services.workflow_service import WorkflowService
from services.cache_service import CacheService
from services.performance_service import PerformanceService
from services.filesystem_service import FilesystemService
from services.download_service import DownloadService
from services.workflow_state import get_workflow_state, reset_workflow_state
import urllib.parse

try:
    import pandas as pd
    PANDAS_AVAILABLE = True
except ImportError:
    PANDAS_AVAILABLE = False
    print("AVERTISSEMENT: pandas non disponible. Les fichiers Excel ne pourront pas être traités.")

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

try:
    import pynvml
    if config.ENABLE_GPU_MONITORING:
        PYNVML_AVAILABLE = True
        pynvml.nvmlInit()
        logger.info("GPU monitoring initialized successfully")
    else:
        PYNVML_AVAILABLE = False
        logger.info("GPU monitoring disabled by configuration")
except ImportError:
    PYNVML_AVAILABLE = False
    logger.warning("pynvml not available. GPU monitoring disabled.")
except Exception as e:
    PYNVML_AVAILABLE = False
    logger.error(f"Failed to initialize GPU monitoring: {e}")

BASE_PATH_SCRIPTS = config.BASE_PATH_SCRIPTS
PYTHON_VENV_EXE = config.PYTHON_VENV_EXE

PARALLEL_TRACKING_SCRIPT_PATH = BASE_PATH_SCRIPTS / "workflow_scripts" / "step5" / "run_tracking_manager.py"

os.environ['ROOT_SCAN_DIR'] = str(BASE_PATH_SCRIPTS / "projets_extraits")
KEYWORD_FILTER_TRACKING_ENV = "Camille"
os.environ['FOLDER_KEYWORD'] = KEYWORD_FILTER_TRACKING_ENV
SUBDIR_FILTER_TRACKING_ENV = "docs"
os.environ['SUBFOLDER_NAME'] = SUBDIR_FILTER_TRACKING_ENV
os.environ.setdefault('TRACKING_DISABLE_GPU', '1')
os.environ.setdefault('TRACKING_CPU_WORKERS', '15')

LOGS_BASE_DIR = BASE_PATH_SCRIPTS / "logs"
os.makedirs(LOGS_BASE_DIR, exist_ok=True)

for step in range(1, 8):
    os.makedirs(LOGS_BASE_DIR / f"step{step}", exist_ok=True)

STEP0_PREP_LOG_DIR = Path(os.environ.get('STEP0_PREP_LOG_DIR_ENV', str(LOGS_BASE_DIR / "step1")))
MEDIA_ENCODER_LOGS_DIR = Path(os.environ.get('MEDIA_ENCODER_LOGS_DIR_ENV', str(LOGS_BASE_DIR / "step2")))
SCENE_DETECT_LOG_DIR = Path(os.environ.get('SCENE_DETECT_LOG_DIR_ENV', str(LOGS_BASE_DIR / "step3")))
AUDIO_ANALYSIS_LOG_DIR = Path(os.environ.get('AUDIO_ANALYSIS_LOG_DIR_ENV', str(LOGS_BASE_DIR / "step4")))

BASE_TRACKING_LOG_SEARCH_PATH = Path(os.environ.get('BASE_TRACKING_LOG_SEARCH_PATH_ENV', str(BASE_PATH_SCRIPTS)))
BASE_TRACKING_PROGRESS_SEARCH_PATH = Path(os.environ.get('BASE_TRACKING_PROGRESS_SEARCH_PATH_ENV', str(BASE_PATH_SCRIPTS)))
HF_AUTH_TOKEN_ENV = os.environ.get("HF_AUTH_TOKEN")

security_config = SecurityConfig()

RENDER_APP_CALLBACK_URL_ENV = os.environ.get("RENDER_APP_CALLBACK_URL")
RENDER_APP_CALLBACK_TOKEN_ENV = os.environ.get("RENDER_APP_CALLBACK_TOKEN")
RENDER_REGISTER_URL_ENDPOINT_ENV = os.environ.get("RENDER_REGISTER_URL_ENDPOINT")

RENDER_REGISTER_TOKEN_ENV = security_config.RENDER_REGISTER_TOKEN
INTERNAL_WORKER_COMMS_TOKEN_ENV = security_config.INTERNAL_WORKER_TOKEN

WEBHOOK_MONITOR_INTERVAL = config.WEBHOOK_MONITOR_INTERVAL
LOCAL_DOWNLOADS_DIR = config.LOCAL_DOWNLOADS_DIR
DOWNLOAD_HISTORY_FILE = config.BASE_PATH_SCRIPTS / "download_history.json"

def create_app(config_class=None):
    """
    Create and configure Flask application with modular architecture.

    Args:
        config_class: Configuration class to use (defaults to main config)

    Returns:
        Configured Flask application
    """
    app = Flask(__name__)

    app.config.update({
        'SECRET_KEY': config.SECRET_KEY,
        'DEBUG': config.DEBUG,
        'CACHE_TYPE': 'SimpleCache',
        'CACHE_DEFAULT_TIMEOUT': 300,
        'SEND_FILE_MAX_AGE_DEFAULT': 0
    })

    import logging
    logging.getLogger('werkzeug').setLevel(logging.WARNING)

    try:
        config.validate()
        logger.info("Application configuration validated successfully")
    except ValueError as e:
        logger.error(f"Configuration validation failed: {e}")

    cache = Cache(app)

    app._cache_service_initialized = False
    app._services_initialized = False
    app._cache_instance = cache

    app.register_blueprint(api_bp, url_prefix='/api')
    app.register_blueprint(workflow_bp)

    logger.info("Flask application created with modular architecture")
    return app

APP_FLASK = create_app()
APP_LOGGER = APP_FLASK.logger

with APP_FLASK.app_context():
    CACHE = APP_FLASK.extensions['cache']

KEYWORD_FILTER_TRACKING_ENV = os.environ.get('KEYWORD_FILTER_TRACKING_ENV', "Camille")
SUBDIR_FILTER_TRACKING_ENV = os.environ.get('SUBDIR_FILTER_TRACKING_ENV', "docs")

if not HF_AUTH_TOKEN_ENV:
    APP_LOGGER.warning("HF_AUTH_TOKEN environment variable not set. Step 4 (Audio Analysis) will fail if executed.")
else:
    APP_LOGGER.info("HF_AUTH_TOKEN environment variable found and will be used for Analyze Audio.")

try:
    security_config.validate_tokens()
    logger.info("Security tokens validated successfully")
    if INTERNAL_WORKER_COMMS_TOKEN_ENV:
        logger.info(f"CFG TOKEN: INTERNAL_WORKER_COMMS_TOKEN_ENV configured: '...{INTERNAL_WORKER_COMMS_TOKEN_ENV[-5:]}'")
    if RENDER_REGISTER_TOKEN_ENV:
        logger.info(f"CFG TOKEN: RENDER_REGISTER_TOKEN_ENV configured: '...{RENDER_REGISTER_TOKEN_ENV[-5:]}'")
except ValueError as e:
    logger.error(f"Security configuration error: {e}")
    logger.error("Application will continue but some endpoints will be INSECURE")

workflow_commands_config = WorkflowCommandsConfig(
    base_path=BASE_PATH_SCRIPTS,
    hf_token=HF_AUTH_TOKEN_ENV
)

workflow_state = get_workflow_state()
workflow_state.initialize_all_steps(workflow_commands_config.get_all_step_keys())



_services_initialized = False
_services_lock = threading.Lock()

def initialize_services():
    """Initialize all services with proper configuration."""
    global _services_initialized

    with _services_lock:
        if _services_initialized:
            logger.debug("Services already initialized, skipping")
            return

        try:
            with APP_FLASK.app_context():
                if not APP_FLASK._cache_service_initialized:
                    CacheService.initialize(APP_FLASK._cache_instance)
                    APP_FLASK._cache_service_initialized = True
                    logger.info("CacheService initialized")

                if not APP_FLASK._services_initialized:
                    CSVService.initialize()
                    WorkflowService.initialize(workflow_commands_config.get_config())
                    PerformanceService.start_background_monitoring()
                    APP_FLASK._services_initialized = True
                    logger.info("All services initialized successfully")

            _services_initialized = True

        except Exception as e:
            logger.error(f"Service initialization failed: {e}")

_app_initialized = False
_app_init_lock = threading.Lock()

def init_app():
    global _app_initialized

    with _app_init_lock:
        if _app_initialized:
            return APP_FLASK

        APP_LOGGER.handlers.clear()

        logs_dir = BASE_PATH_SCRIPTS / "logs"
        logs_dir.mkdir(exist_ok=True)

        log_file_path = logs_dir / "app.log"
        file_handler = logging.FileHandler(log_file_path, mode='w', encoding='utf-8')  # 'w' mode to start fresh each time
        file_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(threadName)s - %(message)s [in %(pathname)s:%(lineno)d]')
        file_handler.setFormatter(file_formatter)

        console_handler = logging.StreamHandler(sys.stdout)
        console_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(threadName)s - %(message)s')
        console_handler.setFormatter(console_formatter)

        APP_LOGGER.addHandler(file_handler)
        APP_LOGGER.addHandler(console_handler)
        APP_LOGGER.propagate = False

        is_debug_mode = os.environ.get("FLASK_DEBUG") == "1"
        APP_LOGGER.setLevel(logging.DEBUG)

        APP_LOGGER.info(f"=== COMPREHENSIVE LOGGING INITIALIZED ===")
        APP_LOGGER.info(f"Log file: {log_file_path}")
        APP_LOGGER.info(f"Debug mode: {is_debug_mode}")
        APP_LOGGER.info(f"Logger level: {APP_LOGGER.level}")
        APP_LOGGER.info(f"=== STARTING APPLICATION ===")

        root_logger = logging.getLogger()
        root_logger.setLevel(logging.DEBUG)
        if not root_logger.handlers:
            root_logger.addHandler(file_handler)
            root_logger.addHandler(console_handler)

        APP_LOGGER.info("--- Lanceur de Workflow Démarré (Version Ubuntu) ---")

        APP_LOGGER.info("🚀🚀🚀 [STARTUP_TEST] UPDATED CODE WITH COMPREHENSIVE DEBUGGING IS RUNNING! 🚀🚀🚀")
        APP_LOGGER.info(f"🔍 [FILE_PATH_TEST] EXECUTING FROM: {__file__}")
        APP_LOGGER.info(f"🔍 [FILE_PATH_TEST] WORKING DIRECTORY: {os.getcwd()}")
        APP_LOGGER.info(f"🔍 [FILE_PATH_TEST] PYTHON EXECUTABLE: {sys.executable}")
        APP_LOGGER.info(f"BASE_PATH_SCRIPTS: {BASE_PATH_SCRIPTS}")
        APP_LOGGER.info(f"Script de tracking parallèle: {PARALLEL_TRACKING_SCRIPT_PATH}")
        APP_LOGGER.info(f"Répertoire de travail: {BASE_PATH_SCRIPTS / 'projets_extraits'}")

        os.makedirs(BASE_PATH_SCRIPTS / 'projets_extraits', exist_ok=True)

        if not HF_AUTH_TOKEN_ENV:
            APP_LOGGER.warning("HF_AUTH_TOKEN non défini. L'étape 4 (Analyse Audio) nécessitera cette variable d'environnement.")

        if not INTERNAL_WORKER_COMMS_TOKEN_ENV:
            APP_LOGGER.warning("INTERNAL_WORKER_COMMS_TOKEN non défini. API critiques INSECURES.")
        else:
            APP_LOGGER.info("INTERNAL_WORKER_COMMS_TOKEN configuré correctement.")

        if not RENDER_REGISTER_TOKEN_ENV:
            APP_LOGGER.warning("RENDER_REGISTER_TOKEN non défini. Fonctionnalités de rendu INSECURES.")
        else:
            APP_LOGGER.info("RENDER_REGISTER_TOKEN configuré correctement.")

        initialize_services()

        if not getattr(APP_FLASK, "_polling_threads_started", False):
            remote_poll_thread = threading.Thread(target=poll_remote_trigger, name="RemoteWorkflowPoller")
            remote_poll_thread.daemon = True
            remote_poll_thread.start()

            csv_monitor_thread = threading.Thread(target=csv_monitor_service, name="CSVMonitorService")
            csv_monitor_thread.daemon = True
            csv_monitor_thread.start()

            APP_FLASK._polling_threads_started = True

        APP_LOGGER.info("WEBHOOK MONITOR: Système de monitoring activé et prêt (Webhook uniquement).")

        _app_initialized = True
        return APP_FLASK

initialize_services()

REMOTE_TRIGGER_URL = os.environ.get('REMOTE_TRIGGER_URL', "https://render-signal-server.onrender.com/api/check_trigger")
REMOTE_POLLING_INTERVAL = int(os.environ.get('REMOTE_POLLING_INTERVAL', "15"))

REMOTE_SEQUENCE_STEP_KEYS = [
    "STEP1",
    "STEP2",
    "STEP3",
    "STEP4",
    "STEP5",
    "STEP6",
    "STEP7"
]

if PYNVML_AVAILABLE:
    atexit.register(pynvml.nvmlShutdown)
    print("INFO: La fonction de nettoyage pynvml.nvmlShutdown a été enregistrée pour la sortie de l'application.")
def format_duration_seconds(seconds_total: float) -> str:
    if seconds_total is None or seconds_total < 0: return "N/A"
    seconds_total = int(seconds_total)
    hours, remainder = divmod(seconds_total, 3600)
    minutes, seconds = divmod(remainder, 60)
    time_str = ""
    if hours > 0: time_str += f"{hours}h "
    if minutes > 0 or hours > 0: time_str += f"{minutes}m "
    time_str += f"{seconds}s"
    return time_str.strip() if time_str else "0s"

def create_frontend_safe_config(config_dict: dict) -> dict:
    frontend_config = {}
    for step_key, step_data_orig in config_dict.items():
        frontend_step_data = {}
        for key, value in step_data_orig.items():
            if key == "progress_patterns":
                pass
            elif isinstance(value, Path):
                frontend_step_data[key] = str(value)
            elif key == "cmd" and isinstance(value, list):
                frontend_step_data[key] = [str(item) for item in value]
            elif key == "specific_logs" and isinstance(value, list):
                safe_logs = []
                for log_entry in value:
                    safe_entry = log_entry.copy()
                    if 'path' in safe_entry and isinstance(safe_entry['path'], Path):
                        safe_entry['path'] = str(safe_entry['path'])
                    safe_logs.append(safe_entry)
                frontend_step_data[key] = safe_logs
            else:
                frontend_step_data[key] = value
        frontend_config[step_key] = frontend_step_data
    return frontend_config

def execute_csv_download_worker(dropbox_url, timestamp_str, fallback_url=None, original_filename=None):
    LOCAL_DOWNLOADS_DIR.mkdir(parents=True, exist_ok=True)
    
    download_id = f"csv_{uuid.uuid4().hex[:8]}"
    
    download_info = {
        'id': download_id,
        'filename': 'Détermination en cours...',
        'original_url': dropbox_url,
        'url': dropbox_url,
        'url_type': 'dropbox',
        'status': 'pending',
        'progress': 0,
        'message': 'En attente de démarrage...',
        'timestamp': datetime.now(),
        'display_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'csv_timestamp': timestamp_str
    }
    
    CSVService.add_csv_download(download_id, download_info)
    
    def progress_callback(status, progress, message):
        """Callback to update CSVService with download progress."""
        update_kwargs = {
            'progress': progress,
            'message': message,
            'display_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'timestamp': datetime.now()
        }
        CSVService.update_csv_download(download_id, status, **update_kwargs)
    
    try:
        urls_to_try = [dropbox_url]
        if fallback_url and str(fallback_url).strip() and str(fallback_url).strip() != str(dropbox_url).strip():
            urls_to_try.append(str(fallback_url).strip())

        forced_name = str(original_filename).strip() if original_filename else None

        result = None
        last_attempt_url = dropbox_url
        for attempt_url in urls_to_try:
            last_attempt_url = attempt_url
            result = DownloadService.download_dropbox_file(
                url=attempt_url,
                timestamp=timestamp_str,
                output_dir=LOCAL_DOWNLOADS_DIR,
                progress_callback=progress_callback,
                forced_filename=forced_name
            )
            if result and result.success:
                break

        if result and result.success:
            CSVService.update_csv_download(
                download_id,
                'completed',
                progress=100,
                message=result.message,
                filename=result.filename,
                display_timestamp=datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                timestamp=datetime.now()
            )
            
            try:
                CSVService.add_to_download_history_with_timestamp(dropbox_url, timestamp_str)
                if fallback_url and str(fallback_url).strip():
                    CSVService.add_to_download_history_with_timestamp(str(fallback_url).strip(), timestamp_str)
            except Exception as e:
                APP_LOGGER.error(f"Error adding to download history: {e}")
            
            APP_LOGGER.info(f"CSV DOWNLOAD: File '{result.filename}' downloaded successfully ({result.size_bytes} bytes)")
        else:
            CSVService.update_csv_download(
                download_id,
                'failed',
                message=result.message if result else 'N/A',
                filename=result.filename if (result and result.filename) else 'N/A',
                display_timestamp=datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                timestamp=datetime.now()
            )
            APP_LOGGER.error(
                f"CSV DOWNLOAD: Failed - {(result.message if result else 'N/A')} (last_url={last_attempt_url})"
            )
            
    except Exception as e:
        error_msg = f"Unexpected error: {str(e)}"
        APP_LOGGER.error(f"CSV DOWNLOAD: {error_msg}", exc_info=True)
        CSVService.update_csv_download(
            download_id,
            'failed',
            message=error_msg,
            display_timestamp=datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            timestamp=datetime.now()
        )
    
    APP_LOGGER.info(f"CSV DOWNLOAD: Worker for {download_id} completed")

def csv_monitor_service():
    """Service de monitoring Webhook qui s'exécute en arrière-plan."""
    APP_LOGGER.info("WEBHOOK MONITOR: Service démarré.")

    from services.csv_service import CSVService

    while True:
        try:
            workflow_state.update_csv_monitor_status(
                status="checking",
                last_check=datetime.now().isoformat(),
                error=None
            )

            try:
                CSVService._check_csv_for_downloads()
                APP_LOGGER.debug("Webhook monitor check completed successfully")
            except Exception as check_error:
                APP_LOGGER.error(f"Webhook monitor check error: {check_error}")
                workflow_state.update_csv_monitor_status(
                    status="error",
                    last_check=datetime.now().isoformat(),
                    error=str(check_error)
                )
                time.sleep(WEBHOOK_MONITOR_INTERVAL)
                continue

            workflow_state.update_csv_monitor_status(
                status="active",
                last_check=datetime.now().isoformat(),
                error=None
            )

        except Exception as e:
            error_msg = f"Erreur dans le service CSV monitor: {e}"
            APP_LOGGER.error(error_msg, exc_info=True)
            workflow_state.update_csv_monitor_status(
                status="error",
                last_check=datetime.now().isoformat(),
                error=error_msg
            )

        time.sleep(WEBHOOK_MONITOR_INTERVAL)


def run_process_async(step_key: str):
    from services.workflow_service import WorkflowService
    
    APP_LOGGER.info(f"[RUN_PROCESS] Starting execution for {step_key}")

    projects_dir = os.path.join(BASE_PATH_SCRIPTS, 'projets_extraits')
    os.makedirs(projects_dir, exist_ok=True)

    config = workflow_commands_config.get_step_config(step_key)
    if not config:
        APP_LOGGER.error(f"Invalid step_key: {step_key}")
        return
    workflow_state.update_step_status(step_key, 'starting')
    workflow_state.clear_step_log(step_key)
    workflow_state.append_step_log(step_key, f"--- Lancement de: {html.escape(config['display_name'])} ---\n")
    workflow_state.update_step_info(
        step_key,
        return_code=None,
        progress_current=0,
        progress_total=0,
        progress_text='',
        start_time_epoch=time.time(),
        duration_str=None
    )


    
    cmd_str_list = [str(c) for c in config['cmd']]
    temp_json_path_for_tracking = None

    if step_key == "STEP5":
        workflow_state.append_step_log(step_key, "Préparation de l'étape de tracking : recherche des vidéos à traiter...\n")
        try:
            videos_to_process = WorkflowService.prepare_tracking_step(
                BASE_TRACKING_LOG_SEARCH_PATH,
                KEYWORD_FILTER_TRACKING_ENV,
                SUBDIR_FILTER_TRACKING_ENV
            )
            
            if not videos_to_process:
                APP_LOGGER.info(f"{step_key}: No videos require tracking, completing immediately")
                workflow_state.append_step_log(step_key, "Toutes les vidéos candidates semblent déjà traitées (aucun .mp4/.mov/... sans .json trouvé). Étape terminée.\n")
                workflow_state.update_step_info(step_key, status='completed', return_code=0)
                start_time = workflow_state.get_step_field(step_key, 'start_time_epoch')
                workflow_state.set_step_field(step_key, 'duration_str', WorkflowService.calculate_step_duration(start_time))
                return

            temp_json_path_for_tracking = WorkflowService.create_tracking_temp_file(videos_to_process)
            cmd_str_list.extend(["--videos_json_path", str(temp_json_path_for_tracking)])
            workflow_state.append_step_log(step_key, f"{len(videos_to_process)} vidéo(s) ajoutée(s) au lot de traitement.\nLe script gestionnaire va maintenant prendre le relais.\n\n")

        except Exception as e_prep:
            APP_LOGGER.error(f"{step_key}: Preparation failed - {e_prep}", exc_info=True)
            error_msg = f"Erreur lors de la préparation de l'étape de tracking: {e_prep}"
            workflow_state.append_step_log(step_key, html.escape(error_msg))
            workflow_state.update_step_info(step_key, status='failed', return_code=-1)
            return

    workflow_state.append_step_log(step_key, f"Commande: {html.escape(' '.join(cmd_str_list))}\n")
    workflow_state.append_step_log(step_key, f"Dans: {html.escape(str(config['cwd']))}\n\n")
    
    step_progress_patterns = config.get("progress_patterns", {})
    total_pattern_re = step_progress_patterns.get("total")
    current_pattern_re = step_progress_patterns.get("current")
    current_success_line_pattern_re = step_progress_patterns.get("current_success_line_pattern")
    current_item_counter = 0

    try:
        APP_LOGGER.info(f"[SUBPROCESS_DEBUG] {step_key} executing command: {cmd_str_list}")
        APP_LOGGER.info(f"[SUBPROCESS_DEBUG] {step_key} working directory: {config['cwd']}")

        process_env = os.environ.copy()
        process_env["PYTHONIOENCODING"] = "UTF-8"; process_env["PYTHONUTF8"] = "1"

        if step_key == "STEP3":
            try:
                process_env["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
            except Exception as _e:
                APP_LOGGER.warning(f"Unable to set PYTORCH_CUDA_ALLOC_CONF: {_e}")

        if step_key == "STEP4":
            try:
                process_env["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:32"
                process_env["AUDIO_PARTIAL_SUCCESS_OK"] = "1"
            except Exception as _e:
                APP_LOGGER.warning(f"Unable to set PYTORCH_CUDA_ALLOC_CONF for STEP4: {_e}")

        process = subprocess.Popen(
            cmd_str_list, cwd=str(config['cwd']),
            stdout=subprocess.PIPE, stderr=subprocess.STDOUT,
            text=True, bufsize=1, encoding='utf-8', errors='replace', env=process_env
        )

        APP_LOGGER.info(f"[SUBPROCESS_DEBUG] {step_key} subprocess started successfully (PID: {process.pid})")

        workflow_state.set_step_process(step_key, process)
        workflow_state.update_step_status(step_key, 'running')
        running_status_start_time = time.time()

        log_deque = workflow_state.get_step_log_deque(step_key)

        if process.stdout:
            for line in iter(process.stdout.readline, ''):
                line_strip = line.strip()

                if line_strip.startswith("[Progression-MultiLine]"):
                    progress_data = line_strip.replace("[Progression-MultiLine]", "", 1)
                    text_progress = progress_data.replace(" || ", "\n")
                    workflow_state.set_step_field(step_key, 'progress_text', text_progress)
                    continue

                if '\r' in line or '\x1b[' in line or '\033[' in line:
                    continue

                if log_deque is not None:
                    log_deque.append(html.escape(line))
                try:
                    APP_LOGGER.debug(f"[{step_key}] SCRIPT_OUT: {line_strip}")
                except UnicodeEncodeError:
                    APP_LOGGER.debug(f"[{step_key}] SCRIPT_OUT (ascii): {line_strip.encode('ascii', 'replace').decode('ascii')}")

                if total_pattern_re:
                    total_match = total_pattern_re.search(line_strip)
                    if total_match:
                        try:
                            workflow_state.set_step_field(step_key, 'progress_total', int(total_match.group(1)))
                            files_completed = workflow_state.get_step_field(step_key, 'files_completed')
                            if files_completed is None or not isinstance(files_completed, int):
                                workflow_state.set_step_field(step_key, 'files_completed', 0)
                        except (ValueError, IndexError):
                            APP_LOGGER.warning(f"[{step_key}] ProgTotal parse error: {line_strip}")

                if current_pattern_re:
                    current_match = current_pattern_re.search(line_strip)
                    if current_match:
                        try:
                            groups = current_match.groups()

                            if len(groups) >= 3 and groups[0].isdigit() and groups[1].isdigit():
                                current_num = int(groups[0])
                                total_num = int(groups[1])
                                filename = groups[2].strip()
                                workflow_state.set_step_field(step_key, 'progress_current', current_num)
                                if workflow_state.get_step_field(step_key, 'progress_total', 0) == 0:
                                    workflow_state.set_step_field(step_key, 'progress_total', total_num)
                                if filename:
                                    workflow_state.set_step_field(step_key, 'progress_text', html.escape(filename))
                            elif len(groups) >= 1 and step_key == 'STEP3':
                                filename = groups[0].strip()
                                if filename:
                                    workflow_state.set_step_field(step_key, 'progress_text', html.escape(filename))
                                progress_total = workflow_state.get_step_field(step_key, 'progress_total', 0)
                                if progress_total > 0:
                                    files_completed = int(workflow_state.get_step_field(step_key, 'files_completed', max(0, int(workflow_state.get_step_field(step_key, 'progress_current', 0)))))
                                    workflow_state.set_step_field(step_key, 'progress_current', min(progress_total, max(files_completed, 0) + 1))
                                    workflow_state.set_step_field(step_key, 'progress_current_fractional', min(float(progress_total), float(workflow_state.get_step_field(step_key, 'progress_current', 0)) - 0.0 + 0.01))
                            else:
                                filename = groups[0].strip() if len(groups) >= 1 else ""
                                percent = int(groups[1]) if len(groups) >= 2 and str(groups[1]).isdigit() else None
                                if filename:
                                    workflow_state.set_step_field(step_key, 'progress_text', html.escape(filename))
                                progress_total = workflow_state.get_step_field(step_key, 'progress_total', 0)
                                if percent is not None and progress_total > 0:
                                    files_completed = int(workflow_state.get_step_field(step_key, 'files_completed', max(0, int(workflow_state.get_step_field(step_key, 'progress_current', 0)))))
                                    current_file_progress = max(0.0, min(0.99, percent / 100.0))
                                    overall_progress = (files_completed + current_file_progress)
                                    workflow_state.set_step_field(step_key, 'progress_current_fractional', max(0.0, min(float(progress_total), overall_progress)))
                        except (ValueError, IndexError):
                            APP_LOGGER.warning(f"[{step_key}] ProgCurrent parse error: {line_strip}")

                internal_pattern_re = step_progress_patterns.get("internal")
                if internal_pattern_re:
                    internal_match = internal_pattern_re.search(line_strip)
                    if internal_match:
                        try:
                            groups = internal_match.groups()

                            if len(groups) >= 4:
                                current_batch = int(groups[0])
                                total_batches = int(groups[1])
                                percent = int(groups[2])
                                filename = groups[3].strip() if groups[3] else ""
                            elif len(groups) >= 2:
                                filename = groups[0].strip() if groups[0] else ""
                                percent = int(groups[1])
                                current_batch = percent
                                total_batches = 100
                            else:
                                continue

                            progress_total = workflow_state.get_step_field(step_key, 'progress_total', 0)
                            if progress_total > 0:
                                files_completed = int(workflow_state.get_step_field(step_key, 'files_completed', max(0, int(workflow_state.get_step_field(step_key, 'progress_current', 0)))))
                                current_file_progress = max(0.0, min(0.99, percent / 100.0))
                                overall_progress_files = files_completed + current_file_progress
                                overall_progress_files = max(0.0, min(float(progress_total), overall_progress_files))
                                workflow_state.set_step_field(step_key, 'progress_current_fractional', overall_progress_files)

                            if filename:
                                workflow_state.set_step_field(step_key, 'progress_text', html.escape(f"{filename} ({percent}%)"))

                        except (ValueError, IndexError):
                            APP_LOGGER.warning(f"[{step_key}] Internal progress parse error: {line_strip}")

                internal_simple_re = step_progress_patterns.get("internal_simple")
                if internal_simple_re:
                    internal_simple_match = internal_simple_re.search(line_strip)
                    if internal_simple_match:
                        try:
                            batches = int(internal_simple_match.group(1))
                            filename = internal_simple_match.group(2).strip() if internal_simple_match.group(2) else ""
                            progress_total = workflow_state.get_step_field(step_key, 'progress_total', 0)
                            if progress_total > 0:
                                files_completed = int(workflow_state.get_step_field(step_key, 'files_completed', max(0, int(workflow_state.get_step_field(step_key, 'progress_current', 0)))))
                                current_file_progress = 0.01
                                overall_progress_files = files_completed + current_file_progress
                                overall_progress_files = max(0.0, min(float(progress_total), overall_progress_files))
                                workflow_state.set_step_field(step_key, 'progress_current_fractional', overall_progress_files)
                            if filename:
                                workflow_state.set_step_field(step_key, 'progress_text', html.escape(filename))
                        except Exception:
                            APP_LOGGER.warning(f"[{step_key}] Internal simple progress parse error: {line_strip}")

                if current_success_line_pattern_re:
                    success_match = current_success_line_pattern_re.search(line_strip)
                    if success_match:
                        current_item_counter += 1
                        workflow_state.set_step_field(step_key, 'progress_current', current_item_counter)
                        workflow_state.set_step_field(step_key, 'files_completed', current_item_counter)
                        workflow_state.set_step_field(step_key, 'progress_current_fractional', None)
                        if step_progress_patterns.get("current_item_text_from_success_line"):
                            try:
                                workflow_state.set_step_field(step_key, 'progress_text', html.escape(success_match.group(1).strip()))
                            except IndexError:
                                pass
            
            process.stdout.close()
        subprocess_start_time = time.time()
        process.wait()
        subprocess_duration = time.time() - subprocess_start_time

        APP_LOGGER.info(f"[SUBPROCESS_DEBUG] {step_key} subprocess completed in {subprocess_duration:.2f} seconds (return_code: {process.returncode})")

        running_status_duration = time.time() - running_status_start_time
        min_running_time = 0.6

        if running_status_duration < min_running_time:
            sleep_time = min_running_time - running_status_duration
            APP_LOGGER.info(f"[TIMING_FIX] {step_key} ensuring minimum running time: sleeping {sleep_time:.3f}s (total running time will be {min_running_time:.3f}s)")
            time.sleep(sleep_time)

        workflow_state.update_step_info(
            step_key,
            return_code=process.returncode,
            status='completed' if process.returncode == 0 else 'failed'
        )
        log_suffix = "terminé avec succès" if process.returncode == 0 else f"a échoué (code: {process.returncode})"
        workflow_state.append_step_log(step_key, f"\n--- {html.escape(config['display_name'])} {log_suffix} ---")

        status = workflow_state.get_step_status(step_key)
        progress_total = workflow_state.get_step_field(step_key, 'progress_total', 0)
        progress_current = workflow_state.get_step_field(step_key, 'progress_current', 0)
        if status == 'completed' and progress_total > 0 and progress_current < progress_total:
            workflow_state.set_step_field(step_key, 'progress_current', progress_total)
        progress_text = workflow_state.get_step_field(step_key, 'progress_text', '')
        if not progress_text and status == 'completed':
            workflow_state.set_step_field(step_key, 'progress_text', "Terminé")
    except FileNotFoundError:
        APP_LOGGER.error(f"[EARLY_RETURN_DEBUG] {step_key} failing - executable not found: {cmd_str_list[0] if cmd_str_list else 'N/A'}")

        error_msg = f"Erreur: Exécutable non trouvé pour {step_key}: {cmd_str_list[0]}"
        workflow_state.append_step_log(step_key, html.escape(error_msg))
        workflow_state.update_step_info(step_key, status='failed', return_code=-1)
        APP_LOGGER.error(error_msg)
    except Exception as e:
        APP_LOGGER.error(f"[EARLY_RETURN_DEBUG] {step_key} failing - general exception: {e}")

        error_msg = f"Erreur exécution {step_key}: {str(e)}"
        workflow_state.append_step_log(step_key, html.escape(error_msg))
        workflow_state.update_step_info(step_key, status='failed', return_code=-1)
        APP_LOGGER.error(f"Exception run_process_async pour {step_key}: {e}", exc_info=True)
    finally:
        start_time = workflow_state.get_step_field(step_key, 'start_time_epoch')
        workflow_state.set_step_field(step_key, 'duration_str', WorkflowService.calculate_step_duration(start_time))
        workflow_state.set_step_process(step_key, None)
        if temp_json_path_for_tracking and temp_json_path_for_tracking.exists():
            try:
                os.remove(temp_json_path_for_tracking)
                APP_LOGGER.info(f"Fichier temporaire de tracking '{temp_json_path_for_tracking.name}' supprimé.")
            except Exception as e_clean:
                APP_LOGGER.error(f"Impossible de supprimer le fichier temporaire de tracking '{temp_json_path_for_tracking.name}': {e_clean}")


def execute_step_sequence_worker(steps_to_run_list: list, sequence_type: str ="Custom"):
    """Execute a sequence of workflow steps.
    
    This function has been migrated to use WorkflowState for sequence management.
    
    Args:
        steps_to_run_list: List of step keys to execute in order
        sequence_type: Type of sequence ('Full', 'Remote', 'Custom', etc.)
    """
    APP_LOGGER.info("🔥🔥🔥 [SEQUENCE_WORKER_TEST] UPDATED SEQUENCE WORKER WITH DEBUGGING IS RUNNING! 🔥🔥🔥")
    
    if sequence_type != "InternalPollingCheck" and workflow_state.is_sequence_running():
        APP_LOGGER.warning(f"{sequence_type.upper()} SEQUENCE: Tentative de lancement alors qu'une séquence est déjà en cours.")
        return
    
    if not workflow_state.start_sequence(sequence_type):
        APP_LOGGER.warning(f"{sequence_type.upper()} SEQUENCE: Could not start - already running")
        return
    
    APP_LOGGER.info(f"{sequence_type.upper()} SEQUENCE: Séquence démarrée.")
    all_steps_succeeded = True; sequence_summary_data = []
    try:
        APP_LOGGER.info(f"{sequence_type.upper()} SEQUENCE: Thread démarré pour {len(steps_to_run_list)} étapes: {steps_to_run_list}")

        APP_LOGGER.info(f"[CACHE_CLEAR_TEST] *** UPDATED CODE IS RUNNING - CACHE CLEARED SUCCESSFULLY ***")
        for i, step_key in enumerate(steps_to_run_list):
            step_config = workflow_commands_config.get_step_config(step_key)
            if not step_config:
                APP_LOGGER.error(f"{sequence_type.upper()} SEQUENCE: Clé invalide '{step_key}'. Interruption.")
                all_steps_succeeded = False; sequence_summary_data.append({"name": f"Étape Invalide ({html.escape(step_key)})", "status": "Erreur de config", "duration": "0s", "success": False}); break
            step_display_name = step_config['display_name']
            APP_LOGGER.info(f"{sequence_type.upper()} SEQUENCE: Lancement étape {i+1}/{len(steps_to_run_list)}: '{step_display_name}' ({step_key})")
            
            workflow_state.update_step_info(
                step_key,
                status='idle',
                progress_current=0,
                progress_total=0,
                progress_text='',
                start_time_epoch=None,
                duration_str=None
            )

            current_status = workflow_state.get_step_status(step_key)

            try:
                run_process_async(step_key)
                final_status = workflow_state.get_step_status(step_key)
                return_code = workflow_state.get_step_field(step_key, 'return_code')
                APP_LOGGER.info(f"[SEQUENCE_DEBUG] run_process_async completed for {step_key} (final status: {final_status}, return_code: {return_code})")
            except Exception as e:
                APP_LOGGER.error(f"[SEQUENCE_DEBUG] Exception in run_process_async for {step_key}: {e}", exc_info=True)
                workflow_state.update_step_info(step_key, status='failed', return_code=-1)
            
            step_info = workflow_state.get_step_info(step_key)
            duration_str_step = step_info.get('duration_str', 'N/A')
            if step_info['status'] == 'completed':
                sequence_summary_data.append({"name": html.escape(step_display_name), "status": "Réussie", "duration": duration_str_step, "success": True})
            else:
                all_steps_succeeded = False
                sequence_summary_data.append({"name": html.escape(step_display_name), "status": f"Échouée ({step_info['status']})", "duration": duration_str_step, "success": False})
                APP_LOGGER.error(f"{sequence_type.upper()} SEQUENCE: Étape '{html.escape(step_display_name)}' Échouée. Interruption.")
                break 
        final_overall_status_text = "Terminée avec succès" if all_steps_succeeded else "Terminée avec erreurs"
        summary_log = [f"{s['name']}: {s['status']} ({s['duration']})" for s in sequence_summary_data]
        full_summary_log_text = f"Séquence {sequence_type} {final_overall_status_text}. Détails: " + " | ".join(summary_log)
        APP_LOGGER.info(f"{sequence_type.upper()} SEQUENCE: {full_summary_log_text}")
        
        workflow_state.complete_sequence(success=all_steps_succeeded, message=full_summary_log_text, sequence_type=sequence_type)
    except Exception as e_seq:
        APP_LOGGER.error(f"{sequence_type.upper()} SEQUENCE: Erreur inattendue dans le worker de séquence: {e_seq}", exc_info=True)
        all_steps_succeeded = False
        workflow_state.complete_sequence(success=False, message=f"Erreur critique durant la séquence: {e_seq}", sequence_type=sequence_type)
    finally:
        if workflow_state.is_sequence_running():
            workflow_state.complete_sequence(success=False, message="Séquence terminée de façon inattendue", sequence_type=sequence_type)
        APP_LOGGER.info(f"{sequence_type.upper()} SEQUENCE: Séquence terminée.")

def run_full_sequence_from_remote():
    """Launch full sequence from remote trigger.
    
    Migrated to use WorkflowState for sequence management.
    """
    APP_LOGGER.info("REMOTE TRIGGER: Demande de lancement de la séquence complète reçue.")
    
    if workflow_state.is_sequence_running():
        APP_LOGGER.warning("REMOTE TRIGGER: Séquence complète non lancée, une autre séquence est déjà en cours.")
        return
    
    seq_thread = threading.Thread(target=execute_step_sequence_worker, args=(list(REMOTE_SEQUENCE_STEP_KEYS), "Remote"))
    seq_thread.daemon = True
    seq_thread.start()

def poll_remote_trigger():
    """Poll remote trigger URL for pending commands.
    
    Migrated to use WorkflowState for sequence status checking.
    """
    APP_LOGGER.info(f"REMOTE POLLER: Démarré. Interrogation de {REMOTE_TRIGGER_URL} toutes les {REMOTE_POLLING_INTERVAL}s.")
    
    while True:
        time.sleep(REMOTE_POLLING_INTERVAL)
        
        if workflow_state.is_sequence_running():
            APP_LOGGER.debug("REMOTE POLLER: Une séquence est marquée comme en cours, attente.")
            continue
        
        if workflow_state.is_any_step_running():
            APP_LOGGER.debug("REMOTE POLLER: Une étape individuelle est active. Attente.")
            continue
        
        try:
            APP_LOGGER.debug(f"REMOTE POLLER: Vérification de {REMOTE_TRIGGER_URL}.")
            response = requests.get(REMOTE_TRIGGER_URL, timeout=10)
            response.raise_for_status()
            data = response.json()
            
            if data.get('command_pending'):
                APP_LOGGER.info(f"REMOTE POLLER: Commande reçue! Payload: {data.get('payload')}")
                
                if not workflow_state.is_sequence_running():
                    run_full_sequence_from_remote()
                else:
                    APP_LOGGER.warning("REMOTE POLLER: Signal reçu, mais une séquence est déjà en cours.")
            else:
                APP_LOGGER.debug("REMOTE POLLER: Aucune commande en attente.")
        except requests.exceptions.RequestException as e:
            APP_LOGGER.warning(f"REMOTE POLLER: Erreur communication {REMOTE_TRIGGER_URL}: {e}")
        except Exception as e_poll:
            APP_LOGGER.error(f"REMOTE POLLER: Erreur inattendue: {e_poll}", exc_info=True)

@APP_FLASK.route('/test-slideshow-fixes')
def test_slideshow_fixes():
    """Serve the slideshow fixes test page."""
    APP_LOGGER.debug("Serving slideshow fixes test page")

    try:
        with open('test_dom_slideshow_fixes.html', 'r', encoding='utf-8') as f:
            test_content = f.read()

        return test_content, 200, {'Content-Type': 'text/html; charset=utf-8'}

    except Exception as e:
        APP_LOGGER.error(f"Error serving test page: {e}")
        return f"Error loading test page: {e}", 500

@APP_FLASK.route('/favicon.ico')
def favicon():
    """
    Handle favicon.ico requests to prevent 404 errors in browser console.
    Returns a 204 No Content response since we don't have a favicon file.
    """
    APP_LOGGER.debug("Favicon requested - returning 204 No Content")
    return '', 204

def get_current_workflow_status_summary():
    """Get comprehensive workflow status summary including CSV downloads.
    
    Migrated to use WorkflowState for all state access.
    """
    overall_status_code_val = "idle"
    overall_status_text_display_val = "Prêt et en attente."
    current_step_name_val = None
    progress_current_val = 0
    progress_total_val = 0
    active_step_key_found = None
    
    is_sequence_globally_active = workflow_state.is_sequence_running()
    
    if is_sequence_globally_active:
        sequence_outcome = workflow_state.get_sequence_outcome()
        active_sequence_type = sequence_outcome.get("type", "Inconnue") if sequence_outcome.get("status", "").startswith("running_") else "Inconnue"
        
        for step_key_seq in REMOTE_SEQUENCE_STEP_KEYS:
            step_status = workflow_state.get_step_status(step_key_seq)
            if step_status in ['running', 'starting']:
                active_step_key_found = step_key_seq
                overall_status_code_val = f"sequence_running_{active_sequence_type.lower()}"
                break
    
    if not active_step_key_found:
        all_steps_info = workflow_state.get_all_steps_info()
        for step_key_ind, info_ind in all_steps_info.items():
            if info_ind['status'] in ['running', 'starting']:
                active_step_key_found = step_key_ind
                overall_status_code_val = f"step_running_{step_key_ind}"
                break
    
    if active_step_key_found:
        active_step_info = workflow_state.get_step_info(active_step_key_found)
        active_step_config = workflow_commands_config.get_step_config(active_step_key_found)
        current_step_name_val = active_step_config['display_name'] if active_step_config else 'Étape inconnue'
        progress_current_val = active_step_info['progress_current']
        progress_total_val = active_step_info['progress_total']
        
        if overall_status_code_val.startswith("sequence_running"):
            seq_type_disp = overall_status_code_val.split("_")[-1].capitalize()
            overall_status_text_display_val = f"Séquence {seq_type_disp} - En cours: {current_step_name_val}"
        elif overall_status_code_val.startswith("step_running"):
            overall_status_text_display_val = f"En cours: {current_step_name_val}"
    else:
        sequence_outcome = workflow_state.get_sequence_outcome()
        if sequence_outcome and sequence_outcome.get("timestamp"):
            last_seq_time_str = sequence_outcome["timestamp"]
            if last_seq_time_str.endswith('Z'):
                last_seq_time_str = last_seq_time_str[:-1] + '+00:00'
            try:
                last_seq_dt = datetime.fromisoformat(last_seq_time_str)
                if last_seq_dt.tzinfo is None:
                    last_seq_dt = last_seq_dt.replace(tzinfo=timezone.utc)
                time_since_last_seq = datetime.now(timezone.utc) - last_seq_dt
                
                if time_since_last_seq < timedelta(hours=1):
                    seq_type_display = sequence_outcome.get('type', 'N/A')
                    if sequence_outcome.get("status") == "success":
                        overall_status_code_val = "completed_success_recent"
                        overall_status_text_display_val = f"Terminé avec succès (Séquence {seq_type_display})"
                        current_step_name_val = sequence_outcome.get("message", "Détails non disponibles.")
                    elif sequence_outcome.get("status") == "error":
                        overall_status_code_val = "completed_error_recent"
                        overall_status_text_display_val = f"Terminé avec erreur(s) (Séquence {seq_type_display})"
                        current_step_name_val = sequence_outcome.get("message", "Détails non disponibles.")
                    elif sequence_outcome.get("status", "").startswith("running_"):
                        seq_type_running = sequence_outcome.get("type", "N/A")
                        overall_status_code_val = f"sequence_starting_{seq_type_running.lower()}"
                        overall_status_text_display_val = f"Démarrage Séquence {seq_type_running}..."
                else:
                    overall_status_code_val = "idle_after_completion"
                    overall_status_text_display_val = "Prêt (dernière opération terminée il y a >1h)."
            except ValueError as e_ts:
                APP_LOGGER.warning(f"Error parsing sequence outcome timestamp '{sequence_outcome['timestamp']}': {e_ts}")
                overall_status_code_val = "idle_parse_error"
                overall_status_text_display_val = "Prêt (erreur parsing dernier statut)."
        else:
            overall_status_code_val = "idle_initial"
            overall_status_text_display_val = "Prêt et en attente (jamais exécuté)."

    active_list_csv = workflow_state.get_active_csv_downloads_list()
    kept_list_csv = workflow_state.get_kept_csv_downloads_list()

    all_csv_downloads_intermediate = active_list_csv + kept_list_csv
    all_csv_downloads_intermediate.sort(key=lambda x: x.get('timestamp', datetime.min), reverse=True)

    recent_downloads_summary_val = []
    for item_csv in all_csv_downloads_intermediate[:5]:
        recent_downloads_summary_val.append({
            "filename": item_csv.get('filename', 'N/A'),
            "status": item_csv.get('status', 'N/A'),
            "timestamp": item_csv.get('display_timestamp', 'N/A'),
            "csv_timestamp": item_csv.get('csv_timestamp', 'N/A')
        })
    status_text_detail_val = datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')

    return {
        "overall_status_code": overall_status_code_val,
        "overall_status_text_display": overall_status_text_display_val,
        "current_step_name": current_step_name_val,
        "status_text_detail": status_text_detail_val,
        "progress_current": progress_current_val,
        "progress_total": progress_total_val,
        "recent_downloads": recent_downloads_summary_val,
        "last_updated_utc": datetime.now(timezone.utc).isoformat(),
        "last_sequence_summary": workflow_state.get_sequence_outcome()
    }


if __name__ == '__main__':
    init_app()

    APP_FLASK.run(
        debug=config.DEBUG,
        host=config.HOST,
        port=config.PORT,
        threaded=True,
        use_reloader=False
    )
```

## File: config/settings.py
```python
"""
Centralized configuration management for workflow_mediapipe.

This module provides environment-based configuration management
following the project's development guidelines.
"""

import os
import logging
import subprocess
from pathlib import Path
from dataclasses import dataclass, field
from typing import Optional, List

logger = logging.getLogger(__name__)


def _parse_bool(raw: Optional[str], default: bool) -> bool:
    if raw is None:
        return default
    return raw.strip().lower() in {"1", "true", "yes", "y", "on"}


def _parse_optional_int(raw: Optional[str]) -> Optional[int]:
    if raw is None:
        return None
    raw = raw.strip()
    if not raw:
        return None
    try:
        return int(raw)
    except Exception:
        return None


def _parse_optional_positive_int(raw: Optional[str]) -> Optional[int]:
    value = _parse_optional_int(raw)
    if value is None:
        return None
    if value <= 0:
        return None
    return value


def _parse_csv_list(raw: Optional[str]) -> List[str]:
    if raw is None:
        return []
    parts = [p.strip() for p in raw.split(",")]
    return [p for p in parts if p]


@dataclass
class Config:
    """
    Centralized configuration class for the workflow_mediapipe application.
    
    All configuration values are loaded from environment variables with
    sensible defaults to maintain backward compatibility.
    """
    
    # Flask Application Settings
    SECRET_KEY: str = os.environ.get('FLASK_SECRET_KEY', 'dev-key-change-in-production')
    DEBUG: bool = os.environ.get('DEBUG', 'false').lower() == 'true'
    HOST: str = os.environ.get('FLASK_HOST', '0.0.0.0')
    PORT: int = int(os.environ.get('FLASK_PORT', '5000'))
    
    # Security Tokens (loaded from environment)
    INTERNAL_WORKER_TOKEN: Optional[str] = os.environ.get('INTERNAL_WORKER_COMMS_TOKEN')
    RENDER_REGISTER_TOKEN: Optional[str] = os.environ.get('RENDER_REGISTER_TOKEN')
    
    # Webhook JSON Source (single data source for monitoring)
    WEBHOOK_JSON_URL: str = os.environ.get(
        'WEBHOOK_JSON_URL',
        'https://webhook.kidpixel.fr/data/webhook_links.json'
    )
    WEBHOOK_TIMEOUT: int = int(os.environ.get('WEBHOOK_TIMEOUT', '10'))
    WEBHOOK_CACHE_TTL: int = int(os.environ.get('WEBHOOK_CACHE_TTL', '60'))
    WEBHOOK_MONITOR_INTERVAL: int = int(os.environ.get('WEBHOOK_MONITOR_INTERVAL', '15'))
    
    # Directory Configuration
    BASE_PATH_SCRIPTS: Path = Path(os.environ.get(
        'BASE_PATH_SCRIPTS_ENV', 
        os.path.dirname(os.path.abspath(__file__ + '/../'))
    ))
    CACHE_ROOT_DIR: Path = Path(os.environ.get('CACHE_ROOT_DIR', '/mnt/cache'))
    LOCAL_DOWNLOADS_DIR: Path = Path(os.environ.get(
        'LOCAL_DOWNLOADS_DIR', 
        Path.home() / 'Téléchargements'
    ))
    DISABLE_EXPLORER_OPEN: bool = _parse_bool(os.environ.get('DISABLE_EXPLORER_OPEN'), default=False)
    ENABLE_EXPLORER_OPEN: bool = _parse_bool(os.environ.get('ENABLE_EXPLORER_OPEN'), default=False)
    DOWNLOAD_HISTORY_SHARED_GROUP: Optional[str] = os.environ.get('DOWNLOAD_HISTORY_SHARED_GROUP')
    DOWNLOAD_HISTORY_DB_PATH: Path = Path(os.environ.get('DOWNLOAD_HISTORY_DB_PATH', ''))
    # LOGS_DIR is normalized in __post_init__ to be absolute under BASE_PATH_SCRIPTS by default.
    # If LOGS_DIR is set in env and is relative, it will be resolved against BASE_PATH_SCRIPTS.
    LOGS_DIR: Path = Path(os.environ.get('LOGS_DIR', ''))
    # Virtual environments base directory (defaults to project root if not set)
    VENV_BASE_DIR: Optional[Path] = Path(os.environ.get('VENV_BASE_DIR', '')) if os.environ.get('VENV_BASE_DIR') else None
    # Projects directory for visualization/timeline features
    PROJECTS_DIR: Path = Path(os.environ.get('PROJECTS_DIR', '')) if os.environ.get('PROJECTS_DIR') else None
    # Archives directory for persistent analysis results (timeline)
    ARCHIVES_DIR: Path = Path(os.environ.get('ARCHIVES_DIR', '')) if os.environ.get('ARCHIVES_DIR') else None
    
    # Python Environment Configuration
    PYTHON_VENV_EXE: str = os.environ.get('PYTHON_VENV_EXE_ENV', '')
    
    # Processing Configuration
    MAX_CPU_WORKERS: int = int(os.environ.get(
        'MAX_CPU_WORKERS', 
        str(max(1, os.cpu_count() - 2 if os.cpu_count() else 2))
    ))
    
    # Polling Intervals (in milliseconds for frontend, seconds for backend)
    POLLING_INTERVAL: int = int(os.environ.get('POLLING_INTERVAL', '1000'))
    LOCAL_DOWNLOAD_POLLING_INTERVAL: int = int(os.environ.get('LOCAL_DOWNLOAD_POLLING_INTERVAL', '3000'))

    SYSTEM_MONITOR_POLLING_INTERVAL: int = int(os.environ.get('SYSTEM_MONITOR_POLLING_INTERVAL', '5000'))
    
    # MediaPipe Configuration
    MP_LANDMARKER_MIN_DETECTION_CONFIDENCE: float = float(os.environ.get(
        'MP_LANDMARKER_MIN_DETECTION_CONFIDENCE', '0.5'
    ))
    MP_LANDMARKER_MIN_TRACKING_CONFIDENCE: float = float(os.environ.get(
        'MP_LANDMARKER_MIN_TRACKING_CONFIDENCE', '0.5'
    ))
    
    # GPU Configuration
    ENABLE_GPU_MONITORING: bool = os.environ.get('ENABLE_GPU_MONITORING', 'true').lower() == 'true'
    
    # Lemonfox API Configuration (STEP4 alternative)
    LEMONFOX_API_KEY: Optional[str] = os.environ.get('LEMONFOX_API_KEY')
    LEMONFOX_TIMEOUT_SEC: int = int(os.environ.get('LEMONFOX_TIMEOUT_SEC', '300'))
    LEMONFOX_EU_DEFAULT: bool = os.environ.get('LEMONFOX_EU_DEFAULT', '0') == '1'

    LEMONFOX_DEFAULT_LANGUAGE: Optional[str] = os.environ.get("LEMONFOX_DEFAULT_LANGUAGE")
    LEMONFOX_DEFAULT_PROMPT: Optional[str] = os.environ.get("LEMONFOX_DEFAULT_PROMPT")
    LEMONFOX_SPEAKER_LABELS_DEFAULT: bool = _parse_bool(
        os.environ.get("LEMONFOX_SPEAKER_LABELS_DEFAULT"),
        default=True,
    )
    LEMONFOX_DEFAULT_MIN_SPEAKERS: Optional[int] = _parse_optional_int(
        os.environ.get("LEMONFOX_DEFAULT_MIN_SPEAKERS")
    )
    LEMONFOX_DEFAULT_MAX_SPEAKERS: Optional[int] = _parse_optional_int(
        os.environ.get("LEMONFOX_DEFAULT_MAX_SPEAKERS")
    )
    LEMONFOX_TIMESTAMP_GRANULARITIES: List[str] = field(
        default_factory=lambda: _parse_csv_list(os.environ.get("LEMONFOX_TIMESTAMP_GRANULARITIES", "word"))
    )
    LEMONFOX_SPEECH_GAP_FILL_SEC: float = float(os.environ.get("LEMONFOX_SPEECH_GAP_FILL_SEC", "0.15"))
    LEMONFOX_SPEECH_MIN_ON_SEC: float = float(os.environ.get("LEMONFOX_SPEECH_MIN_ON_SEC", "0.0"))
    LEMONFOX_MAX_UPLOAD_MB: Optional[int] = _parse_optional_positive_int(
        os.environ.get("LEMONFOX_MAX_UPLOAD_MB")
    )
    LEMONFOX_ENABLE_TRANSCODE: bool = _parse_bool(
        os.environ.get("LEMONFOX_ENABLE_TRANSCODE"),
        default=False,
    )
    LEMONFOX_TRANSCODE_AUDIO_CODEC: str = os.environ.get("LEMONFOX_TRANSCODE_AUDIO_CODEC", "aac")
    LEMONFOX_TRANSCODE_BITRATE_KBPS: int = int(os.environ.get("LEMONFOX_TRANSCODE_BITRATE_KBPS", "96"))

    STEP4_USE_LEMONFOX: bool = os.environ.get('STEP4_USE_LEMONFOX', '0') == '1'
    
    # STEP5 Object Detection Configuration
    # Model selection for fallback object detection when face detection fails (MediaPipe only)
    STEP5_OBJECT_DETECTOR_MODEL: str = os.environ.get(
        'STEP5_OBJECT_DETECTOR_MODEL',
        'efficientdet_lite2'  # Default: current baseline, backward compatible
    )
    STEP5_OBJECT_DETECTOR_MODEL_PATH: Optional[str] = os.environ.get('STEP5_OBJECT_DETECTOR_MODEL_PATH')
    STEP5_ENABLE_OBJECT_DETECTION: bool = os.environ.get('STEP5_ENABLE_OBJECT_DETECTION', '0') == '1'
    
    # STEP5 Performance Optimizations (opencv_yunet_pyfeat)
    STEP5_ENABLE_PROFILING: bool = os.environ.get('STEP5_ENABLE_PROFILING', '0') == '1'
    STEP5_ONNX_INTRA_OP_THREADS: int = int(os.environ.get('STEP5_ONNX_INTRA_OP_THREADS', '2'))
    STEP5_ONNX_INTER_OP_THREADS: int = int(os.environ.get('STEP5_ONNX_INTER_OP_THREADS', '1'))
    STEP5_BLENDSHAPES_THROTTLE_N: int = int(os.environ.get('STEP5_BLENDSHAPES_THROTTLE_N', '1'))  # 1 = every frame (no throttling)
    STEP5_YUNET_MAX_WIDTH: int = int(os.environ.get('STEP5_YUNET_MAX_WIDTH', '640'))  # Max width for YuNet detection (coordinates rescaled)

    STEP5_OPENCV_MAX_FACES: Optional[int] = _parse_optional_positive_int(
        os.environ.get('STEP5_OPENCV_MAX_FACES')
    )
    STEP5_OPENCV_JAWOPEN_SCALE: float = float(os.environ.get('STEP5_OPENCV_JAWOPEN_SCALE', '1.0'))

    STEP5_MEDIAPIPE_MAX_FACES: Optional[int] = _parse_optional_positive_int(
        os.environ.get('STEP5_MEDIAPIPE_MAX_FACES')
    )
    STEP5_MEDIAPIPE_JAWOPEN_SCALE: float = float(os.environ.get('STEP5_MEDIAPIPE_JAWOPEN_SCALE', '1.0'))
    STEP5_MEDIAPIPE_MAX_WIDTH: Optional[int] = _parse_optional_positive_int(
        os.environ.get('STEP5_MEDIAPIPE_MAX_WIDTH')
    )

    # STEP5 OpenSeeFace Engine Configuration
    # Lightweight CPU tracking via ONNX Runtime (OpenSeeFace-style models)
    STEP5_OPENSEEFACE_MODELS_DIR: Optional[str] = os.environ.get('STEP5_OPENSEEFACE_MODELS_DIR')
    STEP5_OPENSEEFACE_MODEL_ID: int = int(os.environ.get('STEP5_OPENSEEFACE_MODEL_ID', '1'))
    STEP5_OPENSEEFACE_DETECTION_MODEL_PATH: Optional[str] = os.environ.get('STEP5_OPENSEEFACE_DETECTION_MODEL_PATH')
    STEP5_OPENSEEFACE_LANDMARK_MODEL_PATH: Optional[str] = os.environ.get('STEP5_OPENSEEFACE_LANDMARK_MODEL_PATH')
    STEP5_OPENSEEFACE_DETECT_EVERY_N: int = int(os.environ.get('STEP5_OPENSEEFACE_DETECT_EVERY_N', '1'))
    STEP5_OPENSEEFACE_MAX_WIDTH: int = int(
        os.environ.get('STEP5_OPENSEEFACE_MAX_WIDTH')
        or os.environ.get('STEP5_YUNET_MAX_WIDTH', '640')
    )
    STEP5_OPENSEEFACE_DETECTION_THRESHOLD: float = float(os.environ.get('STEP5_OPENSEEFACE_DETECTION_THRESHOLD', '0.6'))
    STEP5_OPENSEEFACE_MAX_FACES: int = int(os.environ.get('STEP5_OPENSEEFACE_MAX_FACES', '1'))
    STEP5_OPENSEEFACE_JAWOPEN_SCALE: float = float(os.environ.get('STEP5_OPENSEEFACE_JAWOPEN_SCALE', '1.0'))

    STEP5_EOS_ENV_PYTHON: Optional[str] = os.environ.get('STEP5_EOS_ENV_PYTHON')
    STEP5_EOS_MODELS_DIR: Optional[str] = os.environ.get('STEP5_EOS_MODELS_DIR')
    STEP5_EOS_SFM_MODEL_PATH: Optional[str] = os.environ.get('STEP5_EOS_SFM_MODEL_PATH')
    STEP5_EOS_EXPRESSION_BLENDSHAPES_PATH: Optional[str] = os.environ.get('STEP5_EOS_EXPRESSION_BLENDSHAPES_PATH')
    STEP5_EOS_LANDMARK_MAPPER_PATH: Optional[str] = os.environ.get('STEP5_EOS_LANDMARK_MAPPER_PATH')
    STEP5_EOS_EDGE_TOPOLOGY_PATH: Optional[str] = os.environ.get('STEP5_EOS_EDGE_TOPOLOGY_PATH')
    STEP5_EOS_MODEL_CONTOUR_PATH: Optional[str] = os.environ.get('STEP5_EOS_MODEL_CONTOUR_PATH')
    STEP5_EOS_CONTOUR_LANDMARKS_PATH: Optional[str] = os.environ.get('STEP5_EOS_CONTOUR_LANDMARKS_PATH')
    STEP5_EOS_FIT_EVERY_N: int = int(os.environ.get('STEP5_EOS_FIT_EVERY_N', '1'))
    STEP5_EOS_MAX_FACES: Optional[int] = _parse_optional_positive_int(
        os.environ.get('STEP5_EOS_MAX_FACES')
    )
    STEP5_EOS_MAX_WIDTH: int = int(os.environ.get('STEP5_EOS_MAX_WIDTH', '1280'))
    STEP5_EOS_JAWOPEN_SCALE: float = float(os.environ.get('STEP5_EOS_JAWOPEN_SCALE', '1.0'))
    
    def __post_init__(self):
        """Post-initialization to ensure paths are Path objects and create directories."""
        # Ensure all path attributes are Path objects
        if isinstance(self.BASE_PATH_SCRIPTS, str):
            self.BASE_PATH_SCRIPTS = Path(self.BASE_PATH_SCRIPTS)
        if isinstance(self.CACHE_ROOT_DIR, str):
            self.CACHE_ROOT_DIR = Path(self.CACHE_ROOT_DIR)
        if isinstance(self.LOCAL_DOWNLOADS_DIR, str):
            self.LOCAL_DOWNLOADS_DIR = Path(self.LOCAL_DOWNLOADS_DIR)
        if isinstance(self.LOGS_DIR, str):
            self.LOGS_DIR = Path(self.LOGS_DIR)

        if isinstance(self.DOWNLOAD_HISTORY_DB_PATH, str):
            self.DOWNLOAD_HISTORY_DB_PATH = Path(self.DOWNLOAD_HISTORY_DB_PATH)
        
        # Default VENV_BASE_DIR to BASE_PATH_SCRIPTS if not set
        if self.VENV_BASE_DIR is None or (isinstance(self.VENV_BASE_DIR, str) and not self.VENV_BASE_DIR):
            self.VENV_BASE_DIR = self.BASE_PATH_SCRIPTS
        elif isinstance(self.VENV_BASE_DIR, str):
            self.VENV_BASE_DIR = Path(self.VENV_BASE_DIR)

        # Resolve PYTHON_VENV_EXE via VENV_BASE_DIR logic.
        # If PYTHON_VENV_EXE_ENV is provided and is relative, resolve it against VENV_BASE_DIR.
        if not self.PYTHON_VENV_EXE:
            self.PYTHON_VENV_EXE = str(self.get_venv_python("env"))
        else:
            python_exe_path = Path(self.PYTHON_VENV_EXE)
            if not python_exe_path.is_absolute():
                self.PYTHON_VENV_EXE = str((self.VENV_BASE_DIR / python_exe_path).resolve())
        
        # Normalize LOGS_DIR to avoid CWD-dependent side effects when importing config from step scripts.
        # Default to <BASE_PATH_SCRIPTS>/logs if not provided. If provided and relative, make it absolute
        # under BASE_PATH_SCRIPTS. This prevents accidental creation of logs under working directories
        # like 'projets_extraits/logs' when steps run with a different CWD.
        if (not str(self.LOGS_DIR)) or (str(self.LOGS_DIR).strip() == '.'):
            self.LOGS_DIR = (self.BASE_PATH_SCRIPTS / 'logs').resolve()
        elif not self.LOGS_DIR.is_absolute():
            self.LOGS_DIR = (self.BASE_PATH_SCRIPTS / self.LOGS_DIR).resolve()

        if (not str(self.DOWNLOAD_HISTORY_DB_PATH)) or (str(self.DOWNLOAD_HISTORY_DB_PATH).strip() == '.'):
            self.DOWNLOAD_HISTORY_DB_PATH = (self.BASE_PATH_SCRIPTS / 'download_history.sqlite3').resolve()
        elif not self.DOWNLOAD_HISTORY_DB_PATH.is_absolute():
            self.DOWNLOAD_HISTORY_DB_PATH = (self.BASE_PATH_SCRIPTS / self.DOWNLOAD_HISTORY_DB_PATH).resolve()

        if (not str(self.CACHE_ROOT_DIR)) or (str(self.CACHE_ROOT_DIR).strip() == '.'):
            self.CACHE_ROOT_DIR = Path('/mnt/cache')
        elif not self.CACHE_ROOT_DIR.is_absolute():
            self.CACHE_ROOT_DIR = (self.BASE_PATH_SCRIPTS / self.CACHE_ROOT_DIR).resolve()
        else:
            self.CACHE_ROOT_DIR = self.CACHE_ROOT_DIR.resolve()

        # Default PROJECTS_DIR if not set
        if self.PROJECTS_DIR is None or (isinstance(self.PROJECTS_DIR, str) and not self.PROJECTS_DIR):
            self.PROJECTS_DIR = self.BASE_PATH_SCRIPTS / 'projets_extraits'
        elif isinstance(self.PROJECTS_DIR, str):
            self.PROJECTS_DIR = Path(self.PROJECTS_DIR)
        # Default ARCHIVES_DIR if not set
        if self.ARCHIVES_DIR is None or (isinstance(self.ARCHIVES_DIR, str) and not self.ARCHIVES_DIR):
            self.ARCHIVES_DIR = self.BASE_PATH_SCRIPTS / 'archives'
        elif isinstance(self.ARCHIVES_DIR, str):
            self.ARCHIVES_DIR = Path(self.ARCHIVES_DIR)
            
        # Create necessary directories
        self._create_directories()
    
    def _create_directories(self) -> None:
        """Create necessary directories if they don't exist."""
        directories_to_create = [
            self.LOGS_DIR,
            self.LOGS_DIR / 'step1',
            self.LOGS_DIR / 'step2',
            self.LOGS_DIR / 'step3',
            self.LOGS_DIR / 'step4',
            self.LOGS_DIR / 'step5',
            self.LOGS_DIR / 'step6',
            self.LOGS_DIR / 'step7',
            # Ensure projects directory exists by default to avoid confusion
            self.PROJECTS_DIR,
            # Ensure archives directory exists
            self.ARCHIVES_DIR,
        ]
        
        for directory in directories_to_create:
            try:
                directory.mkdir(parents=True, exist_ok=True)
                logger.debug(f"Ensured directory exists: {directory}")
            except Exception as e:
                logger.error(f"Failed to create directory {directory}: {e}")
    
    def validate(self, strict: bool = None) -> bool:
        """
        Validate the configuration and ensure all required settings are present.

        Args:
            strict: If None, uses DEBUG mode to determine strictness.
                   If True, raises errors. If False, logs warnings.

        Returns:
            bool: True if configuration is valid

        Raises:
            ValueError: If required configuration is missing or invalid and strict=True
        """
        if strict is None:
            strict = not self.DEBUG  # Strict in production, lenient in development

        errors = []
        warnings = []

        # Security validation
        if not self.INTERNAL_WORKER_TOKEN:
            msg = "INTERNAL_WORKER_COMMS_TOKEN environment variable is required"
            if strict:
                errors.append(msg)
            else:
                warnings.append(msg)
                # Set development default
                self.INTERNAL_WORKER_TOKEN = "dev-internal-worker-token"

        if not self.RENDER_REGISTER_TOKEN:
            msg = "RENDER_REGISTER_TOKEN environment variable is required"
            if strict:
                errors.append(msg)
            else:
                warnings.append(msg)
                # Set development default
                self.RENDER_REGISTER_TOKEN = "dev-render-register-token"

        # Production security checks
        if not self.DEBUG and self.SECRET_KEY in ['dev-key-change-in-production', 'dev-secret-key-change-in-production-12345678901234567890']:
            errors.append("FLASK_SECRET_KEY must be changed in production (DEBUG=false)")

        # Webhook validation (single data source)
        if not self.WEBHOOK_JSON_URL:
            msg = "WEBHOOK_JSON_URL must be set"
            if strict:
                errors.append(msg)
            else:
                warnings.append(msg)
        if self.WEBHOOK_TIMEOUT <= 0:
            warnings.append("WEBHOOK_TIMEOUT should be > 0; using default")
        if self.WEBHOOK_CACHE_TTL < 0:
            warnings.append("WEBHOOK_CACHE_TTL should be >= 0; using default")
        
        # Path validation
        if not self.BASE_PATH_SCRIPTS.exists():
            warnings.append(f"Base scripts path does not exist: {self.BASE_PATH_SCRIPTS}")
        
        if not self.LOCAL_DOWNLOADS_DIR.exists():
            warnings.append(f"Downloads directory does not exist: {self.LOCAL_DOWNLOADS_DIR}")
        
        # Python executable validation
        python_exe_path = Path(self.PYTHON_VENV_EXE)
        if not python_exe_path.exists():
            warnings.append(f"Python executable not found: {python_exe_path}")
        
        # Log warnings
        for warning in warnings:
            logger.warning(warning)
        
        # Log warnings
        if warnings:
            for warning in warnings:
                logger.warning(f"Configuration warning: {warning}")
            if not strict:
                logger.warning("Using development defaults - NOT SUITABLE FOR PRODUCTION")

        # Raise errors if any
        if errors:
            error_msg = f"Configuration validation failed: {'; '.join(errors)}"
            logger.error(error_msg)
            raise ValueError(error_msg)

        if warnings and not strict:
            logger.info("Configuration validation completed with warnings (development mode)")
        else:
            logger.info("Configuration validation successful")
        return True
    
    def get_venv_path(self, venv_name: str) -> Path:
        """
        Get the path to a virtual environment.
        
        Args:
            venv_name: Name of the virtual environment (e.g., 'env', 'audio_env', 'tracking_env')
            
        Returns:
            Path object to the virtual environment directory
        """
        return self.VENV_BASE_DIR / venv_name
    
    def get_venv_python(self, venv_name: str) -> Path:
        """
        Get the path to the Python executable in a virtual environment.
        
        Args:
            venv_name: Name of the virtual environment
            
        Returns:
            Path object to the Python executable
        """
        return self.get_venv_path(venv_name) / "bin" / "python"
    
    def get_allowed_base_paths(self) -> List[Path]:
        """
        Get list of allowed base paths for file operations.
        
        Returns:
            List of Path objects representing allowed base directories
        """
        return [
            self.BASE_PATH_SCRIPTS,
            self.LOCAL_DOWNLOADS_DIR,
            self.LOGS_DIR,
            self.BASE_PATH_SCRIPTS / 'workflow_scripts',
            self.BASE_PATH_SCRIPTS / 'static',
            self.BASE_PATH_SCRIPTS / 'templates',
            self.BASE_PATH_SCRIPTS / 'utils',
        ]
    
    def to_dict(self) -> dict:
        """
        Convert configuration to dictionary for serialization.
        
        Returns:
            Dictionary representation of configuration (excluding sensitive data)
        """
        config_dict = {}
        for key, value in self.__dict__.items():
            # Exclude sensitive information
            if 'TOKEN' in key or 'SECRET' in key:
                config_dict[key] = '***HIDDEN***' if value else None
            elif isinstance(value, Path):
                config_dict[key] = str(value)
            else:
                config_dict[key] = value
        
        return config_dict
    
    @staticmethod
    def check_gpu_availability() -> dict:
        """
        Vérifier la disponibilité GPU pour STEP5 (MediaPipe + OpenSeeFace).
        
        Returns:
            dict: {
                'available': bool,
                'reason': str (si non disponible),
                'vram_total_gb': float (si disponible),
                'vram_free_gb': float (si disponible),
                'cuda_version': str (si disponible),
                'onnx_cuda': bool,
                'tensorflow_gpu': bool
            }
        """
        result = {
            'available': False,
            'reason': '',
            'onnx_cuda': False,
            'tensorflow_gpu': False
        }
        
        # Check PyTorch CUDA (indicateur général)
        try:
            import torch
            if not torch.cuda.is_available():
                result['reason'] = 'CUDA not available (PyTorch check)'
                return result
            
            # Vérifier VRAM
            vram_total = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # Go
            vram_free = (torch.cuda.mem_get_info()[0]) / (1024**3)  # Go
            
            result['vram_total_gb'] = round(vram_total, 2)
            result['vram_free_gb'] = round(vram_free, 2)
            result['cuda_version'] = torch.version.cuda
            
            # Vérifier VRAM minimale (2 Go libres recommandés)
            if vram_free < 1.5:
                result['reason'] = f'VRAM insuffisante ({vram_free:.1f} Go libres < 1.5 Go)'
                return result
        
        except ImportError:
            result['reason'] = 'PyTorch not installed in tracking_env'
            return result
        except Exception as e:
            result['reason'] = f'PyTorch CUDA check failed: {e}'
            return result
        
        # Check ONNXRuntime CUDA provider (requis pour OpenSeeFace / InsightFace)
        try:
            import onnxruntime as ort
            if 'CUDAExecutionProvider' in ort.get_available_providers():
                result['onnx_cuda'] = True
        except ImportError:
            pass
        except Exception as e:
            logger.warning(f"ONNXRuntime check failed: {e}")

        # Optional external ONNXRuntime CUDA check (useful when ORT GPU lives in a dedicated venv)
        if not result.get('onnx_cuda'):
            ort_gpu_python = os.environ.get('STEP5_INSIGHTFACE_ENV_PYTHON', '').strip()
            if ort_gpu_python:
                try:
                    ort_check_code = (
                        "import sys\n"
                        "import onnxruntime as ort\n"
                        "providers = ort.get_available_providers()\n"
                        "sys.stdout.write('1' if 'CUDAExecutionProvider' in providers else '0')"
                    )
                    completed = subprocess.run(
                        [ort_gpu_python, "-c", ort_check_code],
                        capture_output=True,
                        text=True,
                        timeout=15,
                    )
                    if completed.returncode == 0:
                        result['onnx_cuda'] = completed.stdout.strip() == "1"
                    else:
                        logger.warning(
                            "ONNXRuntime GPU check failed (external env returned code %s): %s",
                            completed.returncode,
                            completed.stderr.strip(),
                        )
                except FileNotFoundError:
                    logger.warning(
                        "STEP5_INSIGHTFACE_ENV_PYTHON '%s' introuvable pour la vérification GPU ONNXRuntime",
                        ort_gpu_python,
                    )
                except subprocess.TimeoutExpired:
                    logger.warning("ONNXRuntime GPU check timed out via STEP5_INSIGHTFACE_ENV_PYTHON")
                except Exception as exc:
                    logger.warning(f"ONNXRuntime GPU check failed via STEP5_INSIGHTFACE_ENV_PYTHON: {exc}")
        
        # Déterminer disponibilité finale (InsightFace s'appuie uniquement sur ONNX Runtime GPU)
        if result['onnx_cuda']:
            result['available'] = True
        else:
            result['reason'] = 'ONNXRuntime GPU indisponible (installer onnxruntime-gpu)'
        
        return result
    
    @staticmethod
    def is_step5_gpu_enabled() -> bool:
        """
        Vérifier si le mode GPU STEP5 est activé via configuration.
        
        Returns:
            bool: True si STEP5_ENABLE_GPU=1
        """
        return _parse_bool(os.environ.get('STEP5_ENABLE_GPU'), default=False)
    
    @staticmethod
    def get_step5_gpu_engines() -> List[str]:
        """
        Récupérer la liste des moteurs STEP5 autorisés à utiliser le GPU.
        
        Returns:
            List[str]: ['mediapipe_landmarker', 'openseeface', ...]
        """
        engines_str = os.environ.get('STEP5_GPU_ENGINES', '')
        engines = _parse_csv_list(engines_str)
        # Normaliser les noms
        return [e.strip().lower() for e in engines if e.strip()]
    
    @staticmethod
    def get_step5_gpu_max_vram_mb() -> int:
        """
        Récupérer la limite VRAM maximale pour STEP5 GPU (Mo).
        
        Returns:
            int: Limite en Mo (défaut: 2048)
        """
        return _parse_optional_positive_int(
            os.environ.get('STEP5_GPU_MAX_VRAM_MB')
        ) or 2048


# Global configuration instance
config = Config()
```

## File: static/css/layout.css
```css
.workflow-wrapper {
    display: flex;
    width: 100%;
    max-width: 1600px;
    flex-grow: 1;
}

/* Local Downloads panel animation (class-based for max browser compatibility) */
.local-downloads-section {
    opacity: 0;
    transform: translateX(30px) scale(0.98);
    transition: opacity 0.4s cubic-bezier(0.25, 0.46, 0.45, 0.94),
                transform 0.4s cubic-bezier(0.25, 0.46, 0.45, 0.94);
}

.local-downloads-section.downloads-visible {
    opacity: 1;
    transform: translateX(0) scale(1);
}

@media (prefers-reduced-motion: reduce) {
    .local-downloads-section { transition: none !important; transform: none !important; }
}

/* Keep vertical scrollbar always present to avoid layout jank during height changes */
html, body { overflow-y: scroll; }

.steps-column {
    flex: 1 1 100%;
    padding: 10px 20px;
    display: flex;
    flex-direction: column;
    align-items: center;
    transition: flex-basis 0.5s ease-in-out, padding 0.5s ease-in-out, opacity 0.5s ease-in-out;
    opacity: 1;
    scroll-margin-top: 0;
}

/* Compact mode: grid layout (multiple cards per row, no large vertical gaps) */
.workflow-wrapper.compact-mode .steps-column {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
    grid-auto-flow: row dense; /* backfill holes to reduce empty spaces between rows */
    gap: 16px 16px; /* row gap, column gap */
    align-items: start; /* cards height adapt to content by default */
    align-content: start; /* pack rows at the top to avoid large vertical gaps between groups */
    justify-items: center; /* center cards if column wider than minmax */
    padding: 10px 20px;
    transition: opacity 0.5s ease-in-out;
}

/* Desktop optimization: uniformize row heights to avoid subtle misalignment */
@media (min-width: 1200px) {
  .workflow-wrapper.compact-mode .steps-column {
    align-items: start;
  }
}

/* (removed flex-specific sizing for steps; grid handles column widths) */

.workflow-wrapper.logs-active .steps-column {
    flex-basis: 48%;
    align-items: stretch;
    opacity: 1;
}

/* COMPACT MODE: Overlay logs panel to avoid grid reflow and saccades */
.workflow-wrapper.compact-mode .logs-column {
    position: fixed;
    right: 16px;
    /* Align approximately below unified controls; adjust if needed */
    top: 120px;
    width: min(50vw, 800px);
    height: calc(100vh - 140px);
    z-index: 20;
    box-shadow: -5px 0 15px rgba(0,0,0,0.25);
}
.workflow-wrapper.compact-mode .logs-column {
    transition: opacity 0.3s ease, transform 0.3s ease;
}
.workflow-wrapper.compact-mode:not(.logs-active) .logs-column {
    opacity: 0;
    pointer-events: none;
    transform: translateX(30px);
}
.workflow-wrapper.compact-mode.logs-active .logs-column {
    opacity: 1;
    pointer-events: auto;
    transform: translateX(0);
}

/* In compact mode, keep steps column width unchanged when logs open (no flex-basis change) */
.workflow-wrapper.compact-mode.logs-active .steps-column {
    flex-basis: auto;
    /* Reserve space for the fixed logs panel so steps never go underneath it */
    margin-right: min(50vw, 800px);
    padding-right: 16px; /* small gutter between steps and logs */
    min-height: calc(100vh - 140px); /* ensure sticky active card has space */
    /* Subtle parallax/brightness for depth while logs overlay is visible */
    transform: scale(0.985);
    filter: brightness(0.95);
    transition: transform 0.35s ease, filter 0.35s ease;
}

/* Keep the active step visually pinned relative to the logs panel (consistent vertical position) */
.workflow-wrapper.compact-mode.logs-active .steps-column .step.active-for-log-panel {
    position: sticky;
    top: 120px; /* align with .logs-column top for visual consistency */
    z-index: 1; /* ensure above siblings during transitions */
    /* Span full width across grid */
    grid-column: 1 / -1;
}

/* Entering phase: hide non-active steps to avoid saccade before panel is fully active */
.workflow-wrapper.compact-mode.logs-entering .steps-column .step:not(.active-for-log-panel) {
    opacity: 0 !important;
    transform: translateY(8px) scale(0.97) !important;
}
.workflow-wrapper.compact-mode.logs-active .steps-column .step:not(.active-for-log-panel) {
    display: none !important; /* fully remove from layout to keep stable height/scrollbar */
}
.workflow-wrapper.logs-active .steps-column .step.active-for-log-panel {
    opacity: 1;
    transform: translateY(0) scale(1);
    pointer-events: auto;
    transition: opacity 0.3s ease-in-out, transform 0.3s ease-in-out;
}

/* Ensure explicit visible state when logs panel is closed */
.workflow-wrapper:not(.logs-active) .steps-column .step {
    opacity: 1;
    transform: translateY(0) scale(1);
    transition: opacity 0.4s ease-in-out, transform 0.4s ease-in-out;
}

.logs-column {
    flex: 0 0 0%;
    width: 0%;
    opacity: 0;
    transform: translateX(30px);
    background-color: var(--bg-card);
    border-left: 1px solid var(--border-color);
    padding: 0;
    overflow-y: auto;
    transition: flex-basis 0.5s ease-in-out, width 0.5s ease-in-out, opacity 0.4s 0.1s ease-in-out, transform 0.5s ease-in-out, padding 0.5s ease-in-out;
    display: flex;
    flex-direction: column;
    box-shadow: -5px 0 15px rgba(0,0,0,0.2);
    max-height: calc(100vh - 40px);
}

.workflow-wrapper.logs-active .logs-column {
    flex-basis: 50%;
    width: 50%;
    opacity: 1;
    transform: translateX(0);
    padding: 20px 20px 20px 52px;
 }

.workflow-wrapper.compact-mode .step-details-panel {
    position: fixed;
    right: 16px;
    top: 120px;
    width: min(30vw, 420px);
    height: calc(100vh - 140px);
    z-index: 19;
    opacity: 0;
    pointer-events: none;
    transform: translateX(30px);
    transition: opacity 0.3s ease, transform 0.3s ease;
 }

.workflow-wrapper.compact-mode.details-active .step-details-panel {
    opacity: 1;
    pointer-events: auto;
    transform: translateX(0);
 }

.workflow-wrapper.compact-mode.details-active .steps-column {
    margin-right: min(30vw, 420px);
    padding-right: 16px;
 }

.workflow-wrapper.compact-mode.logs-active.details-active .steps-column {
    margin-right: min(50vw, 800px);
    padding-right: 16px;
 }

.workflow-wrapper.compact-mode.logs-active .step-details-panel {
    opacity: 0;
    pointer-events: none;
    transform: translateX(30px);
 }

/* Unified controls section compatibility with slideshow mode */
.workflow-wrapper.slideshow-mode ~ .unified-controls-section,
.unified-controls-section + .workflow-wrapper.slideshow-mode {
    /* Ensure proper spacing and visibility during slideshow */
    z-index: 1;
    position: relative;
 }
```

## File: static/apiService.js
```javascript
// --- START OF REFACTORED apiService.js ---

import { POLLING_INTERVAL } from './constants.js';
import * as ui from './uiUpdater.js';
import * as dom from './domElements.js';
import { appState } from './state/AppState.js';
import { showNotification, sendBrowserNotification } from './utils.js';
import { soundEvents } from './soundManager.js';
import { pollingManager } from './utils/PollingManager.js';
import { errorHandler } from './utils/ErrorHandler.js';


/**
 * Fetch helper that toggles a loading state on a button during the request.
 * It adds data-loading="true" and disables the button while the fetch runs.
 * @param {string} url
 * @param {RequestInit} options
 * @param {HTMLElement|string|null} buttonElOrId - element or element id
 * @returns {Promise<any>} parsed JSON response (or throws on network/error)
 */
export async function fetchWithLoadingState(url, options = {}, buttonElOrId = null) {
    let btn = null;
    if (typeof buttonElOrId === 'string') {
        btn = document.getElementById(buttonElOrId);
    } else if (buttonElOrId && buttonElOrId.nodeType === 1) {
        btn = buttonElOrId;
    }

    try {
        if (btn) {
            btn.setAttribute('data-loading', 'true');
            btn.disabled = true;
        }
        const response = await fetch(url, options);
        const data = await response.json().catch(() => ({}));
        if (!response.ok) {
            throw new Error((data && data.message) || `Erreur HTTP ${response.status}`);
        }
        return data;
    } finally {
        if (btn) {
            btn.removeAttribute('data-loading');
            btn.disabled = false;
        }
    }
}

/**
 * Centralized function to handle UI updates and state changes when a step fails.
 * This avoids code duplication in runStepAPI and performPoll.
 * @param {string} stepKey - The key of the step that failed.
 * @param {Error} error - The error object.
 * @param {string} errorSource - A string indicating where the error occurred (e.g., 'Lancement', 'Polling').
 */
function handleStepFailure(stepKey, error, errorSource) {
    const errorMessage = error.message || 'Erreur inconnue.';
    console.error(`[API handleStepFailure] Erreur ${errorSource} pour ${stepKey}:`, error);

    if (errorMessage.includes('Étape inconnue')) {
        console.warn(`[API handleStepFailure] Étape '${stepKey}' n'est pas reconnue.`);
    }

    soundEvents.errorEvent();

    showNotification(`Erreur ${errorSource} ${stepKey}: ${errorMessage}`, 'error');

    const statusEl = document.getElementById(`status-${stepKey}`);
    if (statusEl) {
        statusEl.textContent = `Erreur: ${errorMessage.substring(0, 50)}`;
        statusEl.className = 'status-badge status-failed';
    }

    if (appState.getStateProperty('activeStepKeyForLogsPanel') === stepKey) {
        ui.updateMainLogOutputUI(`<i>Erreur d'initiation: ${errorMessage}</i>`);
    }

    const runButton = document.querySelector(`.run-button[data-step="${stepKey}"]`);
    if (runButton) {
        runButton.disabled = !!appState.getStateProperty('isAnySequenceRunning');
    }
    const cancelButton = document.querySelector(`.cancel-button[data-step="${stepKey}"]`);
    if (cancelButton) {
        cancelButton.disabled = true;
    }

    ui.stopStepTimer(stepKey);
    const progressBar = document.getElementById(`progress-bar-${stepKey}`);
    if (progressBar) {
        progressBar.style.backgroundColor = 'var(--red)';
    }

    stopPollingAPI(stepKey);
}


export async function runStepAPI(stepKey) {
    // --- UI setup (unchanged) ---
    ui.resetStepTimerDisplay(stepKey);
    const statusEl = document.getElementById(`status-${stepKey}`);
    if(statusEl) { statusEl.textContent = 'Initiation...'; statusEl.className = 'status-badge status-initiated'; }
    const runButton = document.querySelector(`.run-button[data-step="${stepKey}"]`);
    if(runButton) runButton.disabled = true;
    const cancelButton = document.querySelector(`.cancel-button[data-step="${stepKey}"]`);
    if(cancelButton) cancelButton.disabled = false;

    if (appState.getStateProperty('activeStepKeyForLogsPanel') === stepKey) {
        ui.updateMainLogOutputUI('<i>Initiation du processus...</i>');
    }

    try {
        const data = await fetchWithLoadingState(`/run/${stepKey}`, { method: 'POST' }, runButton);
        console.log(`[API runStepAPI] Réponse pour ${stepKey}:`, data);

        if (data.status === 'initiated') {
            const statusEl = document.getElementById(`status-${stepKey}`);
            if(statusEl) { statusEl.textContent = 'Lancé'; statusEl.className = 'status-badge status-starting'; }
            ui.startStepTimer(stepKey);
            console.log(`[API runStepAPI] Appel de startPollingAPI pour ${stepKey}`);
            startPollingAPI(stepKey);
            return true;
        } else {
            throw new Error(data.message || `Réponse invalide du serveur pour le lancement de ${stepKey}.`);
        }
    } catch (error) {
        handleStepFailure(stepKey, error, 'Lancement');
        return false;
    }
}

function appendItalicLineToMainLog(panelEl, message) {
    if (!panelEl) return;
    const br = document.createElement('br');
    const i = document.createElement('i');
    i.textContent = String(message ?? '');
    panelEl.appendChild(br);
    panelEl.appendChild(i);
}

export async function cancelStepAPI(stepKey) {
    if (appState.getStateProperty('activeStepKeyForLogsPanel') === stepKey) {
        appendItalicLineToMainLog(dom.mainLogOutputPanel, 'Annulation en cours...');
    }

    try {
        const cancelUrl = `/cancel/${stepKey}`;
        const fullUrl = new URL(cancelUrl, window.location.origin).href;
        console.log(`[CANCEL DEBUG] Attempting to cancel ${stepKey}:`);
        console.log(`  - Relative URL: ${cancelUrl}`);
        console.log(`  - Full URL: ${fullUrl}`);
        console.log(`  - Current origin: ${window.location.origin}`);
        console.log(`  - Current port: ${window.location.port}`);

        const cancelButton = document.querySelector(`.cancel-button[data-step="${stepKey}"]`);
        const data = await fetchWithLoadingState(cancelUrl, { method: 'POST' }, cancelButton);
        console.log(`[CANCEL DEBUG] Response received (ok):`, data);
        showNotification(data.message || "Annulation demandée", 'info');

        if (appState.getStateProperty('activeStepKeyForLogsPanel') === stepKey) {
            appendItalicLineToMainLog(dom.mainLogOutputPanel, data.message || 'Annulation demandée');
        }
    } catch (error) {
        console.error(`Erreur annulation ${stepKey}:`, error);

        errorHandler.handleApiError(`cancel/${stepKey}`, error, { stepKey });

        if (appState.getStateProperty('activeStepKeyForLogsPanel') === stepKey) {
            appendItalicLineToMainLog(dom.mainLogOutputPanel, `Erreur communication pour annulation: ${error.toString()}`);
        }
    }
}

export function startPollingAPI(stepKey, isAutoModeHighFrequency = false) {
    stopPollingAPI(stepKey);

    const pollingInterval = isAutoModeHighFrequency ? 200 : POLLING_INTERVAL;
    console.log(`[API startPollingAPI] 🚀 Polling démarré pour ${stepKey}. Intervalle: ${pollingInterval}ms ${isAutoModeHighFrequency ? '(AutoMode high-frequency)' : '(normal)'}`);

    const performPoll = async () => {
        try {
            const pollStartTime = performance.now();
            console.log(`[API POLL] Fetching status for ${stepKey} at ${new Date().toISOString()}`);

            const response = await fetch(`/status/${stepKey}`);
            if (!response.ok) {
                console.warn(`[API performPoll] Erreur ${response.status} lors du polling pour ${stepKey}. Arrêt du polling.`);
                stopPollingAPI(stepKey);
                if (!appState.getStateProperty('isAnySequenceRunning')) {
                    handleStepFailure(stepKey, new Error(`Erreur statut (${response.status})`), 'Polling');
                }
                return;
            }
            const data = await response.json();

            const pollEndTime = performance.now();
            const statusEmoji = data.status === 'running' ? '🔄' : data.status === 'completed' ? '✅' : data.status === 'failed' ? '❌' : '⚪';
            console.log(`[API POLL RESPONSE] ${statusEmoji} ${stepKey} (${(pollEndTime - pollStartTime).toFixed(2)}ms): status="${data.status}", progress=${data.progress_current}/${data.progress_total}, return_code=${data.return_code}`);

            const previousStatus = appState.getStateProperty(`processInfo.${stepKey}.status`) || 'unknown';
            appState.setState({ processInfo: { [stepKey]: data } }, 'process_info_polled');

            if (typeof data.is_any_sequence_running === 'boolean') {
                appState.setState({ isAnySequenceRunning: data.is_any_sequence_running }, 'sequence_running_polled');
            }

            ui.updateStepCardUI(stepKey, data);

            const workflowWrapper = typeof dom.getWorkflowWrapper === 'function' ? dom.getWorkflowWrapper() : dom.workflowWrapper;
            if (appState.getStateProperty('activeStepKeyForLogsPanel') === stepKey && workflowWrapper && workflowWrapper.classList.contains('logs-active')) {
                ui.updateMainLogOutputUI(data.log.join(''));
            }
            const isTerminal = ['completed', 'failed'].includes(data.status);
            if (isTerminal && previousStatus !== data.status) {
                const title = data.status === 'completed' ? '✅ Étape terminée' : '❌ Étape en erreur';
                const body = `${stepKey} — statut: ${data.status}`;
                sendBrowserNotification(title, body).catch(() => {});
            }

            const shouldStopPolling = isTerminal ||
                                    (data.status === 'idle' && !appState.getStateProperty('autoModeEnabled'));

            if (shouldStopPolling) {
                console.log(`[API performPoll] Statut final '${data.status}' pour ${stepKey}. Arrêt du polling.`);
                stopPollingAPI(stepKey);
            } else if (data.status === 'idle' && appState.getStateProperty('autoModeEnabled')) {
                console.log(`[API performPoll] ⚪ ${stepKey} idle mais AutoMode actif - maintien du polling pour détecter les transitions`);
            }
        } catch (error) {
            console.error(`[API performPoll] Erreur CATCH polling ${stepKey}:`, error);
            stopPollingAPI(stepKey);
            if (!appState.getStateProperty('isAnySequenceRunning')) {
                handleStepFailure(stepKey, error, 'Polling');
            }
        }
    };

    const pollingId = pollingManager.startPolling(
        `step-${stepKey}`,
        performPoll,
        pollingInterval, // Use dynamic interval (200ms for AutoMode, 500ms for normal)
        { immediate: true, maxErrors: 3 }
    );
}

export function stopPollingAPI(stepKey) {
    pollingManager.stopPolling(`step-${stepKey}`);
    console.log(`[API stopPollingAPI] Polling arrêté pour ${stepKey}.`);
}

export async function fetchSpecificLogAPI(stepKey, logIndex, logName, buttonElOrId = null) {
    ui.updateSpecificLogUI(logName, null, "<i>Chargement...</i>");
    try {
        let data;
        const url = `/get_specific_log/${stepKey}/${logIndex}`;
        if (buttonElOrId) {
            data = await fetchWithLoadingState(url, {}, buttonElOrId);
        } else {
            const response = await fetch(url);
            data = await response.json();
            if (!response.ok) {
                throw new Error(data.error || `Erreur HTTP ${response.status}`);
            }
        }
        ui.updateSpecificLogUI(logName, data.path, data.content);
    } catch (error) {
        console.error(`Erreur fetch log spécifique ${stepKey}/${logIndex}:`, error);
        ui.updateSpecificLogUI(logName, null, '', true, `Erreur de communication: ${error.toString()}`);
    }
}


export async function fetchInitialStatusAPI(stepKey) {
    try {
        const response = await fetch(`/status/${stepKey}`);
        if (!response.ok) {
            console.warn(`Initial status fetch failed for ${stepKey}: ${response.status}. Using fallback.`);
            appState.setState({
                processInfo: {
                    [stepKey]: appState.getStateProperty(`processInfo.${stepKey}`) || {
                        status: 'idle', log: [], progress_current: 0, progress_total: 0, progress_text: '',
                        is_any_sequence_running: false
                    }
                }
            }, 'process_info_initial_fallback');
        } else {
            const data = await response.json();
            appState.setState({ processInfo: { [stepKey]: data } }, 'process_info_initial');

            if (typeof data.is_any_sequence_running === 'boolean') {
                appState.setState({ isAnySequenceRunning: data.is_any_sequence_running }, 'sequence_running_initial');
            }
        }

        const stepInfo = appState.getStateProperty(`processInfo.${stepKey}`) || {
            status: 'idle', log: [], progress_current: 0, progress_total: 0, progress_text: '',
            is_any_sequence_running: false
        };

        if (stepKey === 'clear_disk_cache') {
            ui.updateClearCacheGlobalButtonState(stepInfo.status);
        } else {
            ui.updateStepCardUI(stepKey, stepInfo);
        }
        
        if (['running', 'starting', 'initiated'].includes(stepInfo.status)) {
            console.log(`[API fetchInitialStatusAPI] Étape ${stepKey} en cours au démarrage. Lancement du polling.`);
            startPollingAPI(stepKey);
        }

    } catch (err) {
        console.error(`Erreur CATCH fetchInitialStatusAPI pour ${stepKey}:`, err);
        const fallbackData = appState.getStateProperty(`processInfo.${stepKey}`) || {
            status: 'idle', log: [], progress_current: 0, progress_total: 0, progress_text: '',
            is_any_sequence_running: false
        };
        if (stepKey === 'clear_disk_cache') {
            ui.updateClearCacheGlobalButtonState(fallbackData.status);
        } else {
            ui.updateStepCardUI(stepKey, fallbackData);
        }
    }
}

export async function fetchLocalDownloadsStatusAPI() {
    try {
        const response = await fetch('/api/csv_downloads_status');
        if (!response.ok) {
            console.warn(`Erreur lors de la récupération du statut des téléchargements CSV: ${response.status}`);
            return [];
        }
        return await response.json();
    } catch (error) {
        console.error("Erreur réseau fetchLocalDownloadsStatusAPI:", error);
        return [];
    }
}



export async function fetchCSVMonitorStatusAPI() {
    try {
        const response = await fetch('/api/csv_monitor_status');
        if (!response.ok) {
            throw new Error(`Erreur HTTP ${response.status}`);
        }
        return await response.json();
    } catch (error) {
        console.error("Erreur fetchCSVMonitorStatusAPI:", error);
        return {
            csv_monitor: { status: "error", last_check: null, error: "Impossible de récupérer le statut" },
            auto_mode_enabled: false,
            csv_url: "",
            check_interval: 15
        };
    }
}
```

## File: static/popupManager.js
```javascript
import * as dom from './domElements.js';
import { appState } from './state/AppState.js';
import { DOMUpdateUtils } from './utils/DOMBatcher.js';

function handlePopupKeydown(event) {
    const popupOverlay = event.currentTarget;
    if (event.key === 'Escape') {
        closePopupUI(popupOverlay);
    }
    if (event.key === 'Tab') {
        const focusableElements = Array.from(popupOverlay.querySelectorAll('button, [href], input, select, textarea, [tabindex]:not([tabindex="-1"])')).filter(el => el.offsetParent !== null);
        if (focusableElements.length === 0) return;

        const firstElement = focusableElements[0];
        const lastElement = focusableElements[focusableElements.length - 1];

        if (event.shiftKey) {
            if (document.activeElement === firstElement) {
                lastElement.focus();
                event.preventDefault();
            }
        } else {
            if (document.activeElement === lastElement) {
                firstElement.focus();
                event.preventDefault();
            }
        }
    }
}

export function openPopupUI(popupOverlay) {
    if (!popupOverlay) return;

    const currentFocused = document.activeElement;
    if (currentFocused &&
        currentFocused !== document.body &&
        currentFocused !== document.documentElement &&
        typeof currentFocused.focus === 'function' &&
        currentFocused.nodeType === Node.ELEMENT_NODE) {
        appState.setState({ focusedElementBeforePopup: currentFocused }, 'popup_focus_store');
        console.debug('[POPUP] Stored focusable element:', {
            tagName: currentFocused.tagName,
            id: currentFocused.id || 'no-id',
            className: currentFocused.className || 'no-class'
        });
    } else {
        appState.setState({ focusedElementBeforePopup: null }, 'popup_focus_store');
        console.debug('[POPUP] No valid focusable element to store');
    }

    popupOverlay.style.display = 'flex';
    popupOverlay.setAttribute('data-visible', 'true');
    popupOverlay.setAttribute('aria-hidden', 'false');
    const focusableElements = popupOverlay.querySelectorAll('button, [href], input, select, textarea, [tabindex]:not([tabindex="-1"])');

    let elementToFocus = null;
    for (let i = 0; i < focusableElements.length; i++) {
        const element = focusableElements[i];
        if (element.offsetParent !== null && typeof element.focus === 'function') {
            elementToFocus = element;
            break;
        }
    }

    if (elementToFocus) {
        try {
            elementToFocus.focus();
        } catch (error) {
            console.warn('[POPUP] Failed to focus element in popup:', error);
        }
    }

    popupOverlay.addEventListener('keydown', handlePopupKeydown);
}

export function closePopupUI(popupOverlay) {
    if (!popupOverlay) return;
    popupOverlay.removeAttribute('data-visible');
    popupOverlay.setAttribute('aria-hidden', 'true');
    popupOverlay.style.display = 'none';
    popupOverlay.removeEventListener('keydown', handlePopupKeydown);
    const prevFocused = appState.getStateProperty('focusedElementBeforePopup');

    if (prevFocused && typeof prevFocused.focus === 'function') {
        try {
            if (prevFocused.isConnected && document.hasFocus()) {
                if (document.contains(prevFocused) && prevFocused.offsetParent !== null) {
                    prevFocused.focus();
                } else {
                    console.debug('[POPUP] Previous focused element no longer focusable, skipping focus restoration');
                }
            }
        } catch (error) {
            console.warn('[POPUP] Failed to restore focus to previous element:', error);
        }
    } else if (prevFocused) {
        const elementInfo = {
            tagName: prevFocused.tagName || 'unknown',
            id: prevFocused.id || 'no-id',
            className: prevFocused.className || 'no-class',
            nodeType: prevFocused.nodeType || 'unknown',
            hasFocusMethod: typeof prevFocused.focus === 'function'
        };
        console.debug('[POPUP] Previous focused element is not focusable:', elementInfo);
    }

    appState.setState({ focusedElementBeforePopup: null }, 'popup_focus_clear');
}

function resolveSequenceSummaryElements() {
    const overlay = typeof dom.getSequenceSummaryPopupOverlay === 'function'
        ? dom.getSequenceSummaryPopupOverlay()
        : dom.sequenceSummaryPopupOverlay;
    const list = typeof dom.getSequenceSummaryList === 'function'
        ? dom.getSequenceSummaryList()
        : dom.sequenceSummaryList;
    return { overlay, list };
}

export function showSequenceSummaryUI(results, overallSuccess, sequenceName = "Séquence", overallDuration = null) {
    const { overlay, list } = resolveSequenceSummaryElements();
    if (!overlay || !list) {
        console.error("Éléments DOM pour la popup de résumé non trouvés!");
        return;
    }
    const summaryTitle = overlay.querySelector("h3");
    if (summaryTitle) summaryTitle.textContent = `Résumé: ${sequenceName}`;

    list.innerHTML = '';
    if (overallDuration && typeof overallDuration === 'string') {
        const totalItem = document.createElement('li');
        totalItem.style.fontWeight = 'bold';
        totalItem.style.marginBottom = '8px';
        totalItem.style.paddingBottom = '8px';
        totalItem.style.borderBottom = `1px solid var(--border-color)`;
        const safeDuration = DOMUpdateUtils.escapeHtml(overallDuration);
        totalItem.innerHTML = `<span class="status-icon" style="color:var(--accent-color);">⏱️</span> Durée totale: ${safeDuration}`;
        list.appendChild(totalItem);
    }
    results.forEach(result => {
        const listItem = document.createElement('li');
        const icon = result.success ? '<span class="status-icon status-completed" style="color:var(--green);">✔️</span>' : '<span class="status-icon status-failed" style="color:var(--red);">❌</span>';
        const safeName = DOMUpdateUtils.escapeHtml(String(result.name ?? ''));
        const safeDurationText = result.duration && result.duration !== "N/A" ? DOMUpdateUtils.escapeHtml(String(result.duration)) : "";
        const durationText = safeDurationText ? `<span class="duration">(${safeDurationText})</span>` : "";
        listItem.innerHTML = `${icon} ${safeName}: ${result.success ? 'Terminée avec succès' : 'Échouée ou annulée'} ${durationText}`;
        list.appendChild(listItem);
    });

    const overallStatusItem = document.createElement('li');
    overallStatusItem.style.fontWeight = 'bold';
    overallStatusItem.style.marginTop = '10px';
    overallStatusItem.style.paddingTop = '10px';
    overallStatusItem.style.borderTop = `1px solid var(--border-color)`;
    const safeSequenceName = DOMUpdateUtils.escapeHtml(String(sequenceName ?? 'Séquence'));
    if (overallSuccess) {
        overallStatusItem.innerHTML = `<span class="status-icon status-completed" style="color:var(--green);">🎉</span> ${safeSequenceName} terminée avec succès !`;
    } else {
        overallStatusItem.innerHTML = `<span class="status-icon status-failed" style="color:var(--red);">⚠️</span> ${safeSequenceName} a rencontré une ou plusieurs erreurs.`;
    }
    list.appendChild(overallStatusItem);
    openPopupUI(overlay);
}

export function showCustomSequenceConfirmUI() {
    dom.customSequenceConfirmList.innerHTML = '';
    const selectedStepsOrder = appState.getStateProperty('selectedStepsOrder') || [];
    selectedStepsOrder.forEach((stepKey, index) => {
        const stepElement = document.getElementById(`step-${stepKey}`);
        const stepName = stepElement ? stepElement.dataset.stepName : stepKey;
        const li = document.createElement('li');
        const safeStepName = DOMUpdateUtils.escapeHtml(String(stepName ?? ''));
        li.innerHTML = `<span class="order-prefix">${index + 1}.</span> ${safeStepName}`;
        dom.customSequenceConfirmList.appendChild(li);
    });
    openPopupUI(dom.customSequenceConfirmPopupOverlay);
}
```

## File: static/sequenceManager.js
```javascript
import { POLLING_INTERVAL } from './constants.js';
import * as ui from './uiUpdater.js';
import { runStepAPI } from './apiService.js';
import { showSequenceSummaryUI } from './popupManager.js';
import { formatElapsedTime } from './utils.js';
import { scrollToActiveStep, isSequenceAutoScrollEnabled } from './scrollManager.js';

import { appState } from './state/AppState.js';

import { soundEvents } from './soundManager.js';

/**
 * Executes and tracks a single step within a sequence.
 * This helper function encapsulates all logic for one step, from initiation to completion.
 * @private
 * @param {string} stepKey - The unique key for the step.
 * @param {string} sequenceName - The name of the parent sequence.
 * @param {number} currentStepNum - The step's number in the sequence (e.g., 1, 2, 3...).
 * @param {number} totalSteps - The total number of steps in the sequence.
 * @returns {Promise<object>} A promise that resolves to a result object: { name, success, duration }.
 */
async function _executeSingleStep(stepKey, sequenceName, currentStepNum, totalSteps) {
    const stepConfig = ui.getStepsConfig()[stepKey];
    const stepDisplayName = stepConfig ? stepConfig.display_name : stepKey;

    console.log(`[SEQ_MGR] ${sequenceName} - Step ${currentStepNum}/${totalSteps}: ${stepDisplayName} (${stepKey})`);

    ui.updateGlobalProgressUI(`${sequenceName} - Étape ${currentStepNum}/${totalSteps}: ${stepDisplayName}`,
        Math.round(((currentStepNum - 1) / totalSteps) * 100)
    );

    if (stepKey !== 'clear_disk_cache') {
        ui.openLogPanelUI(stepKey, true);
        ui.setActiveStepForLogPanelUI(stepKey);
        
        if (isSequenceAutoScrollEnabled()) {
            setTimeout(() => {
                scrollToActiveStep(stepKey, { behavior: 'smooth', scrollDelay: 0 });
            }, 0);
        }
    }

    const stepInitiated = await runStepAPI(stepKey);
    if (!stepInitiated) {
        console.error(`[SEQ_MGR] Initiation FAILED for ${stepKey}`);
        ui.updateGlobalProgressUI(`ÉCHEC: L'étape "${stepDisplayName}" n'a pas pu être initiée. Séquence interrompue.`,
            Math.round(((currentStepNum - 1) / totalSteps) * 100), true
        );
        return { name: stepDisplayName, success: false, duration: "N/A (échec initiation)" };
    }

    ui.startStepTimer(stepKey);
    console.log(`[SEQ_MGR] Started timer for ${stepKey}`);

    try {
        ui.setActiveStepForLogPanelUI(stepKey);
    } catch (e) {
        console.debug('[SEQ_MGR] setActiveStepForLogPanelUI post-start failed (non-fatal):', e);
    }

    let timerData = appState.getStateProperty(`stepTimers.${stepKey}`);
    if (timerData && timerData.startTime) {
        console.log(`[SEQ_MGR] Timer verified for ${stepKey}, start time:`, timerData.startTime);
    } else {
        console.error(`[SEQ_MGR] Timer NOT properly started for ${stepKey}:`, timerData);
    }

    console.log(`[SEQ_MGR] Waiting for completion of ${stepKey}`);
    const stepCompleted = await waitForStepCompletionInSequence(stepKey);

    ui.stopStepTimer(stepKey);
    console.log(`[SEQ_MGR] Stopped timer for ${stepKey}`);

    timerData = appState.getStateProperty(`stepTimers.${stepKey}`);
    const duration = (timerData?.elapsedTimeFormatted) || "N/A";

    console.log(`[SEQ_MGR] Timer data for ${stepKey}:`, {
        timerData,
        duration,
        startTime: timerData?.startTime,
        elapsedTimeFormatted: timerData?.elapsedTimeFormatted
    });

    if (!stepCompleted) {
        console.error(`[SEQ_MGR] Execution FAILED for ${stepKey}`);

        ui.updateGlobalProgressUI(`ÉCHEC: L'étape "${stepDisplayName}" a échoué. Séquence interrompue.`,
            Math.round((currentStepNum / totalSteps) * 100), true
        );
        return { name: stepDisplayName, success: false, duration };
    }

    console.log(`[SEQ_MGR] Step ${stepDisplayName} completed successfully.`);

    soundEvents.stepSuccess();

    return { name: stepDisplayName, success: true, duration };
}

export async function runStepSequence(stepsToExecute, sequenceName = "Séquence") {
    console.log(`[SEQ_MGR] Starting sequence: ${sequenceName} with steps:`, stepsToExecute);
    ui.updateGlobalUIForSequenceState(true);
    ui.updateGlobalProgressUI(`Démarrage de la ${sequenceName}...`, 0);

    const sequenceStart = Date.now();

    const sequenceResults = [];
    const totalStepsInThisSequence = stepsToExecute.length;
    const isAutoModeSequence = sequenceName === "AutoMode";
    let sequenceFailed = false;

    if (isAutoModeSequence) {
        appState.setState({ ui: { autoModeLogPanelOpened: false } }, 'auto_mode_sequence_reset');
    }

    for (let i = 0; i < stepsToExecute.length; i++) {
        const stepKey = stepsToExecute[i];
        const currentStepNum = i + 1;

        const result = await _executeSingleStep(stepKey, sequenceName, currentStepNum, totalStepsInThisSequence);
        sequenceResults.push(result);

        if (!result.success) {
            sequenceFailed = true;
            break; // Exit the loop immediately on failure
        }

        if (i < stepsToExecute.length - 1) {
            const nextStepKey = stepsToExecute[i + 1];
            if (nextStepKey && nextStepKey !== 'clear_disk_cache') {
                try {
                    ui.openLogPanelUI(nextStepKey, true);
                    ui.setActiveStepForLogPanelUI(nextStepKey);
                    
                    if (isSequenceAutoScrollEnabled()) {
                        setTimeout(() => {
                            scrollToActiveStep(nextStepKey, { behavior: 'smooth', scrollDelay: 0 });
                        }, 0);
                    }
                } catch (e) {
                    console.debug('[SEQ_MGR] Pre-focus next step failed (non-fatal):', e);
                }
            }
            ui.updateGlobalProgressUI(`${sequenceName} - Étape ${currentStepNum}/${totalStepsInThisSequence}: ${result.name} terminée.`,
                Math.round((currentStepNum / totalStepsInThisSequence) * 100)
            );
        }
    }

    console.log(`[SEQ_MGR] Sequence ${sequenceName} finished. sequenceFailed: ${sequenceFailed}`);

    if (sequenceFailed) {
    } else {
        ui.updateGlobalProgressUI(`${sequenceName} terminée avec succès ! 🎉`, 100);
        soundEvents.workflowCompletion();
    }

    if (sequenceResults.length > 0) {
        const overallDuration = formatElapsedTime(new Date(sequenceStart));
        showSequenceSummaryUI(sequenceResults, !sequenceFailed, sequenceName, overallDuration);
    } else {
        console.warn(`[SEQ_MGR] No results to show for sequence ${sequenceName}`);
    }

    ui.updateGlobalUIForSequenceState(false);
    if (isAutoModeSequence) {
        appState.setState({ ui: { autoModeLogPanelOpened: false } }, 'auto_mode_sequence_reset_end');
    }
}

function waitForStepCompletionInSequence(stepKey) {
    return new Promise((resolve) => {
        const intervalIdForLog = `wait_${stepKey}_${Date.now()}`;
        console.log(`[SEQ_MGR - ${intervalIdForLog}] Waiting for final status...`);

        const checkInterval = setInterval(() => {
            const data = appState.getStateProperty(`processInfo.${stepKey}`);

            if (!data) {
                return;
            }

            if (data.status === 'completed') {
                console.log(`[SEQ_MGR - ${intervalIdForLog}] Resolved as COMPLETED.`);
                clearInterval(checkInterval);
                resolve(true);
            } else if (data.status === 'failed' || data.return_code === -9) {
                console.error(`[SEQ_MGR - ${intervalIdForLog}] Resolved as FAILED or CANCELLED.`);
                clearInterval(checkInterval);
                resolve(false);
            }
            }, POLLING_INTERVAL / 2);
    });
}
```

## File: static/eventHandlers.js
```javascript
import * as dom from './domElements.js';
import * as ui from './uiUpdater.js';
import * as api from './apiService.js';
import { runStepSequence } from './sequenceManager.js';
import { defaultSequenceableStepsKeys } from './constants.js';
import { showNotification } from './utils.js';
import { openPopupUI, closePopupUI, showCustomSequenceConfirmUI } from './popupManager.js';
import { scrollToStepImmediate } from './scrollManager.js';
import { soundEvents } from './soundManager.js';
import { appState } from './state/AppState.js';

function getIsAnySequenceRunning() {
    return !!appState.getStateProperty('isAnySequenceRunning');
}

function getSelectedStepsOrder() {
    return appState.getStateProperty('selectedStepsOrder') || [];
}

function setSelectedStepsOrder(order) {
    const safeOrder = Array.isArray(order) ? [...order] : [];
    appState.setState({ selectedStepsOrder: safeOrder }, 'selected_steps_order_update');
}

function resolveElement(getterFn, legacyValue = null) {
    if (typeof getterFn === 'function') {
        try {
            return getterFn();
        } catch (_) {
            return legacyValue;
        }
    }
    return legacyValue;
}

function resolveCollection(getterFn, legacyValue = null) {
    const resolved = resolveElement(getterFn, legacyValue);
    if (!resolved) return [];
    return Array.from(resolved);
}

export function initializeEventHandlers() {
    const closeLogButton = resolveElement(dom.getCloseLogPanelButton, dom.closeLogPanelButton);
    if (closeLogButton) {
        closeLogButton.addEventListener('click', ui.closeLogPanelUI);
    }

    const runButtons = resolveCollection(dom.getAllRunButtons, dom.allRunButtons);
    runButtons.forEach(button => {
        button.addEventListener('click', async () => {
            try {
                if (getIsAnySequenceRunning()) {
                    showNotification("Une séquence est déjà en cours. Veuillez attendre sa fin.", 'warning');
                    return;
                }
                const stepKey = button.dataset.step;
                ui.updateMainLogOutputUI('');
                const specificLogContainer = resolveElement(dom.getSpecificLogContainerPanel, dom.specificLogContainerPanel);
                if (specificLogContainer) specificLogContainer.style.display = 'none';
                ui.openLogPanelUI(stepKey);

                // Scroll to the step immediately when manually triggered
                scrollToStepImmediate(stepKey, { scrollDelay: 0 });

                // Play workflow start sound for individual step execution
                soundEvents.workflowStart();

                await api.runStepAPI(stepKey);
            } catch (error) {
                console.error('[EVENT] Error in run button handler:', error);
                showNotification("Erreur lors de l'exécution de l'étape", 'error');
            }
        });
    });

    const cancelButtons = resolveCollection(dom.getAllCancelButtons, dom.allCancelButtons);
    cancelButtons.forEach(button => {
        button.addEventListener('click', async () => {
            try {
                const stepKey = button.dataset.step;
                await api.cancelStepAPI(stepKey);
            } catch (error) {
                console.error('[EVENT] Error in cancel button handler:', error);
                showNotification("Erreur lors de l'annulation de l'étape", 'error');
            }
        });
    });

    const specificLogButtons = resolveCollection(dom.getAllSpecificLogButtons, dom.allSpecificLogButtons);
    specificLogButtons.forEach(button => {
        button.addEventListener('click', async () => {
            const stepKey = button.dataset.step;
            const logIndex = button.dataset.logIndex;
            const workflowWrapper = resolveElement(dom.getWorkflowWrapper, dom.workflowWrapper);

            if (!workflowWrapper || !workflowWrapper.classList.contains('logs-active') || appState.getStateProperty('activeStepKeyForLogsPanel') !== stepKey) {
                ui.openLogPanelUI(stepKey);
                try {
                    const statusResponse = await fetch(`/status/${stepKey}`);
                    if (!statusResponse.ok) throw new Error(`Erreur statut: ${statusResponse.status}`);
                    const statusData = await statusResponse.json();
                    ui.updateMainLogOutputUI(statusData.log ? statusData.log.join('') : '<i>Log principal non disponible.</i>');
                } catch (error) {
                    console.error("Erreur chargement statut pour log panel:", error);
                    ui.updateMainLogOutputUI(`<i>Erreur chargement log principal: ${error.message}</i>`);
                }
            }
            // Pass the clicked button to enable loading state (spinner + disabled)
            await api.fetchSpecificLogAPI(stepKey, logIndex, button.textContent.trim(), button);
        });
    });

    const runAllButton = resolveElement(dom.getRunAllButton, dom.runAllButton);
    if (runAllButton) {
        runAllButton.addEventListener('click', async () => {
            if (getIsAnySequenceRunning()) {
                showNotification("Séquence déjà en cours.", 'warning'); return;
            }
            // Play workflow start sound for complete sequence 1-6
            soundEvents.workflowStart();
            await runStepSequence(defaultSequenceableStepsKeys, "Séquence 0-4");
        });
    }

    const customSequenceCheckboxes = resolveCollection(dom.getCustomSequenceCheckboxes, dom.customSequenceCheckboxes);
    customSequenceCheckboxes.forEach(checkbox => {
        checkbox.addEventListener('change', (event) => {
            const stepKey = event.target.dataset.stepKey;
            const stepCard = document.getElementById(`step-${stepKey}`);
            const orderNumberEl = document.getElementById(`order-${stepKey}`);
            let currentOrder = getSelectedStepsOrder();

            // Play checkbox interaction sound
            soundEvents.checkboxInteraction();

            if (event.target.checked) {
                if (!currentOrder.includes(stepKey)) {
                    currentOrder.push(stepKey);
                    if (stepCard) stepCard.classList.add('custom-sequence-selected');
                }
            } else {
                const index = currentOrder.indexOf(stepKey);
                if (index > -1) currentOrder.splice(index, 1);
                if (stepCard) stepCard.classList.remove('custom-sequence-selected');
            }
            setSelectedStepsOrder(currentOrder);
            document.querySelectorAll('.step-selection-order-number').forEach(el => { el.textContent = ''; });
            getSelectedStepsOrder().forEach((sk, idx) => {
                const orderEl = document.getElementById(`order-${sk}`);
                if (orderEl) orderEl.textContent = idx + 1;
            });
            ui.updateCustomSequenceButtonsUI();
        });
    });

    const clearCustomSequenceButton = resolveElement(dom.getClearCustomSequenceButton, dom.clearCustomSequenceButton);
    if (clearCustomSequenceButton) {
        clearCustomSequenceButton.addEventListener('click', () => {
            setSelectedStepsOrder([]);
            customSequenceCheckboxes.forEach(cb => {
                cb.checked = false;
                const stepCard = document.getElementById(`step-${cb.dataset.stepKey}`);
                if (stepCard) stepCard.classList.remove('custom-sequence-selected');
                const orderEl = document.getElementById(`order-${cb.dataset.stepKey}`);
                if (orderEl) orderEl.textContent = '';
            });
            ui.updateCustomSequenceButtonsUI();
        });
    }

    const runCustomSequenceButton = resolveElement(dom.getRunCustomSequenceButton, dom.runCustomSequenceButton);
    if (runCustomSequenceButton) {
        runCustomSequenceButton.addEventListener('click', () => {
            if (getSelectedStepsOrder().length === 0) {
                showNotification("Veuillez sélectionner au moins une étape.", 'warning');
                return;
            }
            if (getIsAnySequenceRunning()) {
                showNotification("Une autre séquence est déjà en cours.", 'warning'); return;
            }
            showCustomSequenceConfirmUI();
        });
    }

    const confirmRunCustomSequenceButton = resolveElement(dom.getConfirmRunCustomSequenceButton, dom.confirmRunCustomSequenceButton);
    const customSequenceConfirmOverlay = resolveElement(dom.getCustomSequenceConfirmPopupOverlay, dom.customSequenceConfirmPopupOverlay);
    if (confirmRunCustomSequenceButton) {
        confirmRunCustomSequenceButton.addEventListener('click', async () => {
            closePopupUI(customSequenceConfirmOverlay);
            if (getIsAnySequenceRunning()) {
                showNotification("Une autre séquence est déjà en cours.", 'warning'); return;
            }
            // Loading state on confirm button and disable run-custom while executing
            try {
                confirmRunCustomSequenceButton.setAttribute('data-loading', 'true');
                confirmRunCustomSequenceButton.disabled = true;
                if (runCustomSequenceButton) runCustomSequenceButton.disabled = true;

                // Play workflow start sound for custom sequence
                soundEvents.workflowStart();
                await runStepSequence(getSelectedStepsOrder(), "Séquence Personnalisée");
            } finally {
                confirmRunCustomSequenceButton.removeAttribute('data-loading');
                confirmRunCustomSequenceButton.disabled = false;
                if (runCustomSequenceButton) runCustomSequenceButton.disabled = getIsAnySequenceRunning();
            }
        });
    }

    const cancelRunCustomSequenceButton = resolveElement(dom.getCancelRunCustomSequenceButton, dom.cancelRunCustomSequenceButton);
    if (cancelRunCustomSequenceButton) {
        cancelRunCustomSequenceButton.addEventListener('click', () => {
            closePopupUI(customSequenceConfirmOverlay);
        });
    }
    const closeSummaryPopupButton = resolveElement(dom.getCloseSummaryPopupButton, dom.closeSummaryPopupButton);
    const sequenceSummaryOverlay = resolveElement(dom.getSequenceSummaryPopupOverlay, dom.sequenceSummaryPopupOverlay);
    if (closeSummaryPopupButton) {
        closeSummaryPopupButton.addEventListener('click', () => {
            closePopupUI(sequenceSummaryOverlay);
        });
    }

    // --- DELETION START: Suppression du gestionnaire d'événement pour le bouton de cache ---
    /*
    if (dom.clearCacheGlobalButton) {
        dom.clearCacheGlobalButton.addEventListener('click', async () => {
            const stepKey = 'clear_disk_cache'; 
            // ... (toute la logique interne est supprimée) ...
        });
    }
    */
    // --- DELETION END ---



    // Sound control toggle event handler
    if (dom.getSoundToggle()) {
        // Import sound manager functions
        import('./soundManager.js').then(({ isSoundEnabled, setSoundEnabled }) => {
            // Initialize the toggle state from localStorage
            const isEnabled = isSoundEnabled();
            dom.getSoundToggle().checked = isEnabled;
            if (dom.getSoundStatus()) {
                dom.getSoundStatus().textContent = isEnabled ? 'Activé' : 'Désactivé';
            }

            dom.getSoundToggle().addEventListener('change', (event) => {
                const enabled = event.target.checked;
                setSoundEnabled(enabled);
                if (dom.getSoundStatus()) {
                    dom.getSoundStatus().textContent = enabled ? 'Activé' : 'Désactivé';
                }
                console.log(`[EVENT] Sound effects ${enabled ? 'enabled' : 'disabled'} by user`);
            });
        });
    }
}
```

## File: templates/index_new.html
```html
<!DOCTYPE html>
<html lang="fr">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Workflow Manager</title>
    <!-- Core styles (always loaded first) -->
    <link rel="stylesheet" href="{{ url_for('static', filename='css/variables.css') }}?v={{ cache_buster }}">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/themes.css') }}?v={{ cache_buster }}">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}?v={{ cache_buster }}">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}?v={{ cache_buster }}">

    <!-- Component styles -->
    <link rel="stylesheet" href="{{ url_for('static', filename='css/components/notifications.css') }}?v={{ cache_buster }}">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/components/controls.css') }}?v={{ cache_buster }}">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/components/steps.css') }}?v={{ cache_buster }}">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/components/logs.css') }}?v={{ cache_buster }}">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/components/workflow-buttons.css') }}?v={{ cache_buster }}">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/components/popups.css') }}?v={{ cache_buster }}">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/components/downloads.css') }}?v={{ cache_buster }}">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/components/widgets.css') }}?v={{ cache_buster }}">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/components/csv-workflow-prompt.css') }}?v={{ cache_buster }}">
    <!-- Utilities and responsive (loaded last) -->
    <link rel="stylesheet" href="{{ url_for('static', filename='css/utils/animations.css') }}?v={{ cache_buster }}">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/features/responsive.css') }}?v={{ cache_buster }}">
</head>

<body>
    <!-- DELETION: Le bouton de cache est déjà supprimé, c'est parfait. -->

    <div id="notifications-area" aria-live="assertive"></div>

    <!-- Unified Controls Section (Top Sticky Bar) -->
    <div class="topbar-affix" id="topbar-affix">
        <div class="unified-controls-section unified-controls--topbar" id="topbar-controls">
            <div class="workflow-controls">
                <div class="sequence-controls">
                    <div class="control-group control-group--primary" role="group" aria-label="Actions principales du workflow">
                        <button id="run-all-steps-button">✨ Lancer le Workflow Complet (1-7)</button>
                    </div>
                    <div class="control-group control-group--secondary" role="group" aria-label="Actions secondaires du workflow">
                        <button id="run-custom-sequence-button" disabled>🎯 Lancer Séquence Personnalisée</button>
                        <button id="clear-custom-sequence-button" disabled>🗑️ Vider Séquence</button>
                        <button id="toggle-local-downloads" class="downloads-toggle" aria-pressed="true" title="Afficher/Masquer les Téléchargements Locaux">📥 Téléchargements</button>
                        <button id="settings-toggle" class="settings-toggle" aria-haspopup="true" aria-expanded="false" aria-controls="settings-panel" title="Afficher les options">⚙️ Settings</button>
                    </div>
                </div>
            </div>

            <div class="global-progress-affix" id="global-progress-affix">
                <div class="global-progress-container" id="global-progress-container">
                    <div id="global-progress-bar" role="progressbar" aria-valuenow="0" aria-valuemin="0" aria-valuemax="100">0%</div>
                </div>
                <div id="global-progress-text" aria-live="polite"></div>
            </div>
            <div id="settings-panel" class="settings-panel" hidden>
                <section class="settings-section" aria-label="Affichage et préférences">
                    <p class="settings-title">Affichage & Préférences</p>
                    <div class="settings-row settings-row--stacked">
                        <div class="theme-selector-container settings-block">
                            <label for="theme-selector" class="theme-selector-label">Thème:</label>
                            <select id="theme-selector" class="theme-selector" aria-label="Sélectionner un thème visuel">
                                <!-- Options populated by themeManager.js -->
                            </select>
                        </div>

                        <div id="sound-control-widget" class="sound-control-widget settings-block">
                            <label class="btn-like-switch" for="sound-toggle" aria-label="Activer/Désactiver les effets sonores">
                                🔊 Effets Sonores
                                <input type="checkbox" id="sound-toggle" checked>
                            </label>
                        </div>
                    </div>
                </section>

                        </div>
        </div>

    </div>

    <div id="topbar-spacer" aria-hidden="true"></div>

    <div class="local-downloads-section card-like-section">
        <h2>
            <span class="section-icon">📥</span> Téléchargements Locaux
        </h2>
        <div class="local-downloads-list-container">
            <ul id="local-downloads-list" class="status-list" aria-live="polite">
                <li class="placeholder">Aucune activité de téléchargement locale récente.</li>
            </ul>
        </div>
    </div>

    <div class="workflow-wrapper compact-mode" id="workflow-wrapper">
        <div class="steps-column" id="steps-column">
            <section id="workflow-steps" class="workflow-pipeline" role="region" aria-label="Pipeline de traitement">
                <div class="pipeline-timeline" role="list">
                    <div class="timeline-axis" aria-hidden="true"></div>
                    {% for step_key, config in steps_config.items() %} {# On s'assure de ne pas afficher une étape qui n'aurait pas de config #}
                    {% if config %}
                    <div class="timeline-row">
                        <div class="timeline-rail-column" aria-hidden="true">
                            <div class="timeline-node" data-step="{{ step_key }}"></div>
                        </div>
                        <div class="timeline-cards-column">
                            <div class="step timeline-step" id="step-{{ step_key }}" data-step-key="{{ step_key }}" data-step-name="{{ config.display_name }}" data-status="idle" role="listitem" tabindex="0" aria-controls="step-details-panel" aria-expanded="false">
                                <div class="timeline-content">
                            <div class="timeline-head">
                                <div class="step-title-group">
                                {# --- MODIFICATION: Logique des icônes mise à jour pour l'inversion --- #}
                                <h2><span class="step-icon">{% if step_key == 'STEP1' %}🗜️{% elif step_key == 'STEP2' %}🔄{% elif step_key == 'STEP3' %}✂️{% elif step_key == 'STEP4' %}🔊{% elif step_key == 'STEP5' %}👀{% elif step_key == 'STEP6' %}🧩{% elif step_key == 'STEP7' %}📦{% else %}⚙️{% endif %}</span>{{ config.display_name }}</h2>
                                <span class="step-state-chip state-idle" id="state-chip-{{ step_key }}" aria-live="polite">Prêt</span>
                                </div>
                                <div class="step-selection-control">
                                    <span class="step-selection-order-number" id="order-{{step_key}}"></span>
                                    <input type="checkbox" class="custom-sequence-checkbox" data-step-key="{{ step_key }}" title="Sélectionner pour séquence personnalisée" aria-label="Sélectionner {{ config.display_name }} pour séquence personnalisée">
                                </div>
                            </div>
                            <div class="node-actions step-controls">
                                <button class="run-button" data-step="{{ step_key }}">Lancer</button>
                                <button class="cancel-button" data-step="{{ step_key }}" disabled>Annuler</button>
                            </div>
                            <div class="timeline-body">
                                <div class="status-line">
                                    Statut:
                                    <span id="status-{{ step_key }}" class="status-badge status-idle" aria-live="polite">Prêt</span>
                                    <span class="timer" id="timer-{{ step_key }}"></span>
                                </div>
                                <div class="step-progress-container" id="progress-container-{{ step_key }}" style="display: none;">
                                    <div class="progress-bar-wrapper">
                                        <div class="progress-bar-step" id="progress-bar-{{ step_key }}" role="progressbar" aria-valuenow="0" aria-valuemin="0" aria-valuemax="100">0%</div>
                                    </div>
                                    <div class="progress-text-step" id="progress-text-{{ step_key }}" aria-live="polite"></div>
                                </div>
                                {% if config.specific_logs %}
                                <div class="specific-log-controls-wrapper">
                                    <h4>Logs Spécifiques :</h4>
                                    <div id="log-panel-specific-buttons-container-{{ step_key }}">
                                        {% for log_conf in config.specific_logs %}
                                        <button class="specific-log-button" data-step="{{ step_key }}" data-log-index="{{ loop.index0 }}">{{ log_conf.name }}</button>
                                        {% endfor %}
                                    </div>
                                </div>
                                {% endif %} {# --- DELETION: La zone de progression des workers de tracking est supprimée --- #}
                            </div>
                        </div>
                    </div>
                        </div>
                    </div>
                    {% endif %}
                    {% endfor %}
                </div>
                <div class="timeline-scroll-spacer" aria-hidden="true"></div>
                <aside id="step-details-panel" class="step-details-panel" role="complementary" aria-label="Détails de l'étape" hidden>
                    <div class="step-details-header">
                        <div class="step-details-title" id="step-details-title">Détails</div>
                        <button id="close-step-details" class="step-details-close" type="button" aria-label="Fermer le panneau détails">×</button>
                    </div>
                    <div class="step-details-body">
                        <div class="step-details-meta">
                            <span id="step-details-status" class="status-badge status-idle" aria-live="polite">Prêt</span>
                            <span id="step-details-timer" class="timer"></span>
                        </div>
                        <div class="step-details-progress">
                            <div id="step-details-progress-text" class="progress-text-step" aria-live="polite"></div>
                        </div>
                        <div class="step-details-actions">
                            <button id="step-details-run" class="run-button" type="button" disabled>Lancer</button>
                            <button id="step-details-cancel" class="cancel-button" type="button" disabled>Annuler</button>
                            <button id="step-details-open-logs" class="step-details-open-logs" type="button" disabled>Logs</button>
                        </div>
                    </div>
                </aside>
            </section>
        </div>
        <div class="logs-column" id="logs-column-global">
            <div class="log-panel-header">
                <div class="log-panel-header-main">
                    <span id="log-panel-title">Logs</span>
                    <button id="close-log-panel" title="Fermer le panneau des logs" aria-label="Fermer le panneau des logs">×</button>
                </div>
                <div class="log-panel-subheader" id="log-panel-subheader" aria-live="polite">
                    <span id="log-panel-context-step">Aucune étape active</span>
                    <span id="log-panel-context-status"></span>
                    <span id="log-panel-context-timer"></span>
                </div>
                <div class="log-panel-specific-buttons" id="log-panel-specific-buttons-container" aria-label="Logs spécifiques"></div>
            </div>
            <div class="log-container" id="main-log-container-panel">
                <div class="log-header">
                    Log Principal (<span id="current-step-log-name-panel">Aucune étape active</span>)
                </div>
                <div class="log-output" id="main-log-output-panel" aria-live="polite" aria-atomic="true"></div>
            </div>
            <div class="log-container" id="specific-log-container-panel" style="display:none;">
                <div class="log-header" id="specific-log-header-text-panel">Log Spécifique</div>
                <div class="specific-log-path" id="specific-log-path-info-panel"></div>
                <div class="specific-log-output" id="specific-log-output-content-panel" aria-live="polite" aria-atomic="true"></div>
            </div>
        </div>
    </div>

    <!-- Popups de confirmation et de résumé -->
    <div id="sequence-summary-popup-overlay" class="popup-overlay" role="dialog" aria-modal="true" aria-labelledby="sequence-summary-title">
        <div class="popup-content">
            <h3 id="sequence-summary-title">Résumé de la Séquence</h3>
            <ul id="sequence-summary-list" class="popup-list"></ul>
            <button id="close-summary-popup" class="popup-cancel-button">Fermer</button>
        </div>
    </div>
    <!-- --- BLOC MANQUANT À AJOUTER CI-DESSOUS --- -->
    <div id="custom-sequence-confirm-popup-overlay" class="popup-overlay" role="dialog" aria-modal="true" aria-labelledby="custom-sequence-confirm-title">
        <div class="popup-content">
            <h3 id="custom-sequence-confirm-title">Confirmer la Séquence Personnalisée</h3>
            <p>
                Les étapes suivantes seront lancées dans cet ordre :
            </p>
            <ul id="custom-sequence-confirm-list" class="popup-list"></ul>
            <div class="popup-buttons">
                <button id="confirm-run-custom-sequence-button" class="popup-confirm-button">Lancer la Séquence</button>
                <button id="cancel-run-custom-sequence-button" class="popup-cancel-button">Annuler</button>
            </div>
        </div>
    </div>

    <!-- Widget de Monitoring Système (mis à jour) -->
    <div id="system-monitor-widget" class="system-monitor-widget">
        <div class="monitor-header">
            <span class="monitor-icon">💻</span>
            <span class="monitor-title">Moniteur Système</span>
            <button id="system-monitor-minimize" class="monitor-close" aria-label="Réduire le moniteur" title="Réduire (affichage compact)">×</button>
        </div>
        <!-- Ligne compacte visible en mode réduit -->
        <div id="monitor-compact-line" class="monitor-compact-line" aria-hidden="true" style="display:none;">
            C: <span id="compact-cpu"></span> · R: <span id="compact-ram"></span> · G: <span id="compact-gpu"></span>
        </div>
        <div class="monitor-item">
            <span class="monitor-label">CPU</span>
            <div class="monitor-bar-container">
                <div id="cpu-monitor-bar" class="monitor-bar"></div>
            </div>
            <span id="cpu-monitor-value" class="monitor-value">... %</span>
        </div>
        <div class="monitor-item">
            <span class="monitor-label">RAM</span>
            <div class="monitor-bar-container">
                <div id="ram-monitor-bar" class="monitor-bar"></div>
            </div>
            <span id="ram-monitor-value" class="monitor-value">... %</span>
        </div>
        <div id="ram-monitor-details" class="monitor-details">... / ... GB</div>

        <!-- --- MODIFICATION: Ajout de la section GPU --- -->
        <div id="gpu-monitor-section" style="display: none;">
            <!-- Caché par défaut, affiché par JS si GPU détecté -->
            <div class="monitor-item">
                <span class="monitor-label">GPU</span>
                <div class="monitor-bar-container">
                    <div id="gpu-monitor-bar" class="monitor-bar"></div>
                </div>
                <span id="gpu-monitor-value" class="monitor-value">... %</span>
            </div>
            <div id="gpu-monitor-details" class="monitor-details">... °C | ... / ... GB</div>
        </div>
        <div id="gpu-monitor-error" class="monitor-details" style="display: none; color: var(--red);"></div>
        <!-- --- FIN DE LA MODIFICATION --- -->
    </div>

    
    <script id="steps-config-data" type="application/json">{{ steps_config | tojson | safe }}</script>
    <script src="{{ url_for('static', filename='main.js') }}?v={{ cache_buster }}" type="module" defer></script>
</body>

</html>
```

## File: static/main.js
```javascript
import * as dom from './domElements.js';
import * as ui from './uiUpdater.js';
import * as api from './apiService.js';
import { initializeEventHandlers } from './eventHandlers.js';
import { POLLING_INTERVAL } from './constants.js';
import { showNotification } from './utils.js';

window.showNotification = showNotification;
import { showSequenceSummaryUI } from './popupManager.js';
import { scrollToStepImmediate } from './scrollManager.js';

import { initializeSoundManager } from './soundManager.js';
import { pollingManager } from './utils/PollingManager.js';
import { errorHandler } from './utils/ErrorHandler.js';
import { performanceMonitor } from './utils/PerformanceMonitor.js';
import { domBatcher, DOMUpdateUtils } from './utils/DOMBatcher.js';

import { performanceOptimizer } from './utils/PerformanceOptimizer.js';
import { appState } from './state/AppState.js';
import { initializeCSVDownloadMonitor } from './csvDownloadMonitor.js';
import { themeManager } from './themeManager.js';
import { reportViewer } from './reportViewer.js';
import { fetchWithLoadingState } from './apiService.js';

import { initializeStepDetailsPanel } from './stepDetailsPanel.js';

window.addEventListener('unhandledrejection', (event) => {
    console.error('[MAIN] Unhandled promise rejection:', event.reason);

    // Check if this is the specific browser extension error we're trying to fix
    if (event.reason && event.reason.message &&
        event.reason.message.includes('message channel closed')) {
        console.debug('[MAIN] Suppressing browser extension message channel error');
        event.preventDefault(); // Prevent the error from appearing in console
        return;
    }
});

function setupLocalDownloadsToggle() {
    const section = document.querySelector('.local-downloads-section');
    const btn = document.getElementById('toggle-local-downloads');
    if (!section || !btn) return;

    let visible = true;
    try {
        const stored = localStorage.getItem('ui.localDownloadsVisible');
        if (stored !== null) visible = stored === 'true';
    } catch (_) {}

    if (visible) {
        section.style.display = '';
    } else {
        section.style.display = 'none';
    }

    applyLocalDownloadsVisibility(section, btn, visible);

    btn.addEventListener('click', () => {
        visible = !(btn.getAttribute('aria-pressed') === 'true');
        applyLocalDownloadsVisibility(section, btn, visible);
        try { localStorage.setItem('ui.localDownloadsVisible', String(visible)); } catch (_) {}
        appState.setState({ ui: { localDownloadsVisible: visible } }, 'downloads_visibility_toggle');
        // Clear alert state if revealing
        if (visible) {
            btn.classList.remove('downloads-toggle--alert');
            try { localStorage.removeItem('ui.localDownloadsAlertedOnce'); } catch (_) {}
        }
    });

    updateDownloadsToggleAlert(appState.getStateProperty('csvDownloads') || []);
}

function applyLocalDownloadsVisibility(section, btn, visible) {
    domBatcher.scheduleUpdate('downloads-visibility-toggle', () => {
        if (visible) {
            section.style.display = '';
            section.classList.remove('minimized');
            btn.setAttribute('aria-pressed', 'true');
            btn.classList.remove('downloads-toggle--hidden');
            // Focus and highlight the Downloads section title for accessibility feedback
            requestAnimationFrame(() => {
                const title = section.querySelector('h2');
                if (title) {
                    safeFocusAndHighlight(title);
                }
            });
        } else {
            btn.setAttribute('aria-pressed', 'false');
            btn.classList.add('downloads-toggle--hidden');
            section.style.display = 'none';
        }
    });
}

function safeFocusAndHighlight(el) {
    if (!el || typeof el !== 'object') return;
    try {
        // Make sure element can be focused without altering tab order permanently
        let removeTabIndex = false;
        if (el !== document.body && el.tabIndex === -1) {
            // Already programmatically focusable
        } else if (el !== document.body && (el.getAttribute && el.getAttribute('tabindex') === null)) {
            el.setAttribute('tabindex', '-1');
            removeTabIndex = true;
        }
        el.focus && el.focus();
        // Apply highlight flash
        if (el.classList) {
            el.classList.add('section-focus-highlight');
            setTimeout(() => { try { el.classList.remove('section-focus-highlight'); } catch(_){} }, 450);
        }
        if (removeTabIndex) {
            setTimeout(() => { try { el.removeAttribute('tabindex'); } catch(_){} }, 500);
        }
    } catch(_){}
}

function updateDownloadsToggleAlert(downloads) {
    const btn = document.getElementById('toggle-local-downloads');
    const section = document.querySelector('.local-downloads-section');
    if (!btn || !section) return;

    const visible = btn.getAttribute('aria-pressed') === 'true';
    const list = Array.isArray(downloads) ? downloads : [];
    const inProgress = list.some(d => {
        const s = (d && d.status) ? String(d.status).toLowerCase() : '';
        return s === 'downloading' || s === 'starting' || s === 'pending';
    });

    if (!visible && inProgress) {
        btn.classList.add('downloads-toggle--alert');
        try {
            const alerted = localStorage.getItem('ui.localDownloadsAlertedOnce') === 'true';
            if (!alerted && typeof window.showNotification === 'function') {
                window.showNotification('Téléchargements', 'Des téléchargements locaux sont en cours. Cliquez pour afficher.');
                localStorage.setItem('ui.localDownloadsAlertedOnce', 'true');
            }
        } catch (_) {}
    } else {
        btn.classList.remove('downloads-toggle--alert');
        try { localStorage.removeItem('ui.localDownloadsAlertedOnce'); } catch (_) {}
    }
}

window.addEventListener('error', (event) => {
    console.error('[MAIN] Uncaught error:', event.error);
    errorHandler.handleApiError('uncaught-error', event.error);
});

let stepsConfigData = {};
try {
    stepsConfigData = JSON.parse(document.getElementById('steps-config-data').textContent);
} catch (e) {
    console.error("Could not parse steps_config_data:", e);
}
ui.setStepsConfig(stepsConfigData);

function initializeProcessInfoFromDOM() {
    const initialProcessInfo = {};
    document.querySelectorAll('.step').forEach(s => {
        const stepKey = s && s.dataset ? s.dataset.stepKey : null;
        if (!stepKey) return;
        initialProcessInfo[stepKey] = {
            status: 'idle',
            log: [],
            progress_current: 0,
            progress_total: 0,
            progress_text: '',
            is_any_sequence_running: false
        };
    });
    appState.setState({ processInfo: initialProcessInfo }, 'process_info_init');
}

const LOCAL_DOWNLOAD_POLLING_INTERVAL = POLLING_INTERVAL * 2;

const SYSTEM_MONITOR_POLLING_INTERVAL = 5000;



function initializeStateManagement() {
    console.log('Initializing state management and performance monitoring...');

    if (window.location.hostname === 'localhost' || window.location.hostname === '127.0.0.1') {
        appState.subscribe((newState, oldState, source) => {
            console.debug(`[StateManagement] State changed from ${source}:`, {
                changes: findStateChanges(oldState, newState),
                newState: newState
            });
        });
    }

    appState.subscribeToProperty('isAnySequenceRunning', (newValue, oldValue) => {
        if (newValue !== oldValue) {
            ui.updateGlobalUIForSequenceState(newValue);
        }
    });

    appState.subscribeToProperty('activeStepKeyForLogsPanel', (newValue, oldValue) => {
        if (newValue !== oldValue && newValue) {
            domBatcher.scheduleUpdate('logs-panel-update', () => {
                console.debug(`Active step for logs changed to: ${newValue}`);
            });
        }
    });

    console.log('Performance monitoring initialized automatically');

    setTimeout(() => {
        if (typeof CacheService !== 'undefined' && CacheService.warm_cache) {
            CacheService.warm_cache();
        }
    }, 1000);

    console.log('State management and performance monitoring initialized successfully');
}

function findStateChanges(oldState, newState) {
    const changes = {};

    function compareObjects(old, current, path = '') {
        for (const key in current) {
            const currentPath = path ? `${path}.${key}` : key;

            if (typeof current[key] === 'object' && current[key] !== null && !Array.isArray(current[key])) {
                if (typeof old[key] === 'object' && old[key] !== null) {
                    compareObjects(old[key], current[key], currentPath);
                } else {
                    changes[currentPath] = { from: old[key], to: current[key] };
                }
            } else if (old[key] !== current[key]) {
                changes[currentPath] = { from: old[key], to: current[key] };
            }
        }
    }

    compareObjects(oldState, newState);
    return changes;
}

async function pollLocalDownloadsStatus() {
    if (!dom.getLocalDownloadsList()) return;

    try {
        const downloads = await api.fetchLocalDownloadsStatusAPI();
        ui.updateLocalDownloadsListUI(downloads);

        // Update app state with current downloads for CSV monitoring
        appState.setState({ csvDownloads: downloads }, 'downloads_polled');

        // Update toggle alert state if section is hidden and activity occurs
        updateDownloadsToggleAlert(downloads);

        // Clear error state on success
        errorHandler.clearErrors('localDownloadsStatus', {
            elementId: 'local-downloads-list'
        });

    } catch (error) {
        // Handle error with proper user feedback and exponential backoff
        const delay = await errorHandler.handlePollingError('localDownloadsStatus', error, {
            elementId: 'local-downloads-list'
        });

        // Apply delay if needed (PollingManager will handle this automatically)
        if (delay > 0) {
            console.debug(`Applying ${delay}ms delay for localDownloadsStatus polling`);
        }
    }
}



async function pollSystemMonitor() {
    const monitorWidget = document.getElementById('system-monitor-widget');
    if (!monitorWidget) {
        console.warn('[MAIN] System monitor widget not found during polling');
        return;
    }

    try {
        const response = await fetch('/api/system_monitor');
        if (!response.ok) {
            console.warn(`[MAIN] System monitor API failed: ${response.status}`);
            monitorWidget.style.opacity = '0.5';
            return;
        }
        const data = await response.json();
        console.debug('[MAIN] System monitor data received:', data);

        // Batch DOM updates to minimize reflows
        domBatcher.scheduleUpdate('system-monitor-update', () => {
            // CPU
            const cpuBar = document.getElementById('cpu-monitor-bar');
            const cpuValue = document.getElementById('cpu-monitor-value');
            if (cpuBar && cpuValue) {
                const cpuPercent = data.cpu_percent || 0;
                cpuBar.style.width = `${cpuPercent}%`;
                cpuValue.textContent = `${cpuPercent.toFixed(1)} %`;
                cpuBar.dataset.usageLevel = cpuPercent > 85 ? 'high' : cpuPercent > 60 ? 'medium' : 'low';
            }

            // RAM
            const ramBar = document.getElementById('ram-monitor-bar');
            const ramValue = document.getElementById('ram-monitor-value');
            const ramDetails = document.getElementById('ram-monitor-details');
            if (ramBar && ramValue && ramDetails && data.memory) {
                const memPercent = data.memory.percent || 0;
                ramBar.style.width = `${memPercent}%`;
                ramValue.textContent = `${memPercent.toFixed(1)} %`;
                ramDetails.textContent = `${data.memory.used_gb.toFixed(2)} / ${data.memory.total_gb.toFixed(2)} GB`;
                ramBar.dataset.usageLevel = memPercent > 85 ? 'high' : memPercent > 70 ? 'medium' : 'low';
            }

            // GPU
            const gpuSection = document.getElementById('gpu-monitor-section');
            const gpuError = document.getElementById('gpu-monitor-error');
            if (gpuSection && gpuError) {
                if (data.gpu && !data.gpu.error) {
                    gpuSection.style.display = 'block';
                    gpuError.style.display = 'none';

                    const gpuBar = document.getElementById('gpu-monitor-bar');
                    const gpuValue = document.getElementById('gpu-monitor-value');
                    const gpuDetails = document.getElementById('gpu-monitor-details');

                    const gpuPercent = data.gpu.utilization_percent || 0;
                    gpuBar.style.width = `${gpuPercent}%`;
                    gpuValue.textContent = `${gpuPercent.toFixed(1)} %`;
                    gpuBar.dataset.usageLevel = gpuPercent > 85 ? 'high' : gpuPercent > 60 ? 'medium' : 'low';

                    const temp = data.gpu.temperature_c || 'N/A';
                    const memUsed = data.gpu.memory ? data.gpu.memory.used_gb.toFixed(2) : 'N/A';
                    const memTotal = data.gpu.memory ? data.gpu.memory.total_gb.toFixed(2) : 'N/A';
                    gpuDetails.textContent = `${temp}°C | ${memUsed} / ${memTotal} GB`;
                } else {
                    gpuSection.style.display = 'none';
                    if (data.gpu && data.gpu.error) {
                        gpuError.textContent = data.gpu.error;
                        gpuError.style.display = 'block';
                    }
                }
            }

            // Compact line values (shown when minimized)
            const compactLine = document.getElementById('monitor-compact-line');
            if (compactLine) {
                const compactCpu = document.getElementById('compact-cpu');
                const compactRam = document.getElementById('compact-ram');
                const compactGpu = document.getElementById('compact-gpu');

                const cpuPercent = data.cpu_percent || 0;
                const memPercent = (data.memory && data.memory.percent) ? data.memory.percent : 0;

                if (compactCpu) compactCpu.textContent = `${cpuPercent.toFixed(1)}%`;
                if (compactRam && data.memory) {
                    const used = (typeof data.memory.used_gb === 'number') ? data.memory.used_gb.toFixed(1) : 'N/A';
                    const total = (typeof data.memory.total_gb === 'number') ? data.memory.total_gb.toFixed(1) : 'N/A';
                    compactRam.textContent = `${memPercent.toFixed(1)}% (${used}/${total}G)`;
                }
                if (compactGpu) {
                    if (data.gpu && !data.gpu.error && typeof data.gpu.utilization_percent === 'number') {
                        const temp = (typeof data.gpu.temperature_c === 'number') ? data.gpu.temperature_c : 'N/A';
                        const gUsed = data.gpu.memory && typeof data.gpu.memory.used_gb === 'number' ? data.gpu.memory.used_gb.toFixed(1) : 'N/A';
                        const gTotal = data.gpu.memory && typeof data.gpu.memory.total_gb === 'number' ? data.gpu.memory.total_gb.toFixed(1) : 'N/A';
                        compactGpu.textContent = `${data.gpu.utilization_percent.toFixed(1)}% (${temp}C)`;
                    } else if (data.gpu && data.gpu.error) {
                        compactGpu.textContent = 'err';
                    } else {
                        compactGpu.textContent = 'N/A';
                    }
                }
            }

            monitorWidget.style.opacity = '1';
        });

        // Clear error state on success
        errorHandler.clearErrors('systemMonitor', {
            elementId: 'system-monitor-widget'
        });

    } catch (error) {
        // Handle error with proper user feedback
        const delay = await errorHandler.handlePollingError('systemMonitor', error, {
            elementId: 'system-monitor-widget'
        });

        // Visual feedback for system monitor errors
        domBatcher.scheduleUpdate('system-monitor-error', () => {
            monitorWidget.style.opacity = '0.5';
        });

        if (delay > 0) {
            console.debug(`Applying ${delay}ms delay for systemMonitor polling`);
        }
    }
}

document.addEventListener('DOMContentLoaded', async () => {
    ui.closeLogPanelUI();
    if (dom.sequenceSummaryPopupOverlay) dom.sequenceSummaryPopupOverlay.style.display = 'none';
    
    if (dom.customSequenceConfirmPopupOverlay) dom.customSequenceConfirmPopupOverlay.style.display = 'none';

    // Initialize theme system
    themeManager.init();

    if (document.getElementById('report-overlay')) {
        reportViewer.init();
    }


    const initialStatusPromises = [];
    const allStepKeysForInitialStatus = Object.keys(stepsConfigData);

    initializeProcessInfoFromDOM();

    allStepKeysForInitialStatus.forEach(stepKey => {
        initialStatusPromises.push(api.fetchInitialStatusAPI(stepKey));
    });



    try {
        await Promise.all(initialStatusPromises);
    } catch (error) {
        console.warn("[Main.js DOMContentLoaded] Error fetching some initial statuses:", error);
    } finally {
        ui.updateGlobalUIForSequenceState(appState.getStateProperty('isAnySequenceRunning'));
        ui.updateCustomSequenceButtonsUI();
    }

    initializeEventHandlers();

    // Initialize sound manager
    initializeSoundManager();

    // Initialize CSV download monitor for auto-workflow prompts
    initializeCSVDownloadMonitor();

    // Initialize state management and performance monitoring
    initializeStateManagement();

    // Initialize polling with proper resource management
    if (dom.getLocalDownloadsList()) {
        pollingManager.startPolling(
            'localDownloadsStatus',
            pollLocalDownloadsStatus,
            LOCAL_DOWNLOAD_POLLING_INTERVAL,
            { immediate: true }
        );
    }

    // Setup unified-controls toggle for Local Downloads visibility
    setupLocalDownloadsToggle();




    // System monitor polling - with retry mechanism
    const startSystemMonitorPolling = () => {
        const widget = document.getElementById('system-monitor-widget');
        if (widget) {
            console.log('[MAIN] ✅ Starting system monitor polling');
            pollingManager.startPolling(
                'systemMonitor',
                pollSystemMonitor,
                SYSTEM_MONITOR_POLLING_INTERVAL,
                { immediate: true }
            );
            return true;
        } else {
            console.warn('[MAIN] ⚠️ System monitor widget not found, retrying...');
            return false;
        }
    };

    // Try immediately, then retry if needed
    if (!startSystemMonitorPolling()) {
        setTimeout(() => {
            if (!startSystemMonitorPolling()) {
                console.error('[MAIN] ❌ System monitor widget not found after retry');
            }
        }, 1000);
    }

    setupCompactMode();

    setupSettingsPanel();

    setupSystemMonitorMinimize();

    setupKeyboardShortcuts();

    initializeStepDetailsPanel();
});

function setupCompactMode() {
    const wrapper = typeof dom.getWorkflowWrapper === 'function'
        ? dom.getWorkflowWrapper()
        : dom.workflowWrapper;
    if (!wrapper) {
        console.warn('[COMPACT] Wrapper not found, skipping setup');
        return;
    }

    // Force compact mode as the only mode
    appState.setState({ ui: { compactMode: true } }, 'compact_forced_default');
    try { localStorage.setItem('ui.compactMode', 'true'); } catch (_) {}

    // Apply immediately and keep in sync if state changes elsewhere
    applyCompactClass(wrapper, true);
    appState.subscribeToProperty('ui.compactMode', (newVal) => {
        applyCompactClass(wrapper, !!newVal);
    });
}

function applyCompactClass(wrapper, enabled) {
    domBatcher.scheduleUpdate('compact-mode-toggle', () => {
        if (enabled) {
            wrapper.classList.add('compact-mode');
        } else {
            wrapper.classList.remove('compact-mode');
        }
    });
}


function setupSystemMonitorMinimize() {
    const widget = document.getElementById('system-monitor-widget');
    const btn = document.getElementById('system-monitor-minimize');
    const compactLine = document.getElementById('monitor-compact-line');
    if (!widget || !btn) {
        console.warn('[SYSTEM-MONITOR] Elements not found, skipping minimize setup');
        return;
    }

    // Init from storage
    let stored = null;
    try { stored = localStorage.getItem('ui.systemMonitorMinimized'); } catch (_) {}
    const minimized = stored === 'true';
    appState.setState({ ui: { systemMonitorMinimized: minimized } }, 'system_monitor_init');
    applySystemMonitorMinimized(widget, compactLine, minimized);

    // Subscribe to state changes
    appState.subscribeToProperty('ui.systemMonitorMinimized', (newVal) => {
        applySystemMonitorMinimized(widget, compactLine, !!newVal);
        try { localStorage.setItem('ui.systemMonitorMinimized', (!!newVal).toString()); } catch (_) {}
    });

    // Click on button to minimize
    btn.addEventListener('click', (e) => {
        e.stopPropagation();
        const current = !!appState.getStateProperty('ui.systemMonitorMinimized');
        appState.setState({ ui: { systemMonitorMinimized: !current } }, 'system_monitor_toggle');
    });

    // Click on widget restores when minimized
    widget.addEventListener('click', () => {
        const isMinimized = widget.classList.contains('minimized');
        if (isMinimized) {
            appState.setState({ ui: { systemMonitorMinimized: false } }, 'system_monitor_restore_click');
        }
    });
}

function applySystemMonitorMinimized(widget, compactLine, minimized) {
    domBatcher.scheduleUpdate('system-monitor-minimized-toggle', () => {
        if (minimized) {
            widget.classList.add('minimized');
            if (compactLine) {
                compactLine.style.display = 'flex';
                compactLine.setAttribute('aria-hidden', 'false');
            }
        } else {
            widget.classList.remove('minimized');
            if (compactLine) {
                compactLine.style.display = 'none';
                compactLine.setAttribute('aria-hidden', 'true');
            }
        }
    });
}

function setupSettingsPanel() {
    const toggle = dom.getSettingsToggle();
    const panel = dom.getSettingsPanel();
    if (!toggle || !panel) {
        console.warn('[SETTINGS] Elements not found, skipping setup');
        return;
    }

    // Init from storage (optional), then AppState
    try {
        const stored = localStorage.getItem('ui.settingsOpen');
        if (stored !== null) {
            appState.setState({ ui: { settingsOpen: stored === 'true' } }, 'settings_init_storage');
        }
    } catch (e) {
        console.debug('[SETTINGS] localStorage not available', e);
    }

    // Apply initial state
    const initialOpen = !!appState.getStateProperty('ui.settingsOpen');
    applySettingsPanel(panel, toggle, initialOpen);

    // Keep DOM in sync with state changes
    appState.subscribeToProperty('ui.settingsOpen', (open) => {
        applySettingsPanel(panel, toggle, !!open);
    });

    // Toggle handler
    toggle.addEventListener('click', () => {
        const next = !appState.getStateProperty('ui.settingsOpen');
        appState.setState({ ui: { settingsOpen: next } }, 'settings_toggle');
        try { localStorage.setItem('ui.settingsOpen', String(next)); } catch (_) {}
    });
}

function applySettingsPanel(panel, toggle, open) {
    domBatcher.scheduleUpdate('settings-panel-update', () => {
        if (!panel || !toggle) return;
        if (open) {
            panel.classList.add('open');
            panel.hidden = false;
        } else {
            panel.classList.remove('open');
            panel.hidden = true;
        }
        toggle.setAttribute('aria-expanded', open ? 'true' : 'false');
        toggle.setAttribute('aria-label', open ? 'Refermer les réglages' : 'Ouvrir les réglages');
    });
}

function setupKeyboardShortcuts() {
    document.addEventListener('keydown', (e) => {
        // Only handle shortcuts when not typing in inputs
        if (e.target && (e.target.tagName === 'INPUT' || e.target.tagName === 'TEXTAREA' || e.target.contentEditable === 'true')) {
            return;
        }

        // Toggle Settings panel with 'S' key
        if (e.key === 's' || e.key === 'S') {
            e.preventDefault();
            const toggle = dom.getSettingsToggle();
            if (toggle) {
                toggle.click();
            }
        }
    });
}
```

## File: static/domElements.js
```javascript
const _SAFE_STEP_KEY_PATTERN = /^[A-Za-z0-9_-]+$/;

function byId(id) {
    return document.getElementById(id);
}

function bySelectorAll(selector) {
    return document.querySelectorAll(selector);
}

export const getWorkflowWrapper = () => byId('workflow-wrapper');
export const getStepsColumn = () => byId('steps-column');
export const getLogsColumnGlobal = () => byId('logs-column-global');
export const getLogPanelTitle = () => byId('log-panel-title');
export const getLogPanelSubheader = () => byId('log-panel-subheader');
export const getLogPanelContextStep = () => byId('log-panel-context-step');
export const getLogPanelContextStatus = () => byId('log-panel-context-status');
export const getLogPanelContextTimer = () => byId('log-panel-context-timer');
export const getLogPanelSpecificButtonsContainer = () => byId('log-panel-specific-buttons-container');
export const getMainLogContainerPanel = () => byId('main-log-container-panel');
export const getMainLogOutputPanel = () => byId('main-log-output-panel');
export const getCurrentStepLogNamePanel = () => byId('current-step-log-name-panel');
export const getSpecificLogContainerPanel = () => byId('specific-log-container-panel');
export const getSpecificLogHeaderTextPanel = () => byId('specific-log-header-text-panel');
export const getSpecificLogPathInfoPanel = () => byId('specific-log-path-info-panel');
export const getSpecificLogOutputContentPanel = () => byId('specific-log-output-content-panel');
export const getRunAllButton = () => byId('run-all-steps-button');
export const getTopbarAffix = () => byId('topbar-affix');
export const getTopbarControls = () => byId('topbar-controls');
export const getGlobalProgressAffix = () => byId('global-progress-affix');
export const getGlobalProgressContainer = () => byId('global-progress-container');
export const getGlobalProgressBar = () => byId('global-progress-bar');
export const getGlobalProgressText = () => byId('global-progress-text');
export const getSequenceSummaryPopupOverlay = () => byId('sequence-summary-popup-overlay');
export const getSequenceSummaryList = () => byId('sequence-summary-list');
export const getCloseSummaryPopupButton = () => byId('close-summary-popup');
export const getRunCustomSequenceButton = () => byId('run-custom-sequence-button');
export const getClearCustomSequenceButton = () => byId('clear-custom-sequence-button');
export const getCustomSequenceCheckboxes = () => bySelectorAll('.custom-sequence-checkbox');
export const getCustomSequenceConfirmPopupOverlay = () => byId('custom-sequence-confirm-popup-overlay');
export const getCustomSequenceConfirmList = () => byId('custom-sequence-confirm-list');
export const getConfirmRunCustomSequenceButton = () => byId('confirm-run-custom-sequence-button');
export const getCancelRunCustomSequenceButton = () => byId('cancel-run-custom-sequence-button');
export const getNotificationsArea = () => byId('notifications-area');

// Lazy DOM element getters to ensure elements are available when accessed
export function getAllStepDivs() {
    const elements = document.querySelectorAll('.step');
    console.debug(`[DOM] getAllStepDivs found ${elements.length} elements`);
    return elements;
}

export function getAllRunButtons() {
    const elements = document.querySelectorAll('.run-button');
    console.debug(`[DOM] getAllRunButtons found ${elements.length} elements`);
    return elements;
}

export function getAllCancelButtons() {
    const elements = document.querySelectorAll('.cancel-button');
    console.debug(`[DOM] getAllCancelButtons found ${elements.length} elements`);
    return elements;
}

export function getAllSpecificLogButtons() {
    const elements = document.querySelectorAll('.specific-log-button');
    console.debug(`[DOM] getAllSpecificLogButtons found ${elements.length} elements`);
    return elements;
}

// Enhanced step element getter with validation
export function getStepElement(stepKey) {
    if (!stepKey) {
        console.warn('[DOM] getStepElement called with invalid stepKey:', stepKey);
        return null;
    }

    if (!_SAFE_STEP_KEY_PATTERN.test(String(stepKey))) {
        console.warn('[DOM] getStepElement called with unsafe stepKey (refusing to query by id):', stepKey);
        return null;
    }

    const element = document.getElementById(`step-${stepKey}`);
    if (!element) {
        console.warn(`[DOM] Step element not found: step-${stepKey}`);
        console.debug('[DOM] Available step elements:',
            Array.from(document.querySelectorAll('[id^="step-"]')).map(el => el.id));
    }

    return element;
}

// Validate DOM structure for debugging
export function validateDOMStructure() {
    const results = {
        stepElements: getAllStepDivs().length,
        runButtons: getAllRunButtons().length,
        cancelButtons: getAllCancelButtons().length,
        workflowWrapper: !!getWorkflowWrapper(),
        stepsColumn: !!getStepsColumn(),
        issues: []
    };

    // Check for common issues
    if (results.stepElements === 0) {
        results.issues.push('No step elements found (.step)');
    }

    if (!results.workflowWrapper) {
        results.issues.push('Workflow wrapper not found (#workflow-wrapper)');
    }

    if (!results.stepsColumn) {
        results.issues.push('Steps column not found (#steps-column)');
    }

    console.debug('[DOM] Structure validation:', results);
    return results;
}

// Legacy exports for backward compatibility (will be deprecated)
export const allStepDivs = getAllStepDivs();
export const allRunButtons = getAllRunButtons();
export const allCancelButtons = getAllCancelButtons();
export const allSpecificLogButtons = getAllSpecificLogButtons();
export const closeLogPanelButton = document.getElementById('close-log-panel');

export const localDownloadsList = document.getElementById('local-downloads-list');



// ÉLÉMENTS POUR LE CONTRÔLE SONORE
export const soundToggle = document.getElementById('sound-toggle');
export const soundStatus = document.getElementById('sound-status');
export const soundControlWidget = document.getElementById('sound-control-widget');


// ÉLÉMENTS POUR LE PANNEAU DE RÉGLAGES (top bar)
export const settingsToggle = document.getElementById('settings-toggle');
export const settingsPanel = document.getElementById('settings-panel');

// New getter functions for lazy DOM access
export const getCloseLogPanelButton = () => byId('close-log-panel');
export const getLocalDownloadsList = () => byId('local-downloads-list');

// ÉLÉMENTS POUR LE CONTRÔLE SONORE
export const getSoundToggle = () => byId('sound-toggle');
export const getSoundStatus = () => byId('sound-status');
export const getSoundControlWidget = () => byId('sound-control-widget');

// ÉLÉMENTS POUR LE PANNEAU DE RÉGLAGES (top bar)
export const getSettingsToggle = () => byId('settings-toggle');
export const getSettingsPanel = () => byId('settings-panel');
```

## File: static/uiUpdater.js
```javascript
import { formatElapsedTime, showNotification } from './utils.js';
import * as dom from './domElements.js';
import { appState } from './state/AppState.js';
import { scrollToActiveStep, isAutoScrollEnabled } from './scrollManager.js';

const lastProgressTextByStep = {};

const _lastAutoCenterTsByStep = {};
const _AUTO_CENTER_THROTTLE_MS = 700;

import { soundEvents } from './soundManager.js';
import { domBatcher, DOMUpdateUtils } from './utils/DOMBatcher.js';
import { performanceOptimizer } from './utils/PerformanceOptimizer.js';

let _stepDetailsPanelModulePromise = null;

const STATUS_UI_MAP = {
    running: { label: 'En cours', badgeClass: 'status-running', chipClass: 'state-running', icon: '⏱️' },
    starting: { label: 'Préparation', badgeClass: 'status-running', chipClass: 'state-running', icon: '⚙️' },
    initiated: { label: 'Initialisation', badgeClass: 'status-running', chipClass: 'state-running', icon: '⚙️' },
    completed: { label: 'Terminé', badgeClass: 'status-completed', chipClass: 'state-success', icon: '✅' },
    success: { label: 'Terminé', badgeClass: 'status-success', chipClass: 'state-success', icon: '✅' },
    failed: { label: 'Échec', badgeClass: 'status-failed', chipClass: 'state-error', icon: '❌' },
    error: { label: 'Erreur', badgeClass: 'status-error', chipClass: 'state-error', icon: '⚠️' },
    cancelled: { label: 'Annulé', badgeClass: 'status-cancelled', chipClass: 'state-error', icon: '⛔' },
    warning: { label: 'Attention', badgeClass: 'status-warning', chipClass: 'state-warning', icon: '⚠️' },
    paused: { label: 'En pause', badgeClass: 'status-warning', chipClass: 'state-warning', icon: '⏸️' },
    idle: { label: 'Prêt', badgeClass: 'status-idle', chipClass: 'state-idle', icon: '🕒' },
    pending: { label: 'En attente', badgeClass: 'status-warning', chipClass: 'state-warning', icon: '⏳' }
};

let STEPS_CONFIG_FROM_SERVER = {};
export function setStepsConfig(config) {
    STEPS_CONFIG_FROM_SERVER = config;
}

function getWorkflowWrapperElement() {
    return typeof dom.getWorkflowWrapper === 'function' ? dom.getWorkflowWrapper() : dom.workflowWrapper;
}

function getLogsColumnElement() {
    return typeof dom.getLogsColumnGlobal === 'function' ? dom.getLogsColumnGlobal() : dom.logsColumnGlobal;
}

function resolveElement(getterFn, legacyValue = null) {
    if (typeof getterFn === 'function') {
        try {
            return getterFn();
        } catch (_) {
            return legacyValue || null;
        }
    }
    return legacyValue || null;
}

function getIsAnySequenceRunning() {
    return !!appState.getStateProperty('isAnySequenceRunning');
}

function getActiveStepKeyForLogs() {
    return appState.getStateProperty('activeStepKeyForLogsPanel');
}

function setActiveStepKeyForLogs(stepKey) {
    appState.setState({ activeStepKeyForLogsPanel: stepKey }, 'setActiveStepKeyForLogs');
}

function getSelectedStepsOrder() {
    return appState.getStateProperty('selectedStepsOrder') || [];
}

function getProcessInfo(stepKey) {
    if (!stepKey) return null;
    return appState.getStateProperty(`processInfo.${stepKey}`) || null;
}

function setProcessInfo(stepKey, info) {
    if (!stepKey) return;
    appState.setState({ processInfo: { [stepKey]: info } }, 'process_info_update');
}

function getStepTimers() {
    return appState.getStateProperty('stepTimers') || {};
}

function getStepTimer(stepKey) {
    return getStepTimers()[stepKey];
}

function setStepTimer(stepKey, timerData, source = 'setStepTimer') {
    const timers = getStepTimers();
    appState.setState({ stepTimers: { ...timers, [stepKey]: timerData } }, source);
}

function deleteStepTimer(stepKey) {
    const timers = getStepTimers();
    if (!timers || !Object.prototype.hasOwnProperty.call(timers, stepKey)) return;
    const { [stepKey]: _removed, ...remaining } = timers;
    appState.setState({ stepTimers: remaining }, 'deleteStepTimer');
}

function onWrapperTransitionEndOnce(callback, fallbackMs = 500) {
    const el = getWorkflowWrapperElement();
    if (!el) { callback(); return; }
    let called = false;
    const handler = (e) => {
        if (called) return;
        called = true;
        el.removeEventListener('transitionend', handler);
        callback();
    };
    el.addEventListener('transitionend', handler, { once: true });
    setTimeout(() => {
        if (called) return;
        try { el.removeEventListener('transitionend', handler); } catch (_) {}
        callback();
    }, fallbackMs);
}

function hideNonActiveSteps(activeStepKey, hidden) {
    try {
        const stepDivs = dom.getAllStepDivs();
        stepDivs.forEach(el => {
            const isActive = activeStepKey && el.id === `step-${activeStepKey}`;
            if (!isActive && hidden) {
                el.classList.add('steps-hidden');
            } else if (isActive && hidden) {
                el.classList.remove('steps-hidden');
            } else if (!hidden) {
                el.classList.remove('steps-hidden');
            }
        });
    } catch (e) {
        console.warn('[UI] hideNonActiveSteps error', e);
    }
}

let previousDownloadIds = new Set();
export function getStepsConfig() {
    return STEPS_CONFIG_FROM_SERVER;
}

function normalizeStatus(status) {
    return typeof status === 'string' ? status.toLowerCase() : 'idle';
}

function getStatusMeta(status) {
    const normalized = normalizeStatus(status);
    return STATUS_UI_MAP[normalized] || STATUS_UI_MAP.idle;
}

function getStepDisplayNameForLogPanel(stepKey) {
    if (!stepKey) return '';
    const config = getStepsConfig();
    const stepConfig = config ? config[stepKey] : null;
    if (stepConfig && stepConfig.display_name) return stepConfig.display_name;

    const stepEl = document.getElementById(`step-${stepKey}`);
    const datasetName = stepEl && stepEl.dataset ? stepEl.dataset.stepName : null;
    if (datasetName) return datasetName;

    return stepKey.replace(/_/g, ' ').replace(/\b\w/g, l => l.toUpperCase());
}

function updateLogPanelContextUI(stepKey) {
    const displayName = stepKey ? getStepDisplayNameForLogPanel(stepKey) : '';

    const statusEl = stepKey ? document.getElementById(`status-${stepKey}`) : null;
    const timerEl = stepKey ? document.getElementById(`timer-${stepKey}`) : null;

    const contextStepEl = resolveElement(dom.getLogPanelContextStep, dom.logPanelContextStep);
    const contextStatusEl = resolveElement(dom.getLogPanelContextStatus, dom.logPanelContextStatus);
    const contextTimerEl = resolveElement(dom.getLogPanelContextTimer, dom.logPanelContextTimer);

    if (contextStepEl) {
        contextStepEl.textContent = stepKey ? displayName : 'Aucune étape active';
    }
    if (contextStatusEl) {
        contextStatusEl.textContent = statusEl ? (statusEl.textContent || '') : '';
    }
    if (contextTimerEl) {
        contextTimerEl.textContent = timerEl ? (timerEl.textContent || '') : '';
    }
}

function clearLogPanelSpecificButtons() {
    const container = resolveElement(dom.getLogPanelSpecificButtonsContainer, dom.logPanelSpecificButtonsContainer);
    if (!container) return;

    while (container.firstChild) {
        container.removeChild(container.firstChild);
    }
}

function positionLogsPanelNearActiveStep(stepKey) {
    if (!stepKey) return;
    const workflowWrapper = getWorkflowWrapperElement();
    const logsColumn = getLogsColumnElement();
    if (!workflowWrapper || !logsColumn) return;
    if (!workflowWrapper.classList.contains('compact-mode')) return;

    const activeStepElement = document.getElementById(`step-${stepKey}`);
    if (!activeStepElement || typeof activeStepElement.getBoundingClientRect !== 'function') return;

    const rect = activeStepElement.getBoundingClientRect();
    const minTop = 120;
    const minHeight = 280;
    const bottomPadding = 20;

    const maxTop = Math.max(minTop, (window.innerHeight || 800) - minHeight - bottomPadding);
    const targetTop = Math.max(minTop, Math.min(Math.round(rect.top), maxTop));

    logsColumn.style.top = `${targetTop}px`;
    logsColumn.style.height = `${Math.max(minHeight, (window.innerHeight || 800) - targetTop - bottomPadding)}px`;
}

function updateStepStateChip(stepKey, status) {
    const chip = document.getElementById(`state-chip-${stepKey}`);
    if (!chip) return;
    const meta = getStatusMeta(status);
    chip.className = `step-state-chip ${meta.chipClass}`;
    chip.textContent = `${meta.icon} ${meta.label}`;
}

export function startStepTimer(stepKey) {
    const existingTimer = getStepTimer(stepKey);
    if (existingTimer && existingTimer.intervalId) {
        clearInterval(existingTimer.intervalId);
    }

    const startTime = Date.now();
    setStepTimer(stepKey, {
        startTime: startTime,
        startTimeDate: new Date(startTime),
        intervalId: null,
        elapsedTimeFormatted: "0s"
    }, 'startStepTimer');

    if (stepKey !== 'clear_disk_cache') {
        domBatcher.scheduleUpdate(`timer-init-${stepKey}`, () => {
            const timerEl = document.getElementById(`timer-${stepKey}`);
            if (timerEl) timerEl.textContent = "(0s)";
        });
    }

    const newIntervalId = setInterval(() => {
        const currentTimer = getStepTimer(stepKey);
        if (!currentTimer || (!currentTimer.startTime && !currentTimer.startTimeDate)) {
            if (currentTimer && currentTimer.intervalId) clearInterval(currentTimer.intervalId);
            return;
        }

        const startTimeToUse = currentTimer.startTime ? new Date(currentTimer.startTime) : currentTimer.startTimeDate;
        const elapsedTimeStr = formatElapsedTime(startTimeToUse);
        setStepTimer(stepKey, { ...currentTimer, elapsedTimeFormatted: elapsedTimeStr }, 'timer_tick');

        if (stepKey !== 'clear_disk_cache') {
            domBatcher.scheduleUpdate(`timer-update-${stepKey}`, () => {
                const timerEl = document.getElementById(`timer-${stepKey}`);
                if (timerEl) timerEl.textContent = `(${elapsedTimeStr})`;
            });
        }
    }, 1000);

    const currentTimerData = getStepTimer(stepKey);
    if (currentTimerData) {
        setStepTimer(stepKey, { ...currentTimerData, intervalId: newIntervalId }, 'timer_interval_set');
    }
}

export function stopStepTimer(stepKey) {
    const timerData = getStepTimer(stepKey);
    if (timerData && timerData.intervalId) {
        clearInterval(timerData.intervalId);
        setStepTimer(stepKey, { ...timerData, intervalId: null }, 'timer_interval_cleared');
    }
    const updatedTimerData = getStepTimer(stepKey);
    if (updatedTimerData && (updatedTimerData.startTime || updatedTimerData.startTimeDate)) {
        const startTimeToUse = updatedTimerData.startTime ? new Date(updatedTimerData.startTime) : updatedTimerData.startTimeDate;
        const elapsedTimeStr = formatElapsedTime(startTimeToUse);
        setStepTimer(stepKey, { ...updatedTimerData, elapsedTimeFormatted: elapsedTimeStr }, 'timer_stopped');
        if (stepKey !== 'clear_disk_cache') {
            const timerEl = document.getElementById(`timer-${stepKey}`);
            if (timerEl) timerEl.textContent = `(Terminé en ${elapsedTimeStr})`;
        }
    }
}

export function resetStepTimerDisplay(stepKey) {
    if (stepKey !== 'clear_disk_cache') {
        const timerEl = document.getElementById(`timer-${stepKey}`);
        if (timerEl) timerEl.textContent = "";
    }
    deleteStepTimer(stepKey);
}

export function updateGlobalUIForSequenceState(isRunning) {
    const runAllButton = resolveElement(dom.getRunAllButton, dom.runAllButton);
    const runCustomSequenceButton = resolveElement(dom.getRunCustomSequenceButton, dom.runCustomSequenceButton);
    const clearCustomSequenceButton = resolveElement(dom.getClearCustomSequenceButton, dom.clearCustomSequenceButton);
    const customSequenceCheckboxes = resolveElement(dom.getCustomSequenceCheckboxes, dom.customSequenceCheckboxes) || [];

    if (runAllButton) runAllButton.disabled = isRunning;
    if (runCustomSequenceButton) runCustomSequenceButton.disabled = isRunning || getSelectedStepsOrder().length === 0;
    if (clearCustomSequenceButton) clearCustomSequenceButton.disabled = isRunning || getSelectedStepsOrder().length === 0;

    customSequenceCheckboxes.forEach(cb => cb.disabled = isRunning);

    Object.keys(STEPS_CONFIG_FROM_SERVER).forEach(stepKeyConfig => {
        const runButton = document.querySelector(`.run-button[data-step="${stepKeyConfig}"]`);
        const cancelButton = document.querySelector(`.cancel-button[data-step="${stepKeyConfig}"]`);
        const stepInfo = getProcessInfo(stepKeyConfig);

        if (runButton) runButton.disabled = isRunning;

        if (cancelButton) {
            if (stepInfo && ['running', 'starting', 'initiated'].includes(stepInfo.status)) {
                cancelButton.disabled = false;
            } else {
                cancelButton.disabled = true;
            }
        }
    });
}

export function setActiveStepForLogPanelUI(stepKey) {
    console.log(`[UI] setActiveStepForLogPanelUI, new active step for logs: ${stepKey}`);
    setActiveStepKeyForLogs(stepKey);

    const allStepDivs = dom.getAllStepDivs();
    allStepDivs.forEach(s => {
        s.classList.remove('active-for-log-panel');
    });
    if (stepKey && stepKey !== 'clear_disk_cache') {
        const activeStepElement = document.getElementById(`step-${stepKey}`);
        if (activeStepElement) {
            activeStepElement.classList.add('active-for-log-panel');

            const workflowWrapper = getWorkflowWrapperElement();
            const logsOpen = workflowWrapper && workflowWrapper.classList.contains('logs-active');
            if (logsOpen) {
                hideNonActiveSteps(stepKey, true);
            }
            if (isAutoScrollEnabled() && !logsOpen) {
                console.log(`[UI] Auto-scrolling to active step: ${stepKey}`);
                scrollToActiveStep(stepKey);
            }
        }
    }

    clearLogPanelSpecificButtons();

    if (stepKey) {
        const config = getStepsConfig();
        const stepConfig = config ? config[stepKey] : null;
        const displayName = stepConfig ? stepConfig.display_name : stepKey.replace(/_/g, ' ').replace(/\b\w/g, l => l.toUpperCase());
        console.log(`[UI] setActiveStepForLogPanelUI, displayName for logs: ${displayName}`);

        const logPanelTitle = resolveElement(dom.getLogPanelTitle, dom.logPanelTitle);
        const currentStepLogName = resolveElement(dom.getCurrentStepLogNamePanel, dom.currentStepLogNamePanel);
        if(logPanelTitle) logPanelTitle.textContent = `Logs: ${displayName}`;
        if(currentStepLogName) currentStepLogName.textContent = displayName;
        updateLogPanelContextUI(stepKey);

        const buttonsContainer = resolveElement(dom.getLogPanelSpecificButtonsContainer, dom.logPanelSpecificButtonsContainer);
        if (stepConfig && stepConfig.specific_logs && stepConfig.specific_logs.length > 0 && buttonsContainer) {
            stepConfig.specific_logs.forEach((logConf, index) => {
                const button = document.createElement('button');
                button.className = 'specific-log-button';
                button.textContent = logConf.name;
                button.dataset.step = stepKey;
                button.dataset.logIndex = index;
                button.addEventListener('click', async () => {
                    const apiModule = await import('./apiService.js');
                    await apiModule.fetchSpecificLogAPI(stepKey, index, logConf.name);
                });
                buttonsContainer.appendChild(button);
            });
        }
    } else {
        const logPanelTitle = resolveElement(dom.getLogPanelTitle, dom.logPanelTitle);
        const currentStepLogName = resolveElement(dom.getCurrentStepLogNamePanel, dom.currentStepLogNamePanel);
        if(logPanelTitle) logPanelTitle.textContent = "Logs";
        if(currentStepLogName) currentStepLogName.textContent = "Aucune étape active";
        updateLogPanelContextUI(null);
    }
}

async function fetchAndDisplayLogsForPanel(stepKeyToFocus) {
    console.log(`[UI] fetchAndDisplayLogsForPanel called for: ${stepKeyToFocus}. Current active log panel: ${getActiveStepKeyForLogs()}`);
    if (!stepKeyToFocus) return;

    const stepConfig = getStepsConfig()[stepKeyToFocus];
    const displayName = stepConfig ? (stepConfig.display_name || stepKeyToFocus) : stepKeyToFocus;

    const mainLogOutputPanel = resolveElement(dom.getMainLogOutputPanel, dom.mainLogOutputPanel);
    const mainLogContainer = resolveElement(dom.getMainLogContainerPanel, dom.mainLogContainerPanel);
    const specificLogContainer = resolveElement(dom.getSpecificLogContainerPanel, dom.specificLogContainerPanel);

    if (mainLogOutputPanel) {
        mainLogOutputPanel.textContent = `Chargement des logs pour ${displayName}...`;
    }

    if(mainLogContainer) mainLogContainer.style.display = 'flex';
    if(specificLogContainer) specificLogContainer.style.display = 'none';

    try {
        const response = await fetch(`/status/${stepKeyToFocus}`);
        if (!response.ok) {
            console.error(`[UI] fetchAndDisplayLogsForPanel - fetch failed for ${stepKeyToFocus}: ${response.status}`);
            throw new Error(`Erreur ${response.status} lors de la récupération des logs pour ${displayName}`);
        }
        const data = await response.json();
        setProcessInfo(stepKeyToFocus, { ...(getProcessInfo(stepKeyToFocus) || {}), ...data });
        console.log(`[UI] fetchAndDisplayLogsForPanel - response for: ${stepKeyToFocus}, Log content length: ${data.log ? data.log.length : 'N/A'}`);

        if (getActiveStepKeyForLogs() === stepKeyToFocus && mainLogOutputPanel) {
            console.log(`[UI] fetchAndDisplayLogsForPanel - Updating main log for ${stepKeyToFocus} with ${data.log ? data.log.length : 0} lines.`);
            updateMainLogOutputUI(data.log.join(''));
        } else {
            console.log(`[UI] fetchAndDisplayLogsForPanel - Log focus changed. Current: ${getActiveStepKeyForLogs()}, Fetched for: ${stepKeyToFocus}. Not updating main log panel.`);
        }
    } catch (error) {
        console.error(`[UI] fetchAndDisplayLogsForPanel - CATCH error for ${stepKeyToFocus}:`, error);
        const logPanel = resolveElement(dom.getMainLogOutputPanel, dom.mainLogOutputPanel);
        if (getActiveStepKeyForLogs() === stepKeyToFocus && logPanel) {
            logPanel.textContent = `Erreur: ${error?.message || 'Erreur inconnue'}`;
        }
    }
}

export function openLogPanelUI(stepKeyToFocus, forceOpen = false) {
    const workflowWrapper = getWorkflowWrapperElement();
    if (!workflowWrapper) {
        console.warn('[UI] openLogPanelUI aborted: workflow wrapper missing.');
        return;
    }

    const currentActiveLogStep = getActiveStepKeyForLogs();
    const isPanelOpen = workflowWrapper.classList.contains('logs-active');
    console.log(`[UI] openLogPanelUI called for: ${stepKeyToFocus}, forceOpen: ${forceOpen}, currentActive: ${currentActiveLogStep}, isPanelOpen: ${isPanelOpen}`);

    if (forceOpen) {
        console.log(`[UI] Forcing panel open/update for ${stepKeyToFocus}`);
        workflowWrapper.classList.add('logs-entering');
        setActiveStepForLogPanelUI(stepKeyToFocus);
        hideNonActiveSteps(stepKeyToFocus, true);
        requestAnimationFrame(() => {
            workflowWrapper.classList.add('logs-active');
            onWrapperTransitionEndOnce(() => {
                workflowWrapper.classList.remove('logs-entering');
            }, 500);
        });
        positionLogsPanelNearActiveStep(stepKeyToFocus);
        fetchAndDisplayLogsForPanel(stepKeyToFocus);
        return;
    }

    if (isPanelOpen && currentActiveLogStep && currentActiveLogStep !== stepKeyToFocus) {
        console.log(`[UI] Log panel already open for ${currentActiveLogStep}, switching to ${stepKeyToFocus}.`);
        setActiveStepForLogPanelUI(stepKeyToFocus);
        hideNonActiveSteps(stepKeyToFocus, true);
        positionLogsPanelNearActiveStep(stepKeyToFocus);
        fetchAndDisplayLogsForPanel(stepKeyToFocus);
        return;
    }

    if (isPanelOpen && currentActiveLogStep === stepKeyToFocus) {
        console.log(`[UI] Panel already open for ${stepKeyToFocus}. Refreshing its content.`);
        fetchAndDisplayLogsForPanel(stepKeyToFocus);
        return;
    }

    console.log(`[UI] Opening panel for ${stepKeyToFocus} (or was closed/open for null).`);
    workflowWrapper.classList.add('logs-entering');
    setActiveStepForLogPanelUI(stepKeyToFocus);
    hideNonActiveSteps(stepKeyToFocus, true);
    requestAnimationFrame(() => {
        workflowWrapper.classList.add('logs-active');
        onWrapperTransitionEndOnce(() => {
            workflowWrapper.classList.remove('logs-entering');
        }, 500);
    });
    positionLogsPanelNearActiveStep(stepKeyToFocus);
    fetchAndDisplayLogsForPanel(stepKeyToFocus);
}

export function closeLogPanelUI() {
    const workflowWrapper = getWorkflowWrapperElement();
    if (!workflowWrapper) {
        console.warn('[CLOSE_LOG] Workflow wrapper missing; aborting close sequence.');
        setActiveStepForLogPanelUI(null);
        const mainLogOutputPanel = resolveElement(dom.getMainLogOutputPanel, dom.mainLogOutputPanel);
        const specificLogContainer = resolveElement(dom.getSpecificLogContainerPanel, dom.specificLogContainerPanel);
        if (mainLogOutputPanel) mainLogOutputPanel.textContent = "";
        if (specificLogContainer) specificLogContainer.style.display = 'none';
        const logsColumn = getLogsColumnElement();
        if (logsColumn) {
            logsColumn.style.removeProperty('top');
            logsColumn.style.removeProperty('height');
        }
        clearLogPanelSpecificButtons();
        return;
    }

    console.log('[CLOSE_LOG] Starting closeLogPanelUI, current classes:', workflowWrapper.className);
    workflowWrapper.classList.add('logs-leaving');
    console.log('[CLOSE_LOG] Added logs-leaving class, new classes:', workflowWrapper.className);

    requestAnimationFrame(() => {
        console.log('[CLOSE_LOG] In RAF, removing logs-active class, current classes:', workflowWrapper.className);
        workflowWrapper.classList.remove('logs-active');

        onWrapperTransitionEndOnce(() => {
            console.log('[CLOSE_LOG] Cleanup - removing logs-leaving class, current classes:', workflowWrapper.className);
            workflowWrapper.classList.remove('logs-leaving');
            hideNonActiveSteps(null, false);
        }, 500);
    });

    setActiveStepForLogPanelUI(null);
    const mainLogOutputPanel = resolveElement(dom.getMainLogOutputPanel, dom.mainLogOutputPanel);
    const specificLogContainer = resolveElement(dom.getSpecificLogContainerPanel, dom.specificLogContainerPanel);
    if(mainLogOutputPanel) mainLogOutputPanel.textContent = "";
    if(specificLogContainer) specificLogContainer.style.display = 'none';

    const logsColumn = getLogsColumnElement();
    if (logsColumn) {
        logsColumn.style.removeProperty('top');
        logsColumn.style.removeProperty('height');
    }
    clearLogPanelSpecificButtons();
}

export function updateStepCardUI(stepKey, data) {
    console.group(`[PROGRESS DEBUG] updateStepCardUI - ${stepKey}`);
    console.log('Raw data received:', {
        progress_current: data.progress_current,
        progress_total: data.progress_total,
        progress_current_fractional: data.progress_current_fractional,
        status: data.status,
        progress_text: data.progress_text,
        timestamp: new Date().toISOString()
    });

    performanceOptimizer.measureDomUpdate(`updateStepCard-${stepKey}`, () => {
        try {
            const statusEl = document.getElementById(`status-${stepKey}`);
            const runButton = document.querySelector(`.run-button[data-step="${stepKey}"]`);
            const cancelButton = document.querySelector(`.cancel-button[data-step="${stepKey}"]`);
            const workflowWrapper = getWorkflowWrapperElement();

            const normalizedStatus = normalizeStatus(data.status || 'idle');
            const statusMeta = getStatusMeta(normalizedStatus);

            if (statusEl) {
                statusEl.textContent = statusMeta.label;
                statusEl.className = `status-badge ${statusMeta.badgeClass}`;
            }

            const stepCardEl = document.getElementById(`step-${stepKey}`);
            if (stepCardEl) {
                stepCardEl.setAttribute('data-status', normalizedStatus);
            }

            updateStepStateChip(stepKey, normalizedStatus);

            if (runButton && cancelButton) {
                const isCurrentlyRunningOrStarting = ['running', 'starting', 'initiated'].includes(normalizedStatus);
                runButton.disabled = isCurrentlyRunningOrStarting || getIsAnySequenceRunning();
                cancelButton.disabled = !isCurrentlyRunningOrStarting;
            }

            const logsOpen = workflowWrapper && workflowWrapper.classList.contains('logs-active');
            if (logsOpen && ['running', 'starting', 'initiated'].includes(normalizedStatus)) {
                if (getActiveStepKeyForLogs() !== stepKey) {
                    setActiveStepForLogPanelUI(stepKey);
                    hideNonActiveSteps(stepKey, true);
                }
            }

            if (logsOpen && getActiveStepKeyForLogs() === stepKey) {
                updateLogPanelContextUI(stepKey);
            }

            if (['completed', 'failed'].includes(normalizedStatus) || (normalizedStatus === 'idle' && getStepTimer(stepKey))) {
                stopStepTimer(stepKey);
            } else if (normalizedStatus === 'idle' && !getStepTimer(stepKey)) {
                resetStepTimerDisplay(stepKey);
            } else if (['running', 'starting', 'initiated'].includes(normalizedStatus) && !getStepTimer(stepKey)?.intervalId) {
                // TODO: Implement proper timer resumption after page reload
                // Date: 2026-01-19
                // Owner: kidpixel
                // Issue: startStepTimer doesn't resume from existing startTime
                // Solution needed: Backend should provide duration_str for running steps
            }

            const progressContainer = document.getElementById(`progress-container-${stepKey}`);
            const progressBar = document.getElementById(`progress-bar-${stepKey}`);
            const progressTextEl = document.getElementById(`progress-text-${stepKey}`);

            let percentage = 0;

            if (progressContainer && progressBar && progressTextEl) {
                if (data.progress_total > 0) {
                    let currentProgress = data.progress_current_fractional || data.progress_current;

                    if (data.progress_current_fractional === null && data.progress_text) {
                        const isSpecialRunning = (['STEP3','STEP4','STEP5'].includes(stepKey)) && ['running','starting','initiated'].includes(normalizedStatus);
                        if (!isSpecialRunning) {
                            const percentMatch = data.progress_text.match(/(\d+)%/);
                            if (percentMatch) {
                                const textPercent = parseInt(percentMatch[1]);
                                currentProgress = (textPercent / 100) * data.progress_total;
                                console.log(`[PROGRESS FALLBACK] ${stepKey}: Extracted ${textPercent}% from text, using fractional: ${currentProgress}`);
                            }
                        }
                    }

                    percentage = Math.round((currentProgress / data.progress_total) * 100);
                    percentage = Math.min(percentage, 100);

                    if ((['STEP3','STEP4','STEP5'].includes(stepKey)) && ['running', 'starting', 'initiated'].includes(normalizedStatus)) {
                        if (percentage >= 100) {
                            percentage = 99;
                        }
                        if (data.progress_total > 0 && data.progress_current === data.progress_total) {
                            percentage = Math.min(percentage, 99);
                        }
                    }

                    console.log(`[PROGRESS CALC] ${stepKey}:`, {
                        progress_current: data.progress_current,
                        progress_current_fractional: data.progress_current_fractional,
                        progress_total: data.progress_total,
                        currentProgress: currentProgress,
                        calculatedPercentage: (currentProgress / data.progress_total) * 100,
                        finalPercentage: percentage,
                        status: data.status,
                        progress_text: data.progress_text
                    });

                    let displayCurrent = data.progress_current;
                    if ((!displayCurrent || displayCurrent === 0) && typeof data.progress_current_fractional === 'number' && data.progress_current_fractional > 0) {
                        const frac = Math.max(0, Math.min(data.progress_total, data.progress_current_fractional));
                        displayCurrent = Math.min(data.progress_total, Math.floor(frac) + 1);
                    }

                    progressContainer.style.display = 'block';
                    progressBar.style.backgroundColor = 'var(--blue)';
                    progressBar.style.width = `${percentage}%`;
                    progressBar.textContent = `${percentage}%`;
                    progressBar.setAttribute('aria-valuenow', percentage);

                    if (['running','starting','initiated'].includes(normalizedStatus)) {
                        progressBar.setAttribute('data-active', 'true');
                    } else {
                        progressBar.removeAttribute('data-active');
                    }

                    const candidateText = (data.progress_text && data.progress_text.trim()) ? data.progress_text : (lastProgressTextByStep[stepKey] || '');
                    if (data.progress_text && data.progress_text.trim()) {
                        lastProgressTextByStep[stepKey] = data.progress_text;
                    }
                    const subText = candidateText ? `${candidateText} (${displayCurrent}/${data.progress_total})` : `${displayCurrent}/${data.progress_total}`;
                    progressTextEl.textContent = subText;

                    const shouldAutoCenter = getIsAnySequenceRunning() && ['running', 'starting', 'initiated'].includes(normalizedStatus);
                    if (shouldAutoCenter) {
                        const logsOpenNow = workflowWrapper && workflowWrapper.classList.contains('logs-active');
                        if (!logsOpenNow) {
                            const now = performance.now();
                            const lastTs = _lastAutoCenterTsByStep[stepKey] || 0;
                            if ((now - lastTs) > _AUTO_CENTER_THROTTLE_MS) {
                                _lastAutoCenterTsByStep[stepKey] = now;
                                requestAnimationFrame(() => {
                                    scrollToActiveStep(stepKey, { behavior: 'auto', scrollDelay: 0 });
                                });
                            }
                        }
                    }

                    if (candidateText && ['running','starting','initiated'].includes(data.status)) {
                        progressTextEl.setAttribute('data-processing', 'true');
                    } else {
                        progressTextEl.removeAttribute('data-processing');
                    }

                    if (['STEP3','STEP4','STEP5'].includes(stepKey)) {
                        const stepNames = { STEP3: 'Étape 3 — Transitions', STEP4: 'Étape 4 — Audio', STEP5: 'Étape 5 — Tracking' };
                        try { updateGlobalProgressUI(`${stepNames[stepKey] || stepKey}: ${subText}`, percentage, false); } catch (_) {}
                    }
                } else if (data.status === 'completed' && data.progress_total === 0) {
                    percentage = 0;
                    console.log(`[PROGRESS CALC] ${stepKey}: Completed with no work (0%)`);
                } else if (data.status === 'completed' && data.progress_total > 0) {
                    percentage = 100;
                    console.log(`[PROGRESS CALC] ${stepKey}: Completed with work (100%)`);
                } else if (['running', 'starting', 'initiated'].includes(data.status) && data.progress_total === 0) {
                    percentage = 0;
                    console.log(`[PROGRESS CALC] ${stepKey}: Running with no progress tracking (0%)`);
                }
            } else if (['running', 'starting', 'initiated'].includes(data.status) && data.progress_total === 0) {
                progressContainer.style.display = 'block';
                progressBar.style.backgroundColor = 'var(--blue)';
                progressBar.setAttribute('data-active', 'true');
                const runningText = (data.progress_text && data.progress_text.trim()) ? data.progress_text : (lastProgressTextByStep[stepKey] || "En cours d'exécution...");
                if (data.progress_text && data.progress_text.trim()) lastProgressTextByStep[stepKey] = data.progress_text;
                progressTextEl.textContent = runningText;

                if (['STEP3','STEP4','STEP5'].includes(stepKey)) {
                    const stepNames = { STEP3: 'Étape 3 — Transitions', STEP4: 'Étape 4 — Audio', STEP5: 'Étape 5 — Tracking' };
                    const globalText = `${stepNames[stepKey] || stepKey}: ${runningText || 'En cours...'}`;
                    try { updateGlobalProgressUI(globalText, 0, false); } catch (_) {}
                }

                if (runningText && runningText.trim()) {
                    progressTextEl.setAttribute('data-processing', 'true');
                } else {
                    progressTextEl.removeAttribute('data-processing');
                }
            } else if (data.status === 'completed') {
                progressContainer.style.display = 'block';
                progressBar.style.backgroundColor = 'var(--green)';
                progressBar.removeAttribute('data-active');

                if (data.progress_total === 0) {
                    let noWorkText = "Aucun élément à traiter";
                    if (data.progress_text && data.progress_text.trim() !== "") {
                        noWorkText = data.progress_text;
                    }
                    progressTextEl.textContent = noWorkText;
                    progressBar.style.width = '10%';
                    progressBar.textContent = '✓';
                } else {
                    let baseCompletionText = `Terminé (${data.progress_current}/${data.progress_total})`;
                    if (data.progress_text && data.progress_text.toLowerCase() !== "terminé" && data.progress_text.trim() !== "") {
                        baseCompletionText = `${data.progress_text} (${data.progress_current}/${data.progress_total})`;
                    }
                    const config = STEPS_CONFIG_FROM_SERVER[stepKey];
                    if (config && config.post_completion_message_ui) {
                        progressTextEl.textContent = `${baseCompletionText}\n${config.post_completion_message_ui}`;
                    } else {
                        progressTextEl.textContent = baseCompletionText;
                    }

                    if (['STEP3','STEP4','STEP5'].includes(stepKey)) {
                        const stepNames = { STEP3: 'Étape 3 — Transitions', STEP4: 'Étape 4 — Audio', STEP5: 'Étape 5 — Tracking' };
                        try { updateGlobalProgressUI(`${stepNames[stepKey] || stepKey}: Terminé`, 100, false); } catch (_) {}
                    }
                    delete lastProgressTextByStep[stepKey];
                }
            } else if (data.status === 'failed') {
                progressContainer.style.display = 'block';
                progressBar.style.backgroundColor = 'var(--red)';
                let failureText = `Échec`;
                if (data.progress_total > 0) failureText += ` à ${data.progress_current}/${data.progress_total}`;
                if (data.progress_text) failureText += `: ${data.progress_text}`;
                progressTextEl.textContent = failureText;
                progressBar.removeAttribute('data-active');
                progressTextEl.removeAttribute('data-processing');

                if (['STEP3','STEP4','STEP5'].includes(stepKey)) {
                    const stepNames = { STEP3: 'Étape 3 — Transitions', STEP4: 'Étape 4 — Audio', STEP5: 'Étape 5 — Tracking' };
                    try { updateGlobalProgressUI(`${stepNames[stepKey] || stepKey}: ${failureText}`, percentage, true); } catch (_) {}
                }
                delete lastProgressTextByStep[stepKey];
            } else if (data.status === 'starting' || data.status === 'initiated') {
                progressContainer.style.display = 'block';
                progressBar.style.width = `0%`;
                progressBar.textContent = `0%`;
                progressBar.style.backgroundColor = 'var(--blue)';
                progressBar.setAttribute('data-active', 'true');
                progressTextEl.textContent = "Démarrage...";
            } else {
                progressContainer.style.display = 'none';
                progressBar.setAttribute('aria-valuenow', 0);
            }

            const anyRunning = !!document.querySelector('.step[data-status="running"], .step[data-status="starting"], .step[data-status="initiated"]');
            if (workflowWrapper) {
                if (anyRunning) {
                    workflowWrapper.classList.add('any-step-running');
                    if (['running','starting','initiated'].includes(data.status)) {
                        workflowWrapper.setAttribute('data-active-step', stepKey);
                    } else if (!document.querySelector(`.step[data-status="running"], .step[data-status="starting"], .step[data-status="initiated"]`)) {
                        workflowWrapper.removeAttribute('data-active-step');
                    }
                } else {
                    workflowWrapper.classList.remove('any-step-running');
                    workflowWrapper.removeAttribute('data-active-step');
                }
            }

            try {
                if (!_stepDetailsPanelModulePromise) {
                    _stepDetailsPanelModulePromise = import('./stepDetailsPanel.js');
                }
                _stepDetailsPanelModulePromise
                    .then((mod) => {
                        if (mod && typeof mod.refreshStepDetailsPanelIfOpen === 'function') {
                            mod.refreshStepDetailsPanelIfOpen(stepKey);
                        }
                    })
                    .catch((e) => {
                        console.debug('[UI] Step details module not available:', e);
                    });
            } catch (_) {}
        } catch (_) {}

        console.groupEnd();
    });
}

export function updateCustomSequenceButtonsUI() {
    const hasSelection = getSelectedStepsOrder().length > 0;
    if (dom.runCustomSequenceButton) dom.runCustomSequenceButton.disabled = !hasSelection || getIsAnySequenceRunning();
    if (dom.clearCustomSequenceButton) dom.clearCustomSequenceButton.disabled = !hasSelection || getIsAnySequenceRunning();
}

export function updateGlobalProgressUI(text, percentage, isError = false) {
    if(dom.globalProgressAffix) dom.globalProgressAffix.style.display = 'flex';
    if(dom.globalProgressContainer) dom.globalProgressContainer.style.display = 'block';
    if(dom.globalProgressText) {
        dom.globalProgressText.style.display = 'block';
        dom.globalProgressText.textContent = text;
        dom.globalProgressText.style.color = isError ? 'var(--red)' : 'var(--text-secondary)';
    }
    if(dom.globalProgressBar) {
        dom.globalProgressBar.style.width = `${percentage}%`;
        dom.globalProgressBar.textContent = `${percentage}%`;
        dom.globalProgressBar.setAttribute('aria-valuenow', percentage);
        dom.globalProgressBar.style.backgroundColor = isError ? 'var(--red)' : 'var(--green)';
    }
}

export function updateSpecificLogUI(logName, path, content, isError = false, errorMessage = '') {
    domBatcher.scheduleUpdate('specific-log-ui', () => {
        const headerText = resolveElement(dom.getSpecificLogHeaderTextPanel, dom.specificLogHeaderTextPanel);
        const pathInfo = resolveElement(dom.getSpecificLogPathInfoPanel, dom.specificLogPathInfoPanel);
        const outputContent = resolveElement(dom.getSpecificLogOutputContentPanel, dom.specificLogOutputContentPanel);
        const specificLogContainer = resolveElement(dom.getSpecificLogContainerPanel, dom.specificLogContainerPanel);
        const mainLogContainer = resolveElement(dom.getMainLogContainerPanel, dom.mainLogContainerPanel);

        if(headerText) headerText.textContent = isError ? `Erreur chargement "${logName}"` : `Log Spécifique: "${logName}"`;
        if(pathInfo) pathInfo.textContent = path ? `(Source: ${path})` : "";
        if (isError) {
            if(outputContent) {
                const escapedErrorMessage = DOMUpdateUtils.escapeHtml(errorMessage);
                outputContent.innerHTML = `<span class="log-line log-error">${escapedErrorMessage}</span>`;
            }
        } else {
            const styledContent = parseAndStyleLogContent(content);
            if(outputContent) outputContent.innerHTML = styledContent;
        }
        if(specificLogContainer) specificLogContainer.style.display = 'flex';
        if(mainLogContainer) mainLogContainer.style.display = 'none';
        if(outputContent) outputContent.scrollTop = 0;
    });
}

const _LOG_LINE_EMPTY_OR_WHITESPACE_PATTERN = /^\s*$/;

const _LOG_TIMESTAMP_PATTERN = /^(?:\d{4}-\d{2}-\d{2}|\d{2}:\d{2}:\d{2})/;
const _LOG_ERROR_PATTERN = /(?:erreur|error|échec|failed|exception|critical|fatal|crash)/i;
const _LOG_WARNING_PATTERN = /(?:warning|attention|avertissement|warn|caution|deprecated)/i;
const _LOG_SUCCESS_PATTERN = /(?:success|réussi|terminé|completed|finished|done|✓|✔|ok\b)/i;
const _LOG_INFO_PATTERN = /(?:info|information|démarrage|starting|lancement|initiated|status)/i;
const _LOG_DEBUG_PATTERN = /(?:debug|trace|verbose|détail)/i;
const _LOG_COMMAND_PATTERN = /^(?:commande:|command:|executing:|exécution:|\$|>)/i;
const _LOG_PROGRESS_PATTERN = /(?:\d+%|\d+\/\d+|progress|progression|chargement|loading|téléchargement|downloading)/i;

const _LOG_PATTERNS = [
    {
        regex: _LOG_ERROR_PATTERN,
        type: 'error'
    },
    {
        regex: _LOG_WARNING_PATTERN,
        type: 'warning'
    },
    {
        regex: _LOG_SUCCESS_PATTERN,
        type: 'success'
    },
    {
        regex: _LOG_PROGRESS_PATTERN,
        type: 'progress'
    },
    {
        regex: _LOG_COMMAND_PATTERN,
        type: 'command'
    },
    {
        regex: _LOG_INFO_PATTERN,
        type: 'info'
    },
    {
        regex: _LOG_TIMESTAMP_PATTERN,
        type: 'info'
    },
    {
        regex: _LOG_DEBUG_PATTERN,
        type: 'debug'
    }
];

const _COMPILED_LOG_PATTERNS = _LOG_PATTERNS.map(p => ({
    ...p,
    regex: new RegExp(p.regex.source, p.regex.flags)
}));

/**
 * Parse and style log content with CSS classes for different log types.
 * Escapes all HTML to prevent XSS.
 * 
 * @param {string} rawContent - Raw log content
 * @returns {string} - Styled HTML content
 */
export function parseAndStyleLogContent(rawContent) {
    if (!rawContent || typeof rawContent !== 'string') {
        return rawContent || '';
    }

const lines = rawContent.split('\n');
    const styledLines = new Array(lines.length);

    for (let i = 0; i < lines.length; i++) {
        const line = lines[i];
        if (line === '' || _LOG_LINE_EMPTY_OR_WHITESPACE_PATTERN.test(line)) {
            styledLines[i] = line;
            continue;
        }

        const escapedLine = DOMUpdateUtils.escapeHtml(line);

        let logType = 'default';
        for (let j = 0; j < _COMPILED_LOG_PATTERNS.length; j++) {
            const pattern = _COMPILED_LOG_PATTERNS[j];
            if (pattern.regex.test(line)) {
                logType = pattern.type;
                break;
            }
        }

        styledLines[i] = logType !== 'default'
            ? `<span class="log-line log-${logType}">${escapedLine}</span>`
            : escapedLine;
    }

    return styledLines.join('\n');
}

export function updateMainLogOutputUI(htmlContent) {
    if(dom.mainLogOutputPanel) {
const styledContent = parseAndStyleLogContent(htmlContent);
        dom.mainLogOutputPanel.innerHTML = styledContent;
    }
    if(dom.mainLogOutputPanel) dom.mainLogOutputPanel.scrollTop = dom.mainLogOutputPanel.scrollHeight;

    if(dom.mainLogContainerPanel) dom.mainLogContainerPanel.style.display = 'flex';
    if(dom.specificLogContainerPanel) dom.specificLogContainerPanel.style.display = 'none';
}

export function updateLocalDownloadsListUI(downloadsData) {
    if (!dom.getLocalDownloadsList()) return;
    dom.getLocalDownloadsList().innerHTML = '';
    if (!downloadsData || downloadsData.length === 0) {
        const li = document.createElement('li');
        li.textContent = 'Aucune activité de téléchargement locale récente.';
        li.classList.add('placeholder');
        dom.getLocalDownloadsList().appendChild(li);
        return;
    }

const currentDownloadIds = new Set();
    downloadsData.forEach(download => {
        if (download.id) {
            currentDownloadIds.add(download.id);
if (!previousDownloadIds.has(download.id) &&
                (download.status === 'pending' || download.status === 'downloading')) {
                console.log(`[SOUND] New CSV download detected: ${download.filename}`);
                soundEvents.csvDownloadInitiation();

const filename = download.filename && download.filename !== 'Détermination en cours...'
                    ? download.filename.substring(0, 30) + (download.filename.length > 30 ? '...' : '')
                    : 'nouveau fichier';
                showNotification(`Mode Auto: Téléchargement démarré - ${filename}`, "info", 5000);
            }
        }
    });

previousDownloadIds = currentDownloadIds;

    downloadsData.forEach(download => {
        const li = document.createElement('li');
        li.classList.add(`download-status-${download.status}`);

        const escapedOriginalUrl = DOMUpdateUtils.escapeHtml(download.original_url || '');
        const escapedFilename = DOMUpdateUtils.escapeHtml(download.filename || 'Nom inconnu');
        const escapedStatus = DOMUpdateUtils.escapeHtml(download.status || '');
        const escapedDisplayTimestamp = DOMUpdateUtils.escapeHtml(download.display_timestamp || 'N/A');

        const timestampSpan = `<span class="timestamp">${escapedDisplayTimestamp}</span>`;
        const filenameSpan = `<span class="filename" title="${escapedOriginalUrl}">${escapedFilename}</span>`;
        let statusText = `Statut: <span class="status-text">${escapedStatus}</span>`;
        let progressText = '';
        if (download.status === 'downloading' && typeof download.progress === 'number') {
            progressText = ` <span class="progress-percentage">(${download.progress}%)</span>`;
        }
        if (download.message) {
            const escapedMessage = DOMUpdateUtils.escapeHtml(download.message);
            const messagePreview = escapedMessage.substring(0, 50) + (escapedMessage.length > 50 ? '...' : '');
            statusText += ` <span class="message" title="${escapedMessage}">${messagePreview}</span>`;
        }
        li.innerHTML = `${timestampSpan} - ${filenameSpan} - ${statusText}${progressText}`;
        dom.getLocalDownloadsList().appendChild(li);
    });
}

export function updateClearCacheGlobalButtonState(status, message = '') {
    if (!dom.clearCacheGlobalButton) return;

    dom.clearCacheGlobalButton.classList.remove('idle', 'running', 'completed', 'failed');
    const textSpan = dom.clearCacheGlobalButton.querySelector('.button-text');
    const currentStepInfo = getProcessInfo('clear_disk_cache');

const isOtherSequenceRunning = getIsAnySequenceRunning() && currentStepInfo?.status !== 'running';


    switch (status) {
        case 'idle':
            dom.clearCacheGlobalButton.disabled = isOtherSequenceRunning;
            if (textSpan) textSpan.textContent = "Vider le Cache";
            dom.clearCacheGlobalButton.classList.add('idle');
            break;
        case 'starting':
        case 'initiated':
            dom.clearCacheGlobalButton.disabled = true;
            if (textSpan) textSpan.textContent = "Lancement...";
            dom.clearCacheGlobalButton.classList.add('running');
            break;
        case 'running':
            dom.clearCacheGlobalButton.disabled = true;
            if (textSpan) textSpan.textContent = "Nettoyage...";
            dom.clearCacheGlobalButton.classList.add('running');
            break;
        case 'completed':
            dom.clearCacheGlobalButton.disabled = isOtherSequenceRunning;
            if (textSpan) textSpan.textContent = "Cache Vidé";
            dom.clearCacheGlobalButton.classList.add('completed');
            showNotification("Nettoyage du cache disque terminé avec succès.", "success");
            setTimeout(() => updateClearCacheGlobalButtonState('idle'), 5000);
            break;
        case 'failed':
            dom.clearCacheGlobalButton.disabled = isOtherSequenceRunning;
            if (textSpan) textSpan.textContent = "Échec Nettoyage";
            dom.clearCacheGlobalButton.classList.add('failed');
            let notifMessage = "Échec du nettoyage du cache disque.";
            if (message && typeof message === 'string' && message.trim() !== '' && !message.startsWith('<')) {
                notifMessage += ` Détail: ${message.substring(0,100)}`;
            }
            showNotification(notifMessage, "error");
            setTimeout(() => updateClearCacheGlobalButtonState('idle'), 8000);
            break;
        default:
            dom.clearCacheGlobalButton.disabled = isOtherSequenceRunning;
            if (textSpan) textSpan.textContent = "Vider le Cache";
            dom.clearCacheGlobalButton.classList.add('idle');
    }
}
```
