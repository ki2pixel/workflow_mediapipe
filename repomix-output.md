This file is a merged representation of a subset of the codebase, containing specifically included files and files not matching ignore patterns, combined into a single document by Repomix.
The content has been processed where line numbers have been added.

# File Summary

## Purpose
This file contains a packed representation of a subset of the repository's contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

## File Format
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  a. A header with the file path (## File: path/to/file)
  b. The full contents of the file in a code block

## Usage Guidelines
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
- Pay special attention to the Repository Description. These contain important context and guidelines specific to this project.

## Notes
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: app_new.py, routes/**, services/**, workflow_scripts/**/*.py, utils/**, config/**, static/**, templates/**
- Files matching these patterns are excluded: archives/**, sound-design/**, assets/**, transnetv2-weights/**, workflow_scripts/**/models/**, workflow_scripts/**/assets/**, logs/**, debug/**, download_history*.json, download_history*.bak, download_history*.sqlite3, download_history*.sqlite3-wal, download_history*.sqlite3-shm, *.mp3, *.wav, *.tflite, *.task, *.onnx, *.pth, *.zip, *.tar, *.tar.gz, *.rar, *.7z
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Line numbers have been added to the beginning of each line
- Long base64 data strings (e.g., data:image/png;base64,...) have been truncated to reduce token count
- Files are sorted by Git change count (files with more changes are at the bottom)

# User Provided Header
Workflow MediaPipe — extrait optimisé pour IA.
Ce bundle exclut les artefacts volumineux (archives, modèles, logs) pour rester sous la limite de Repomix.
Consultez `docs/workflow/guides/REPOMIX_USAGE.md` pour l'utilisation et `.windsurf/rules/codingstandards.md` pour les règles obligatoires.

# Directory Structure
```
config/
  __init__.py
  optimal_tv_config.json
  security.py
  settings.py
  step3_transnet.json
  workflow_commands.py
routes/
  __init__.py
  api_routes.py
  workflow_routes.py
services/
  deprecated/
    airtable_service.py
    mysql_service.py
    README.md
  __init__.py
  cache_service.py
  csv_service.py
  download_history_repository.py
  download_service.py
  filesystem_service.py
  lemonfox_audio_service.py
  monitoring_service.py
  performance_service.py
  report_service.py
  results_archiver.py
  visualization_service.py
  webhook_service.py
  workflow_service.py
  workflow_state.py
static/
  css/
    components/
      controls.css
      csv-workflow-prompt.css
      downloads.css
      logs.css
      notifications.css
      popups.css
      steps.css
      widgets.css
      workflow-buttons.css
    features/
      reports.css
      responsive.css
      stats-dashboard.css
    utils/
      animations.css
    base.css
    layout.css
    themes.css
    variables.css
  state/
    AppState.js
  test-exports/
    fetchWithLoadingState.js
  utils/
    DOMBatcher.js
    ErrorHandler.js
    PerformanceMonitor.js
    PerformanceOptimizer.js
    PollingManager.js
  apiService.js
  constants.js
  csvDownloadMonitor.js
  csvWorkflowPrompt.js
  domElements.js
  eventHandlers.js
  main.js
  popupManager.js
  reportViewer.js
  scrollManager.js
  sequenceManager.js
  soundManager.js
  state.js
  stepDetailsPanel.js
  themeManager.js
  uiUpdater.js
  utils.js
templates/
  reports/
    analysis_report.html
    monthly_archive_report.html
    project_report.html
  index_new.html
utils/
  enhanced_speaking_detection.py
  filename_security.py
  mediapipe_asset_helper.py
  performance.py
  resource_manager.py
  simple_profiling.py
  tracking_optimizations.py
  transnetv2_library.py
  worker_wrapper.py
workflow_scripts/
  step1/
    extract_archives.py
  step2/
    convert_videos.py
  step3/
    run_transnet.py
    transnetv2_pytorch.py
  step4/
    run_audio_analysis_lemonfox.py
    run_audio_analysis.py
  step5/
    face_engines.py
    object_detector_registry.py
    onnx_facemesh_detector.py
    process_video_worker_multiprocessing.py
    process_video_worker.py
    pyfeat_blendshape_extractor.py
    run_tracking_manager.py
  step6/
    json_reducer.py
  step7/
    finalize_and_copy.py
app_new.py
```

# Files

## File: config/__init__.py
```python
1: """Config package marker for pytest/package imports."""
```

## File: config/optimal_tv_config.json
```json
1: {
2:   "segmentation": {
3:     "min_duration_off": 0.15
4:   },
5:   "clustering": {
6:     "min_cluster_size": 3
7:   }
8: }
```

## File: config/security.py
```python
  1: """
  2: Security configuration and authentication management for workflow_mediapipe.
  3: 
  4: This module provides secure token management and authentication decorators
  5: following the project's security guidelines.
  6: """
  7: 
  8: import os
  9: import logging
 10: from functools import wraps
 11: from typing import Optional
 12: from flask import request, jsonify
 13: 
 14: logger = logging.getLogger(__name__)
 15: 
 16: 
 17: class SecurityConfig:
 18:     """
 19:     Centralized security configuration management.
 20:     
 21:     Loads security tokens from environment variables and provides
 22:     validation methods to ensure proper configuration.
 23:     """
 24:     
 25:     def __init__(self):
 26:         """Initialize security configuration from environment variables."""
 27:         self.INTERNAL_WORKER_TOKEN = os.environ.get('INTERNAL_WORKER_COMMS_TOKEN')
 28:         self.RENDER_REGISTER_TOKEN = os.environ.get('RENDER_REGISTER_TOKEN')
 29:         
 30:     def validate_tokens(self, strict: bool = True) -> bool:
 31:         """
 32:         Validate that all required security tokens are configured.
 33: 
 34:         Args:
 35:             strict: If True, raise errors for missing tokens. If False, log warnings.
 36: 
 37:         Returns:
 38:             bool: True if all tokens are valid, False otherwise
 39: 
 40:         Raises:
 41:             ValueError: If required tokens are missing and strict=True
 42:         """
 43:         errors = []
 44:         warnings = []
 45: 
 46:         if not self.INTERNAL_WORKER_TOKEN:
 47:             msg = "INTERNAL_WORKER_COMMS_TOKEN environment variable is required"
 48:             if strict:
 49:                 errors.append(msg)
 50:             else:
 51:                 warnings.append(msg)
 52:                 # Set a development default
 53:                 self.INTERNAL_WORKER_TOKEN = "dev-internal-worker-token"
 54:                 logger.warning(f"{msg} - using development default")
 55: 
 56:         if not self.RENDER_REGISTER_TOKEN:
 57:             msg = "RENDER_REGISTER_TOKEN environment variable is required"
 58:             if strict:
 59:                 errors.append(msg)
 60:             else:
 61:                 warnings.append(msg)
 62:                 # Set a development default
 63:                 self.RENDER_REGISTER_TOKEN = "dev-render-register-token"
 64:                 logger.warning(f"{msg} - using development default")
 65: 
 66:         if errors:
 67:             error_msg = f"Security configuration errors: {'; '.join(errors)}"
 68:             logger.error(error_msg)
 69:             raise ValueError(error_msg)
 70: 
 71:         if warnings:
 72:             logger.warning(f"Security configuration warnings: {'; '.join(warnings)}")
 73:             logger.warning("Using development defaults - NOT SUITABLE FOR PRODUCTION")
 74:             return False
 75: 
 76:         logger.info("Security tokens validated successfully")
 77:         return True
 78:     
 79:     def get_token(self, token_name: str) -> Optional[str]:
 80:         """
 81:         Get a specific token by name.
 82:         
 83:         Args:
 84:             token_name: Name of the token to retrieve
 85:             
 86:         Returns:
 87:             Token value or None if not found
 88:         """
 89:         return getattr(self, token_name, None)
 90: 
 91: 
 92: def require_internal_worker_token(func):
 93:     """
 94:     Decorator for endpoints requiring internal worker authentication.
 95:     
 96:     Validates the X-Worker-Token header against the configured
 97:     INTERNAL_WORKER_COMMS_TOKEN.
 98:     
 99:     Args:
100:         func: Flask route function to protect
101:         
102:     Returns:
103:         Decorated function with token validation
104:     """
105:     @wraps(func)
106:     def wrapper(*args, **kwargs):
107:         security_config = SecurityConfig()
108:         
109:         if not security_config.INTERNAL_WORKER_TOKEN:
110:             logger.error("Internal worker token not configured")
111:             return jsonify({"error": "Authentication not configured"}), 500
112:         
113:         received_token = request.headers.get('X-Worker-Token')
114:         if not received_token:
115:             logger.warning("Missing X-Worker-Token header in request")
116:             return jsonify({"error": "Missing authentication token"}), 401
117:             
118:         if received_token != security_config.INTERNAL_WORKER_TOKEN:
119:             logger.warning(f"Invalid worker token received from {request.remote_addr}")
120:             return jsonify({"error": "Invalid authentication token"}), 401
121:         
122:         logger.debug("Internal worker token validated successfully")
123:         return func(*args, **kwargs)
124:     
125:     return wrapper
126: 
127: 
128: def require_render_register_token(func):
129:     """
130:     Decorator for endpoints requiring render registration authentication.
131:     
132:     Validates the X-Render-Token header against the configured
133:     RENDER_REGISTER_TOKEN.
134:     
135:     Args:
136:         func: Flask route function to protect
137:         
138:     Returns:
139:         Decorated function with token validation
140:     """
141:     @wraps(func)
142:     def wrapper(*args, **kwargs):
143:         security_config = SecurityConfig()
144:         
145:         if not security_config.RENDER_REGISTER_TOKEN:
146:             logger.error("Render register token not configured")
147:             return jsonify({"error": "Authentication not configured"}), 500
148:         
149:         received_token = request.headers.get('X-Render-Token')
150:         if not received_token:
151:             logger.warning("Missing X-Render-Token header in request")
152:             return jsonify({"error": "Missing authentication token"}), 401
153:             
154:         if received_token != security_config.RENDER_REGISTER_TOKEN:
155:             logger.warning(f"Invalid render token received from {request.remote_addr}")
156:             return jsonify({"error": "Invalid authentication token"}), 401
157:         
158:         logger.debug("Render register token validated successfully")
159:         return func(*args, **kwargs)
160:     
161:     return wrapper
162: 
163: 
164: def validate_file_path(file_path: str, allowed_base_paths: list) -> bool:
165:     """
166:     Validate file path to prevent directory traversal attacks.
167:     
168:     Args:
169:         file_path: Path to validate
170:         allowed_base_paths: List of allowed base directory paths
171:         
172:     Returns:
173:         bool: True if path is safe, False otherwise
174:     """
175:     try:
176:         from pathlib import Path
177:         
178:         # Resolve the path to handle any .. or . components
179:         resolved_path = Path(file_path).resolve()
180:         
181:         # Check if the resolved path starts with any allowed base path
182:         for base_path in allowed_base_paths:
183:             base_resolved = Path(base_path).resolve()
184:             try:
185:                 resolved_path.relative_to(base_resolved)
186:                 return True
187:             except ValueError:
188:                 continue
189:                 
190:         logger.warning(f"File path validation failed for: {file_path}")
191:         return False
192:         
193:     except Exception as e:
194:         logger.error(f"Error validating file path {file_path}: {e}")
195:         return False
```

## File: config/step3_transnet.json
```json
 1: {
 2:   "threshold": 0.5,
 3:   "window": 100,
 4:   "stride": 50,
 5:   "padding": 25,
 6:   "device": "auto",
 7:   "ffmpeg_threads": 0,
 8:   "mixed_precision": false,
 9:   "amp_dtype": "float16",
10:   "num_workers": 1,
11:   "torchscript": false,
12:   "warmup": true,
13:   "warmup_batches": 2,
14:   "torchscript_auto_fallback": true
15: }
```

## File: config/workflow_commands.py
```python
  1: #!/usr/bin/env python3
  2: # -*- coding: utf-8 -*-
  3: """
  4: Workflow Commands Configuration
  5: 
  6: Centralized configuration for all workflow steps.
  7: Defines commands, working directories, log paths, and progress patterns.
  8: """
  9: 
 10: import re
 11: from pathlib import Path
 12: from typing import Dict, Any, List, Optional
 13: import logging
 14: 
 15: from config.settings import config
 16: 
 17: logger = logging.getLogger(__name__)
 18: 
 19: 
 20: class WorkflowCommandsConfig:
 21:     """Centralized workflow commands configuration.
 22:     
 23:     This class provides configuration for all 7 workflow steps, including:
 24:     - Command line arguments
 25:     - Working directories
 26:     - Log file locations
 27:     - Progress parsing patterns
 28:     - Display names for UI
 29:     """
 30:     
 31:     def __init__(self, base_path: Path = None, hf_token: str = None):
 32:         """Initialize workflow commands configuration.
 33:         
 34:         Args:
 35:             base_path: Base path for scripts (defaults to config.BASE_PATH_SCRIPTS)
 36:             hf_token: HuggingFace authentication token for Step 4
 37:         """
 38:         self.base_path = base_path or config.BASE_PATH_SCRIPTS
 39:         self.hf_token = hf_token
 40:         
 41:         # Setup log directories
 42:         self.logs_base_dir = self.base_path / "logs"
 43:         self._ensure_log_directories()
 44:         
 45:         # Build configuration
 46:         self._config = self._build_configuration()
 47:         
 48:         logger.info(f"WorkflowCommandsConfig initialized with base_path: {self.base_path}")
 49:     
 50:     def _ensure_log_directories(self) -> None:
 51:         """Ensure all log directories exist."""
 52:         self.logs_base_dir.mkdir(exist_ok=True)
 53:         for step in range(1, 8):
 54:             (self.logs_base_dir / f"step{step}").mkdir(exist_ok=True)
 55:     
 56:     def _build_configuration(self) -> Dict[str, Dict[str, Any]]:
 57:         """Build the complete workflow commands configuration.
 58:         
 59:         Returns:
 60:             Dictionary mapping step keys to their configuration
 61:         """
 62:         return {
 63:             "STEP1": self._get_step1_config(),
 64:             "STEP2": self._get_step2_config(),
 65:             "STEP3": self._get_step3_config(),
 66:             "STEP4": self._get_step4_config(),
 67:             "STEP5": self._get_step5_config(),
 68:             "STEP6": self._get_step6_config(),
 69:             "STEP7": self._get_step7_config(),
 70:         }
 71:     
 72:     def _get_step1_config(self) -> Dict[str, Any]:
 73:         """Get configuration for Step 1: Archive Extraction.
 74:         
 75:         Returns:
 76:             Step 1 configuration dictionary
 77:         """
 78:         step1_log_dir = self.logs_base_dir / "step1"
 79:         
 80:         return {
 81:             "display_name": "1. Extraction des archives",
 82:             "cmd": [
 83:                 str(config.get_venv_python("env")),
 84:                 str(self.base_path / "workflow_scripts" / "step1" / "extract_archives.py"),
 85:                 "--source-dir", str(Path.home() / "Téléchargements")
 86:             ],
 87:             "cwd": str(self.base_path),
 88:             "specific_logs": [
 89:                 {
 90:                     "name": "Log Extraction",
 91:                     "type": "directory_latest",
 92:                     "path": step1_log_dir,
 93:                     "pattern": "*.log",
 94:                     "lines": 150
 95:                 },
 96:                 {
 97:                     "name": "Liste Archives Traitées",
 98:                     "type": "file",
 99:                     "path": step1_log_dir / "processed_archives.txt",
100:                     "lines": 50
101:                 }
102:             ],
103:             "progress_patterns": {
104:                 "total": re.compile(r"Trouvé (\d+) archive\(s\) à traiter", re.IGNORECASE),
105:                 "current_success_line_pattern": re.compile(
106:                     r"Extraction terminée pour (.*?)$", re.IGNORECASE
107:                 ),
108:                 "current_item_text_from_success_line": True
109:             }
110:         }
111:     
112:     def _get_step2_config(self) -> Dict[str, Any]:
113:         """Get configuration for Step 2: Video Conversion.
114:         
115:         Returns:
116:             Step 2 configuration dictionary
117:         """
118:         step2_log_dir = self.logs_base_dir / "step2"
119:         
120:         return {
121:             "display_name": "2. Conversion des vidéos",
122:             "cmd": [
123:                 str(config.get_venv_python("env")),
124:                 str(self.base_path / "workflow_scripts" / "step2" / "convert_videos.py")
125:             ],
126:             "cwd": str(self.base_path / "projets_extraits"),
127:             "specific_logs": [
128:                 {
129:                     "name": "Log Conversion",
130:                     "type": "directory_latest",
131:                     "path": step2_log_dir,
132:                     "pattern": "*.log",
133:                     "lines": 200
134:                 }
135:             ],
136:             "progress_patterns": {
137:                 "total": re.compile(r"TOTAL_VIDEOS_TO_PROCESS:\s*(\d+)", re.IGNORECASE),
138:                 "current": re.compile(
139:                     r"--- Traitement de la vidéo \((\d+)/(\d+)\): (.*?) ---", re.IGNORECASE
140:                 )
141:             }
142:         }
143:     
144:     def _get_step3_config(self) -> Dict[str, Any]:
145:         """Get configuration for Step 3: Scene Detection.
146:         
147:         Returns:
148:             Step 3 configuration dictionary
149:         """
150:         step3_log_dir = self.logs_base_dir / "step3"
151:         
152:         return {
153:             "display_name": "3. Analyse des transitions",
154:             "cmd": [
155:                 str(config.get_venv_python("transnet_env")),
156:                 str(self.base_path / "workflow_scripts" / "step3" / "run_transnet.py"),
157:             ],
158:             "cwd": str(self.base_path / "projets_extraits"),
159:             "specific_logs": [
160:                 {
161:                     "name": "Log Analyse Transitions",
162:                     "type": "directory_latest",
163:                     "path": step3_log_dir,
164:                     "pattern": "*.log",
165:                     "lines": 150
166:                 }
167:             ],
168:             "progress_patterns": {
169:                 # Accept both underscore and space variants
170:                 "total": re.compile(r"TOTAL[_ ]VIDEOS[_ ]TO[_ ]PROCESS:\s*(\d+)", re.IGNORECASE),
171:                 "current": re.compile(r"PROCESSING[_ ]VIDEO:\s*(.*)$", re.IGNORECASE),
172:                 "internal_simple": re.compile(
173:                     r"INTERNAL[_ ]PROGRESS:\s*(\d+)\s*batches\s*-\s*(.*)$", re.IGNORECASE
174:                 ),
175:                 "current_success_line_pattern": re.compile(
176:                     r"Succès:\s*(.*?)(?:\.csv|\.json)\s+créé", re.IGNORECASE
177:                 ),
178:                 "current_item_text_from_success_line": True
179:             }
180:         }
181:     
182:     def _get_step4_config(self) -> Dict[str, Any]:
183:         """Get configuration for Step 4: Audio Analysis.
184:         
185:         Returns:
186:             Step 4 configuration dictionary
187:         """
188:         step4_log_dir = self.logs_base_dir / "step4"
189: 
190:         step4_script_name = "run_audio_analysis_lemonfox.py" if config.STEP4_USE_LEMONFOX else "run_audio_analysis.py"
191:         cmd = [
192:             str(config.get_venv_python("audio_env")),
193:             str(self.base_path / "workflow_scripts" / "step4" / step4_script_name),
194:             "--log_dir", str(step4_log_dir),
195:         ]
196:         
197:         return {
198:             "display_name": "4. Analyse audio",
199:             "cmd": cmd,
200:             "cwd": str(self.base_path / "projets_extraits"),
201:             "specific_logs": [
202:                 {
203:                     "name": "Log Analyse Audio",
204:                     "type": "directory_latest",
205:                     "path": step4_log_dir,
206:                     "pattern": "*.log",
207:                     "lines": 150
208:                 }
209:             ],
210:             "progress_patterns": {
211:                 "total": re.compile(r"TOTAL_AUDIO_TO_ANALYZE:\s*(\d+)", re.IGNORECASE),
212:                 "current": re.compile(r"ANALYZING_AUDIO:\s*(\d+)/(\d+):\s*(.*)", re.IGNORECASE),
213:                 "internal": re.compile(
214:                     r"INTERNAL_PROGRESS:\s*(\d+)/(\d+)\s*frames\s*\((\d+)%\)\s*-\s*(.*)",
215:                     re.IGNORECASE
216:                 ),
217:                 "current_success_line_pattern": re.compile(
218:                     r"Succès: analyse audio terminée pour (.*?)$", re.IGNORECASE
219:                 ),
220:                 "current_item_text_from_success_line": True
221:             }
222:         }
223:     
224:     def _get_step5_config(self) -> Dict[str, Any]:
225:         """Get configuration for Step 5: Tracking Analysis.
226:         
227:         Returns:
228:             Step 5 configuration dictionary
229:         """
230:         step5_log_dir = self.logs_base_dir / "step5"
231:         
232:         return {
233:             "display_name": "5. Analyse du tracking",
234:             "cmd": [
235:                 str(config.get_venv_python("tracking_env")),
236:                 str(self.base_path / "workflow_scripts" / "step5" / "run_tracking_manager.py")
237:             ],
238:             "cwd": str(self.base_path / "projets_extraits"),
239:             "specific_logs": [
240:                 {
241:                     "name": "Log Tracking Manager",
242:                     "type": "directory_latest",
243:                     "path": step5_log_dir,
244:                     "pattern": "manager_tracking*.log",
245:                     "lines": 100
246:                 },
247:                 {
248:                     "name": "Log Worker CPU",
249:                     "type": "directory_latest",
250:                     "path": step5_log_dir,
251:                     "pattern": "*worker_CPU*.log",
252:                     "lines": 100
253:                 },
254:                 {
255:                     "name": "Log Worker GPU",
256:                     "type": "directory_latest",
257:                     "path": step5_log_dir,
258:                     "pattern": "*worker_GPU*.log",
259:                     "lines": 100
260:                 }
261:             ],
262:             "progress_patterns": {
263:                 "total": re.compile(r"Vidéos à traiter: (\d+)", re.IGNORECASE),
264:                 "current": re.compile(r"Traitement de (.*?):\s*(\d+)%", re.IGNORECASE),
265:                 "internal": re.compile(r"(.*?):\s*(\d+)%", re.IGNORECASE),
266:                 "current_success_line_pattern": re.compile(
267:                     r"\[Gestionnaire\] Succès pour (.*?)$", re.IGNORECASE
268:                 ),
269:                 "current_item_text_from_success_line": True
270:             },
271:             "post_completion_message_ui": "Traitement du tracking terminé."
272:         }
273:     
274:     def _get_step6_config(self) -> Dict[str, Any]:
275:         """Get configuration for Step 6: JSON Reduction.
276:         
277:         Returns:
278:             Step 6 configuration dictionary
279:         """
280:         step6_log_dir = self.logs_base_dir / "step6"
281:         
282:         return {
283:             "display_name": "6. Réduction JSON",
284:             "cmd": [
285:                 str(config.get_venv_python("env")),
286:                 str(self.base_path / "workflow_scripts" / "step6" / "json_reducer.py"),
287:                 "--log_dir", str(step6_log_dir),
288:                 "--work_dir", str(self.base_path / "projets_extraits")
289:             ],
290:             "cwd": str(self.base_path / "projets_extraits"),
291:             "specific_logs": [
292:                 {
293:                     "name": "Log Réduction JSON",
294:                     "type": "directory_latest",
295:                     "path": step6_log_dir,
296:                     "pattern": "*.log",
297:                     "lines": 150
298:                 }
299:             ],
300:             "progress_patterns": {
301:                 "total": re.compile(r"TOTAL_JSON_TO_REDUCE:\s*(\d+)", re.IGNORECASE),
302:                 "current": re.compile(r"REDUCING_JSON:\s*(\d+)/(\d+):\s*(.*)", re.IGNORECASE),
303:                 "internal": re.compile(
304:                     r"INTERNAL_PROGRESS:\s*(\d+)/(\d+)\s*items\s*\((\d+)%\)\s*-\s*(.*)",
305:                     re.IGNORECASE
306:                 ),
307:                 "current_success_line_pattern": re.compile(
308:                     r"Succès: réduction JSON terminée pour (.*?)$", re.IGNORECASE
309:                 ),
310:                 "current_item_text_from_success_line": True
311:             }
312:         }
313:     
314:     def _get_step7_config(self) -> Dict[str, Any]:
315:         """Get configuration for Step 7: Finalization.
316:         
317:         Returns:
318:             Step 7 configuration dictionary
319:         """
320:         step7_log_dir = self.logs_base_dir / "step7"
321:         
322:         return {
323:             "display_name": "7. Finalisation",
324:             "cmd": [
325:                 str(config.get_venv_python("env")),
326:                 str(self.base_path / "workflow_scripts" / "step7" / "finalize_and_copy.py")
327:             ],
328:             "cwd": str(self.base_path / "projets_extraits"),
329:             "specific_logs": [
330:                 {
331:                     "name": "Log Finalisation",
332:                     "type": "directory_latest",
333:                     "path": step7_log_dir,
334:                     "pattern": "*.log",
335:                     "lines": 150
336:                 }
337:             ],
338:             "progress_patterns": {
339:                 "total": re.compile(r"(\d+) projet\(s\) à finaliser", re.IGNORECASE),
340:                 "current_success_line_pattern": re.compile(
341:                     r"Finalisation terminée pour '(.*?)'", re.IGNORECASE
342:                 ),
343:                 "current_item_text_from_success_line": True
344:             },
345:             "post_completion_message_ui": "Finalisation des projets terminée."
346:         }
347:     
348:     # ========== Public API ==========
349:     
350:     def get_config(self) -> Dict[str, Dict[str, Any]]:
351:         """Get the complete workflow commands configuration.
352:         
353:         Returns:
354:             Dictionary mapping step keys to their configuration
355:         """
356:         return self._config.copy()
357:     
358:     def get_step_config(self, step_key: str) -> Optional[Dict[str, Any]]:
359:         """Get configuration for a specific step.
360:         
361:         Args:
362:             step_key: Step identifier (e.g., 'STEP1', 'STEP2')
363:             
364:         Returns:
365:             Step configuration dictionary or None if not found
366:         """
367:         return self._config.get(step_key)
368:     
369:     def validate_step_key(self, step_key: str) -> bool:
370:         """Validate if a step key exists in configuration.
371:         
372:         Args:
373:             step_key: Step identifier to validate
374:             
375:         Returns:
376:             True if step key is valid
377:         """
378:         return step_key in self._config
379:     
380:     def get_all_step_keys(self) -> List[str]:
381:         """Get all valid step keys.
382:         
383:         Returns:
384:             List of step keys
385:         """
386:         return list(self._config.keys())
387:     
388:     def get_step_display_name(self, step_key: str) -> Optional[str]:
389:         """Get display name for a step.
390:         
391:         Args:
392:             step_key: Step identifier
393:             
394:         Returns:
395:             Display name or None if step not found
396:         """
397:         step_config = self.get_step_config(step_key)
398:         return step_config.get('display_name') if step_config else None
399:     
400:     def get_step_command(self, step_key: str) -> Optional[List[str]]:
401:         """Get command line for a step.
402:         
403:         Args:
404:             step_key: Step identifier
405:             
406:         Returns:
407:             Command line as list of strings or None if step not found
408:         """
409:         step_config = self.get_step_config(step_key)
410:         return step_config.get('cmd') if step_config else None
411:     
412:     def get_step_cwd(self, step_key: str) -> Optional[str]:
413:         """Get working directory for a step.
414:         
415:         Args:
416:             step_key: Step identifier
417:             
418:         Returns:
419:             Working directory path or None if step not found
420:         """
421:         step_config = self.get_step_config(step_key)
422:         return step_config.get('cwd') if step_config else None
423:     
424:     def update_hf_token(self, hf_token: str) -> None:
425:         """Update HuggingFace token and rebuild Step 4 configuration.
426:         
427:         Args:
428:             hf_token: New HuggingFace authentication token
429:         """
430:         self.hf_token = hf_token
431:         self._config["STEP4"] = self._get_step4_config()
432:         logger.info("HuggingFace token updated in configuration")
433:     
434:     def __repr__(self) -> str:
435:         """String representation of configuration."""
436:         return f"WorkflowCommandsConfig(base_path={self.base_path}, steps={len(self._config)})"
```

## File: routes/__init__.py
```python
1: # routes package initialization
```

## File: routes/api_routes.py
```python
  1: """
  2: API Routes Blueprint
  3: Handles all API endpoints for system monitoring, step status, and remote communication.
  4: """
  5: 
  6: import logging
  7: import time
  8: from functools import wraps
  9: from flask import Blueprint, jsonify, request
 10: from config.security import require_internal_worker_token, require_render_register_token
 11: from services.monitoring_service import MonitoringService
 12: from services.workflow_service import WorkflowService
 13: from services.performance_service import PerformanceService
 14: from services.filesystem_service import FilesystemService
 15: from services.visualization_service import VisualizationService
 16: from services.lemonfox_audio_service import LemonfoxAudioService
 17: 
 18: logger = logging.getLogger(__name__)
 19: 
 20: # Create API blueprint
 21: api_bp = Blueprint('api', __name__)
 22: 
 23: 
 24: def measure_api(endpoint_name: str):
 25:     """Decorator to measure API response time and record it via PerformanceService.
 26: 
 27:     Args:
 28:         endpoint_name: Logical name of the endpoint for metrics.
 29:     """
 30:     def decorator(fn):
 31:         @wraps(fn)
 32:         def wrapper(*args, **kwargs):
 33:             start = time.perf_counter()
 34:             status_code = 200
 35:             try:
 36:                 resp = fn(*args, **kwargs)
 37:                 # Flask can return a tuple (payload, status)
 38:                 if isinstance(resp, tuple) and len(resp) >= 2:
 39:                     status_code = resp[1]
 40:                 return resp
 41:             except Exception:
 42:                 status_code = 500
 43:                 raise
 44:             finally:
 45:                 elapsed_ms = (time.perf_counter() - start) * 1000.0
 46:                 try:
 47:                     PerformanceService.record_api_response_time(endpoint_name, elapsed_ms, status_code)
 48:                 except Exception:
 49:                     logger.debug("Failed to record API performance metric", exc_info=True)
 50:         return wrapper
 51:     return decorator
 52: 
 53: @api_bp.route('/system_monitor')
 54: @measure_api('/api/system_monitor')
 55: def system_monitor():
 56:     """
 57:     Get current system resource usage (CPU, RAM, GPU).
 58: 
 59:     Returns:
 60:         JSON response with system status:
 61:         {
 62:             "cpu_percent": float,
 63:             "memory": {
 64:                 "percent": float,
 65:                 "used_gb": float,
 66:                 "total_gb": float
 67:             },
 68:             "gpu": dict|null
 69:         }
 70: 
 71:     Status Codes:
 72:         200: Success
 73:         500: Server error
 74:     """
 75:     try:
 76:         # Thin controller: delegate to service layer
 77:         status = MonitoringService.get_system_status()
 78:         return jsonify(status)
 79:     except Exception as e:
 80:         logger.error(f"System monitor error: {e}")
 81:         import traceback
 82:         logger.error(f"Traceback: {traceback.format_exc()}")
 83:         return jsonify({"error": "Unable to retrieve system information"}), 500
 84: 
 85: 
 86: @api_bp.route('/system/diagnostics')
 87: @measure_api('/api/system/diagnostics')
 88: def system_diagnostics():
 89:     """
 90:     Get environment diagnostics (Python/FFmpeg versions, GPU availability, filtered config flags).
 91: 
 92:     Returns:
 93:         JSON response with environment info:
 94:         {
 95:             "python": {"version": str, "implementation": str},
 96:             "ffmpeg": {"version": str},
 97:             "gpu": {"available": bool, "name": str|null},
 98:             "config_flags": { ... },
 99:             "timestamp": str
100:         }
101: 
102:     Status Codes:
103:         200: Success
104:         500: Server error
105:     """
106:     try:
107:         info = MonitoringService.get_environment_info()
108:         return jsonify(info)
109:     except Exception as e:
110:         logger.error(f"Diagnostics error: {e}")
111:         return jsonify({"error": "Unable to retrieve diagnostics"}), 500
112: 
113: 
114: @api_bp.route('/step_status/<step_key>')
115: @measure_api('/api/step_status')
116: def step_status(step_key):
117:     """
118:     Get current status of a workflow step.
119:     
120:     Args:
121:         step_key (str): Step identifier (STEP1, STEP2, etc.)
122:         
123:     Returns:
124:         JSON response with step status:
125:         {
126:             "step": str,
127:             "display_name": str,
128:             "status": "idle|running|completed|error",
129:             "progress_current": int,
130:             "progress_total": int,
131:             "progress_text": str,
132:             "return_code": int|null,
133:             "duration_str": str|null,
134:             "is_any_sequence_running": bool
135:         }
136:         
137:     Status Codes:
138:         200: Success
139:         404: Step not found
140:         500: Server error
141:     """
142:     try:
143:         return jsonify(WorkflowService.get_step_status(step_key))
144:     except ValueError as e:
145:         return jsonify({"error": str(e)}), 404
146:     except Exception as e:
147:         logger.error(f"Step status error for {step_key}: {e}")
148:         return jsonify({"error": "Unable to retrieve step status"}), 500
149: 
150: 
151: @api_bp.route('/csv_monitor_status')
152: @measure_api('/api/csv_monitor_status')
153: def csv_monitor_status():
154:     """
155:     Get CSV monitoring service status.
156: 
157:     Returns:
158:         JSON response with CSV monitor status:
159:         {
160:             "csv_monitor": {
161:                 "status": str,
162:                 "last_check": str|null,
163:                 "error": str|null
164:             },
165:             "csv_url": str,
166:             "check_interval": int
167:         }
168: 
169:     Status Codes:
170:         200: Success
171:         500: Server error
172:     """
173:     try:
174:         from services.csv_service import CSVService
175: 
176:         # Use CSVService to get monitor status
177:         monitor_data = CSVService.get_monitor_status()
178: 
179:         return jsonify(monitor_data)
180:     except Exception as e:
181:         logger.error(f"CSV monitor status error: {e}")
182:         return jsonify({"error": "Unable to retrieve CSV monitor status"}), 500
183: 
184: 
185: @api_bp.route('/ping', methods=['GET'])
186: @measure_api('/api/ping')
187: @require_internal_worker_token
188: def api_ping():
189:     """
190:     Health check endpoint for remote servers.
191:     Requires internal worker token authentication.
192: 
193:     Returns:
194:         JSON response:
195:         {
196:             "status": "pong",
197:             "worker_type": "ubuntu_new"
198:         }
199: 
200:     Status Codes:
201:         200: Success
202:         401: Unauthorized
203:     """
204:     return jsonify({"status": "pong", "worker_type": "ubuntu_new"}), 200
205: 
206: 
207: @api_bp.route('/get_remote_status_summary', methods=['GET'])
208: @measure_api('/api/get_remote_status_summary')
209: @require_internal_worker_token
210: def api_get_remote_status_summary():
211:     """
212:     Get workflow status summary for remote servers.
213:     Requires internal worker token authentication.
214: 
215:     Returns:
216:         JSON response with comprehensive workflow status
217: 
218:     Status Codes:
219:         200: Success
220:         401: Unauthorized
221:         500: Server error
222:     """
223:     try:
224:         return jsonify(WorkflowService.get_current_workflow_status_summary())
225:     except Exception as e:
226:         logger.error(f"Remote status summary error: {e}")
227:         return jsonify({"error": "Unable to retrieve status summary"}), 500
228: 
229: 
230: @api_bp.route('/performance/metrics')
231: @measure_api('/api/performance/metrics')
232: def performance_metrics():
233:     """
234:     Get performance metrics and profiling data.
235:     
236:     Returns:
237:         JSON response with performance metrics:
238:         {
239:             "profiling_stats": dict,
240:             "cache_stats": dict,
241:             "system_performance": dict
242:         }
243:         
244:     Status Codes:
245:         200: Success
246:         500: Server error
247:     """
248:     try:
249:         return jsonify(PerformanceService.get_performance_metrics())
250:     except Exception as e:
251:         logger.error(f"Performance metrics error: {e}")
252:         return jsonify({"error": "Unable to retrieve performance metrics"}), 500
253: 
254: 
255: @api_bp.route('/performance/reset', methods=['POST'])
256: @measure_api('/api/performance/reset')
257: def reset_performance_metrics():
258:     """
259:     Reset performance profiling statistics.
260:     
261:     Returns:
262:         JSON response:
263:         {
264:             "status": "success",
265:             "message": "Performance metrics reset"
266:         }
267:         
268:     Status Codes:
269:         200: Success
270:         500: Server error
271:     """
272:     try:
273:         PerformanceService.reset_profiling_stats()
274:         return jsonify({
275:             "status": "success", 
276:             "message": "Performance metrics reset"
277:         })
278:     except Exception as e:
279:         logger.error(f"Performance reset error: {e}")
280:         return jsonify({"error": "Unable to reset performance metrics"}), 500
281: 
282: 
283: @api_bp.route('/cache/stats')
284: @measure_api('/api/cache/stats')
285: def cache_stats():
286:     """
287:     Get cache statistics and hit rates.
288:     
289:     Returns:
290:         JSON response with cache statistics
291:         
292:     Status Codes:
293:         200: Success
294:         500: Server error
295:     """
296:     try:
297:         from services.cache_service import CacheService
298:         return jsonify(CacheService.get_cache_stats())
299:     except Exception as e:
300:         logger.error(f"Cache stats error: {e}")
301:         return jsonify({"error": "Unable to retrieve cache statistics"}), 500
302: 
303: 
304: 
305: @api_bp.route('/cache/search', methods=['GET'])
306: @measure_api('/api/cache/search')
307: def cache_search():
308:     """
309:     Search for folders under /mnt/cache that start with a given number.
310: 
311:     Query Params:
312:         number (str): Numeric identifier, e.g., "115"
313: 
314:     Returns:
315:         JSON response:
316:         {
317:             "number": str,
318:             "matches": [str],
319:             "best_match": str|null
320:         }
321: 
322:     Status Codes:
323:         200: Success
324:         400: Missing parameter
325:         500: Server error
326:     """
327:     try:
328:         number = request.args.get('number', '').strip()
329:         if not number:
330:             return jsonify({"error": "Missing 'number' parameter"}), 400
331: 
332:         result = FilesystemService.find_cache_folder_by_number(number)
333:         return jsonify({
334:             "number": result.number,
335:             "matches": result.matches,
336:             "best_match": result.best_match
337:         })
338:     except Exception as e:
339:         logger.error(f"Cache search error: {e}")
340:         return jsonify({"error": "Unable to search cache"}), 500
341: 
342: 
343: @api_bp.route('/cache/list_today', methods=['GET'])
344: @measure_api('/api/cache/list_today')
345: def cache_list_today():
346:     """
347:     List folders under /mnt/cache that were created/modified today.
348: 
349:     Returns:
350:         JSON response:
351:         {
352:             "folders": [
353:                 {"path": str, "name": str, "number": str|null, "mtime": str}
354:             ]
355:         }
356: 
357:     Status Codes:
358:         200: Success
359:         500: Server error
360:     """
361:     try:
362:         folders = FilesystemService.list_today_cache_folders()
363:         return jsonify({"folders": folders})
364:     except Exception as e:
365:         logger.error(f"Cache list_today error: {e}")
366:         return jsonify({"error": "Unable to list today's cache folders"}), 500
367: 
368: @api_bp.route('/cache/open', methods=['POST'])
369: @measure_api('/api/cache/open')
370: def cache_open():
371:     """
372:     Open a folder path in the system's file explorer (server-side).
373: 
374:     Request Body (JSON):
375:         {
376:             "path": "/mnt/cache/115 Camille",
377:             "select_parent": false   # optional, open parent and preselect target when supported
378:         }
379: 
380:     Returns:
381:         JSON response:
382:         {
383:             "success": bool,
384:             "message": str
385:         }
386: 
387:     Status Codes:
388:         200: Success/Failure message
389:         400: Invalid body
390:         500: Server error
391:     """
392:     try:
393:         data = request.get_json(silent=True) or {}
394:         path = (data.get('path') or '').strip()
395:         select_parent = bool(data.get('select_parent', False))
396:         if not path:
397:             return jsonify({"success": False, "message": "Paramètre 'path' manquant"}), 400
398: 
399:         success, message = FilesystemService.open_path_in_explorer(path, select_parent=select_parent)
400:         return jsonify({"success": success, "message": message})
401:     except Exception as e:
402:         logger.error(f"Cache open error: {e}")
403:         return jsonify({"success": False, "message": "Erreur interne pour l'ouverture du dossier"}), 500
404: @api_bp.route('/cache/clear', methods=['POST'])
405: @measure_api('/api/cache/clear')
406: def clear_cache():
407:     """
408:     Clear application cache.
409: 
410:     Returns:
411:         JSON response:
412:         {
413:             "status": "success",
414:             "message": "Cache cleared"
415:         }
416: 
417:     Status Codes:
418:         200: Success
419:         500: Server error
420:     """
421:     try:
422:         from services.cache_service import CacheService
423:         CacheService.clear_cache()
424:         return jsonify({
425:             "status": "success",
426:             "message": "Cache cleared"
427:         })
428:     except Exception as e:
429:         logger.error(f"Cache clear error: {e}")
430:         return jsonify({"error": "Unable to clear cache"}), 500
431: 
432: 
433: 
434: 
435: 
436: @api_bp.route('/csv_downloads_status')
437: def get_csv_downloads_status():
438:     """
439:     Get CSV downloads status.
440: 
441:     Returns:
442:         JSON response with CSV downloads status
443: 
444:     Status Codes:
445:         200: Success
446:         500: Server error
447:     """
448:     try:
449:         from services.csv_service import CSVService
450:         from datetime import datetime
451: 
452:         # Use CSVService to get download status
453:         downloads_status = CSVService.get_csv_downloads_status()
454: 
455:         # Combine active and recent downloads
456:         active_downloads = downloads_status.get("active_downloads", {})
457:         recent_statuses = downloads_status.get("recent_statuses", [])
458: 
459:         # Convert active downloads dict to list
460:         active_list = list(active_downloads.values()) if active_downloads else []
461: 
462:         # Combine and sort all downloads
463:         all_downloads = active_list + recent_statuses
464:         all_downloads.sort(key=lambda x: x.get('timestamp', datetime.min), reverse=True)
465: 
466:         # Clean timestamps for JSON serialization
467:         json_safe_downloads = []
468:         for download in all_downloads:
469:             download_copy = download.copy()
470:             # Remove datetime objects that can't be JSON serialized
471:             download_copy.pop('timestamp', None)
472:             json_safe_downloads.append(download_copy)
473: 
474:         return jsonify(json_safe_downloads)
475:     except Exception as e:
476:         logger.error(f"CSV downloads status error: {e}")
477:         return jsonify({"error": "Unable to retrieve CSV downloads status"}), 500
478: 
479: 
480: 
481: 
482: 
483: 
484: @api_bp.route('/stats/dashboard')
485: @measure_api('/api/stats/dashboard')
486: def stats_dashboard():
487:     """
488:     Get comprehensive statistics for the performance dashboard.
489: 
490:     Returns:
491:         JSON response with dashboard statistics:
492:         {
493:             "summary": {
494:                 "total_api_calls": int,
495:                 "total_errors": int,
496:                 "avg_response_time_ms": float,
497:                 "error_rate_percent": float
498:             },
499:             "api_metrics": list,
500:             "system_metrics": list,
501:             "step_history": dict,
502:             "alerts": list,
503:             "cache_stats": dict,
504:             "profiling_summary": dict
505:         }
506: 
507:     Status Codes:
508:         200: Success
509:         500: Server error
510:     """
511:     try:
512:         stats = PerformanceService.get_dashboard_stats()
513:         return jsonify(stats)
514:     except Exception as e:
515:         logger.error(f"Dashboard stats error: {e}")
516:         return jsonify({"error": "Unable to retrieve dashboard statistics"}), 500
517: 
518: 
519: @api_bp.route('/stats/history')
520: @measure_api('/api/stats/history')
521: def stats_history():
522:     """
523:     Get historical performance data.
524: 
525:     Query Parameters:
526:         type (str): Data type - "api", "system", or "all" (default: "all")
527:         limit (int): Maximum number of records (default: 100, max: 500)
528: 
529:     Returns:
530:         JSON response with historical data:
531:         {
532:             "data_type": str,
533:             "count": int,
534:             "data": list
535:         }
536: 
537:     Status Codes:
538:         200: Success
539:         400: Invalid parameters
540:         500: Server error
541:     """
542:     try:
543:         data_type = request.args.get('type', 'all')
544:         limit = request.args.get('limit', 100, type=int)
545:         
546:         # Validate parameters
547:         if data_type not in ['api', 'system', 'all']:
548:             return jsonify({"error": "Invalid data type. Must be 'api', 'system', or 'all'"}), 400
549:         
550:         if limit < 1 or limit > 500:
551:             return jsonify({"error": "Limit must be between 1 and 500"}), 400
552:         
553:         history = PerformanceService.get_historical_data(data_type, limit)
554:         return jsonify(history)
555:         
556:     except Exception as e:
557:         logger.error(f"Historical data error: {e}")
558:         return jsonify({"error": "Unable to retrieve historical data"}), 500
559: 
560: 
561: 
562: @api_bp.route('/visualization/projects', methods=['GET'])
563: @measure_api('/api/visualization/projects')
564: def visualization_projects():
565:     """
566:     List available projects and their videos for visualization/reporting.
567: 
568:     Returns:
569:         JSON response:
570:         {
571:             "projects": [
572:                 {"name": str, "path": str, "videos": [str], "video_count": int,
573:                  "has_scenes": bool, "has_audio": bool, "has_tracking": bool, "source": "projects|archives"}
574:             ],
575:             "count": int,
576:             "timestamp": str
577:         }
578: 
579:     Status Codes:
580:         200: Success
581:         500: Server error
582:     """
583:     try:
584:         data = VisualizationService.get_available_projects()
585:         # If service reports an error, surface as 500 while returning payload
586:         if data.get("error"):
587:             return jsonify(data), 500
588:         return jsonify(data)
589:     except Exception as e:
590:         logger.error(f"Visualization projects error: {e}", exc_info=True)
591:         return jsonify({"error": "Unable to list visualization projects"}), 500
592: 
593: 
594: @api_bp.route('/step4/lemonfox_audio', methods=['POST'])
595: @measure_api('/api/step4/lemonfox_audio')
596: def lemonfox_audio_analysis():
597:     """
598:     Process video audio analysis using Lemonfox Speech-to-Text API.
599:     
600:     Generates a STEP4-compatible {video_stem}_audio.json file with frame-by-frame
601:     audio analysis including speaker diarization.
602:     
603:     Request JSON:
604:         {
605:             "project_name": str (required),
606:             "video_name": str (required, relative path within project),
607:             "language": str (optional),
608:             "prompt": str (optional),
609:             "speaker_labels": bool (optional, default: true),
610:             "min_speakers": int (optional),
611:             "max_speakers": int (optional),
612:             "timestamp_granularities": array[str] (optional, e.g., ["word"]),
613:             "eu_processing": bool (optional, uses LEMONFOX_EU_DEFAULT if not set)
614:         }
615:     
616:     Returns:
617:         JSON response on success:
618:         {
619:             "status": "success",
620:             "output_path": str,
621:             "fps": float,
622:             "total_frames": int
623:         }
624:         
625:         JSON response on error:
626:         {
627:             "status": "error",
628:             "error": str
629:         }
630:     
631:     Status Codes:
632:         200: Success
633:         400: Invalid input parameters
634:         404: Project or video not found
635:         502: Lemonfox API upstream error
636:         500: Server error
637:     """
638:     try:
639:         # Parse and validate request
640:         data = request.get_json(silent=True)
641:         if not data:
642:             return jsonify({
643:                 "status": "error",
644:                 "error": "Request body must be JSON"
645:             }), 400
646:         
647:         # Required fields
648:         project_name = data.get('project_name', '').strip()
649:         video_name = data.get('video_name', '').strip()
650:         
651:         if not project_name:
652:             return jsonify({
653:                 "status": "error",
654:                 "error": "project_name is required"
655:             }), 400
656:         
657:         if not video_name:
658:             return jsonify({
659:                 "status": "error",
660:                 "error": "video_name is required"
661:             }), 400
662:         
663:         # Optional fields
664:         language = data.get('language')
665:         prompt = data.get('prompt')
666:         speaker_labels = data.get('speaker_labels')
667:         min_speakers = data.get('min_speakers')
668:         max_speakers = data.get('max_speakers')
669:         timestamp_granularities = data.get('timestamp_granularities')
670:         eu_processing = data.get('eu_processing')
671:         
672:         # Validate types
673:         if min_speakers is not None and not isinstance(min_speakers, int):
674:             return jsonify({
675:                 "status": "error",
676:                 "error": "min_speakers must be an integer"
677:             }), 400
678:         
679:         if max_speakers is not None and not isinstance(max_speakers, int):
680:             return jsonify({
681:                 "status": "error",
682:                 "error": "max_speakers must be an integer"
683:             }), 400
684:         
685:         if timestamp_granularities is not None and not isinstance(timestamp_granularities, list):
686:             return jsonify({
687:                 "status": "error",
688:                 "error": "timestamp_granularities must be an array"
689:             }), 400
690: 
691:         if speaker_labels is not None and not isinstance(speaker_labels, bool):
692:             return jsonify({
693:                 "status": "error",
694:                 "error": "speaker_labels must be a boolean"
695:             }), 400
696:         
697:         # Call service
698:         result = LemonfoxAudioService.process_video_with_lemonfox(
699:             project_name=project_name,
700:             video_name=video_name,
701:             language=language,
702:             prompt=prompt,
703:             speaker_labels=speaker_labels,
704:             min_speakers=min_speakers,
705:             max_speakers=max_speakers,
706:             timestamp_granularities=timestamp_granularities,
707:             eu_processing=eu_processing
708:         )
709:         
710:         if not result.success:
711:             error_msg = result.error or "Unknown error"
712:             
713:             # Determine appropriate status code
714:             if "not found" in error_msg.lower():
715:                 status_code = 404
716:             elif "lemonfox api" in error_msg.lower():
717:                 status_code = 502
718:             elif "not configured" in error_msg.lower():
719:                 status_code = 500
720:             else:
721:                 status_code = 400
722:             
723:             return jsonify({
724:                 "status": "error",
725:                 "error": error_msg
726:             }), status_code
727:         
728:         # Success response
729:         return jsonify({
730:             "status": "success",
731:             "output_path": str(result.output_path),
732:             "fps": result.fps,
733:             "total_frames": result.total_frames
734:         }), 200
735:         
736:     except Exception as e:
737:         logger.error(f"Lemonfox audio analysis error: {e}", exc_info=True)
738:         return jsonify({
739:             "status": "error",
740:             "error": "Internal server error"
741:         }), 500
```

## File: routes/workflow_routes.py
```python
  1: """
  2: Workflow Routes Blueprint
  3: Handles workflow execution, step management, and sequence operations.
  4: """
  5: 
  6: import logging
  7: import time
  8: from functools import wraps
  9: from flask import Blueprint, jsonify, request, render_template, send_from_directory
 10: from services.workflow_service import WorkflowService
 11: from services.cache_service import CacheService
 12: from services.performance_service import PerformanceService
 13: from config.settings import config
 14: 
 15: # Configure route logger to capture all debug statements
 16: logger = logging.getLogger(__name__)
 17: logger.setLevel(logging.DEBUG)  # Ensure we capture all debug statements
 18: 
 19: # Used for cache-busting static assets in templates. This value changes on server restart,
 20: # which is enough to force browsers to fetch updated JS/CSS after a deploy/restart.
 21: _STATIC_CACHE_BUSTER = str(int(time.time()))
 22: 
 23: # Create workflow blueprint
 24: workflow_bp = Blueprint('workflow', __name__)
 25: 
 26: 
 27: def measure_api(endpoint_name: str):
 28:     """Decorator to measure API response time and record it via PerformanceService.
 29: 
 30:     Args:
 31:         endpoint_name: Logical name of the endpoint for metrics.
 32:     """
 33:     def decorator(fn):
 34:         @wraps(fn)
 35:         def wrapper(*args, **kwargs):
 36:             start = time.perf_counter()
 37:             status_code = 200
 38:             try:
 39:                 resp = fn(*args, **kwargs)
 40:                 # Flask can return a tuple (payload, status)
 41:                 if isinstance(resp, tuple) and len(resp) >= 2:
 42:                     status_code = resp[1]
 43:                 return resp
 44:             except Exception:
 45:                 status_code = 500
 46:                 raise
 47:             finally:
 48:                 elapsed_ms = (time.perf_counter() - start) * 1000.0
 49:                 try:
 50:                     PerformanceService.record_api_response_time(endpoint_name, elapsed_ms, status_code)
 51:                 except Exception:
 52:                     logger.debug("Failed to record API performance metric", exc_info=True)
 53:         return wrapper
 54:     return decorator
 55: 
 56: 
 57: @workflow_bp.route('/')
 58: def index():
 59:     """
 60:     Main application page.
 61:     
 62:     Returns:
 63:         Rendered HTML template with steps configuration
 64:         
 65:     Status Codes:
 66:         200: Success
 67:     """
 68:     try:
 69:         # Use cached configuration for better performance
 70:         frontend_safe_steps_config = CacheService.get_cached_frontend_config()
 71:         return render_template(
 72:             'index_new.html',
 73:             steps_config=frontend_safe_steps_config,
 74:             cache_buster=_STATIC_CACHE_BUSTER,
 75:         )
 76:     except Exception as e:
 77:         logger.error(f"Index page error: {e}")
 78:         return render_template('error.html', error="Unable to load application"), 500
 79: 
 80: 
 81: @workflow_bp.route('/run/<step_key>', methods=['POST'])
 82: @measure_api('/workflow/run')
 83: def run_step(step_key):
 84:     """
 85:     Execute a single workflow step.
 86:     
 87:     Args:
 88:         step_key (str): Step identifier (STEP1, STEP2, etc.)
 89:         
 90:     Returns:
 91:         JSON response:
 92:         {
 93:             "status": "initiated|error",
 94:             "message": str
 95:         }
 96:         
 97:     Status Codes:
 98:         202: Step initiated successfully
 99:         404: Step not found
100:         409: Step already running or sequence in progress
101:         500: Server error
102:     """
103:     try:
104:         result = WorkflowService.run_step(step_key)
105:         
106:         if result["status"] == "initiated":
107:             return jsonify(result), 202
108:         elif result["status"] == "error":
109:             if "inconnue" in result["message"]:
110:                 return jsonify(result), 404
111:             elif "en cours" in result["message"]:
112:                 return jsonify(result), 409
113:             else:
114:                 return jsonify(result), 500
115:         else:
116:             return jsonify(result), 500
117:             
118:     except Exception as e:
119:         logger.error(f"Run step error for {step_key}: {e}")
120:         return jsonify({
121:             "status": "error", 
122:             "message": f"Internal error running step {step_key}"
123:         }), 500
124: 
125: 
126: @workflow_bp.route('/run_custom_sequence', methods=['POST'])
127: @measure_api('/workflow/run_custom_sequence')
128: def run_custom_sequence():
129:     """
130:     Execute a custom sequence of workflow steps.
131:     
132:     Request Body:
133:         {
134:             "steps": ["STEP1", "STEP2", ...]
135:         }
136:         
137:     Returns:
138:         JSON response:
139:         {
140:             "status": "initiated|error",
141:             "message": str
142:         }
143:         
144:     Status Codes:
145:         202: Sequence initiated successfully
146:         400: Invalid request data
147:         409: Sequence already running
148:         500: Server error
149:     """
150:     try:
151:         data = request.get_json()
152:         if not data or not isinstance(data.get('steps'), list):
153:             return jsonify({
154:                 "status": "error", 
155:                 "message": "Invalid steps list"
156:             }), 400
157:             
158:         result = WorkflowService.run_custom_sequence(data['steps'])
159:         
160:         if result["status"] == "initiated":
161:             return jsonify(result), 202
162:         elif result["status"] == "error":
163:             if "en cours" in result["message"]:
164:                 return jsonify(result), 409
165:             else:
166:                 return jsonify(result), 400
167:         else:
168:             return jsonify(result), 500
169:             
170:     except Exception as e:
171:         logger.error(f"Custom sequence error: {e}")
172:         return jsonify({
173:             "status": "error", 
174:             "message": "Internal error running custom sequence"
175:         }), 500
176: 
177: 
178: @workflow_bp.route('/status/<step_key>', methods=['GET'])
179: @measure_api('/workflow/status')
180: def get_status(step_key):
181:     """
182:     Get detailed status of a workflow step.
183:     
184:     Args:
185:         step_key (str): Step identifier
186:         
187:     Returns:
188:         JSON response with detailed step status including logs
189:         
190:     Status Codes:
191:         200: Success
192:         404: Step not found
193:         500: Server error
194:     """
195:     try:
196:         status_data = WorkflowService.get_step_status(step_key, include_logs=True)
197: 
198:         # DEBUG: Log what we're returning to the frontend during AutoMode
199:         if status_data.get('is_any_sequence_running', False):
200:             logger.info(f"[ROUTE_DEBUG] /status/{step_key} returning: status='{status_data.get('status')}', progress={status_data.get('progress_current')}/{status_data.get('progress_total')}")
201: 
202:         return jsonify(status_data)
203:     except ValueError as e:
204:         return jsonify({"status": "error", "message": str(e)}), 404
205:     except Exception as e:
206:         logger.error(f"Get status error for {step_key}: {e}")
207:         return jsonify({"error": "Unable to retrieve step status"}), 500
208: 
209: 
210: @workflow_bp.route('/stop/<step_key>', methods=['POST'])
211: @measure_api('/workflow/stop')
212: def stop_step(step_key):
213:     """
214:     Stop a running workflow step.
215:     
216:     Args:
217:         step_key (str): Step identifier
218:         
219:     Returns:
220:         JSON response:
221:         {
222:             "status": "success|error",
223:             "message": str
224:         }
225:         
226:     Status Codes:
227:         200: Success
228:         404: Step not found
229:         409: Step not running
230:         500: Server error
231:     """
232:     try:
233:         result = WorkflowService.stop_step(step_key)
234:         return jsonify(result)
235:     except ValueError as e:
236:         return jsonify({"status": "error", "message": str(e)}), 404
237:     except Exception as e:
238:         logger.error(f"Stop step error for {step_key}: {e}")
239:         return jsonify({
240:             "status": "error", 
241:             "message": f"Internal error stopping step {step_key}"
242:         }), 500
243: 
244: 
245: @workflow_bp.route('/get_specific_log_test/<step_key>/<int:log_index>')
246: def get_specific_log_test(step_key, log_index):
247:     """
248:     Test version of get specific log that bypasses cache service.
249:     """
250:     try:
251:         result = WorkflowService.get_step_log_file(step_key, log_index)
252:         return jsonify(result)
253: 
254:     except Exception as e:
255:         logger.error(f"Test log endpoint error for {step_key}/{log_index}: {e}")
256:         import traceback
257:         logger.error(f"Traceback: {traceback.format_exc()}")
258:         return jsonify({"error": f"Internal error: {str(e)}"}), 500
259: 
260: @workflow_bp.route('/get_specific_log/<step_key>/<int:log_index>')
261: def get_specific_log(step_key, log_index):
262:     """
263:     Get specific log file content for a step.
264:     
265:     Args:
266:         step_key (str): Step identifier
267:         log_index (int): Log file index
268:         
269:     Returns:
270:         JSON response with log content
271:         
272:     Status Codes:
273:         200: Success
274:         404: Step or log not found
275:         500: Server error
276:     """
277:     try:
278:         result = WorkflowService.get_step_log_file(step_key, log_index)
279:         return jsonify(result)
280:     except ValueError as e:
281:         logger.error(f"ValueError in get_specific_log for {step_key}/{log_index}: {e}")
282:         return jsonify({"error": str(e)}), 404
283:     except Exception as e:
284:         logger.error(f"Get specific log error for {step_key}/{log_index}: {e}")
285:         import traceback
286:         logger.error(f"Traceback: {traceback.format_exc()}")
287:         return jsonify({"error": "Unable to retrieve log content"}), 500
288: 
289: 
290: @workflow_bp.route('/sound-design/<filename>')
291: def serve_sound_file(filename):
292:     """
293:     Serve sound files from the sound-design directory.
294:     
295:     Args:
296:         filename (str): Sound file name
297:         
298:     Returns:
299:         Sound file content
300:         
301:     Status Codes:
302:         200: Success
303:         404: File not found
304:     """
305:     try:
306:         sound_dir = config.BASE_PATH_SCRIPTS / 'sound-design'
307:         return send_from_directory(sound_dir, filename)
308:     except Exception as e:
309:         logger.error(f"Sound file serve error for {filename}: {e}")
310:         return jsonify({"error": "Sound file not found"}), 404
311: 
312: 
313: @workflow_bp.route('/test-sound')
314: def test_sound():
315:     """
316:     Serve the sound test page.
317:     
318:     Returns:
319:         Sound test HTML page
320:         
321:     Status Codes:
322:         200: Success
323:         404: File not found
324:     """
325:     try:
326:         return send_from_directory(config.BASE_PATH_SCRIPTS, 'test_sound.html')
327:     except Exception as e:
328:         logger.error(f"Test sound page error: {e}")
329:         return jsonify({"error": "Test sound page not found"}), 404
330: 
331: 
332: @workflow_bp.route('/sequence/status')
333: @measure_api('/workflow/sequence/status')
334: def sequence_status():
335:     """
336:     Get current sequence execution status.
337:     
338:     Returns:
339:         JSON response with sequence status:
340:         {
341:             "is_running": bool,
342:             "current_step": str|null,
343:             "progress": {
344:                 "current": int,
345:                 "total": int
346:             },
347:             "last_outcome": dict
348:         }
349:         
350:     Status Codes:
351:         200: Success
352:         500: Server error
353:     """
354:     try:
355:         return jsonify(WorkflowService.get_sequence_status())
356:     except Exception as e:
357:         logger.error(f"Sequence status error: {e}")
358:         return jsonify({"error": "Unable to retrieve sequence status"}), 500
359: 
360: 
361: @workflow_bp.route('/sequence/stop', methods=['POST'])
362: @measure_api('/workflow/sequence/stop')
363: def stop_sequence():
364:     """
365:     Stop the currently running sequence.
366: 
367:     Returns:
368:         JSON response:
369:         {
370:             "status": "success|error",
371:             "message": str
372:         }
373: 
374:     Status Codes:
375:         200: Success
376:         409: No sequence running
377:         500: Server error
378:     """
379:     try:
380:         result = WorkflowService.stop_sequence()
381:         if result["status"] == "error" and "aucune" in result["message"]:
382:             return jsonify(result), 409
383:         return jsonify(result)
384:     except Exception as e:
385:         logger.error(f"Stop sequence error: {e}")
386:         return jsonify({
387:             "status": "error",
388:             "message": "Internal error stopping sequence"
389:         }), 500
390: 
391: 
392: @workflow_bp.route('/cancel/<step_key>', methods=['POST'])
393: @measure_api('/workflow/cancel')
394: def cancel_step(step_key):
395:     """
396:     Cancel a running workflow step.
397: 
398:     Args:
399:         step_key (str): Step identifier
400: 
401:     Returns:
402:         JSON response:
403:         {
404:             "status": "success|error",
405:             "message": str
406:         }
407: 
408:     Status Codes:
409:         200: Success
410:         400: Step not running or doesn't exist
411:         500: Server error
412:     """
413:     try:
414:         result = WorkflowService.stop_step(step_key)
415:         if result["status"] == "error":
416:             if "not running" in result["message"] or "not found" in result["message"]:
417:                 return jsonify(result), 400
418:             else:
419:                 return jsonify(result), 500
420:         return jsonify(result)
421:     except ValueError as e:
422:         return jsonify({"status": "error", "message": str(e)}), 400
423:     except Exception as e:
424:         logger.error(f"Cancel step error for {step_key}: {e}")
425:         return jsonify({
426:             "status": "error",
427:             "message": f"Internal error cancelling step {step_key}"
428:         }), 500
429: 
430: 
431: # Additional workflow routes that were moved from app_new.py
432: # Note: Duplicate routes removed to prevent conflicts
433: @workflow_bp.route('/sound-design/<filename>')
434: def serve_sound_file_blueprint(filename):
435:     """
436:     Serve sound files from the sound-design directory (moved from app_new.py).
437: 
438:     Args:
439:         filename (str): Sound file name
440: 
441:     Returns:
442:         Sound file content
443: 
444:     Status Codes:
445:         200: Success
446:         404: File not found
447:     """
448:     try:
449:         sound_dir = config.BASE_PATH_SCRIPTS / 'sound-design'
450:         return send_from_directory(sound_dir, filename)
451:     except Exception as e:
452:         logger.error(f"Sound file serve error for {filename}: {e}")
453:         return jsonify({"error": "Sound file not found"}), 404
454: 
455: 
456: @workflow_bp.route('/test-sound')
457: def test_sound_blueprint():
458:     """
459:     Serve the sound test page (moved from app_new.py).
460: 
461:     Returns:
462:         Sound test HTML page
463: 
464:     Status Codes:
465:         200: Success
466:         404: File not found
467:     """
468:     try:
469:         return send_from_directory(config.BASE_PATH_SCRIPTS, 'test_sound.html')
470:     except Exception as e:
471:         logger.error(f"Test sound page error: {e}")
472:         return jsonify({"error": "Test sound page not found"}), 404
```

## File: services/deprecated/airtable_service.py
```python
  1: """
  2: Airtable Service for workflow_mediapipe application.
  3: 
  4: This service handles all Airtable API interactions for monitoring download URLs.
  5: Replaces the legacy CSV/XLSX file monitoring system with a more reliable
  6: Airtable-based solution.
  7: 
  8: Author: Augment Agent
  9: Date: 2025-07-21
 10: """
 11: 
 12: import logging
 13: from typing import Dict, List, Any, Optional
 14: from datetime import datetime, timezone
 15: import time
 16: 
 17: from config.settings import config
 18: from services.cache_service import CacheService
 19: 
 20: logger = logging.getLogger(__name__)
 21: 
 22: try:
 23:     from pyairtable import Api
 24:     AIRTABLE_AVAILABLE = True
 25: except ImportError:
 26:     AIRTABLE_AVAILABLE = False
 27:     logger.warning("pyairtable not available. Airtable integration disabled.")
 28: 
 29: 
 30: class AirtableService:
 31:     """
 32:     Service for managing Airtable integration.
 33:     
 34:     Provides methods for fetching records from Airtable and monitoring
 35:     for new download URLs. Designed to replace the legacy CSV monitoring
 36:     system while maintaining the same interface.
 37:     """
 38:     
 39:     _api_instance: Optional[Any] = None
 40:     _table_instance: Optional[Any] = None
 41:     
 42:     @classmethod
 43:     def _get_api(cls) -> Optional[Any]:
 44:         """
 45:         Get or create Airtable API instance.
 46:         
 47:         Returns:
 48:             Airtable API instance or None if not available
 49:         """
 50:         if not AIRTABLE_AVAILABLE:
 51:             logger.error("Airtable integration not available - pyairtable not installed")
 52:             return None
 53:             
 54:         if not config.AIRTABLE_ACCESS_TOKEN:
 55:             logger.error("Airtable access token not configured")
 56:             return None
 57:             
 58:         if cls._api_instance is None:
 59:             try:
 60:                 cls._api_instance = Api(config.AIRTABLE_ACCESS_TOKEN)
 61:                 logger.info("Airtable API instance created successfully")
 62:             except Exception as e:
 63:                 logger.error(f"Failed to create Airtable API instance: {e}")
 64:                 return None
 65:                 
 66:         return cls._api_instance
 67:     
 68:     @classmethod
 69:     def _get_table(cls) -> Optional[Any]:
 70:         """
 71:         Get or create Airtable table instance.
 72: 
 73:         Returns:
 74:             Airtable table instance or None if not available
 75:         """
 76:         if cls._table_instance is None:
 77:             api = cls._get_api()
 78:             if api is None:
 79:                 return None
 80: 
 81:             try:
 82:                 # Get base by name
 83:                 bases = api.bases()
 84:                 target_base = None
 85: 
 86:                 for base in bases:
 87:                     if base.name == config.AIRTABLE_BASE_NAME:
 88:                         target_base = base
 89:                         break
 90: 
 91:                 if target_base is None:
 92:                     logger.error(f"Airtable base '{config.AIRTABLE_BASE_NAME}' not found")
 93:                     available_bases = [base.name for base in bases]
 94:                     logger.error(f"Available bases: {available_bases}")
 95:                     return None
 96: 
 97:                 # Get table from base - try by name first, then by ID if available
 98:                 try:
 99:                     cls._table_instance = target_base.table(config.AIRTABLE_TABLE_NAME)
100:                     logger.info(f"Airtable table '{config.AIRTABLE_TABLE_NAME}' connected successfully")
101:                 except Exception as table_error:
102:                     logger.warning(f"Failed to connect to table by name: {table_error}")
103: 
104:                     # Try to get table info from schema for better error reporting
105:                     try:
106:                         schema = target_base.schema()
107:                         available_tables = [(table.name, table.id) for table in schema.tables]
108:                         logger.info(f"Available tables: {available_tables}")
109: 
110:                         # Try to find table by name in schema
111:                         for table_info in schema.tables:
112:                             if table_info.name == config.AIRTABLE_TABLE_NAME:
113:                                 logger.info(f"Found table in schema: {table_info.name} (ID: {table_info.id})")
114:                                 # Try using table ID instead of name
115:                                 cls._table_instance = target_base.table(table_info.id)
116:                                 logger.info(f"Connected to table using ID: {table_info.id}")
117:                                 break
118:                         else:
119:                             logger.error(f"Table '{config.AIRTABLE_TABLE_NAME}' not found in schema")
120:                             return None
121: 
122:                     except Exception as schema_error:
123:                         logger.error(f"Failed to get schema information: {schema_error}")
124:                         return None
125: 
126:             except Exception as e:
127:                 logger.error(f"Failed to connect to Airtable table: {e}")
128:                 return None
129: 
130:         return cls._table_instance
131:     
132:     @staticmethod
133:     def test_connection() -> Dict[str, Any]:
134:         """
135:         Test Airtable connection and configuration.
136:         
137:         Returns:
138:             Dictionary with connection test results
139:         """
140:         result = {
141:             "status": "error",
142:             "message": "",
143:             "details": {}
144:         }
145:         
146:         try:
147:             # Check if Airtable is available
148:             if not AIRTABLE_AVAILABLE:
149:                 result["message"] = "pyairtable library not installed"
150:                 result["details"]["pyairtable_available"] = False
151:                 return result
152:             
153:             result["details"]["pyairtable_available"] = True
154:             
155:             # Check configuration
156:             if not config.AIRTABLE_ACCESS_TOKEN:
157:                 result["message"] = "Airtable access token not configured"
158:                 result["details"]["token_configured"] = False
159:                 return result
160:             
161:             result["details"]["token_configured"] = True
162:             result["details"]["base_name"] = config.AIRTABLE_BASE_NAME
163:             result["details"]["table_name"] = config.AIRTABLE_TABLE_NAME
164:             
165:             # Test API connection and schema access
166:             api = AirtableService._get_api()
167:             if api is None:
168:                 result["message"] = "Failed to create API instance"
169:                 result["details"]["api_accessible"] = False
170:                 return result
171: 
172:             result["details"]["api_accessible"] = True
173: 
174:             # Test base access
175:             try:
176:                 bases = api.bases()
177:                 result["details"]["bases_accessible"] = True
178:                 result["details"]["available_bases"] = [base.name for base in bases]
179: 
180:                 # Find target base
181:                 target_base = None
182:                 for base in bases:
183:                     if base.name == config.AIRTABLE_BASE_NAME:
184:                         target_base = base
185:                         break
186: 
187:                 if target_base is None:
188:                     result["message"] = f"Base '{config.AIRTABLE_BASE_NAME}' not found"
189:                     result["details"]["base_found"] = False
190:                     return result
191: 
192:                 result["details"]["base_found"] = True
193:                 result["details"]["base_id"] = target_base.id
194: 
195:             except Exception as e:
196:                 result["message"] = f"Failed to access bases: {e}"
197:                 result["details"]["bases_accessible"] = False
198:                 return result
199: 
200:             # Test schema access
201:             try:
202:                 schema = target_base.schema()
203:                 result["details"]["schema_accessible"] = True
204: 
205:                 # Get table information from schema
206:                 table_info = None
207:                 for table in schema.tables:
208:                     if table.name == config.AIRTABLE_TABLE_NAME:
209:                         table_info = table
210:                         break
211: 
212:                 if table_info is None:
213:                     result["message"] = f"Table '{config.AIRTABLE_TABLE_NAME}' not found in schema"
214:                     result["details"]["table_found_in_schema"] = False
215:                     result["details"]["available_tables"] = [(t.name, t.id) for t in schema.tables]
216:                     return result
217: 
218:                 result["details"]["table_found_in_schema"] = True
219:                 result["details"]["table_id"] = table_info.id
220:                 result["details"]["available_fields"] = [field.name for field in table_info.fields]
221: 
222:                 # Check for required fields in schema
223:                 field_names = [field.name.lower() for field in table_info.fields]
224:                 has_timestamp = any('timestamp' in field or 'time' in field or 'date' in field for field in field_names)
225:                 has_url = any('url' in field and 'dropbox' in field for field in field_names)
226: 
227:                 result["details"]["has_timestamp_field"] = has_timestamp
228:                 result["details"]["has_url_field"] = has_url
229: 
230:             except Exception as e:
231:                 result["message"] = f"Failed to access schema: {e}"
232:                 result["details"]["schema_accessible"] = False
233:                 return result
234: 
235:             # Test table connection
236:             table = AirtableService._get_table()
237:             if table is None:
238:                 result["message"] = "Failed to connect to Airtable table"
239:                 result["details"]["table_accessible"] = False
240:                 return result
241: 
242:             result["details"]["table_accessible"] = True
243: 
244:             # Test record access (this may fail due to permissions)
245:             try:
246:                 records = table.all(max_records=1)
247:                 result["details"]["records_fetchable"] = True
248:                 result["details"]["sample_record_count"] = len(records)
249: 
250:                 if result["details"]["has_timestamp_field"] and result["details"]["has_url_field"]:
251:                     result["status"] = "success"
252:                     result["message"] = "Airtable connection successful"
253:                 else:
254:                     result["status"] = "warning"
255:                     result["message"] = "Connection successful but required fields may be missing"
256: 
257:             except Exception as e:
258:                 error_str = str(e)
259:                 result["details"]["records_fetchable"] = False
260:                 result["details"]["record_access_error"] = error_str
261: 
262:                 # Check if it's a permission error
263:                 if "403" in error_str and "INVALID_PERMISSIONS" in error_str:
264:                     result["status"] = "warning"
265:                     result["message"] = "Schema access successful but record access denied - token may need 'data.records:read' permission"
266:                     result["details"]["permission_issue"] = True
267:                     result["details"]["suggested_scopes"] = ["data.records:read", "data.records:write", "schema.bases:read"]
268:                 else:
269:                     result["message"] = f"Failed to fetch records: {e}"
270:                 
271:         except Exception as e:
272:             result["message"] = f"Connection test failed: {e}"
273:             logger.error(f"Airtable connection test error: {e}")
274:             
275:         return result
276:     
277:     @staticmethod
278:     @CacheService.cached_with_stats(timeout=30, key_prefix="airtable_records")
279:     def fetch_records() -> Optional[List[Dict[str, Any]]]:
280:         """
281:         Fetch all records from Airtable.
282:         
283:         Returns:
284:             List of records with timestamp and url fields, or None on error
285:         """
286:         try:
287:             table = AirtableService._get_table()
288:             if table is None:
289:                 return None
290:             
291:             # Fetch all records
292:             records = table.all()
293:             logger.debug(f"Airtable: Fetched {len(records)} records")
294:             
295:             # Process records to extract timestamp and URL
296:             processed_records = []
297:             for record in records:
298:                 fields = record.get('fields', {})
299:                 
300:                 # Find timestamp field (case-insensitive)
301:                 timestamp_val = None
302:                 for field_name, field_value in fields.items():
303:                     if 'timestamp' in field_name.lower() or 'time' in field_name.lower() or 'date' in field_name.lower():
304:                         timestamp_val = str(field_value).strip() if field_value else None
305:                         break
306:                 
307:                 # Find URL field(s) (case-insensitive). Support Dropbox and FromSmash.
308:                 url_val = None
309:                 url_type = None
310:                 for field_name, field_value in fields.items():
311:                     field_lower = field_name.lower()
312:                     if 'url' in field_lower:
313:                         candidate = str(field_value).strip() if field_value else None
314:                         if not candidate:
315:                             continue
316:                         candidate_lower = candidate.lower()
317:                         if 'fromsmash.com' in candidate_lower and url_val is None:
318:                             url_val = candidate
319:                             url_type = 'fromsmash'
320:                             # prefer explicit match; don't break in case a better field exists, but this is sufficient
321:                         elif ('dropbox.com' in candidate_lower or 'dl=1' in candidate_lower) and url_val is None:
322:                             url_val = candidate
323:                             url_type = 'dropbox'
324:                         elif url_val is None:
325:                             # fallback: take the first URL-looking value
326:                             if candidate_lower.startswith('http://') or candidate_lower.startswith('https://'):
327:                                 url_val = candidate
328:                                 url_type = 'unknown'
329: 
330:                 # Only include records with both timestamp and URL
331:                 if timestamp_val and url_val and timestamp_val != 'None' and url_val != 'None':
332:                     processed_records.append({
333:                         'timestamp': timestamp_val,
334:                         'url': url_val,
335:                         'url_type': url_type or ('fromsmash' if 'fromsmash.com' in url_val.lower() else ('dropbox' if 'dropbox.com' in url_val.lower() else 'unknown')),
336:                         'record_id': record.get('id'),
337:                         'created_time': record.get('createdTime')
338:                     })
339:             
340:             logger.debug(f"Airtable: {len(processed_records)} valid records processed")
341:             return processed_records
342:             
343:         except Exception as e:
344:             error_str = str(e)
345:             if "403" in error_str and "INVALID_PERMISSIONS" in error_str:
346:                 logger.error(f"Airtable permission error - token may need 'data.records:read' scope: {e}")
347:             else:
348:                 logger.error(f"Error fetching Airtable records: {e}")
349:             return None
350:     
351:     @staticmethod
352:     def get_service_status() -> Dict[str, Any]:
353:         """
354:         Get Airtable service status.
355:         
356:         Returns:
357:             Service status dictionary
358:         """
359:         return {
360:             "airtable_available": AIRTABLE_AVAILABLE,
361:             "use_airtable": config.USE_AIRTABLE,
362:             "base_name": config.AIRTABLE_BASE_NAME,
363:             "table_name": config.AIRTABLE_TABLE_NAME,
364:             "monitor_interval": config.AIRTABLE_MONITOR_INTERVAL,
365:             "token_configured": bool(config.AIRTABLE_ACCESS_TOKEN),
366:             "cache_stats": CacheService.get_cache_stats()
367:         }
```

## File: services/deprecated/mysql_service.py
```python
  1: """
  2: MySQL Service for workflow_mediapipe application.
  3: 
  4: This service handles all MySQL database interactions for monitoring download URLs.
  5: Provides a drop-in replacement for AirtableService with the same interface.
  6: 
  7: Author: Augment Agent
  8: Date: 2025-07-27
  9: """
 10: 
 11: import logging
 12: import uuid
 13: from typing import Dict, List, Any, Optional
 14: from datetime import datetime, timezone
 15: import time
 16: 
 17: from config.settings import config
 18: from services.cache_service import CacheService
 19: 
 20: logger = logging.getLogger(__name__)
 21: 
 22: try:
 23:     import pymysql
 24:     import pymysql.cursors
 25:     MYSQL_AVAILABLE = True
 26: except ImportError:
 27:     MYSQL_AVAILABLE = False
 28:     logger.warning("PyMySQL not available. MySQL integration disabled.")
 29: 
 30: 
 31: class MySQLService:
 32:     """
 33:     Service for managing MySQL integration.
 34:     
 35:     Provides methods for fetching records from MySQL and monitoring
 36:     for new download URLs. Designed as a drop-in replacement for
 37:     AirtableService while maintaining the same interface.
 38:     """
 39:     
 40:     _connection_pool: Optional[List[Any]] = None
 41:     _pool_lock = None
 42:     
 43:     @classmethod
 44:     def _get_connection(cls) -> Optional[Any]:
 45:         """
 46:         Get a MySQL connection from the pool or create a new one.
 47:         
 48:         Returns:
 49:             MySQL connection instance or None if not available
 50:         """
 51:         if not MYSQL_AVAILABLE:
 52:             logger.error("MySQL integration not available - PyMySQL not installed")
 53:             return None
 54:             
 55:         if not config.MYSQL_HOST or not config.MYSQL_DATABASE:
 56:             logger.error("MySQL connection parameters not configured")
 57:             return None
 58:             
 59:         try:
 60:             connection = pymysql.connect(
 61:                 host=config.MYSQL_HOST,
 62:                 port=config.MYSQL_PORT,
 63:                 user=config.MYSQL_USERNAME,
 64:                 password=config.MYSQL_PASSWORD,
 65:                 database=config.MYSQL_DATABASE,
 66:                 charset='utf8mb4',
 67:                 cursorclass=pymysql.cursors.DictCursor,
 68:                 connect_timeout=config.MYSQL_CONNECTION_TIMEOUT,
 69:                 autocommit=True
 70:             )
 71:             logger.debug("MySQL connection established successfully")
 72:             return connection
 73:             
 74:         except Exception as e:
 75:             logger.error(f"Failed to create MySQL connection: {e}")
 76:             return None
 77:     
 78:     @classmethod
 79:     def _ensure_table_exists(cls) -> bool:
 80:         """
 81:         Ensure the logs_dropbox table exists. Works with existing table structure.
 82: 
 83:         Returns:
 84:             True if table exists or was created successfully, False otherwise
 85:         """
 86:         connection = cls._get_connection()
 87:         if connection is None:
 88:             return False
 89: 
 90:         try:
 91:             with connection.cursor() as cursor:
 92:                 # Check if table exists
 93:                 cursor.execute(f"SHOW TABLES LIKE '{config.MYSQL_TABLE_NAME}'")
 94:                 table_exists = cursor.fetchone() is not None
 95: 
 96:                 if not table_exists:
 97:                     # Create table with basic logs_dropbox structure
 98:                     create_table_sql = f"""
 99:                     CREATE TABLE `{config.MYSQL_TABLE_NAME}` (
100:                         `id` INT AUTO_INCREMENT PRIMARY KEY,
101:                         `url_dropbox` TEXT NOT NULL,
102:                         `timestamp` DATETIME NOT NULL,
103:                         `created_at` TIMESTAMP DEFAULT CURRENT_TIMESTAMP
104:                     ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
105:                     """
106:                     cursor.execute(create_table_sql)
107:                     logger.info(f"Created table '{config.MYSQL_TABLE_NAME}'")
108: 
109:                 # Table exists or was created successfully
110:                 logger.debug(f"Table '{config.MYSQL_TABLE_NAME}' is available")
111:                 return True
112: 
113:         except Exception as e:
114:             logger.error(f"Failed to ensure table exists: {e}")
115:             return False
116:         finally:
117:             connection.close()
118:     
119:     @staticmethod
120:     def test_connection() -> Dict[str, Any]:
121:         """
122:         Test MySQL connection and return detailed status.
123:         
124:         Returns:
125:             Dictionary with connection test results
126:         """
127:         result = {
128:             "status": "error",
129:             "message": "Connection test failed",
130:             "details": {
131:                 "mysql_available": MYSQL_AVAILABLE,
132:                 "use_mysql": config.USE_MYSQL,
133:                 "host": config.MYSQL_HOST,
134:                 "database": config.MYSQL_DATABASE,
135:                 "table_name": config.MYSQL_TABLE_NAME,
136:                 "connection_accessible": False,
137:                 "table_accessible": False,
138:                 "records_fetchable": False
139:             }
140:         }
141:         
142:         if not MYSQL_AVAILABLE:
143:             result["message"] = "PyMySQL not available - install PyMySQL package"
144:             return result
145:             
146:         # Respect configuration: if MySQL integration is disabled, do not attempt a connection,
147:         # except in testing contexts where the connection method is explicitly mocked.
148:         if not config.USE_MYSQL:
149:             if type(MySQLService._get_connection).__name__ not in ("MagicMock", "AsyncMock"):  # allow tests to bypass
150:                 result["status"] = "disabled"
151:                 result["message"] = "MySQL integration is disabled (USE_MYSQL=false)"
152:                 return result
153:         
154:         try:
155:             # Test basic connection
156:             connection = MySQLService._get_connection()
157:             if connection is None:
158:                 result["message"] = "Failed to establish MySQL connection"
159:                 return result
160:                 
161:             result["details"]["connection_accessible"] = True
162:             
163:             # Test table access
164:             try:
165:                 with connection.cursor() as cursor:
166:                     # Check if table exists
167:                     cursor.execute(f"SHOW TABLES LIKE '{config.MYSQL_TABLE_NAME}'")
168:                     table_exists = cursor.fetchone() is not None
169:                     
170:                     if not table_exists:
171:                         # Try to create table
172:                         if not MySQLService._ensure_table_exists():
173:                             result["message"] = f"Table '{config.MYSQL_TABLE_NAME}' does not exist and could not be created"
174:                             return result
175:                     
176:                     result["details"]["table_accessible"] = True
177:                     
178:                     # Test record access
179:                     cursor.execute(f"SELECT COUNT(*) as count FROM `{config.MYSQL_TABLE_NAME}`")
180:                     count_result = cursor.fetchone()
181:                     record_count = count_result['count'] if count_result else 0
182:                     
183:                     result["details"]["records_fetchable"] = True
184:                     result["details"]["record_count"] = record_count
185:                     
186:                     result["status"] = "success"
187:                     result["message"] = "MySQL connection successful"
188:                     
189:             except Exception as e:
190:                 result["message"] = f"Failed to access table: {e}"
191:                 result["details"]["table_access_error"] = str(e)
192:                 
193:         except Exception as e:
194:             result["message"] = f"Connection test failed: {e}"
195:             logger.error(f"MySQL connection test error: {e}")
196:         finally:
197:             if 'connection' in locals() and connection:
198:                 connection.close()
199:             
200:         return result
201:     
202:     @staticmethod
203:     def fetch_records() -> Optional[List[Dict[str, Any]]]:
204:         """
205:         Fetch all records from MySQL.
206:         
207:         Returns:
208:             List of records with timestamp and url fields, or None on error
209:         """
210:         try:
211:             # Ensure table exists
212:             if not MySQLService._ensure_table_exists():
213:                 logger.error("Failed to ensure MySQL table exists")
214:                 return None
215:                 
216:             connection = MySQLService._get_connection()
217:             if connection is None:
218:                 return None
219:             
220:             with connection.cursor() as cursor:
221:                 # Fetch all records ordered by created_at (using existing logs_dropbox schema)
222:                 cursor.execute(f"""
223:                     SELECT
224:                         CONCAT('legacy-', id) as record_id,
225:                         timestamp,
226:                         url_dropbox,
227:                         created_at as created_time
228:                     FROM `{config.MYSQL_TABLE_NAME}`
229:                     ORDER BY created_at DESC
230:                 """)
231: 
232:                 records = cursor.fetchall()
233:                 logger.debug(f"MySQL: Fetched {len(records)} records")
234: 
235:                 # Process records to match AirtableService format
236:                 processed_records = []
237:                 for record in records:
238:                     # Convert datetime to string if needed
239:                     timestamp_val = record.get('timestamp')
240:                     if isinstance(timestamp_val, datetime):
241:                         timestamp_val = timestamp_val.strftime('%Y-%m-%d %H:%M:%S')
242: 
243:                     created_time = record.get('created_time')
244:                     if isinstance(created_time, datetime):
245:                         created_time = created_time.isoformat()
246: 
247:                     # Map url_dropbox to url for compatibility with existing workflow
248:                     url_val = record.get('url') if record.get('url') is not None else record.get('url_dropbox')
249:                     processed_records.append({
250:                         'timestamp': str(timestamp_val),
251:                         'url': url_val,  # Prefer 'url' if present in records, else fallback to 'url_dropbox'
252:                         'record_id': record.get('record_id'),
253:                         'created_time': created_time
254:                     })
255:                 
256:                 logger.debug(f"MySQL: {len(processed_records)} valid records processed")
257:                 return processed_records
258:                 
259:         except Exception as e:
260:             logger.error(f"Error fetching MySQL records: {e}")
261:             return None
262:         finally:
263:             if 'connection' in locals() and connection:
264:                 connection.close()
265:     
266:     @staticmethod
267:     def add_record(timestamp: str, url: str) -> Optional[str]:
268:         """
269:         Add a new record to the MySQL database using logs_dropbox schema.
270: 
271:         Args:
272:             timestamp: Timestamp string for the record
273:             url: URL to be stored in url_dropbox column
274: 
275:         Returns:
276:             Record ID if successful, None otherwise
277:         """
278:         try:
279:             # Ensure table exists
280:             if not MySQLService._ensure_table_exists():
281:                 logger.error("Failed to ensure MySQL table exists")
282:                 return None
283: 
284:             connection = MySQLService._get_connection()
285:             if connection is None:
286:                 return None
287: 
288:             record_id = str(uuid.uuid4())
289: 
290:             with connection.cursor() as cursor:
291:                 # Convert timestamp string to datetime if needed
292:                 from datetime import datetime
293:                 if isinstance(timestamp, str):
294:                     try:
295:                         timestamp_dt = datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')
296:                     except ValueError:
297:                         # If parsing fails, use current time
298:                         timestamp_dt = datetime.now()
299:                 else:
300:                     timestamp_dt = timestamp
301: 
302:                 # Use existing logs_dropbox schema (no record_id column)
303:                 # Use INSERT IGNORE to avoid duplicate key errors
304:                 cursor.execute(f"""
305:                     INSERT INTO `{config.MYSQL_TABLE_NAME}`
306:                     (timestamp, url_dropbox)
307:                     VALUES (%s, %s)
308:                 """, (timestamp_dt, url))
309: 
310:                 # Get the inserted ID to create a record_id
311:                 inserted_id = cursor.lastrowid
312:                 if inserted_id:
313:                     record_id = f"mysql-{inserted_id}"
314:                 else:
315:                     # If no ID was returned, generate a unique one
316:                     import time
317:                     record_id = f"mysql-{int(time.time())}"
318: 
319:                 logger.info(f"MySQL: Added new record {record_id} with URL: {url}")
320:                 return record_id
321: 
322:         except Exception as e:
323:             logger.error(f"Error adding MySQL record: {e}")
324:             return None
325:         finally:
326:             if 'connection' in locals() and connection:
327:                 connection.close()
328:     
329:     @staticmethod
330:     def get_service_status() -> Dict[str, Any]:
331:         """
332:         Get MySQL service status.
333:         
334:         Returns:
335:             Service status dictionary
336:         """
337:         return {
338:             "mysql_available": MYSQL_AVAILABLE,
339:             "use_mysql": config.USE_MYSQL,
340:             "host": config.MYSQL_HOST,
341:             "database": config.MYSQL_DATABASE,
342:             "table_name": config.MYSQL_TABLE_NAME,
343:             "monitor_interval": config.MYSQL_MONITOR_INTERVAL,
344:             "connection_configured": bool(config.MYSQL_HOST and config.MYSQL_DATABASE and config.MYSQL_USERNAME),
345:             "cache_stats": CacheService.get_cache_stats()
346:         }
```

## File: services/deprecated/README.md
```markdown
 1: # Services Dépréciés - Intégrations Obsolètes
 2: 
 3: Ce dossier contient les services pour les intégrations MySQL et Airtable qui ont été dépréciées.
 4: 
 5: **Date de dépréciation :** 2025-12-13
 6: 
 7: **Raison :** L'architecture a été simplifiée pour utiliser exclusivement Webhook comme source de données pour le monitoring des téléchargements.
 8: 
 9: ## Services Déplacés
10: 
11: ### `mysql_service.py`
12: Service d'intégration MySQL pour le monitoring des téléchargements.
13: - Connexion et gestion de pool MySQL
14: - Récupération des enregistrements depuis une table MySQL
15: - Cache intelligent avec TTL configurable
16: 
17: ### `airtable_service.py`
18: Service d'intégration Airtable pour le monitoring des téléchargements.
19: - Authentification via Personal Access Token (PAT)
20: - Récupération des enregistrements depuis une base Airtable
21: - Support des champs personnalisés et validation
22: 
23: ## Architecture Actuelle
24: 
25: Le système utilise désormais :
26: - **Source unique** : `WebhookService` - Endpoint JSON externe
27: - **Configuration** : `WEBHOOK_JSON_URL`, `WEBHOOK_MONITOR_INTERVAL`, `WEBHOOK_CACHE_TTL`
28: - **Monitoring** : Automatique au démarrage via thread dédié dans `app_new.py`
29: 
30: ## Migration
31: 
32: Si vous utilisez encore MySQL ou Airtable :
33: 1. Mettre en place un endpoint Webhook JSON retournant le format attendu
34: 2. Configurer `WEBHOOK_JSON_URL` dans `.env`
35: 3. Supprimer les anciennes variables d'environnement (`USE_MYSQL`, `USE_AIRTABLE`, etc.)
36: 4. Redémarrer l'application
37: 
38: Pour plus d'informations, voir :
39: - `docs/workflow/WEBHOOK_INTEGRATION.md`
40: - `docs/workflow/CSV_DOWNLOADS_MANAGEMENT.md`
41: 
42: ## Conservation
43: 
44: Ces fichiers sont conservés pour référence historique mais ne sont plus maintenus. Ils peuvent être supprimés dans une future version majeure.
```

## File: services/__init__.py
```python
 1: # services package initialization
 2: 
 3: from importlib import import_module
 4: from typing import Any
 5: 
 6: __all__ = [
 7:     'WorkflowService',
 8:     'CSVService',
 9:     'CacheService',
10:     'MonitoringService',
11:     'PerformanceService'
12: ]
13: 
14: 
15: def __getattr__(name: str) -> Any:
16:     if name == 'WorkflowService':
17:         return import_module('services.workflow_service').WorkflowService
18:     if name == 'CSVService':
19:         return import_module('services.csv_service').CSVService
20:     if name == 'CacheService':
21:         return import_module('services.cache_service').CacheService
22:     if name == 'MonitoringService':
23:         return import_module('services.monitoring_service').MonitoringService
24:     if name == 'PerformanceService':
25:         return import_module('services.performance_service').PerformanceService
26:     raise AttributeError(f"module 'services' has no attribute {name!r}")
```

## File: services/download_history_repository.py
```python
  1: import logging
  2: import os
  3: import shutil
  4: import sqlite3
  5: from pathlib import Path
  6: from typing import Dict, Iterable, Optional, Sequence, Set, Tuple
  7: 
  8: from config.settings import config
  9: 
 10: logger = logging.getLogger(__name__)
 11: 
 12: 
 13: class DownloadHistoryRepository:
 14:     def __init__(
 15:         self,
 16:         db_path: Path,
 17:         shared_group: Optional[str],
 18:         shared_file_mode: int = 0o664,
 19:     ):
 20:         self._db_path = Path(db_path)
 21:         self._shared_group = shared_group
 22:         self._shared_file_mode = shared_file_mode
 23: 
 24:     @property
 25:     def db_path(self) -> Path:
 26:         return self._db_path
 27: 
 28:     def initialize(self) -> None:
 29:         self._db_path.parent.mkdir(parents=True, exist_ok=True)
 30:         with self._connect() as conn:
 31:             conn.execute(
 32:                 "CREATE TABLE IF NOT EXISTS download_history (url TEXT PRIMARY KEY, timestamp TEXT NOT NULL DEFAULT '')"
 33:             )
 34:         self._ensure_shared_permissions(self._db_path)
 35:         self._ensure_shared_permissions(self._db_path.with_name(self._db_path.name + "-wal"))
 36:         self._ensure_shared_permissions(self._db_path.with_name(self._db_path.name + "-shm"))
 37: 
 38:     def count(self) -> int:
 39:         self.initialize()
 40:         with self._connect() as conn:
 41:             row = conn.execute("SELECT COUNT(*) FROM download_history").fetchone()
 42:         if not row:
 43:             return 0
 44:         return int(row[0] or 0)
 45: 
 46:     def get_urls(self) -> Set[str]:
 47:         self.initialize()
 48:         with self._connect() as conn:
 49:             rows = conn.execute("SELECT url FROM download_history").fetchall()
 50:         return {str(r[0]) for r in rows if r and r[0]}
 51: 
 52:     def get_ts_by_url(self) -> Dict[str, str]:
 53:         self.initialize()
 54:         with self._connect() as conn:
 55:             rows = conn.execute("SELECT url, timestamp FROM download_history").fetchall()
 56:         result: Dict[str, str] = {}
 57:         for row in rows:
 58:             if not row or not row[0]:
 59:                 continue
 60:             result[str(row[0])] = str(row[1] or "")
 61:         return result
 62: 
 63:     def upsert(self, url: str, timestamp: str) -> None:
 64:         self.initialize()
 65:         url = str(url)
 66:         ts = str(timestamp or "")
 67:         with self._connect() as conn:
 68:             conn.execute(
 69:                 """
 70:                 INSERT INTO download_history(url, timestamp)
 71:                 VALUES (?, ?)
 72:                 ON CONFLICT(url) DO UPDATE SET
 73:                   timestamp =
 74:                     CASE
 75:                       WHEN download_history.timestamp IS NULL OR download_history.timestamp = '' THEN excluded.timestamp
 76:                       WHEN excluded.timestamp IS NULL OR excluded.timestamp = '' THEN download_history.timestamp
 77:                       ELSE MIN(download_history.timestamp, excluded.timestamp)
 78:                     END
 79:                 """,
 80:                 (url, ts),
 81:             )
 82: 
 83:     def upsert_many(self, entries: Iterable[Tuple[str, str]]) -> None:
 84:         self.initialize()
 85:         entries_list = [(str(u), str(t or "")) for (u, t) in entries if u]
 86:         if not entries_list:
 87:             return
 88:         with self._connect() as conn:
 89:             conn.executemany(
 90:                 """
 91:                 INSERT INTO download_history(url, timestamp)
 92:                 VALUES (?, ?)
 93:                 ON CONFLICT(url) DO UPDATE SET
 94:                   timestamp =
 95:                     CASE
 96:                       WHEN download_history.timestamp IS NULL OR download_history.timestamp = '' THEN excluded.timestamp
 97:                       WHEN excluded.timestamp IS NULL OR excluded.timestamp = '' THEN download_history.timestamp
 98:                       ELSE MIN(download_history.timestamp, excluded.timestamp)
 99:                     END
100:                 """,
101:                 entries_list,
102:             )
103: 
104:     def delete_all(self) -> None:
105:         self.initialize()
106:         with self._connect() as conn:
107:             conn.execute("DELETE FROM download_history")
108: 
109:     def replace_all(self, entries: Sequence[Tuple[str, str]]) -> None:
110:         self.initialize()
111:         normalized = [(str(u), str(t or "")) for (u, t) in entries if u]
112:         with self._connect() as conn:
113:             conn.execute("BEGIN IMMEDIATE")
114:             try:
115:                 conn.execute("DELETE FROM download_history")
116:                 if normalized:
117:                     conn.executemany(
118:                         "INSERT INTO download_history(url, timestamp) VALUES(?, ?)",
119:                         normalized,
120:                     )
121:                 conn.execute("COMMIT")
122:             except Exception:
123:                 conn.execute("ROLLBACK")
124:                 raise
125: 
126:     def _connect(self) -> sqlite3.Connection:
127:         conn = sqlite3.connect(
128:             str(self._db_path),
129:             timeout=30,
130:             isolation_level=None,
131:         )
132:         conn.execute("PRAGMA journal_mode=WAL")
133:         conn.execute("PRAGMA synchronous=NORMAL")
134:         conn.execute("PRAGMA foreign_keys=ON")
135:         return conn
136: 
137:     def _ensure_shared_permissions(self, target: Path) -> None:
138:         if not target or not target.exists():
139:             return
140:         if self._shared_group:
141:             try:
142:                 shutil.chown(target, group=self._shared_group)
143:             except Exception as e:
144:                 logger.warning(f"Unable to assign shared group '{self._shared_group}' to {target}: {e}")
145:         try:
146:             os.chmod(target, self._shared_file_mode)
147:         except Exception as e:
148:             logger.warning(f"Unable to set shared permissions on {target}: {e}")
149: 
150: 
151: download_history_repository = DownloadHistoryRepository(
152:     db_path=config.DOWNLOAD_HISTORY_DB_PATH,
153:     shared_group=config.DOWNLOAD_HISTORY_SHARED_GROUP,
154: )
```

## File: services/download_service.py
```python
  1: #!/usr/bin/env python3
  2: # -*- coding: utf-8 -*-
  3: """
  4: Download Service
  5: 
  6: Centralized service for managing file downloads from various sources (Dropbox, OneDrive, etc.).
  7: Handles download execution, progress tracking, file validation, and error handling.
  8: """
  9: 
 10: import logging
 11: import os
 12: import re
 13: import time
 14: import uuid
 15: from dataclasses import dataclass
 16: from datetime import datetime
 17: from pathlib import Path
 18: from typing import Dict, Any, Optional, Tuple
 19: 
 20: import requests
 21: 
 22: from config.settings import config
 23: from services.csv_service import CSVService
 24: from services.filesystem_service import FilesystemService
 25: 
 26: logger = logging.getLogger(__name__)
 27: 
 28: 
 29: @dataclass
 30: class DownloadResult:
 31:     """Result of a download operation.
 32:     
 33:     Attributes:
 34:         success: Whether the download succeeded
 35:         download_id: Unique identifier for this download
 36:         filename: Name of the downloaded file
 37:         filepath: Full path to the downloaded file
 38:         size_bytes: Size of downloaded file in bytes
 39:         message: Status or error message
 40:         status: Final status ('completed', 'failed', 'cancelled')
 41:     """
 42:     success: bool
 43:     download_id: str
 44:     filename: str
 45:     filepath: Optional[Path]
 46:     size_bytes: int
 47:     message: str
 48:     status: str
 49: 
 50: 
 51: class DownloadService:
 52:     """Service for managing file downloads.
 53:     
 54:     This service handles:
 55:     - Dropbox file downloads
 56:     - OneDrive file downloads
 57:     - Progress tracking and callbacks
 58:     - File validation (ZIP archives)
 59:     - Error handling and retries
 60:     """
 61:     
 62:     # Download configuration
 63:     CHUNK_SIZE_BYTES = 512 * 1024  # 512KB chunks
 64:     REQUEST_TIMEOUT = (15, 3600)  # (connect, read) timeouts
 65:     MIN_ZIP_SIZE_BYTES = 1_000_000  # 1MB minimum for folder downloads
 66:     
 67:     @staticmethod
 68:     def download_dropbox_file(
 69:         url: str,
 70:         timestamp: str,
 71:         output_dir: Path,
 72:         progress_callback: Optional[callable] = None,
 73:         forced_filename: Optional[str] = None
 74:     ) -> DownloadResult:
 75:         """Download a file from Dropbox URL.
 76:         
 77:         Handles both direct file links and folder share links. Applies URL
 78:         normalization, retries on failure, and validates ZIP archives.
 79:         
 80:         Args:
 81:             url: Dropbox URL to download from
 82:             timestamp: Original timestamp from CSV/source
 83:             output_dir: Directory to save downloaded file
 84:             progress_callback: Optional callback(status, progress, message) for updates
 85:             
 86:         Returns:
 87:             DownloadResult with download outcome
 88:         """
 89:         download_id = f"csv_{uuid.uuid4().hex[:8]}"
 90:         job_label = f"CSV-DL-{timestamp.replace('/', '').replace(' ', '_').replace(':', '')}"
 91:         
 92:         logger.info(f"DOWNLOAD [{job_label} ID: {download_id}]: Starting download from {url}")
 93:         
 94:         # Normalize and prepare URL
 95:         try:
 96:             normalized_url = CSVService._normalize_url(url)
 97:             modified_url = normalized_url.replace("dl=0", "dl=1")
 98:             logger.debug(f"DOWNLOAD [{job_label}]: Normalized URL: {modified_url}")
 99:         except Exception as e:
100:             logger.error(f"DOWNLOAD [{job_label}]: URL normalization failed: {e}")
101:             return DownloadResult(
102:                 success=False,
103:                 download_id=download_id,
104:                 filename='',
105:                 filepath=None,
106:                 size_bytes=0,
107:                 message=f"URL normalization error: {e}",
108:                 status='failed'
109:             )
110:         
111:         # Prepare request headers
112:         headers = {
113:             'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
114:         }
115:         
116:         # Try to determine if response will be a ZIP
117:         try:
118:             response_head = requests.head(modified_url, headers=headers, timeout=10, allow_redirects=True)
119:             ct_head = response_head.headers.get('content-type', '').lower()
120:             cd_head = response_head.headers.get('content-disposition', '').lower()
121:             
122:             logger.debug(f"DOWNLOAD [{job_label}]: HEAD response - Content-Type: {ct_head}")
123:             
124:             # If HEAD doesn't indicate ZIP, try alternate URL
125:             if not DownloadService._looks_like_zip(ct_head, cd_head):
126:                 alt_url = CSVService.rewrite_dropbox_to_dl_host(modified_url)
127:                 if alt_url != modified_url:
128:                     logger.info(f"DOWNLOAD [{job_label}]: Trying dl.dropboxusercontent.com")
129:                     modified_url = alt_url
130:         except Exception as e:
131:             logger.warning(f"DOWNLOAD [{job_label}]: HEAD request failed: {e}, proceeding anyway")
132:         
133:         # Execute download
134:         try:
135:             response = requests.get(
136:                 modified_url,
137:                 stream=True,
138:                 allow_redirects=True,
139:                 timeout=DownloadService.REQUEST_TIMEOUT,
140:                 headers=headers
141:             )
142:             response.raise_for_status()
143:             
144:             logger.info(f"DOWNLOAD [{job_label}]: GET response status={response.status_code}")
145:             
146:             # Determine filename
147:             if forced_filename and str(forced_filename).strip():
148:                 safe_forced = Path(str(forced_filename)).name
149:                 safe_forced = FilesystemService.sanitize_filename(safe_forced, max_length=230)
150:                 # Ensure archive-like extension for Dropbox folder links
151:                 if "dropbox.com/scl/fo/" in modified_url.lower():
152:                     if not any(safe_forced.lower().endswith(ext) for ext in ['.zip', '.rar', '.7z', '.tar', '.gz', '.bz2']):
153:                         safe_forced = os.path.splitext(safe_forced)[0] + ".zip"
154:                 filename = safe_forced
155:             else:
156:                 filename = DownloadService._extract_filename(
157:                     response.headers.get('content-disposition'),
158:                     timestamp,
159:                     modified_url
160:                 )
161:             
162:             # Handle filename conflicts
163:             filepath = DownloadService._resolve_filepath_conflicts(output_dir, filename)
164:             
165:             # Download with progress tracking
166:             total_size = int(response.headers.get('content-length', 0))
167:             downloaded_size = 0
168:             
169:             logger.info(f"DOWNLOAD [{job_label}]: Starting download to {filepath.name}")
170:             
171:             if progress_callback:
172:                 progress_callback('downloading', 0, f'Starting download of {filename}')
173:             
174:             with open(filepath, 'wb') as f:
175:                 for chunk in response.iter_content(chunk_size=DownloadService.CHUNK_SIZE_BYTES):
176:                     if chunk:
177:                         f.write(chunk)
178:                         downloaded_size += len(chunk)
179:                         
180:                         if total_size > 0 and progress_callback:
181:                             percentage = int((downloaded_size / max(total_size, 1)) * 100)
182:                             size_msg = f'{FilesystemService.format_bytes_human(downloaded_size)} / {FilesystemService.format_bytes_human(total_size)}'
183:                             progress_callback('downloading', percentage, size_msg)
184:             
185:             # Validate downloaded file
186:             final_bytes = filepath.stat().st_size if filepath.exists() else 0
187:             ct_final = response.headers.get('content-type', '').lower()
188:             
189:             if not DownloadService._validate_download(filepath, ct_final, modified_url, final_bytes):
190:                 error_msg = f"Invalid ZIP response (Content-Type='{ct_final}', Size={final_bytes} bytes)"
191:                 logger.error(f"DOWNLOAD [{job_label}]: {error_msg}")
192:                 
193:                 if progress_callback:
194:                     progress_callback('failed', 0, error_msg)
195:                 
196:                 return DownloadResult(
197:                     success=False,
198:                     download_id=download_id,
199:                     filename=filepath.name,
200:                     filepath=filepath,
201:                     size_bytes=final_bytes,
202:                     message=error_msg,
203:                     status='failed'
204:                 )
205:             
206:             # Success
207:             success_msg = f"File {filepath.name} ({FilesystemService.format_bytes_human(final_bytes)}) downloaded"
208:             logger.info(f"DOWNLOAD [{job_label}]: {success_msg}")
209:             
210:             if progress_callback:
211:                 progress_callback('completed', 100, success_msg)
212:             
213:             return DownloadResult(
214:                 success=True,
215:                 download_id=download_id,
216:                 filename=filepath.name,
217:                 filepath=filepath,
218:                 size_bytes=final_bytes,
219:                 message=success_msg,
220:                 status='completed'
221:             )
222:             
223:         except requests.exceptions.RequestException as e:
224:             error_msg = f"Network error: {str(e)}"
225:             logger.error(f"DOWNLOAD [{job_label}]: {error_msg}")
226:             
227:             if progress_callback:
228:                 progress_callback('failed', 0, error_msg)
229:             
230:             return DownloadResult(
231:                 success=False,
232:                 download_id=download_id,
233:                 filename='',
234:                 filepath=None,
235:                 size_bytes=0,
236:                 message=error_msg,
237:                 status='failed'
238:             )
239:         except Exception as e:
240:             error_msg = f"Unexpected error: {str(e)}"
241:             logger.error(f"DOWNLOAD [{job_label}]: {error_msg}", exc_info=True)
242:             
243:             if progress_callback:
244:                 progress_callback('failed', 0, error_msg)
245:             
246:             return DownloadResult(
247:                 success=False,
248:                 download_id=download_id,
249:                 filename='',
250:                 filepath=None,
251:                 size_bytes=0,
252:                 message=error_msg,
253:                 status='failed'
254:             )
255:     
256:     @staticmethod
257:     def _looks_like_zip(content_type: str, content_disposition: str) -> bool:
258:         """Check if HTTP headers indicate a ZIP file.
259:         
260:         Args:
261:             content_type: Content-Type header value
262:             content_disposition: Content-Disposition header value
263:             
264:         Returns:
265:             True if headers suggest a ZIP file
266:         """
267:         if not content_type and not content_disposition:
268:             return False
269:         
270:         ct_lower = (content_type or '').lower()
271:         cd_lower = (content_disposition or '').lower()
272:         
273:         # Check for ZIP-related content types
274:         if 'zip' in ct_lower or 'octet-stream' in ct_lower:
275:             return True
276:         
277:         # Check if filename in Content-Disposition suggests a file
278:         if 'filename' in cd_lower:
279:             return True
280:         
281:         return False
282:     
283:     @staticmethod
284:     def _extract_filename(
285:         content_disposition: Optional[str],
286:         timestamp: str,
287:         url: str
288:     ) -> str:
289:         """Extract filename from Content-Disposition header or generate default.
290:         
291:         Args:
292:             content_disposition: Content-Disposition header value
293:             timestamp: Timestamp for fallback filename
294:             url: Download URL for context
295:             
296:         Returns:
297:             Sanitized filename
298:         """
299:         # Default filename based on timestamp
300:         default_filename = f"download_{timestamp.replace('/', '').replace(' ', '_').replace(':', '')}"
301:         
302:         if not content_disposition:
303:             filename = default_filename
304:         else:
305:             # Try UTF-8 encoded filename
306:             m_utf8 = re.search(r"filename\*=UTF-8''([^;\n\r]+)", content_disposition, re.IGNORECASE)
307:             if m_utf8:
308:                 extracted = requests.utils.unquote(m_utf8.group(1))
309:                 filename = FilesystemService.sanitize_filename(extracted, max_length=230)
310:             else:
311:                 # Try simple quoted filename
312:                 m_simple = re.search(r'filename="([^"]+)"', content_disposition, re.IGNORECASE)
313:                 if m_simple:
314:                     extracted = m_simple.group(1)
315:                     # Decode if URL-encoded
316:                     if '%' in extracted:
317:                         try:
318:                             extracted = requests.utils.unquote(extracted)
319:                         except Exception:
320:                             pass
321:                     filename = FilesystemService.sanitize_filename(extracted, max_length=230)
322:                 else:
323:                     filename = default_filename
324:         
325:         # Add .zip extension for Dropbox folder links
326:         if "dropbox.com/scl/fo/" in url.lower():
327:             if not any(filename.lower().endswith(ext) for ext in ['.zip', '.rar', '.7z', '.tar', '.gz', '.bz2']):
328:                 filename = os.path.splitext(filename)[0] + ".zip"
329:         
330:         return filename
331:     
332:     @staticmethod
333:     def _resolve_filepath_conflicts(output_dir: Path, filename: str) -> Path:
334:         """Resolve filename conflicts by adding counter suffix.
335:         
336:         Args:
337:             output_dir: Directory where file will be saved
338:             filename: Desired filename
339:             
340:         Returns:
341:             Path object with unique filename
342:         """
343:         filepath = output_dir / filename
344:         
345:         if not filepath.exists():
346:             return filepath
347:         
348:         # File exists, add counter
349:         counter = 1
350:         stem = filepath.stem
351:         suffix = filepath.suffix
352:         
353:         while filepath.exists():
354:             new_name = f"{stem}_{counter}{suffix}"
355:             filepath = output_dir / new_name
356:             counter += 1
357:             
358:             if counter > 1000:  # Safety limit
359:                 logger.warning(f"Exceeded conflict resolution limit for {filename}")
360:                 break
361:         
362:         return filepath
363:     
364:     @staticmethod
365:     def _validate_download(
366:         filepath: Path,
367:         content_type: str,
368:         url: str,
369:         size_bytes: int
370:     ) -> bool:
371:         """Validate that downloaded file is valid (especially for ZIP archives).
372:         
373:         Args:
374:             filepath: Path to downloaded file
375:             content_type: Content-Type from response
376:             url: Original download URL
377:             size_bytes: Size of downloaded file
378:             
379:         Returns:
380:             True if file appears valid
381:         """
382:         if not filepath.exists():
383:             return False
384:         
385:         # Check if it's supposed to be a ZIP (folder share)
386:         is_folder_link = '/scl/fo/' in url.lower()
387:         
388:         if not is_folder_link:
389:             # Not a folder link, accept any response
390:             return True
391:         
392:         # For folder links, validate it's actually a ZIP
393:         ct_lower = content_type.lower()
394:         looks_like_zip = 'zip' in ct_lower or 'octet-stream' in ct_lower
395:         
396:         # Size check: folder ZIPs should be at least 1MB
397:         # Smaller responses are likely HTML interstitials
398:         if size_bytes < DownloadService.MIN_ZIP_SIZE_BYTES:
399:             logger.warning(f"Downloaded file too small for folder ZIP: {size_bytes} bytes")
400:             return False
401:         
402:         if not looks_like_zip:
403:             logger.warning(f"Content-Type doesn't indicate ZIP: {content_type}")
404:             return False
405:         
406:         return True
407:     
408:     @staticmethod
409:     def create_progress_callback(
410:         download_id: str,
411:         update_function: callable
412:     ) -> callable:
413:         """Create a progress callback that updates download status.
414:         
415:         Args:
416:             download_id: ID of the download to update
417:             update_function: Function(download_id, status, progress, message) to call
418:             
419:         Returns:
420:             Callback function(status, progress, message)
421:         """
422:         def callback(status: str, progress: int, message: str):
423:             try:
424:                 update_function(download_id, status, progress, message)
425:             except Exception as e:
426:                 logger.error(f"Progress callback error for {download_id}: {e}")
427:         
428:         return callback
```

## File: services/lemonfox_audio_service.py
```python
  1: """
  2: Lemonfox Audio Service
  3: Provides integration with Lemonfox Speech-to-Text API for STEP4 (Audio Analysis).
  4: 
  5: This service:
  6: - Calls Lemonfox API with video files
  7: - Converts Lemonfox transcription output to STEP4-compatible JSON format
  8: - Writes {video_stem}_audio.json files with frame-by-frame audio analysis
  9: - Ensures compatibility with STEP5 (enhanced_speaking_detection.py) and STEP6 (json_reducer.py)
 10: 
 11: Architecture:
 12: - Service layer only (business logic)
 13: - Consumed by API routes (thin controllers)
 14: - All secrets via config.settings
 15: - Security: anti path traversal, input validation
 16: """
 17: 
 18: import os
 19: import json
 20: import logging
 21: import math
 22: import subprocess
 23: import tempfile
 24: import requests
 25: from pathlib import Path
 26: from typing import Callable, Dict, List, Optional, Tuple, Any
 27: from dataclasses import dataclass
 28: 
 29: from config.settings import config
 30: 
 31: logger = logging.getLogger(__name__)
 32: 
 33: # Constants matching STEP4 current implementation
 34: AUDIO_SUFFIX = "_audio.json"
 35: DEFAULT_FPS = 25.0
 36: 
 37: 
 38: @dataclass
 39: class LemonfoxTranscriptionResult:
 40:     """Result from Lemonfox API transcription."""
 41:     success: bool
 42:     segments: List[Dict[str, Any]]
 43:     words: Optional[List[Dict[str, Any]]]
 44:     duration: Optional[float]
 45:     error: Optional[str] = None
 46: 
 47: 
 48: @dataclass
 49: class AudioAnalysisResult:
 50:     """Result of audio analysis processing."""
 51:     success: bool
 52:     output_path: Optional[Path]
 53:     fps: float
 54:     total_frames: int
 55:     error: Optional[str] = None
 56: 
 57: 
 58: @dataclass
 59: class UploadPreparationResult:
 60:     """Upload artifact used for Lemonfox API requests."""
 61:     upload_path: Path
 62:     cleanup: Optional[Callable[[], None]]
 63:     original_size_mb: float
 64:     limit_mb: Optional[float]
 65:     transcoded: bool = False
 66: 
 67: 
 68: class LemonfoxAudioService:
 69:     """Service for Lemonfox-based audio analysis."""
 70: 
 71:     @staticmethod
 72:     def _apply_speech_smoothing(
 73:         timeline: Dict[int, set],
 74:         speech_frames: set[int],
 75:         total_frames: int,
 76:         fps: float,
 77:         gap_fill_sec: float,
 78:         min_on_sec: float,
 79:     ) -> Tuple[Dict[int, set], set[int]]:
 80:         if total_frames <= 0 or fps <= 0:
 81:             return timeline, speech_frames
 82: 
 83:         if not speech_frames:
 84:             return timeline, speech_frames
 85: 
 86:         try:
 87:             gap_fill_sec = float(gap_fill_sec or 0.0)
 88:         except Exception:
 89:             gap_fill_sec = 0.0
 90:         try:
 91:             min_on_sec = float(min_on_sec or 0.0)
 92:         except Exception:
 93:             min_on_sec = 0.0
 94: 
 95:         gap_fill_frames = 0
 96:         if gap_fill_sec > 0:
 97:             gap_fill_frames = int(math.ceil(gap_fill_sec * fps))
 98: 
 99:         min_on_frames = 0
100:         if min_on_sec > 0:
101:             min_on_frames = int(math.ceil(min_on_sec * fps))
102: 
103:         def _build_runs(frames_sorted: List[int]) -> List[Tuple[int, int]]:
104:             runs: List[Tuple[int, int]] = []
105:             run_start = frames_sorted[0]
106:             run_end = frames_sorted[0]
107:             for frame in frames_sorted[1:]:
108:                 if frame == run_end + 1:
109:                     run_end = frame
110:                 else:
111:                     runs.append((run_start, run_end))
112:                     run_start = frame
113:                     run_end = frame
114:             runs.append((run_start, run_end))
115:             return runs
116: 
117:         frames_sorted = sorted(speech_frames)
118: 
119:         if gap_fill_frames > 0:
120:             runs = _build_runs(frames_sorted)
121:             for (prev_start, prev_end), (next_start, next_end) in zip(runs, runs[1:]):
122:                 gap_start = prev_end + 1
123:                 gap_end = next_start - 1
124:                 if gap_end < gap_start:
125:                     continue
126:                 gap_len = gap_end - gap_start + 1
127:                 if gap_len <= gap_fill_frames:
128:                     for f in range(gap_start, gap_end + 1):
129:                         speech_frames.add(f)
130: 
131:             frames_sorted = sorted(speech_frames)
132: 
133:         if min_on_frames > 0 and speech_frames:
134:             runs = _build_runs(frames_sorted)
135:             for run_start, run_end in runs:
136:                 run_len = run_end - run_start + 1
137:                 if run_len >= min_on_frames:
138:                     continue
139:                 for f in range(run_start, run_end + 1):
140:                     speech_frames.discard(f)
141:                     timeline.pop(f, None)
142: 
143:         return timeline, speech_frames
144: 
145:     @staticmethod
146:     def _validate_project_and_video(project_name: str, video_name: str) -> Tuple[bool, Optional[Path], Optional[str]]:
147:         """
148:         Validate and resolve project + video paths securely.
149:         
150:         Args:
151:             project_name: Name of the project (must be a valid folder name)
152:             video_name: Relative path to video within project
153:             
154:         Returns:
155:             Tuple of (success, video_path, error_message)
156:         """
157:         # Security: Reject absolute paths
158:         if os.path.isabs(video_name):
159:             return False, None, "video_name must be a relative path"
160:         
161:         # Security: Reject path traversal attempts (sanitize separators)
162:         sanitized_video_name = (video_name or "").replace("\\", "/").strip()
163:         if ".." in project_name:
164:             return False, None, "Path traversal not allowed"
165:         video_rel_path = Path(sanitized_video_name)
166:         if any(part == ".." for part in video_rel_path.parts):
167:             return False, None, "Path traversal not allowed"
168:         
169:         # Normalize project name (should be a simple folder name)
170:         if "/" in project_name or "\\" in project_name:
171:             return False, None, "project_name must be a simple folder name"
172:         
173:         # Construct and validate project path
174:         projects_dir = config.PROJECTS_DIR
175:         project_path = projects_dir / project_name
176:         
177:         if not project_path.exists():
178:             return False, None, f"Project not found: {project_name}"
179:         
180:         # Resolve video path
181:         video_path = project_path / video_name
182:         
183:         # Security: Ensure final path is within project directory
184:         try:
185:             video_path_resolved = video_path.resolve()
186:             project_path_resolved = project_path.resolve()
187:             
188:             if not str(video_path_resolved).startswith(str(project_path_resolved)):
189:                 return False, None, "Invalid video path (outside project directory)"
190:         except Exception as e:
191:             logger.error(f"Path resolution error: {e}")
192:             return False, None, "Invalid path"
193:         
194:         if not video_path.exists():
195:             return False, None, f"Video not found: {video_name}"
196:         
197:         if not video_path.is_file():
198:             return False, None, f"Video path is not a file: {video_name}"
199:         
200:         return True, video_path, None
201: 
202:     @staticmethod
203:     def _get_video_duration_ffprobe(video_path: Path) -> Optional[float]:
204:         """
205:         Get video duration in seconds using ffprobe.
206:         
207:         Args:
208:             video_path: Path to video file
209:             
210:         Returns:
211:             Duration in seconds or None if failed
212:         """
213:         try:
214:             result = subprocess.run(
215:                 [
216:                     "ffprobe", "-v", "error", "-show_entries", "format=duration",
217:                     "-of", "default=nw=1:nk=1", str(video_path)
218:                 ],
219:                 stdout=subprocess.PIPE,
220:                 stderr=subprocess.PIPE,
221:                 check=True,
222:                 text=True,
223:                 timeout=30
224:             )
225:             return float(result.stdout.strip())
226:         except Exception as e:
227:             logger.warning(f"ffprobe failed for {video_path.name}: {e}")
228:             return None
229: 
230:     @staticmethod
231:     def _call_lemonfox_api(
232:         video_path: Path,
233:         *,
234:         upload_filename: Optional[str] = None,
235:         original_file_size_mb: Optional[float] = None,
236:         limit_mb: Optional[float] = None,
237:         language: Optional[str] = None,
238:         prompt: Optional[str] = None,
239:         speaker_labels: bool = True,
240:         min_speakers: Optional[int] = None,
241:         max_speakers: Optional[int] = None,
242:         timestamp_granularities: Optional[List[str]] = None,
243:         eu_processing: Optional[bool] = None
244:     ) -> LemonfoxTranscriptionResult:
245:         """
246:         Call Lemonfox API to transcribe audio.
247:         
248:         Args:
249:             video_path: Path to the (possibly transcodé) file to upload
250:             upload_filename: Filename to expose to the API (defaults to video_path.name)
251:             original_file_size_mb: Size of the original video (for logging/errors)
252:             limit_mb: Configured upload limit (for logging/errors)
253:             language: Optional language hint
254:             prompt: Optional prompt to guide transcription
255:             speaker_labels: Enable speaker diarization (default: True)
256:             min_speakers: Minimum number of speakers
257:             max_speakers: Maximum number of speakers
258:             timestamp_granularities: List of granularities (e.g., ["word"])
259:             eu_processing: Force EU processing (overrides default from config)
260:             
261:         Returns:
262:             LemonfoxTranscriptionResult
263:         """
264:         api_key = config.LEMONFOX_API_KEY
265:         if not api_key:
266:             return LemonfoxTranscriptionResult(
267:                 success=False,
268:                 segments=[],
269:                 words=None,
270:                 duration=None,
271:                 error="LEMONFOX_API_KEY not configured"
272:             )
273:         
274:         # Determine endpoint (EU or standard)
275:         use_eu = eu_processing if eu_processing is not None else config.LEMONFOX_EU_DEFAULT
276:         base_url = "https://eu-api.lemonfox.ai" if use_eu else "https://api.lemonfox.ai"
277:         endpoint = f"{base_url}/v1/audio/transcriptions"
278:         
279:         # Prepare request
280:         headers = {
281:             "Authorization": f"Bearer {api_key}"
282:         }
283: 
284:         # Build form data as list of tuples to support repeated keys like timestamp_granularities[]
285:         data: List[Tuple[str, str]] = [
286:             ("response_format", "verbose_json"),
287:         ]
288: 
289:         if language:
290:             data.append(("language", language))
291:         if prompt:
292:             data.append(("prompt", prompt))
293:         if speaker_labels:
294:             data.append(("speaker_labels", "true"))
295:         if min_speakers is not None:
296:             data.append(("min_speakers", str(min_speakers)))
297:         if max_speakers is not None:
298:             data.append(("max_speakers", str(max_speakers)))
299:         if timestamp_granularities:
300:             for gran in timestamp_granularities:
301:                 data.append(("timestamp_granularities[]", str(gran)))
302:         
303:         filename_for_api = upload_filename or video_path.name
304: 
305:         try:
306:             with open(video_path, 'rb') as video_file:
307:                 files = {
308:                     "file": (filename_for_api, video_file, "video/mp4")
309:                 }
310:                 
311:                 logger.info(f"Calling Lemonfox API for {video_path.name}...")
312:                 response = requests.post(
313:                     endpoint,
314:                     headers=headers,
315:                     data=data,
316:                     files=files,
317:                     timeout=config.LEMONFOX_TIMEOUT_SEC
318:                 )
319:                 
320:                 if response.status_code != 200:
321:                     if response.status_code == 413:
322:                         size_clause = ""
323:                         if original_file_size_mb is not None:
324:                             size_clause = f" (taille locale ≈ {original_file_size_mb:.2f} MB)"
325:                         limit_clause = ""
326:                         if limit_mb is not None:
327:                             limit_clause = f" (limite configurée: {limit_mb} MB)"
328:                         error_msg = (
329:                             "Lemonfox API error: HTTP 413 - fichier trop volumineux"
330:                             f"{size_clause}{limit_clause}. "
331:                             "Activez LEMONFOX_ENABLE_TRANSCODE ou réduisez la vidéo."
332:                         )
333:                     else:
334:                         error_msg = f"Lemonfox API error: HTTP {response.status_code}"
335:                     try:
336:                         error_detail = response.json()
337:                         error_msg += f" - {error_detail}"
338:                     except Exception:
339:                         error_msg += f" - {response.text[:200]}"
340:                     
341:                     logger.error(error_msg)
342:                     return LemonfoxTranscriptionResult(
343:                         success=False,
344:                         segments=[],
345:                         words=None,
346:                         duration=None,
347:                         error=error_msg
348:                     )
349:                 
350:                 result = response.json()
351:                 
352:                 # Extract segments and words
353:                 segments = result.get("segments", [])
354:                 words = result.get("words", None)
355:                 duration = result.get("duration", None)
356:                 
357:                 logger.info(f"Lemonfox API success: {len(segments)} segments, duration={duration}s")
358:                 
359:                 return LemonfoxTranscriptionResult(
360:                     success=True,
361:                     segments=segments,
362:                     words=words,
363:                     duration=duration,
364:                     error=None
365:                 )
366:                 
367:         except requests.Timeout:
368:             error_msg = f"Lemonfox API timeout after {config.LEMONFOX_TIMEOUT_SEC}s"
369:             logger.error(error_msg)
370:             return LemonfoxTranscriptionResult(
371:                 success=False,
372:                 segments=[],
373:                 words=None,
374:                 duration=None,
375:                 error=error_msg
376:             )
377:         except Exception as e:
378:             error_msg = f"Lemonfox API call failed: {str(e)}"
379:             logger.error(error_msg, exc_info=True)
380:             return LemonfoxTranscriptionResult(
381:                 success=False,
382:                 segments=[],
383:                 words=None,
384:                 duration=None,
385:                 error=error_msg
386:             )
387: 
388:     @staticmethod
389:     def _build_frame_timeline(
390:         transcription: LemonfoxTranscriptionResult,
391:         total_frames: int,
392:         fps: float
393:     ) -> Tuple[Dict[int, set], set]:
394:         """
395:         Build frame-by-frame timeline from Lemonfox segments/words.
396:         
397:         Args:
398:             transcription: Lemonfox transcription result
399:             total_frames: Total number of frames in video
400:             fps: Frames per second
401:             
402:         Returns:
403:             Tuple of:
404:             - timeline: Dictionary mapping frame_num -> set of speaker labels
405:             - speech_frames: Set of frame numbers where speech is present even if no speaker label is available
406:         """
407:         timeline: Dict[int, set] = {}  # frame_num -> set(speaker_labels)
408:         speech_frames: set[int] = set()
409:         
410:         # Use words if available (more granular), otherwise use segments
411:         source = transcription.words if transcription.words else transcription.segments
412:         
413:         if not source:
414:             logger.warning("No segments or words in transcription, returning empty timeline")
415:             return timeline, speech_frames
416:         
417:         # Build speaker label mapping (normalize to SPEAKER_XX format)
418:         speaker_map = {}
419:         speaker_counter = 0
420: 
421:         for item in source:
422:             raw_speaker = item.get("speaker")
423:             if raw_speaker and raw_speaker not in speaker_map:
424:                 speaker_map[raw_speaker] = f"SPEAKER_{speaker_counter:02d}"
425:                 speaker_counter += 1
426: 
427:         logger.info(f"Speaker mapping: {speaker_map}")
428: 
429:         # Build timeline
430:         for item in source:
431:             start = item.get("start")
432:             end = item.get("end")
433:             raw_speaker = item.get("speaker")
434: 
435:             if start is None or end is None:
436:                 continue
437: 
438:             # Convert times to frame numbers (1-based)
439:             start_frame = max(1, int(start * fps))
440:             end_frame = min(total_frames, int(end * fps))
441:             if end_frame < start_frame:
442:                 continue
443: 
444:             # Get normalized speaker label
445:             speaker_label = speaker_map.get(raw_speaker) if raw_speaker else None
446: 
447:             # Track speech presence for each frame in range
448:             for frame_num in range(start_frame, end_frame + 1):
449:                 speech_frames.add(frame_num)
450:                 if frame_num not in timeline:
451:                     timeline[frame_num] = set()
452:                 if speaker_label:
453:                     timeline[frame_num].add(speaker_label)
454:         
455:         logger.info(f"Built timeline with {len(speech_frames)} frames containing speech")
456:         gap_fill_sec = float(getattr(config, "LEMONFOX_SPEECH_GAP_FILL_SEC", 0.0) or 0.0)
457:         min_on_sec = float(getattr(config, "LEMONFOX_SPEECH_MIN_ON_SEC", 0.0) or 0.0)
458:         timeline, speech_frames = LemonfoxAudioService._apply_speech_smoothing(
459:             timeline=timeline,
460:             speech_frames=speech_frames,
461:             total_frames=total_frames,
462:             fps=fps,
463:             gap_fill_sec=gap_fill_sec,
464:             min_on_sec=min_on_sec,
465:         )
466:         return timeline, speech_frames
467: 
468:     @staticmethod
469:     def _calculate_file_size_mb(path: Path) -> float:
470:         size_bytes = path.stat().st_size
471:         return size_bytes / (1024 * 1024)
472: 
473:     @staticmethod
474:     def _transcode_video_for_upload(video_path: Path) -> Tuple[Path, Callable[[], None]]:
475:         """
476:         Create an audio-only MP4 optimized for Lemonfox uploads.
477: 
478:         Returns:
479:             Tuple[path_to_upload, cleanup_callback]
480:         """
481:         tmp_file = tempfile.NamedTemporaryFile(prefix="lemonfox_audio_upload_", suffix=".mp4", delete=False)
482:         tmp_path = Path(tmp_file.name)
483:         tmp_file.close()
484: 
485:         audio_codec = getattr(config, "LEMONFOX_TRANSCODE_AUDIO_CODEC", "aac")
486:         bitrate_kbps = int(getattr(config, "LEMONFOX_TRANSCODE_BITRATE_KBPS", 96))
487: 
488:         cmd = [
489:             "ffmpeg", "-y", "-hide_banner", "-loglevel", "error",
490:             "-i", str(video_path),
491:             "-vn",
492:             "-ac", "1",
493:             "-ar", "16000",
494:             "-c:a", audio_codec,
495:             "-b:a", f"{bitrate_kbps}k",
496:             "-movflags", "+faststart",
497:             str(tmp_path),
498:         ]
499: 
500:         logger.info(
501:             "Transcodage Lemonfox audio-only pour %s via codec=%s bitrate=%skbps",
502:             video_path.name,
503:             audio_codec,
504:             bitrate_kbps,
505:         )
506: 
507:         try:
508:             subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
509:         except subprocess.CalledProcessError as exc:
510:             raise RuntimeError(f"Transcodage Lemonfox échoué pour {video_path.name}: {exc}") from exc
511: 
512:         def _cleanup() -> None:
513:             try:
514:                 tmp_path.unlink(missing_ok=True)
515:             except Exception:
516:                 pass
517: 
518:         return tmp_path, _cleanup
519: 
520:     @staticmethod
521:     def _prepare_upload_artifact(video_path: Path) -> UploadPreparationResult:
522:         """
523:         Ensure the file to upload respects Lemonfox size policy.
524: 
525:         Returns:
526:             UploadPreparationResult describing the artifact to upload.
527: 
528:         Raises:
529:             ValueError if the file exceeds the limit and transcode is disabled.
530:             RuntimeError if transcode fails.
531:         """
532:         original_size_mb = LemonfoxAudioService._calculate_file_size_mb(video_path)
533:         limit_mb = getattr(config, "LEMONFOX_MAX_UPLOAD_MB", None)
534: 
535:         if not limit_mb or original_size_mb <= float(limit_mb):
536:             return UploadPreparationResult(
537:                 upload_path=video_path,
538:                 cleanup=None,
539:                 original_size_mb=original_size_mb,
540:                 limit_mb=limit_mb,
541:                 transcoded=False,
542:             )
543: 
544:         if not getattr(config, "LEMONFOX_ENABLE_TRANSCODE", False):
545:             raise ValueError(
546:                 f"La vidéo ({original_size_mb:.2f} MB) dépasse la limite Lemonfox "
547:                 f"configurée ({limit_mb} MB) et LEMONFOX_ENABLE_TRANSCODE=0."
548:             )
549: 
550:         tmp_path, cleanup = LemonfoxAudioService._transcode_video_for_upload(video_path)
551:         transcoded_size_mb = LemonfoxAudioService._calculate_file_size_mb(tmp_path)
552:         if limit_mb and transcoded_size_mb > float(limit_mb):
553:             cleanup()
554:             raise ValueError(
555:                 f"Le fichier transcodé ({transcoded_size_mb:.2f} MB) dépasse toujours "
556:                 f"la limite Lemonfox ({limit_mb} MB)."
557:             )
558: 
559:         logger.info(
560:             "Lemonfox upload: %s compressé de %.2f MB à %.2f MB (limite=%s MB)",
561:             video_path.name,
562:             original_size_mb,
563:             transcoded_size_mb,
564:             limit_mb,
565:         )
566: 
567:         return UploadPreparationResult(
568:             upload_path=tmp_path,
569:             cleanup=cleanup,
570:             original_size_mb=original_size_mb,
571:             limit_mb=limit_mb,
572:             transcoded=True,
573:         )
574: 
575:     @staticmethod
576:     def _write_step4_json_atomically(
577:         output_path: Path,
578:         video_filename: str,
579:         fps: float,
580:         total_frames: int,
581:         timeline: Dict[int, set],
582:         speech_frames: set
583:     ) -> bool:
584:         """
585:         Write STEP4-compatible JSON atomically.
586:         
587:         Args:
588:             output_path: Target output path
589:             video_filename: Video filename for metadata
590:             fps: Frames per second
591:             total_frames: Total number of frames
592:             timeline: Frame timeline (frame_num -> set of speakers)
593:             
594:         Returns:
595:             True if successful, False otherwise
596:         """
597:         try:
598:             # Write to temporary file first (atomic write pattern)
599:             tmp_path = output_path.with_suffix('.tmp')
600:             
601:             with open(tmp_path, 'w', encoding='utf-8') as f:
602:                 # Write JSON header
603:                 f.write('{\n')
604:                 f.write(f'  "video_filename": "{video_filename}",\n')
605:                 f.write(f'  "total_frames": {total_frames},\n')
606:                 f.write(f'  "fps": {round(fps, 2)},\n')
607:                 f.write('  "frames_analysis": [\n')
608:                 
609:                 # Write frame-by-frame analysis
610:                 for frame_num in range(1, total_frames + 1):
611:                     speakers = sorted(timeline.get(frame_num, set()))
612:                     is_speech = (frame_num in speech_frames) or (len(speakers) > 0)
613:                     timecode_sec = round((frame_num - 1) / fps, 3)
614:                     
615:                     frame_obj = {
616:                         "frame": frame_num,
617:                         "audio_info": {
618:                             "is_speech_present": is_speech,
619:                             "num_distinct_speakers_audio": len(speakers),
620:                             "active_speaker_labels": speakers,
621:                             "timecode_sec": timecode_sec
622:                         }
623:                     }
624:                     
625:                     if frame_num > 1:
626:                         f.write(',\n')
627:                     f.write('    ' + json.dumps(frame_obj))
628:                 
629:                 f.write('\n  ]\n')
630:                 f.write('}\n')
631:             
632:             # Atomic replace (overwrites existing file)
633:             os.replace(tmp_path, output_path)
634:             logger.info(f"Successfully wrote {output_path}")
635:             return True
636:             
637:         except Exception as e:
638:             logger.error(f"Failed to write JSON: {e}", exc_info=True)
639:             # Clean up temp file if it exists
640:             if tmp_path.exists():
641:                 try:
642:                     tmp_path.unlink()
643:                 except Exception:
644:                     pass
645:             return False
646: 
647:     @staticmethod
648:     def process_video_with_lemonfox(
649:         project_name: str,
650:         video_name: str,
651:         language: Optional[str] = None,
652:         prompt: Optional[str] = None,
653:         speaker_labels: Optional[bool] = None,
654:         min_speakers: Optional[int] = None,
655:         max_speakers: Optional[int] = None,
656:         timestamp_granularities: Optional[List[str]] = None,
657:         eu_processing: Optional[bool] = None
658:     ) -> AudioAnalysisResult:
659:         """
660:         Process a video with Lemonfox API and generate STEP4-compatible JSON.
661:         
662:         This is the main service method called by API routes.
663:         
664:         Args:
665:             project_name: Name of the project
666:             video_name: Relative path to video within project
667:             language: Optional language hint
668:             prompt: Optional prompt to guide transcription
669:             speaker_labels: Enable speaker diarization
670:             min_speakers: Minimum number of speakers
671:             max_speakers: Maximum number of speakers
672:             timestamp_granularities: List of granularities (e.g., ["word"])
673:             eu_processing: Force EU processing
674:             
675:         Returns:
676:             AudioAnalysisResult with success status and output details
677:         """
678:         # Step 1: Validate and resolve paths
679:         valid, video_path, error = LemonfoxAudioService._validate_project_and_video(
680:             project_name, video_name
681:         )
682:         if not valid:
683:             return AudioAnalysisResult(
684:                 success=False,
685:                 output_path=None,
686:                 fps=DEFAULT_FPS,
687:                 total_frames=0,
688:                 error=error
689:             )
690:         
691:         # Step 2: Get video duration and calculate frames
692:         duration_sec = LemonfoxAudioService._get_video_duration_ffprobe(video_path)
693:         if duration_sec is None or duration_sec <= 0:
694:             return AudioAnalysisResult(
695:                 success=False,
696:                 output_path=None,
697:                 fps=DEFAULT_FPS,
698:                 total_frames=0,
699:                 error="Could not determine video duration"
700:             )
701:         
702:         fps = DEFAULT_FPS
703:         total_frames = int(round(duration_sec * fps))
704:         
705:         logger.info(f"Video: {video_path.name}, duration={duration_sec:.2f}s, fps={fps}, frames={total_frames}")
706:         
707:         # Step 3: Prepare file for upload (size policy + optional transcode)
708:         try:
709:             upload_prep = LemonfoxAudioService._prepare_upload_artifact(video_path)
710:         except ValueError as size_error:
711:             return AudioAnalysisResult(
712:                 success=False,
713:                 output_path=None,
714:                 fps=fps,
715:                 total_frames=total_frames,
716:                 error=str(size_error),
717:             )
718: 
719:         # Step 4: Call Lemonfox API
720:         effective_language = language
721:         if effective_language is None:
722:             effective_language = getattr(config, "LEMONFOX_DEFAULT_LANGUAGE", None)
723: 
724:         effective_prompt = prompt
725:         if effective_prompt is None:
726:             effective_prompt = getattr(config, "LEMONFOX_DEFAULT_PROMPT", None)
727: 
728:         if speaker_labels is None:
729:             effective_speaker_labels = bool(getattr(config, "LEMONFOX_SPEAKER_LABELS_DEFAULT", True))
730:         else:
731:             effective_speaker_labels = bool(speaker_labels)
732: 
733:         effective_min_speakers = min_speakers
734:         if effective_min_speakers is None:
735:             effective_min_speakers = getattr(config, "LEMONFOX_DEFAULT_MIN_SPEAKERS", None)
736: 
737:         effective_max_speakers = max_speakers
738:         if effective_max_speakers is None:
739:             effective_max_speakers = getattr(config, "LEMONFOX_DEFAULT_MAX_SPEAKERS", None)
740: 
741:         effective_timestamp_granularities = timestamp_granularities
742:         if effective_timestamp_granularities is None:
743:             effective_timestamp_granularities = list(getattr(config, "LEMONFOX_TIMESTAMP_GRANULARITIES", []) or [])
744:         if not effective_timestamp_granularities:
745:             effective_timestamp_granularities = None
746: 
747:         try:
748:             transcription = LemonfoxAudioService._call_lemonfox_api(
749:                 video_path=upload_prep.upload_path,
750:                 upload_filename=video_path.name,
751:                 original_file_size_mb=upload_prep.original_size_mb,
752:                 limit_mb=upload_prep.limit_mb,
753:                 language=effective_language,
754:                 prompt=effective_prompt,
755:                 speaker_labels=effective_speaker_labels,
756:                 min_speakers=effective_min_speakers,
757:                 max_speakers=effective_max_speakers,
758:                 timestamp_granularities=effective_timestamp_granularities,
759:                 eu_processing=eu_processing
760:             )
761:         finally:
762:             if upload_prep.cleanup:
763:                 upload_prep.cleanup()
764:         
765:         if not transcription.success:
766:             return AudioAnalysisResult(
767:                 success=False,
768:                 output_path=None,
769:                 fps=fps,
770:                 total_frames=total_frames,
771:                 error=transcription.error
772:             )
773:         
774:         # Step 4: Build frame timeline
775:         timeline, speech_frames = LemonfoxAudioService._build_frame_timeline(
776:             transcription, total_frames, fps
777:         )
778:         
779:         # Step 5: Write output JSON atomically
780:         output_path = video_path.with_name(f"{video_path.stem}{AUDIO_SUFFIX}")
781:         success = LemonfoxAudioService._write_step4_json_atomically(
782:             output_path=output_path,
783:             video_filename=video_path.name,
784:             fps=fps,
785:             total_frames=total_frames,
786:             timeline=timeline,
787:             speech_frames=speech_frames
788:         )
789:         
790:         if not success:
791:             return AudioAnalysisResult(
792:                 success=False,
793:                 output_path=None,
794:                 fps=fps,
795:                 total_frames=total_frames,
796:                 error="Failed to write output JSON"
797:             )
798:         
799:         return AudioAnalysisResult(
800:             success=True,
801:             output_path=output_path,
802:             fps=fps,
803:             total_frames=total_frames,
804:             error=None
805:         )
```

## File: services/monitoring_service.py
```python
  1: """
  2: System Monitoring Service
  3: Provides centralized system resource monitoring (CPU, RAM, GPU).
  4: """
  5: 
  6: import logging
  7: import time
  8: import sys
  9: import platform
 10: import subprocess
 11: import psutil
 12: from typing import Dict, Any, Optional
 13: from functools import lru_cache
 14: from config.settings import config
 15: 
 16: logger = logging.getLogger(__name__)
 17: 
 18: # GPU monitoring setup
 19: PYNVML_AVAILABLE = False
 20: try:
 21:     import pynvml
 22:     if config.ENABLE_GPU_MONITORING:
 23:         PYNVML_AVAILABLE = True
 24:         pynvml.nvmlInit()
 25:         logger.info("GPU monitoring initialized successfully")
 26:     else:
 27:         logger.info("GPU monitoring disabled by configuration")
 28: except ImportError:
 29:     logger.warning("pynvml not available. GPU monitoring disabled.")
 30: except Exception as e:
 31:     logger.error(f"Failed to initialize GPU monitoring: {e}")
 32: 
 33: 
 34: class MonitoringService:
 35:     """
 36:     Centralized service for system resource monitoring.
 37:     Provides CPU, memory, and GPU usage statistics.
 38:     """
 39:     
 40:     @staticmethod
 41:     @lru_cache(maxsize=1)
 42:     def _get_gpu_device_info() -> Optional[Dict[str, Any]]:
 43:         """
 44:         Get GPU device information (cached).
 45:         
 46:         Returns:
 47:             GPU device info or None if not available
 48:         """
 49:         if not PYNVML_AVAILABLE:
 50:             return None
 51:             
 52:         try:
 53:             handle = pynvml.nvmlDeviceGetHandleByIndex(0)
 54:             name = pynvml.nvmlDeviceGetName(handle)
 55:             # Handle both string and bytes return types
 56:             if isinstance(name, bytes):
 57:                 name = name.decode('utf-8')
 58:             return {
 59:                 "handle": handle,
 60:                 "name": name
 61:             }
 62:         except Exception as e:
 63:             logger.warning(f"Unable to get GPU device info: {e}")
 64:             return None
 65:     
 66:     @staticmethod
 67:     def get_cpu_usage() -> float:
 68:         """
 69:         Get current CPU usage percentage.
 70:         
 71:         Returns:
 72:             CPU usage percentage (0-100)
 73:         """
 74:         try:
 75:             return round(psutil.cpu_percent(interval=0.1), 1)
 76:         except Exception as e:
 77:             logger.error(f"CPU usage error: {e}")
 78:             return 0.0
 79:     
 80:     @staticmethod
 81:     def get_memory_usage() -> Dict[str, float]:
 82:         """
 83:         Get current memory usage statistics.
 84:         
 85:         Returns:
 86:             Memory usage dictionary:
 87:             {
 88:                 "percent": float,
 89:                 "used_gb": float,
 90:                 "total_gb": float,
 91:                 "available_gb": float
 92:             }
 93:         """
 94:         try:
 95:             memory_info = psutil.virtual_memory()
 96:             return {
 97:                 "percent": round(memory_info.percent, 1),
 98:                 "used_gb": round(memory_info.used / (1024**3), 2),
 99:                 "total_gb": round(memory_info.total / (1024**3), 2),
100:                 "available_gb": round(memory_info.available / (1024**3), 2)
101:             }
102:         except Exception as e:
103:             logger.error(f"Memory usage error: {e}")
104:             return {
105:                 "percent": 0.0,
106:                 "used_gb": 0.0,
107:                 "total_gb": 0.0,
108:                 "available_gb": 0.0
109:             }
110:     
111:     @staticmethod
112:     def get_gpu_usage() -> Optional[Dict[str, Any]]:
113:         """
114:         Get current GPU usage statistics.
115:         
116:         Returns:
117:             GPU usage dictionary or None if not available:
118:             {
119:                 "name": str,
120:                 "utilization_percent": int,
121:                 "memory": {
122:                     "percent": float,
123:                     "used_gb": float,
124:                     "total_gb": float
125:                 },
126:                 "temperature_c": int
127:             }
128:         """
129:         if not PYNVML_AVAILABLE:
130:             return None
131:             
132:         device_info = MonitoringService._get_gpu_device_info()
133:         if not device_info:
134:             return {"error": "GPU data not available"}
135:             
136:         try:
137:             handle = device_info["handle"]
138:             utilization = pynvml.nvmlDeviceGetUtilizationRates(handle)
139:             mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
140:             temperature = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)
141:             
142:             return {
143:                 "name": device_info["name"],
144:                 "utilization_percent": utilization.gpu,
145:                 "memory": {
146:                     "percent": round((mem_info.used / mem_info.total) * 100, 1),
147:                     "used_gb": round(mem_info.used / (1024**3), 2),
148:                     "total_gb": round(mem_info.total / (1024**3), 2),
149:                 },
150:                 "temperature_c": temperature
151:             }
152:         except Exception as e:
153:             logger.warning(f"GPU usage error: {e}")
154:             return {"error": "GPU data not available"}
155:     
156:     @staticmethod
157:     def get_disk_usage(path: str = None) -> Dict[str, float]:
158:         """
159:         Get disk usage statistics for a specific path.
160:         
161:         Args:
162:             path: Path to check (defaults to base path)
163:             
164:         Returns:
165:             Disk usage dictionary:
166:             {
167:                 "percent": float,
168:                 "used_gb": float,
169:                 "total_gb": float,
170:                 "free_gb": float
171:             }
172:         """
173:         if path is None:
174:             path = str(config.BASE_PATH_SCRIPTS)
175:             
176:         try:
177:             disk_info = psutil.disk_usage(path)
178:             return {
179:                 "percent": round((disk_info.used / disk_info.total) * 100, 1),
180:                 "used_gb": round(disk_info.used / (1024**3), 2),
181:                 "total_gb": round(disk_info.total / (1024**3), 2),
182:                 "free_gb": round(disk_info.free / (1024**3), 2)
183:             }
184:         except Exception as e:
185:             logger.error(f"Disk usage error for {path}: {e}")
186:             return {
187:                 "percent": 0.0,
188:                 "used_gb": 0.0,
189:                 "total_gb": 0.0,
190:                 "free_gb": 0.0
191:             }
192:     
193:     @staticmethod
194:     def get_system_status() -> Dict[str, Any]:
195:         """
196:         Get comprehensive system status including CPU, memory, GPU, and disk.
197:         
198:         Returns:
199:             System status dictionary:
200:             {
201:                 "cpu_percent": float,
202:                 "memory": dict,
203:                 "gpu": dict|null,
204:                 "disk": dict,
205:                 "timestamp": str
206:             }
207:         """
208:         try:
209:             # Ensure projects directory exists
210:             projects_dir = config.BASE_PATH_SCRIPTS / 'projets_extraits'
211:             projects_dir.mkdir(parents=True, exist_ok=True)
212:             
213:             from datetime import datetime, timezone
214:             
215:             return {
216:                 "cpu_percent": MonitoringService.get_cpu_usage(),
217:                 "memory": MonitoringService.get_memory_usage(),
218:                 "gpu": MonitoringService.get_gpu_usage(),
219:                 "disk": MonitoringService.get_disk_usage(),
220:                 "timestamp": datetime.now(timezone.utc).isoformat()
221:             }
222:         except Exception as e:
223:             logger.error(f"System status error: {e}")
224:             raise
225: 
226:     @staticmethod
227:     def get_environment_info() -> Dict[str, Any]:
228:         """
229:         Provide environment diagnostics useful for troubleshooting.
230: 
231:         Returns:
232:             dict: Environment information including versions, GPU availability,
233:                   and filtered configuration flags (no secrets).
234:         """
235:         # Python
236:         python_version = sys.version.split(" (", 1)[0]
237:         implementation = platform.python_implementation()
238: 
239:         # FFmpeg version (best-effort, non-fatal)
240:         ffmpeg_version = "unknown"
241:         try:
242:             proc = subprocess.run(["ffmpeg", "-version"], capture_output=True, text=True, timeout=2)
243:             if proc.returncode == 0 and proc.stdout:
244:                 first_line = proc.stdout.splitlines()[0].strip()
245:                 ffmpeg_version = first_line
246:             elif proc.stderr:
247:                 # Some builds print to stderr
248:                 first_line = proc.stderr.splitlines()[0].strip()
249:                 ffmpeg_version = first_line
250:         except Exception as e:
251:             logger.debug(f"FFmpeg version check failed: {e}")
252: 
253:         # GPU info (availability + name if possible)
254:         gpu_available = bool(PYNVML_AVAILABLE and MonitoringService._get_gpu_device_info())
255:         gpu_name = None
256:         if gpu_available:
257:             info = MonitoringService._get_gpu_device_info()
258:             if info and isinstance(info.get("name"), str):
259:                 gpu_name = info["name"]
260: 
261:         # Filtered configuration flags (no secrets)
262:         filtered_config = {
263:             "ENABLE_GPU_MONITORING": bool(getattr(config, "ENABLE_GPU_MONITORING", False)),
264:             "DRY_RUN_DOWNLOADS": bool(getattr(config, "DRY_RUN_DOWNLOADS", False)),
265:             "FLASK_DEBUG": bool(getattr(config, "DEBUG", False)),
266:         }
267: 
268:         from datetime import datetime, timezone
269:         return {
270:             "python": {
271:                 "version": python_version,
272:                 "implementation": implementation,
273:             },
274:             "ffmpeg": {
275:                 "version": ffmpeg_version,
276:             },
277:             "gpu": {
278:                 "available": gpu_available,
279:                 "name": gpu_name,
280:             },
281:             "config_flags": filtered_config,
282:             "timestamp": datetime.now(timezone.utc).isoformat(),
283:         }
284:     
285:     @staticmethod
286:     def get_process_info() -> Dict[str, Any]:
287:         """
288:         Get information about the current process.
289:         
290:         Returns:
291:             Process information dictionary:
292:             {
293:                 "pid": int,
294:                 "cpu_percent": float,
295:                 "memory_mb": float,
296:                 "threads": int,
297:                 "uptime_seconds": float
298:             }
299:         """
300:         try:
301:             process = psutil.Process()
302:             create_time = process.create_time()
303:             
304:             return {
305:                 "pid": process.pid,
306:                 "cpu_percent": round(process.cpu_percent(), 1),
307:                 "memory_mb": round(process.memory_info().rss / (1024**2), 1),
308:                 "threads": process.num_threads(),
309:                 "uptime_seconds": round(time.time() - create_time, 1)
310:             }
311:         except Exception as e:
312:             logger.error(f"Process info error: {e}")
313:             return {
314:                 "pid": 0,
315:                 "cpu_percent": 0.0,
316:                 "memory_mb": 0.0,
317:                 "threads": 0,
318:                 "uptime_seconds": 0.0
319:             }
320:     
321:     @staticmethod
322:     def is_system_healthy() -> Dict[str, Any]:
323:         """
324:         Check if system resources are within healthy limits.
325:         
326:         Returns:
327:             Health status dictionary:
328:             {
329:                 "healthy": bool,
330:                 "warnings": list,
331:                 "critical": list
332:             }
333:         """
334:         warnings = []
335:         critical = []
336:         
337:         try:
338:             # Check CPU usage
339:             cpu_percent = MonitoringService.get_cpu_usage()
340:             if cpu_percent > 90:
341:                 critical.append(f"High CPU usage: {cpu_percent}%")
342:             elif cpu_percent > 75:
343:                 warnings.append(f"Elevated CPU usage: {cpu_percent}%")
344:             
345:             # Check memory usage
346:             memory = MonitoringService.get_memory_usage()
347:             if memory["percent"] > 95:
348:                 critical.append(f"Critical memory usage: {memory['percent']}%")
349:             elif memory["percent"] > 85:
350:                 warnings.append(f"High memory usage: {memory['percent']}%")
351:             
352:             # Check disk usage
353:             disk = MonitoringService.get_disk_usage()
354:             if disk["percent"] > 95:
355:                 critical.append(f"Critical disk usage: {disk['percent']}%")
356:             elif disk["percent"] > 85:
357:                 warnings.append(f"High disk usage: {disk['percent']}%")
358:             
359:             # Check GPU temperature if available
360:             gpu = MonitoringService.get_gpu_usage()
361:             if gpu and "temperature_c" in gpu:
362:                 temp = gpu["temperature_c"]
363:                 if temp > 85:
364:                     critical.append(f"High GPU temperature: {temp}°C")
365:                 elif temp > 75:
366:                     warnings.append(f"Elevated GPU temperature: {temp}°C")
367:             
368:             return {
369:                 "healthy": len(critical) == 0,
370:                 "warnings": warnings,
371:                 "critical": critical
372:             }
373:             
374:         except Exception as e:
375:             logger.error(f"Health check error: {e}")
376:             return {
377:                 "healthy": False,
378:                 "warnings": [],
379:                 "critical": [f"Health check failed: {str(e)}"]
380:             }
```

## File: services/performance_service.py
```python
  1: """
  2: Performance Service
  3: Centralized performance monitoring and profiling service.
  4: """
  5: 
  6: import logging
  7: import time
  8: import threading
  9: from collections import defaultdict, deque
 10: from contextlib import contextmanager
 11: from typing import Dict, Any, Optional, List
 12: from datetime import datetime, timezone
 13: 
 14: from services.monitoring_service import MonitoringService
 15: 
 16: logger = logging.getLogger(__name__)
 17: 
 18: # Global profiling statistics
 19: PROFILING_STATS = defaultdict(lambda: {"total_time": 0, "calls": 0, "avg_time": 0})
 20: PROFILING_LOCK = threading.Lock()
 21: 
 22: # Performance metrics history
 23: PERFORMANCE_HISTORY = deque(maxlen=100)  # Keep last 100 measurements
 24: PERFORMANCE_LOCK = threading.Lock()
 25: 
 26: # Performance alerts
 27: PERFORMANCE_ALERTS = deque(maxlen=50)  # Keep last 50 alerts
 28: ALERT_THRESHOLDS = {
 29:     "cpu_percent": 85.0,
 30:     "memory_percent": 90.0,
 31:     "response_time_ms": 1000.0,
 32:     "error_rate_percent": 5.0
 33: }
 34: 
 35: # Background monitoring state
 36: BACKGROUND_MONITORING_ACTIVE = False
 37: MONITORING_THREAD = None
 38: MONITORING_LOCK = threading.Lock()
 39: 
 40: 
 41: class PerformanceService:
 42:     """
 43:     Centralized service for performance monitoring and profiling.
 44:     Tracks system performance, response times, and provides profiling capabilities.
 45:     """
 46:     
 47:     @staticmethod
 48:     def get_performance_metrics() -> Dict[str, Any]:
 49:         """
 50:         Get comprehensive performance metrics.
 51:         
 52:         Returns:
 53:             Performance metrics dictionary:
 54:             {
 55:                 "profiling_stats": dict,
 56:                 "cache_stats": dict,
 57:                 "system_performance": dict,
 58:                 "performance_history": list,
 59:                 "alerts": list
 60:             }
 61:         """
 62:         try:
 63:             from services.cache_service import CacheService
 64:             
 65:             with PROFILING_LOCK:
 66:                 profiling_data = {k: dict(v) for k, v in PROFILING_STATS.items()}
 67:             
 68:             with PERFORMANCE_LOCK:
 69:                 history_data = list(PERFORMANCE_HISTORY)
 70:                 alerts_data = list(PERFORMANCE_ALERTS)
 71:             
 72:             return {
 73:                 "profiling_stats": profiling_data,
 74:                 "cache_stats": CacheService.get_cache_stats(),
 75:                 "system_performance": MonitoringService.get_system_status(),
 76:                 "performance_history": history_data,
 77:                 "alerts": alerts_data,
 78:                 "timestamp": datetime.now(timezone.utc).isoformat()
 79:             }
 80:             
 81:         except Exception as e:
 82:             logger.error(f"Performance metrics error: {e}")
 83:             return {
 84:                 "error": str(e),
 85:                 "timestamp": datetime.now(timezone.utc).isoformat()
 86:             }
 87:     
 88:     @staticmethod
 89:     def reset_profiling_stats() -> None:
 90:         """Reset profiling statistics."""
 91:         global PROFILING_STATS
 92:         
 93:         with PROFILING_LOCK:
 94:             PROFILING_STATS = defaultdict(lambda: {"total_time": 0, "calls": 0, "avg_time": 0})
 95:         
 96:         logger.info("Profiling statistics reset")
 97:     
 98:     @staticmethod
 99:     @contextmanager
100:     def profile_section(section_name: str):
101:         """
102:         Context manager for profiling code sections.
103:         
104:         Args:
105:             section_name: Name of the section being profiled
106:             
107:         Usage:
108:             with PerformanceService.profile_section("video_processing"):
109:                 # Code to profile
110:                 process_video()
111:         """
112:         start_time = time.perf_counter()
113:         try:
114:             yield
115:         finally:
116:             elapsed_time = (time.perf_counter() - start_time) * 1000  # Convert to ms
117:             
118:             with PROFILING_LOCK:
119:                 stats = PROFILING_STATS[section_name]
120:                 stats["total_time"] += elapsed_time
121:                 stats["calls"] += 1
122:                 stats["avg_time"] = stats["total_time"] / stats["calls"]
123:     
124:     @staticmethod
125:     def record_api_response_time(endpoint: str, response_time_ms: float, status_code: int) -> None:
126:         """
127:         Record API response time for performance tracking.
128:         
129:         Args:
130:             endpoint: API endpoint name
131:             response_time_ms: Response time in milliseconds
132:             status_code: HTTP status code
133:         """
134:         try:
135:             timestamp = datetime.now(timezone.utc).isoformat()
136:             
137:             metric = {
138:                 "timestamp": timestamp,
139:                 "endpoint": endpoint,
140:                 "response_time_ms": round(response_time_ms, 2),
141:                 "status_code": status_code,
142:                 "is_error": status_code >= 400
143:             }
144:             
145:             with PERFORMANCE_LOCK:
146:                 PERFORMANCE_HISTORY.append(metric)
147:             
148:             # Check for performance alerts
149:             PerformanceService._check_performance_alerts(metric)
150:             
151:         except Exception as e:
152:             logger.error(f"Response time recording error: {e}")
153:     
154:     @staticmethod
155:     def record_system_metrics() -> None:
156:         """Record current system metrics for trend analysis."""
157:         try:
158:             system_status = MonitoringService.get_system_status()
159:             timestamp = datetime.now(timezone.utc).isoformat()
160:             
161:             metric = {
162:                 "timestamp": timestamp,
163:                 "type": "system",
164:                 "cpu_percent": system_status["cpu_percent"],
165:                 "memory_percent": system_status["memory"]["percent"],
166:                 "disk_percent": system_status.get("disk", {}).get("percent", 0)
167:             }
168:             
169:             # Add GPU metrics if available
170:             if system_status.get("gpu") and "utilization_percent" in system_status["gpu"]:
171:                 metric["gpu_percent"] = system_status["gpu"]["utilization_percent"]
172:                 metric["gpu_memory_percent"] = system_status["gpu"]["memory"]["percent"]
173:             
174:             with PERFORMANCE_LOCK:
175:                 PERFORMANCE_HISTORY.append(metric)
176:             
177:             # Check for system alerts
178:             PerformanceService._check_system_alerts(metric)
179:             
180:         except Exception as e:
181:             logger.error(f"System metrics recording error: {e}")
182:     
183:     @staticmethod
184:     def get_profiling_summary() -> Dict[str, Any]:
185:         """
186:         Get formatted profiling summary.
187:         
188:         Returns:
189:             Profiling summary with top performers and statistics
190:         """
191:         try:
192:             with PROFILING_LOCK:
193:                 if not PROFILING_STATS:
194:                     return {
195:                         "message": "No profiling data collected",
196:                         "total_sections": 0,
197:                         "top_sections": []
198:                     }
199:                 
200:                 # Calculate total time across all sections
201:                 total_time_all = sum(stats["total_time"] for stats in PROFILING_STATS.values())
202:                 
203:                 # Sort by total time descending
204:                 sorted_stats = sorted(
205:                     PROFILING_STATS.items(),
206:                     key=lambda item: item[1]["total_time"],
207:                     reverse=True
208:                 )
209:                 
210:                 # Format top sections
211:                 top_sections = []
212:                 for name, stats in sorted_stats[:10]:  # Top 10
213:                     percentage = (stats["total_time"] / total_time_all * 100) if total_time_all > 0 else 0
214:                     
215:                     top_sections.append({
216:                         "name": name,
217:                         "total_time_ms": round(stats["total_time"], 2),
218:                         "calls": stats["calls"],
219:                         "avg_time_ms": round(stats["avg_time"], 4),
220:                         "percentage": round(percentage, 2)
221:                     })
222:                 
223:                 return {
224:                     "total_sections": len(PROFILING_STATS),
225:                     "total_time_ms": round(total_time_all, 2),
226:                     "top_sections": top_sections
227:                 }
228:                 
229:         except Exception as e:
230:             logger.error(f"Profiling summary error: {e}")
231:             return {"error": str(e)}
232:     
233:     @staticmethod
234:     def get_performance_trends() -> Dict[str, Any]:
235:         """
236:         Analyze performance trends from historical data.
237:         
238:         Returns:
239:             Performance trends analysis
240:         """
241:         try:
242:             with PERFORMANCE_LOCK:
243:                 if len(PERFORMANCE_HISTORY) < 2:
244:                     return {
245:                         "message": "Insufficient data for trend analysis",
246:                         "data_points": len(PERFORMANCE_HISTORY)
247:                     }
248:                 
249:                 # Separate API and system metrics
250:                 api_metrics = [m for m in PERFORMANCE_HISTORY if m.get("endpoint")]
251:                 system_metrics = [m for m in PERFORMANCE_HISTORY if m.get("type") == "system"]
252:                 
253:                 trends = {}
254:                 
255:                 # API response time trends
256:                 if api_metrics:
257:                     response_times = [m["response_time_ms"] for m in api_metrics[-20:]]  # Last 20
258:                     trends["api"] = {
259:                         "avg_response_time_ms": round(sum(response_times) / len(response_times), 2),
260:                         "max_response_time_ms": max(response_times),
261:                         "min_response_time_ms": min(response_times),
262:                         "error_rate_percent": round(
263:                             sum(1 for m in api_metrics[-20:] if m["is_error"]) / len(api_metrics[-20:]) * 100, 2
264:                         )
265:                     }
266:                 
267:                 # System resource trends
268:                 if system_metrics:
269:                     recent_system = system_metrics[-10:]  # Last 10
270:                     trends["system"] = {
271:                         "avg_cpu_percent": round(
272:                             sum(m["cpu_percent"] for m in recent_system) / len(recent_system), 1
273:                         ),
274:                         "avg_memory_percent": round(
275:                             sum(m["memory_percent"] for m in recent_system) / len(recent_system), 1
276:                         ),
277:                         "max_cpu_percent": max(m["cpu_percent"] for m in recent_system),
278:                         "max_memory_percent": max(m["memory_percent"] for m in recent_system)
279:                     }
280:                 
281:                 return {
282:                     "trends": trends,
283:                     "data_points": {
284:                         "api": len(api_metrics),
285:                         "system": len(system_metrics)
286:                     },
287:                     "analysis_period": "last_20_measurements"
288:                 }
289:                 
290:         except Exception as e:
291:             logger.error(f"Performance trends error: {e}")
292:             return {"error": str(e)}
293:     
294:     @staticmethod
295:     def _check_performance_alerts(metric: Dict[str, Any]) -> None:
296:         """
297:         Check for performance alerts based on thresholds.
298:         
299:         Args:
300:             metric: Performance metric to check
301:         """
302:         try:
303:             alerts = []
304:             
305:             # Check API response time
306:             if "response_time_ms" in metric:
307:                 if metric["response_time_ms"] > ALERT_THRESHOLDS["response_time_ms"]:
308:                     alerts.append({
309:                         "type": "slow_response",
310:                         "message": f"Slow API response: {metric['response_time_ms']}ms for {metric['endpoint']}",
311:                         "severity": "warning",
312:                         "timestamp": metric["timestamp"]
313:                     })
314:             
315:             # Add alerts to queue
316:             for alert in alerts:
317:                 PERFORMANCE_ALERTS.append(alert)
318:                 logger.warning(f"Performance alert: {alert['message']}")
319:                 
320:         except Exception as e:
321:             logger.error(f"Performance alert check error: {e}")
322:     
323:     @staticmethod
324:     def _check_system_alerts(metric: Dict[str, Any]) -> None:
325:         """
326:         Check for system resource alerts.
327:         
328:         Args:
329:             metric: System metric to check
330:         """
331:         try:
332:             alerts = []
333:             
334:             if metric.get("cpu_percent", 0) > ALERT_THRESHOLDS["cpu_percent"]:
335:                 alerts.append({
336:                     "type": "cpu_high",
337:                     "severity": "warning",
338:                     "message": f"CPU usage high: {metric['cpu_percent']}%",
339:                     "timestamp": metric["timestamp"]
340:                 })
341:             
342:             if metric.get("memory_percent", 0) > ALERT_THRESHOLDS["memory_percent"]:
343:                 alerts.append({
344:                     "type": "memory_high",
345:                     "severity": "warning",
346:                     "message": f"Memory usage high: {metric['memory_percent']}%",
347:                     "timestamp": metric["timestamp"]
348:                 })
349:             
350:             if alerts:
351:                 with PERFORMANCE_LOCK:
352:                     PERFORMANCE_ALERTS.extend(alerts)
353:                     
354:         except Exception as e:
355:             logger.error(f"System alerts check error: {e}")
356:     
357:     @staticmethod
358:     def clear_alerts() -> None:
359:         """Clear all performance alerts."""
360:         with PERFORMANCE_LOCK:
361:             PERFORMANCE_ALERTS.clear()
362:         logger.info("Performance alerts cleared")
363:     
364:     @staticmethod
365:     def update_alert_thresholds(thresholds: Dict[str, float]) -> None:
366:         """
367:         Update performance alert thresholds.
368:         
369:         Args:
370:             thresholds: Dictionary of threshold values
371:         """
372:         global ALERT_THRESHOLDS
373:         
374:         for key, value in thresholds.items():
375:             if key in ALERT_THRESHOLDS:
376:                 ALERT_THRESHOLDS[key] = value
377:                 logger.info(f"Updated alert threshold {key} to {value}")
378:     
379:     @staticmethod
380:     def start_background_monitoring(interval_seconds: int = 30) -> None:
381:         """
382:         Start background system monitoring thread.
383: 
384:         Args:
385:             interval_seconds: Monitoring interval in seconds
386:         """
387:         global BACKGROUND_MONITORING_ACTIVE, MONITORING_THREAD
388: 
389:         with MONITORING_LOCK:
390:             if BACKGROUND_MONITORING_ACTIVE:
391:                 logger.debug("Background monitoring already active, skipping")
392:                 return
393: 
394:             def monitor_loop():
395:                 global BACKGROUND_MONITORING_ACTIVE
396:                 while BACKGROUND_MONITORING_ACTIVE:
397:                     try:
398:                         PerformanceService.record_system_metrics()
399:                         time.sleep(interval_seconds)
400:                     except Exception as e:
401:                         logger.error(f"Background monitoring error: {e}")
402:                         time.sleep(interval_seconds)
403: 
404:             BACKGROUND_MONITORING_ACTIVE = True
405:             MONITORING_THREAD = threading.Thread(target=monitor_loop, daemon=True)
406:             MONITORING_THREAD.start()
407:             logger.info(f"Background performance monitoring started (interval: {interval_seconds}s)")
408: 
409:     @staticmethod
410:     def stop_background_monitoring() -> None:
411:         """Stop background system monitoring."""
412:         global BACKGROUND_MONITORING_ACTIVE, MONITORING_THREAD
413: 
414:         with MONITORING_LOCK:
415:             if BACKGROUND_MONITORING_ACTIVE:
416:                 BACKGROUND_MONITORING_ACTIVE = False
417:                 logger.info("Background performance monitoring stopped")
418:             else:
419:                 logger.debug("Background monitoring was not active")
420: 
421:     @staticmethod
422:     def get_dashboard_stats() -> Dict[str, Any]:
423:         """
424:         Get comprehensive statistics for the performance dashboard.
425:         
426:         Returns:
427:             Dashboard statistics including:
428:             - API response time trends (last 50)
429:             - System resource trends (last 50)
430:             - Step execution history
431:             - Performance alerts
432:             - Cache statistics
433:             - Summary metrics
434:         """
435:         try:
436:             from services.cache_service import CacheService
437:             
438:             with PERFORMANCE_LOCK:
439:                 history_data = list(PERFORMANCE_HISTORY)
440:             
441:             # Separate API and system metrics
442:             api_metrics = [m for m in history_data if m.get("endpoint")][-50:]
443:             system_metrics = [m for m in history_data if m.get("type") == "system"][-50:]
444:             
445:             # Calculate summary statistics
446:             summary = {
447:                 "total_api_calls": len(api_metrics),
448:                 "total_errors": sum(1 for m in api_metrics if m.get("is_error", False)),
449:                 "avg_response_time_ms": 0,
450:                 "error_rate_percent": 0
451:             }
452:             
453:             if api_metrics:
454:                 response_times = [m["response_time_ms"] for m in api_metrics]
455:                 summary["avg_response_time_ms"] = round(sum(response_times) / len(response_times), 2)
456:                 summary["error_rate_percent"] = round((summary["total_errors"] / len(api_metrics)) * 100, 2)
457:             
458:             # Get step execution data from WorkflowService
459:             step_history = PerformanceService._get_step_execution_summary()
460:             
461:             # Get recent alerts
462:             with PERFORMANCE_LOCK:
463:                 recent_alerts = list(PERFORMANCE_ALERTS)[-20:]
464:             
465:             return {
466:                 "summary": summary,
467:                 "api_metrics": api_metrics,
468:                 "system_metrics": system_metrics,
469:                 "step_history": step_history,
470:                 "alerts": recent_alerts,
471:                 "cache_stats": CacheService.get_cache_stats(),
472:                 "profiling_summary": PerformanceService.get_profiling_summary(),
473:                 "timestamp": datetime.now(timezone.utc).isoformat()
474:             }
475:             
476:         except Exception as e:
477:             logger.error(f"Dashboard stats error: {e}")
478:             return {
479:                 "error": str(e),
480:                 "timestamp": datetime.now(timezone.utc).isoformat()
481:             }
482:     
483:     @staticmethod
484:     def _get_step_execution_summary() -> Dict[str, Any]:
485:         """
486:         Get summary of workflow step executions from profiling data.
487:         
488:         Returns:
489:             Step execution summary with counts and average times
490:         """
491:         try:
492:             with PROFILING_LOCK:
493:                 step_stats = {}
494:                 
495:                 # Filter profiling stats for workflow steps
496:                 for section_name, stats in PROFILING_STATS.items():
497:                     # Check if this is a step execution
498:                     if section_name.startswith("step_") or "STEP" in section_name.upper():
499:                         step_stats[section_name] = {
500:                             "executions": stats["calls"],
501:                             "total_time_ms": round(stats["total_time"], 2),
502:                             "avg_time_ms": round(stats["avg_time"], 2)
503:                         }
504:                 
505:                 return {
506:                     "steps": step_stats,
507:                     "total_step_executions": sum(s["executions"] for s in step_stats.values()),
508:                     "total_time_ms": round(sum(s["total_time_ms"] for s in step_stats.values()), 2)
509:                 }
510:                 
511:         except Exception as e:
512:             logger.error(f"Step execution summary error: {e}")
513:             return {
514:                 "steps": {},
515:                 "total_step_executions": 0,
516:                 "total_time_ms": 0,
517:                 "error": str(e)
518:             }
519:     
520:     @staticmethod
521:     def get_historical_data(data_type: str = "all", limit: int = 100) -> Dict[str, Any]:
522:         """
523:         Get historical performance data for specific type.
524:         
525:         Args:
526:             data_type: Type of data ("api", "system", or "all")
527:             limit: Maximum number of records to return
528:             
529:         Returns:
530:             Historical data filtered by type
531:         """
532:         try:
533:             with PERFORMANCE_LOCK:
534:                 history_data = list(PERFORMANCE_HISTORY)
535:             
536:             if data_type == "api":
537:                 filtered_data = [m for m in history_data if m.get("endpoint")][-limit:]
538:             elif data_type == "system":
539:                 filtered_data = [m for m in history_data if m.get("type") == "system"][-limit:]
540:             else:
541:                 filtered_data = history_data[-limit:]
542:             
543:             return {
544:                 "data_type": data_type,
545:                 "count": len(filtered_data),
546:                 "data": filtered_data,
547:                 "timestamp": datetime.now(timezone.utc).isoformat()
548:             }
549:             
550:         except Exception as e:
551:             logger.error(f"Historical data retrieval error: {e}")
552:             return {
553:                 "error": str(e),
554:                 "timestamp": datetime.now(timezone.utc).isoformat()
555:             }
```

## File: services/report_service.py
```python
   1: """
   2: Report Service
   3: Generates visual reports (HTML) with statistics and infographics for analyzed videos.
   4: Uses Jinja2 templates and aggregates data from VisualizationService.
   5: """
   6: from __future__ import annotations
   7: 
   8: import logging
   9: from pathlib import Path
  10: from typing import Dict, Any, Optional, List
  11: import hashlib
  12: import unicodedata
  13: import re
  14: from datetime import datetime, timezone
  15: from jinja2 import Environment, FileSystemLoader, select_autoescape
  16: 
  17: from config.settings import config
  18: from services.visualization_service import VisualizationService
  19: 
  20: logger = logging.getLogger(__name__)
  21: 
  22: 
  23: class ReportService:
  24:     """Service for generating visual analysis reports."""
  25:     
  26:     _EXCLUDED_ARCHIVE_FILENAMES = {
  27:         "m6+.mov".lower(),
  28:         "BOUCLE BUG 9-16.mov".lower(),
  29:     }
  30:     
  31:     @staticmethod
  32:     def analyze_monthly_report_html(html: str) -> Dict[str, Any]:
  33:         """Analyze an uploaded monthly report HTML and compute counts from the
  34:         'Répartition des Durées par Projet' section only.
  35: 
  36:         Args:
  37:             html: The HTML content of a monthly report.
  38: 
  39:         Returns:
  40:             Dict with computed counts strictly from listed filenames in the
  41:             duration section (what is visually present in the report):
  42:             {
  43:               "total_listed": int,            # total entries across all spans.video-names
  44:               "by_extension": {".mp4": int, "other": int, "noext": int},
  45:               "projects": int|null,           # number of distinct project blocks found
  46:               "build_id": str|null,           # parsed from header if present
  47:               "month": str|null               # parsed from title if present
  48:             }
  49:         """
  50:         try:
  51:             text = html or ""
  52:             section_text = text
  53:             try:
  54:                 h2m = re.search(r"<h2>\s*R[ée]partition des Dur[ée]es par Projet\s*</h2>", text, flags=re.IGNORECASE)
  55:                 if h2m:
  56:                     section_text = text[h2m.end():]
  57:             except Exception:
  58:                 section_text = text
  59:             month_match = re.search(r"Rapport\s+Mensuel\s+Archives\s+[-—]\s+([0-9]{4}-[0-9]{2})", text)
  60:             build_match = re.search(r"Build\s+([a-f0-9]{6,16})", text, re.IGNORECASE)
  61:             month_val = month_match.group(1) if month_match else None
  62:             build_val = build_match.group(1) if build_match else None
  63: 
  64:             blocks = re.findall(r"<div\s+class=\"video-names\"[^>]*>(.*?)</div>", section_text, flags=re.DOTALL | re.IGNORECASE)
  65: 
  66:             total_listed = 0
  67:             mp4 = 0
  68:             other = 0
  69:             noext = 0
  70: 
  71:             def _classify(name: str):
  72:                 nonlocal total_listed, mp4, other, noext
  73:                 clean = re.sub(r"<[^>]+>", "", name)
  74:                 clean = unicodedata.normalize("NFKC", clean).strip()
  75:                 if not clean:
  76:                     return
  77:                 total_listed += 1
  78:                 l = clean.lower()
  79:                 if l.endswith('.mp4'):
  80:                     mp4 += 1
  81:                 else:
  82:                     if '.' in l:
  83:                         other += 1
  84:                     else:
  85:                         noext += 1
  86: 
  87:             for raw in blocks:
  88:                 items = re.findall(r"<div\s+class=\"video-name\"[^>]*>(.*?)</div>", raw, flags=re.DOTALL | re.IGNORECASE)
  89:                 if items:
  90:                     for it in items:
  91:                         _classify(it)
  92:                 else:
  93:                     parts = re.split(r"<br\s*/?>", raw, flags=re.IGNORECASE)
  94:                     buffer = ""
  95:                     for part in parts:
  96:                         txt = re.sub(r"<[^>]+>", "", part)
  97:                         txt = unicodedata.normalize("NFKC", txt).strip()
  98:                         if not txt:
  99:                             continue
 100:                         if not buffer:
 101:                             if txt.lower().endswith('.mp4') or re.search(r"\.[a-z0-9]{2,4}$", txt.lower()):
 102:                                 _classify(txt)
 103:                             else:
 104:                                 buffer = txt
 105:                         else:
 106:                             combined = (buffer + " " + txt).strip()
 107:                             if txt.lower().endswith('.mp4') or combined.lower().endswith('.mp4') or re.search(r"\.[a-z0-9]{2,4}$", txt.lower()) or re.search(r"\.[a-z0-9]{2,4}$", combined.lower()):
 108:                                 _classify(combined)
 109:                                 buffer = ""
 110:                             else:
 111:                                 buffer = combined
 112: 
 113:             if total_listed == 0:
 114:                 spans = re.findall(r"<span\s+class=\"video-names\">(.*?)</span>", section_text, flags=re.DOTALL | re.IGNORECASE)
 115:                 for raw in spans:
 116:                     parts = re.split(r"<br\s*/?>", raw, flags=re.IGNORECASE)
 117:                     buffer = ""
 118:                     for part in parts:
 119:                         txt = re.sub(r"<[^>]+>", "", part)
 120:                         txt = unicodedata.normalize("NFKC", txt).strip()
 121:                         if not txt:
 122:                             continue
 123:                         if not buffer:
 124:                             if txt.lower().endswith('.mp4') or re.search(r"\.[a-z0-9]{2,4}$", txt.lower()):
 125:                                 _classify(txt)
 126:                             else:
 127:                                 buffer = txt
 128:                         else:
 129:                             combined = (buffer + " " + txt).strip()
 130:                             if txt.lower().endswith('.mp4') or combined.lower().endswith('.mp4') or re.search(r"\.[a-z0-9]{2,4}$", txt.lower()) or re.search(r"\.[a-z0-9]{2,4}$", combined.lower()):
 131:                                 _classify(combined)
 132:                                 buffer = ""
 133:                             else:
 134:                                 buffer = combined
 135: 
 136:             if total_listed == 0:
 137:                 items = re.findall(r"<div\s+class=\"video-name\"[^>]*>(.*?)</div>", section_text, flags=re.DOTALL | re.IGNORECASE)
 138:                 for it in items:
 139:                     _classify(it)
 140: 
 141:             proj_blocks = re.findall(r"<strong>\s*([^<]+?)\s*</strong>\s*:", section_text)
 142:             
 143:             counters = re.findall(r"(?:moins\s+de\s+2\s+minutes\s*:\s*(\d+))|(?:entre\s+2\s+et\s*5\s+minutes\s*:\s*(\d+))|(?:plus\s+de\s*5\s+minutes\s*:\s*(\d+))", text, flags=re.IGNORECASE)
 144:             total_from_counters = 0
 145:             for a, b, c in counters:
 146:                 for v in (a, b, c):
 147:                     if v:
 148:                         try:
 149:                             total_from_counters += int(v)
 150:                         except ValueError:
 151:                             pass
 152:             return {
 153:                 "total_listed": total_listed,
 154:                 "by_extension": {".mp4": mp4, "other": other, "noext": noext},
 155:                 "lines_mp4": mp4,
 156:                 "list_items_total": total_listed,
 157:                 "projects": len(proj_blocks) if proj_blocks else 0,
 158:                 "build_id": build_val,
 159:                 "month": month_val,
 160:                 "total_from_counters": total_from_counters,
 161:             }
 162:         except Exception as e:
 163:             logger.error("analyze_monthly_report_html error", exc_info=True)
 164:             return {"error": str(e)}
 165: 
 166:     @staticmethod
 167:     def _get_jinja_env() -> Environment:
 168:         """Get Jinja2 environment for report templates."""
 169:         template_dir = config.BASE_PATH_SCRIPTS / "templates" / "reports"
 170:         template_dir.mkdir(parents=True, exist_ok=True)
 171:         
 172:         env = Environment(
 173:             loader=FileSystemLoader(str(template_dir)),
 174:             autoescape=select_autoescape(['html', 'xml'])
 175:         )
 176:         
 177:         env.filters['format_duration'] = ReportService._format_duration
 178:         env.filters['format_percentage'] = ReportService._format_percentage
 179:         
 180:         return env
 181:     
 182:     @staticmethod
 183:     def _format_duration(seconds: float) -> str:
 184:         """Format duration in seconds to HH:MM:SS."""
 185:         if not seconds:
 186:             return "00:00:00"
 187:         hours = int(seconds // 3600)
 188:         minutes = int((seconds % 3600) // 60)
 189:         secs = int(seconds % 60)
 190:         return f"{hours:02d}:{minutes:02d}:{secs:02d}"
 191:     
 192:     @staticmethod
 193:     def _format_percentage(value: float, decimals: int = 1) -> str:
 194:         """Format percentage value."""
 195:         return f"{value:.{decimals}f}%"
 196:     
 197:     @staticmethod
 198:     def generate_report(
 199:         project_name: str,
 200:         video_name: str,
 201:         format: str = "html"
 202:     ) -> Dict[str, Any]:
 203:         """
 204:         Generate a visual report for a video.
 205:         
 206:         Args:
 207:             project_name: Name of the project
 208:             video_name: Name of the video file
 209:             format: Output format ('html' or 'pdf')
 210:             
 211:         Returns:
 212:             Dictionary with report data and rendered content
 213:         """
 214:         try:
 215:             timeline_data = VisualizationService.get_project_timeline(project_name, video_name)
 216:             
 217:             if "error" in timeline_data:
 218:                 return {"error": timeline_data["error"]}
 219:             
 220:             stats = ReportService._compute_statistics(timeline_data)
 221:             
 222:             context = {
 223:                 "project_name": project_name,
 224:                 "video_name": video_name,
 225:                 "generated_at": datetime.now(timezone.utc).isoformat(),
 226:                 "metadata": timeline_data.get("metadata", {}),
 227:                 "scenes": timeline_data.get("scenes", {}),
 228:                 "audio": timeline_data.get("audio", {}),
 229:                 "tracking": timeline_data.get("tracking", {}),
 230:                 "archive_probe_source": timeline_data.get("archive_probe_source", {}),
 231:                 "statistics": stats,
 232:             }
 233:             
 234:             env = ReportService._get_jinja_env()
 235:             
 236:             try:
 237:                 template = env.get_template("analysis_report.html")
 238:             except Exception as e:
 239:                 logger.warning(f"Template not found, using fallback: {e}")
 240:                 ReportService._create_default_template()
 241:                 template = env.get_template("analysis_report.html")
 242:             
 243:             html_content = template.render(**context)
 244:             
 245:             result = {
 246:                 "project_name": project_name,
 247:                 "video_name": video_name,
 248:                 "format": "html",
 249:                 "html": html_content,
 250:                 "statistics": stats,
 251:                 "timestamp": datetime.now(timezone.utc).isoformat()
 252:             }
 253:             
 254:             return result
 255:             
 256:         except Exception as e:
 257:             logger.error(f"Error generating report: {e}", exc_info=True)
 258:             return {"error": str(e)}
 259: 
 260:     @staticmethod
 261:     def generate_monthly_archive_report(month: str, format: str = "html") -> Dict[str, Any]:
 262:         """Generate a consolidated report for all archived projects in a given month.
 263: 
 264:         Args:
 265:             month: Target month in 'YYYY-MM' format.
 266:             format: Output format (currently 'html').
 267: 
 268:         Returns:
 269:             Dict with rendered HTML and aggregated statistics, or an error.
 270:         """
 271:         try:
 272:             month = (month or "").strip()
 273:             if (not month) or (len(month) != 7) or (month[4] != '-'):
 274:                 return {"error": f"Invalid month format: '{month}'. Expected YYYY-MM."}
 275: 
 276:             avail = VisualizationService.get_available_projects()
 277:             if avail.get("error"):
 278:                 return {"error": avail["error"]}
 279: 
 280:             projects = avail.get("projects", [])
 281:             selected = [
 282:                 p for p in projects
 283:                 if p.get("source") == "archives" and isinstance(p.get("archive_timestamp"), str)
 284:                 and p["archive_timestamp"].startswith(month)
 285:             ]
 286: 
 287:             if not selected:
 288:                 return {"error": f"No archived projects found for month '{month}'"}
 289: 
 290:             monthly_projects: List[Dict[str, Any]] = []
 291:             totals = {
 292:                 "projects": 0,
 293:                 "videos": 0,
 294:                 "video": {"duration_seconds": 0.0, "total_frames": 0, "fps_weighted_sum": 0.0, "fps_weight": 0.0},
 295:                 "scenes": {"total_count": 0},
 296:                 "audio": {"total_segments": 0, "unique_speakers": set(), "total_speech_duration": 0.0},
 297:                 "tracking": {
 298:                     "frames_with_faces": 0,
 299:                     "frames_with_speaking": 0,
 300:                     "max_faces_detected": 0,
 301:                     "face_coverage_percent_sum": 0.0,
 302:                     "face_coverage_entries": 0
 303:                 }
 304:             }
 305: 
 306:             for proj in selected:
 307:                 pname = proj.get("name")
 308:                 videos_all = proj.get("videos", [])
 309:                 filtered_videos = [
 310:                     v for v in videos_all
 311:                     if isinstance(v, str) and v.lower() not in ReportService._EXCLUDED_ARCHIVE_FILENAMES
 312:                 ]
 313:                 totals["projects"] += 1
 314:                 per_video: List[Dict[str, Any]] = []
 315:                 duration_counts = {
 316:                     "lt_2m": 0,            # < 120s
 317:                     "between_2_5m": 0,     # 120s .. 300s (inclusive)
 318:                     "gt_5m": 0,            # > 300s
 319:                 }
 320:                 duration_names = {
 321:                     "lt_2m": [],
 322:                     "between_2_5m": [],
 323:                     "gt_5m": [],
 324:                 }
 325:                 for video in filtered_videos:
 326:                     tl = VisualizationService.get_project_timeline(pname, video)
 327:                     if "error" in tl:
 328:                         per_video.append({"video_name": video, "error": tl["error"]})
 329:                         continue
 330:                     stats = ReportService._compute_statistics(tl)
 331:                     per_video.append({
 332:                         "video_name": video,
 333:                         "metadata": tl.get("metadata", {}),
 334:                         "statistics": stats,
 335:                         "archive_probe_source": tl.get("archive_probe_source", {})
 336:                     })
 337: 
 338:                     md = stats.get("video", {})
 339:                     totals["video"]["duration_seconds"] += float(md.get("duration_seconds", 0) or 0)
 340:                     totals["video"]["total_frames"] += int(md.get("total_frames", 0) or 0)
 341:                     fps = float(md.get("fps", 0) or 0)
 342:                     if fps > 0 and md.get("duration_seconds", 0):
 343:                         w = float(md.get("duration_seconds", 0) or 0)
 344:                         totals["video"]["fps_weighted_sum"] += fps * w
 345:                         totals["video"]["fps_weight"] += w
 346: 
 347:                     sc = stats.get("scenes", {})
 348:                     totals["scenes"]["total_count"] += int(sc.get("total_count", 0) or 0)
 349: 
 350:                     au = stats.get("audio", {})
 351:                     totals["audio"]["total_segments"] += int(au.get("total_segments", 0) or 0)
 352:                     totals["audio"]["total_speech_duration"] += float(au.get("total_speech_duration", 0) or 0)
 353:                     audio_raw = tl.get("audio", {})
 354:                     for sp in audio_raw.get("unique_speakers", []):
 355:                         totals["audio"]["unique_speakers"].add(sp)
 356: 
 357:                     tr = stats.get("tracking", {})
 358:                     totals["tracking"]["frames_with_faces"] += int(tr.get("frames_with_faces", 0) or 0)
 359:                     totals["tracking"]["frames_with_speaking"] += int(tr.get("frames_with_speaking", 0) or 0)
 360:                     totals["tracking"]["max_faces_detected"] = max(
 361:                         totals["tracking"]["max_faces_detected"], int(tr.get("max_faces_detected", 0) or 0)
 362:                     )
 363:                     if tr.get("face_coverage_percent") is not None:
 364:                         totals["tracking"]["face_coverage_percent_sum"] += float(tr.get("face_coverage_percent", 0) or 0)
 365:                         totals["tracking"]["face_coverage_entries"] += 1
 366: 
 367:                     try:
 368:                         dur = float(md.get("duration_seconds", 0) or 0)
 369:                     except Exception:
 370:                         dur = 0.0
 371:                     if dur < 120.0:
 372:                         duration_counts["lt_2m"] += 1
 373:                         duration_names["lt_2m"].append(video)
 374:                     elif dur <= 300.0:
 375:                         duration_counts["between_2_5m"] += 1
 376:                         duration_names["between_2_5m"].append(video)
 377:                     else:
 378:                         duration_counts["gt_5m"] += 1
 379:                         duration_names["gt_5m"].append(video)
 380: 
 381:                 def _norm_for_merge(s: str) -> str:
 382:                     if not isinstance(s, str):
 383:                         return ""
 384:                     x = unicodedata.normalize("NFKC", s)
 385:                     x = re.sub(r"\s+", " ", x).strip().lower()
 386:                     return x
 387: 
 388:                 def _merge_split_names(names: List[str]) -> List[str]:
 389:                     out: List[str] = []
 390:                     i = 0
 391:                     while i < len(names):
 392:                         cur_raw = names[i]
 393:                         cur = _norm_for_merge(cur_raw)
 394:                         if i + 1 < len(names):
 395:                             nxt_raw = names[i + 1]
 396:                             nxt = _norm_for_merge(nxt_raw)
 397:                             if not cur.endswith('.mp4') and nxt.endswith('.mp4') and nxt.startswith(cur):
 398:                                 out.append(nxt_raw.strip())
 399:                                 i += 2
 400:                                 continue
 401:                         out.append(cur_raw.strip())
 402:                         i += 1
 403:                     seen = set()
 404:                     dedup: List[str] = []
 405:                     for item in out:
 406:                         key = _norm_for_merge(item)
 407:                         if key in seen:
 408:                             continue
 409:                         seen.add(key)
 410:                         dedup.append(item)
 411:                     return dedup
 412: 
 413:                 for k in ("lt_2m", "between_2_5m", "gt_5m"):
 414:                     duration_names[k] = _merge_split_names(duration_names.get(k, []) or [])
 415: 
 416:                 duration_counts["lt_2m"] = len(duration_names.get("lt_2m", []) or [])
 417:                 duration_counts["between_2_5m"] = len(duration_names.get("between_2_5m", []) or [])
 418:                 duration_counts["gt_5m"] = len(duration_names.get("gt_5m", []) or [])
 419: 
 420:                 bucket_sum = (
 421:                     int(duration_counts["lt_2m"]) +
 422:                     int(duration_counts["between_2_5m"]) +
 423:                     int(duration_counts["gt_5m"])
 424:                 )
 425:                 totals["videos"] += bucket_sum
 426: 
 427:                 monthly_projects.append({
 428:                     "name": pname,
 429:                     "display_base": proj.get("display_base", pname),
 430:                     "archive_timestamp": proj.get("archive_timestamp"),
 431:                     "videos": per_video,
 432:                     "video_count": bucket_sum,
 433:                     "duration_counts": duration_counts,
 434:                     "duration_names": duration_names,
 435:                 })
 436: 
 437:             consolidated = {
 438:                 "projects": totals["projects"],
 439:                 "videos": totals["videos"],
 440:                 "video": {
 441:                     "duration_seconds": totals["video"]["duration_seconds"],
 442:                     "total_frames": totals["video"]["total_frames"],
 443:                     "fps": (
 444:                         totals["video"]["fps_weighted_sum"] / totals["video"]["fps_weight"]
 445:                         if totals["video"]["fps_weight"] > 0 else 0
 446:                     ),
 447:                 },
 448:                 "scenes": {"total_count": totals["scenes"]["total_count"]},
 449:                 "audio": {
 450:                     "total_segments": totals["audio"]["total_segments"],
 451:                     "unique_speakers": len(totals["audio"]["unique_speakers"]),
 452:                     "total_speech_duration": totals["audio"]["total_speech_duration"],
 453:                     "speech_coverage_percent": (
 454:                         (totals["audio"]["total_speech_duration"] / totals["video"]["duration_seconds"]) * 100.0
 455:                         if totals["video"]["duration_seconds"] > 0 else 0
 456:                     ),
 457:                 },
 458:                 "tracking": {
 459:                     "frames_with_faces": totals["tracking"]["frames_with_faces"],
 460:                     "frames_with_speaking": totals["tracking"]["frames_with_speaking"],
 461:                     "max_faces_detected": totals["tracking"]["max_faces_detected"],
 462:                     "face_coverage_percent": (
 463:                         totals["tracking"]["face_coverage_percent_sum"] / totals["tracking"]["face_coverage_entries"]
 464:                         if totals["tracking"]["face_coverage_entries"] > 0 else 0
 465:                     ),
 466:                 }
 467:             }
 468: 
 469:             def _norm_filename(name: str) -> str:
 470:                 if not isinstance(name, str):
 471:                     return ""
 472:                 s = unicodedata.normalize("NFKC", name)
 473:                 s = s.strip().lower()
 474:                 s = re.sub(r"\s+", " ", s)
 475:                 return s
 476: 
 477:             total_listed = 0
 478:             mp4_listed = 0
 479:             mp4_distinct: set[str] = set()
 480:             mp4_distinct_stems: set[str] = set()
 481:             for p in monthly_projects:
 482:                 dc = p.get("duration_counts", {}) or {}
 483:                 total_listed += int(dc.get("lt_2m", 0)) + int(dc.get("between_2_5m", 0)) + int(dc.get("gt_5m", 0))
 484:                 dn = p.get("duration_names", {}) or {}
 485:                 for key in ("lt_2m", "between_2_5m", "gt_5m"):
 486:                     for raw in dn.get(key, []) or []:
 487:                         nn = _norm_filename(raw)
 488:                         if nn.endswith(".mp4"):
 489:                             mp4_listed += 1
 490:                             mp4_distinct.add(nn)
 491:                             stem = nn[:-4].strip()
 492:                             mp4_distinct_stems.add(stem)
 493: 
 494:             section_summary = {
 495:                 "total_listed": total_listed,
 496:                 "mp4_listed": mp4_listed,
 497:                 "mp4_distinct": len(mp4_distinct),
 498:                 "mp4_distinct_stems": len(mp4_distinct_stems),
 499:             }
 500: 
 501:             generated_at = datetime.now(timezone.utc).isoformat()
 502:             build_id_source = f"{month}|{generated_at}|{totals['projects']}|{totals['videos']}|{len(monthly_projects)}"
 503:             build_id = hashlib.sha256(build_id_source.encode("utf-8")).hexdigest()[:12]
 504: 
 505:             context = {
 506:                 "month": month,
 507:                 "generated_at": generated_at,
 508:                 "projects": monthly_projects,
 509:                 "consolidated": consolidated,
 510:                 "build_id": build_id,
 511:                 "section_summary": section_summary,
 512:             }
 513: 
 514:             env = ReportService._get_jinja_env()
 515:             try:
 516:                 template = env.get_template("monthly_archive_report.html")
 517:             except Exception:
 518:                 ReportService._create_default_monthly_template()
 519:                 template = env.get_template("monthly_archive_report.html")
 520: 
 521:             html_content = template.render(**context)
 522:             return {
 523:                 "format": "html",
 524:                 "html": html_content,
 525:                 "timestamp": datetime.now(timezone.utc).isoformat(),
 526:                 "month": month,
 527:                 "project_count": consolidated["projects"],
 528:                 "video_count": consolidated["videos"],
 529:             }
 530: 
 531:         except Exception as e:
 532:             logger.error(f"Error generating monthly archive report: {e}", exc_info=True)
 533:             return {"error": str(e)}
 534: 
 535:     @staticmethod
 536:     def _create_default_monthly_template() -> None:
 537:         """Create a minimal monthly report template if not present."""
 538:         try:
 539:             template_dir = config.BASE_PATH_SCRIPTS / "templates" / "reports"
 540:             template_dir.mkdir(parents=True, exist_ok=True)
 541:             target = template_dir / "monthly_archive_report.html"
 542:             if not target.exists():
 543:                 target.write_text(
 544:                     """<!doctype html>
 545: <html lang=\"fr\">
 546: <head>
 547:   <meta charset=\"utf-8\" />
 548:   <title>Rapport Mensuel Archives - {{ month }}</title>
 549:   <style>
 550:     body { font-family: system-ui, -apple-system, Segoe UI, Roboto, Arial, sans-serif; padding: 16px; color: #1a1a1a; }
 551:     h1 { margin: 0 0 12px; font-size: 20px; }
 552:     .meta { color: #666; font-size: 12px; margin-bottom: 16px; }
 553:     .summary { background: #f7f7f7; padding: 12px; border-radius: 8px; margin-bottom: 16px; }
 554:     table { width: 100%; border-collapse: collapse; margin-top: 8px; }
 555:     th, td { border: 1px solid #ddd; padding: 8px; font-size: 13px; }
 556:     th { background: #fafafa; text-align: left; }
 557:     .section { margin-top: 20px; }
 558:     ul { margin: 6px 0 12px 18px; }
 559:     li { margin: 2px 0; }
 560:   </style>
 561:   </head>
 562: <body>
 563:   <h1>Rapport Mensuel Archives — {{ month }}</h1>
 564:   <div class=\"meta\">Généré le {{ generated_at }}</div>
 565:   <div class=\"summary\">
 566:     <div><strong>Projets:</strong> {{ consolidated.projects }}</div>
 567:     <div><strong>Vidéos:</strong> {{ consolidated.videos }}</div>
 568:     <div><strong>Durée totale:</strong> {{ consolidated.video.duration_seconds|format_duration }}</div>
 569:     <div><strong>FPS moyen (pondéré):</strong> {{ consolidated.video.fps|format_percentage(1) }}</div>
 570:   </div>
 571:   <h2>Détails par projet</h2>
 572:   <table>
 573:     <thead>
 574:       <tr><th>Projet</th><th>Horodatage</th><th>Vidéos</th><th>Scènes</th><th>Parole (s)</th><th>Faces (frames)</th></tr>
 575:     </thead>
 576:     <tbody>
 577:       {% for p in projects %}
 578:       <tr>
 579:         <td>{{ p.display_base }}</td>
 580:         <td>{{ p.archive_timestamp or '-' }}</td>
 581:         <td style=\"text-align:right\">{{ p.video_count }}</td>
 582:         <td style=\"text-align:right\">{{ p.videos | sum(attribute='statistics.scenes.total_count') }}</td>
 583:         <td style=\"text-align:right\">{{ p.videos | sum(attribute='statistics.audio.total_speech_duration') | round(1) }}</td>
 584:         <td style=\"text-align:right\">{{ p.videos | sum(attribute='statistics.tracking.frames_with_faces') }}</td>
 585:       </tr>
 586:       {% endfor %}
 587:     </tbody>
 588:   </table>
 589: 
 590:   <div class=\"section\">
 591:     <h2>Répartition des Durées par Projet</h2>
 592:     {% for p in projects %}
 593:       <div>
 594:         <strong>{{ p.display_base }}</strong> :
 595:         <ul>
 596:           <li>moins de 2 minutes : {{ p.duration_counts.lt_2m }}</li>
 597:           <li>entre 2 et 5 minutes : {{ p.duration_counts.between_2_5m }}</li>
 598:           <li>plus de 5 minutes : {{ p.duration_counts.gt_5m }}</li>
 599:         </ul>
 600:       </div>
 601:     {% endfor %}
 602:   </div>
 603: </body>
 604: </html>
 605: """,
 606:                     encoding="utf-8",
 607:                 )
 608:         except Exception:
 609:             pass
 610: 
 611:     @staticmethod
 612:     def generate_project_report(
 613:         project_name: str,
 614:         format: str = "html"
 615:     ) -> Dict[str, Any]:
 616:         """Generate a consolidated report for all videos in a project.
 617: 
 618:         Args:
 619:             project_name: Name of the project
 620:             format: Output format ('html' or 'pdf')
 621: 
 622:         Returns:
 623:             Dictionary with consolidated report data and rendered content
 624:         """
 625:         try:
 626:             projects_info = VisualizationService.get_available_projects()
 627:             if projects_info.get("error"):
 628:                 return {"error": projects_info["error"]}
 629: 
 630:             project_entry = None
 631:             for p in projects_info.get("projects", []):
 632:                 if p.get("name") == project_name:
 633:                     project_entry = p
 634:                     break
 635:             if not project_entry:
 636:                 return {"error": f"Project '{project_name}' not found"}
 637: 
 638:             videos = project_entry.get("videos", [])
 639:             if not videos:
 640:                 return {"error": f"No videos found for project '{project_name}'"}
 641: 
 642:             per_video: List[Dict[str, Any]] = []
 643:             totals = {
 644:                 "video": {"duration_seconds": 0.0, "total_frames": 0, "fps_weighted_sum": 0.0, "fps_weight": 0.0},
 645:                 "scenes": {"total_count": 0},
 646:                 "audio": {"total_segments": 0, "unique_speakers": set(), "total_speech_duration": 0.0},
 647:                 "tracking": {
 648:                     "frames_with_faces": 0,
 649:                     "frames_with_speaking": 0,
 650:                     "max_faces_detected": 0,
 651:                     "face_coverage_percent_sum": 0.0,
 652:                     "face_coverage_entries": 0
 653:                 }
 654:             }
 655: 
 656:             for video in videos:
 657:                 timeline = VisualizationService.get_project_timeline(project_name, video)
 658:                 if "error" in timeline:
 659:                     per_video.append({"video_name": video, "error": timeline["error"]})
 660:                     continue
 661: 
 662:                 stats = ReportService._compute_statistics(timeline)
 663:                 per_video.append({
 664:                     "video_name": video,
 665:                     "metadata": timeline.get("metadata", {}),
 666:                     "statistics": stats,
 667:                     "archive_probe_source": timeline.get("archive_probe_source", {})
 668:                 })
 669: 
 670:                 md = stats.get("video", {})
 671:                 totals["video"]["duration_seconds"] += float(md.get("duration_seconds", 0) or 0)
 672:                 totals["video"]["total_frames"] += int(md.get("total_frames", 0) or 0)
 673:                 fps = float(md.get("fps", 0) or 0)
 674:                 if fps > 0 and md.get("duration_seconds", 0):
 675:                     w = float(md.get("duration_seconds", 0) or 0)
 676:                     totals["video"]["fps_weighted_sum"] += fps * w
 677:                     totals["video"]["fps_weight"] += w
 678: 
 679:                 sc = stats.get("scenes", {})
 680:                 totals["scenes"]["total_count"] += int(sc.get("total_count", 0) or 0)
 681: 
 682:                 au = stats.get("audio", {})
 683:                 totals["audio"]["total_segments"] += int(au.get("total_segments", 0) or 0)
 684:                 totals["audio"]["total_speech_duration"] += float(au.get("total_speech_duration", 0) or 0)
 685:                 audio_raw = timeline.get("audio", {})
 686:                 for sp in audio_raw.get("unique_speakers", []):
 687:                     totals["audio"]["unique_speakers"].add(sp)
 688: 
 689:                 tr = stats.get("tracking", {})
 690:                 totals["tracking"]["frames_with_faces"] += int(tr.get("frames_with_faces", 0) or 0)
 691:                 totals["tracking"]["frames_with_speaking"] += int(tr.get("frames_with_speaking", 0) or 0)
 692:                 totals["tracking"]["max_faces_detected"] = max(
 693:                     totals["tracking"]["max_faces_detected"], int(tr.get("max_faces_detected", 0) or 0)
 694:                 )
 695:                 if tr.get("face_coverage_percent") is not None:
 696:                     totals["tracking"]["face_coverage_percent_sum"] += float(tr.get("face_coverage_percent", 0) or 0)
 697:                     totals["tracking"]["face_coverage_entries"] += 1
 698: 
 699:             consolidated = {
 700:                 "video": {
 701:                     "duration_seconds": totals["video"]["duration_seconds"],
 702:                     "total_frames": totals["video"]["total_frames"],
 703:                     "fps": (
 704:                         totals["video"]["fps_weighted_sum"] / totals["video"]["fps_weight"]
 705:                         if totals["video"]["fps_weight"] > 0 else 0
 706:                     ),
 707:                 },
 708:                 "scenes": {
 709:                     "total_count": totals["scenes"]["total_count"],
 710:                 },
 711:                 "audio": {
 712:                     "total_segments": totals["audio"]["total_segments"],
 713:                     "unique_speakers": len(totals["audio"]["unique_speakers"]),
 714:                     "total_speech_duration": totals["audio"]["total_speech_duration"],
 715:                     "speech_coverage_percent": (
 716:                         (totals["audio"]["total_speech_duration"] / totals["video"]["duration_seconds"]) * 100.0
 717:                         if totals["video"]["duration_seconds"] > 0 else 0
 718:                     ),
 719:                 },
 720:                 "tracking": {
 721:                     "frames_with_faces": totals["tracking"]["frames_with_faces"],
 722:                     "frames_with_speaking": totals["tracking"]["frames_with_speaking"],
 723:                     "max_faces_detected": totals["tracking"]["max_faces_detected"],
 724:                     "face_coverage_percent": (
 725:                         totals["tracking"]["face_coverage_percent_sum"] / totals["tracking"]["face_coverage_entries"]
 726:                         if totals["tracking"]["face_coverage_entries"] > 0 else 0
 727:                     ),
 728:                 }
 729:             }
 730: 
 731:             context = {
 732:                 "project_name": project_name,
 733:                 "generated_at": datetime.now(timezone.utc).isoformat(),
 734:                 "videos": per_video,
 735:                 "consolidated": consolidated,
 736:             }
 737: 
 738:             env = ReportService._get_jinja_env()
 739:             try:
 740:                 template = env.get_template("project_report.html")
 741:             except Exception as e:
 742:                 logger.warning(f"Project template not found, using fallback: {e}")
 743:                 ReportService._create_default_project_template()
 744:                 template = env.get_template("project_report.html")
 745: 
 746:             html_content = template.render(**context)
 747: 
 748:             result = {
 749:                 "project_name": project_name,
 750:                 "format": "html",
 751:                 "html": html_content,
 752:                 "consolidated": consolidated,
 753:                 "video_reports": per_video,
 754:                 "timestamp": datetime.now(timezone.utc).isoformat(),
 755:             }
 756: 
 757:             return result
 758: 
 759:         except Exception as e:
 760:             logger.error(f"Error generating project report: {e}", exc_info=True)
 761:             return {"error": str(e)}
 762:     
 763:     @staticmethod
 764:     def _compute_statistics(timeline_data: Dict[str, Any]) -> Dict[str, Any]:
 765:         """Compute statistics from timeline data."""
 766:         metadata = timeline_data.get("metadata", {})
 767:         scenes = timeline_data.get("scenes", {})
 768:         audio = timeline_data.get("audio", {})
 769:         tracking = timeline_data.get("tracking", {})
 770:         
 771:         stats = {
 772:             "video": {
 773:                 "duration_seconds": metadata.get("duration_seconds", 0),
 774:                 "total_frames": metadata.get("total_frames", 0),
 775:                 "fps": metadata.get("fps", 0),
 776:             },
 777:             "scenes": {
 778:                 "total_count": scenes.get("count", 0),
 779:                 "average_duration": 0,
 780:                 "shortest_duration": 0,
 781:                 "longest_duration": 0,
 782:             },
 783:             "audio": {
 784:                 "total_segments": audio.get("segment_count", 0),
 785:                 "unique_speakers": audio.get("speaker_count", 0),
 786:                 "total_speech_duration": 0,
 787:                 "speech_coverage_percent": 0,
 788:             },
 789:             "tracking": {
 790:                 "frames_with_faces": tracking.get("summary", {}).get("frames_with_faces", 0),
 791:                 "frames_with_speaking": tracking.get("summary", {}).get("frames_with_speaking", 0),
 792:                 "max_faces_detected": tracking.get("summary", {}).get("max_faces_detected", 0),
 793:                 "face_coverage_percent": tracking.get("summary", {}).get("face_coverage_percent", 0),
 794:             }
 795:         }
 796:         
 797:         if scenes.get("available") and scenes.get("scenes"):
 798:             scene_list = scenes["scenes"]
 799:             durations = [
 800:                 s.get("end_time_seconds", 0) - s.get("start_time_seconds", 0)
 801:                 for s in scene_list
 802:             ]
 803:             if durations:
 804:                 stats["scenes"]["average_duration"] = sum(durations) / len(durations)
 805:                 stats["scenes"]["shortest_duration"] = min(durations)
 806:                 stats["scenes"]["longest_duration"] = max(durations)
 807:         
 808:         if audio.get("available") and audio.get("segments"):
 809:             segments = audio["segments"]
 810:             speech_durations = [
 811:                 seg.get("end_time", 0) - seg.get("start_time", 0)
 812:                 for seg in segments
 813:             ]
 814:             if speech_durations:
 815:                 total_speech = sum(speech_durations)
 816:                 stats["audio"]["total_speech_duration"] = total_speech
 817:                 video_duration = metadata.get("duration_seconds", 1)
 818:                 if video_duration > 0:
 819:                     stats["audio"]["speech_coverage_percent"] = (total_speech / video_duration) * 100
 820:         
 821:         return stats
 822:     
 823:     @staticmethod
 824:     def _create_default_template():
 825:         """Create a default report template if none exists."""
 826:         template_dir = config.BASE_PATH_SCRIPTS / "templates" / "reports"
 827:         template_dir.mkdir(parents=True, exist_ok=True)
 828:         
 829:         template_path = template_dir / "analysis_report.html"
 830:         
 831:         if template_path.exists():
 832:             return
 833:         
 834:         default_template = """<!DOCTYPE html>
 835: <html lang="fr">
 836: <head>
 837:     <meta charset="UTF-8">
 838:     <meta name="viewport" content="width=device-width, initial-scale=1.0">
 839:     <title>Rapport d'Analyse - {{ video_name }}</title>
 840:     <style>
 841:         * {
 842:             margin: 0;
 843:             padding: 0;
 844:             box-sizing: border-box;
 845:         }
 846:         
 847:         body {
 848:             font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, sans-serif;
 849:             line-height: 1.6;
 850:             color: #333;
 851:             background: #f5f5f5;
 852:             padding: 20px;
 853:         }
 854:         
 855:         .container {
 856:             max-width: 1200px;
 857:             margin: 0 auto;
 858:             background: white;
 859:             padding: 40px;
 860:             border-radius: 8px;
 861:             box-shadow: 0 2px 10px rgba(0,0,0,0.1);
 862:         }
 863:         
 864:         .header {
 865:             border-bottom: 3px solid #007bff;
 866:             padding-bottom: 20px;
 867:             margin-bottom: 30px;
 868:         }
 869:         
 870:         .header h1 {
 871:             color: #007bff;
 872:             margin-bottom: 10px;
 873:         }
 874:         
 875:         .meta-info {
 876:             color: #666;
 877:             font-size: 14px;
 878:         }
 879:         
 880:         .section {
 881:             margin-bottom: 40px;
 882:         }
 883:         
 884:         .section-title {
 885:             color: #007bff;
 886:             border-left: 4px solid #007bff;
 887:             padding-left: 15px;
 888:             margin-bottom: 20px;
 889:             font-size: 24px;
 890:         }
 891:         
 892:         .stats-grid {
 893:             display: grid;
 894:             grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
 895:             gap: 20px;
 896:             margin-bottom: 20px;
 897:         }
 898:         
 899:         .stat-card {
 900:             background: #f8f9fa;
 901:             padding: 20px;
 902:             border-radius: 8px;
 903:             border-left: 4px solid #007bff;
 904:         }
 905:         
 906:         .stat-label {
 907:             color: #666;
 908:             font-size: 12px;
 909:             text-transform: uppercase;
 910:             letter-spacing: 0.5px;
 911:             margin-bottom: 5px;
 912:         }
 913:         
 914:         .stat-value {
 915:             font-size: 28px;
 916:             font-weight: bold;
 917:             color: #007bff;
 918:         }
 919:         
 920:         .stat-unit {
 921:             font-size: 14px;
 922:             color: #666;
 923:             font-weight: normal;
 924:         }
 925:         
 926:         .badge {
 927:             display: inline-block;
 928:             padding: 4px 12px;
 929:             border-radius: 12px;
 930:             font-size: 12px;
 931:             font-weight: 600;
 932:             margin-right: 10px;
 933:         }
 934:         
 935:         .badge-success {
 936:             background: #28a745;
 937:             color: white;
 938:         }
 939:         
 940:         .badge-warning {
 941:             background: #ffc107;
 942:             color: #333;
 943:         }
 944:         
 945:         .badge-info {
 946:             background: #17a2b8;
 947:             color: white;
 948:         }
 949:         
 950:         .badge-archive {
 951:             background: #6c757d;
 952:             color: white;
 953:         }
 954:         
 955:         .progress-bar {
 956:             width: 100%;
 957:             height: 30px;
 958:             background: #e9ecef;
 959:             border-radius: 15px;
 960:             overflow: hidden;
 961:             margin-top: 10px;
 962:         }
 963:         
 964:         .progress-fill {
 965:             height: 100%;
 966:             background: linear-gradient(90deg, #007bff, #0056b3);
 967:             display: flex;
 968:             align-items: center;
 969:             justify-content: center;
 970:             color: white;
 971:             font-weight: bold;
 972:             font-size: 14px;
 973:         }
 974:         
 975:         .footer {
 976:             margin-top: 40px;
 977:             padding-top: 20px;
 978:             border-top: 1px solid #dee2e6;
 979:             text-align: center;
 980:             color: #666;
 981:             font-size: 14px;
 982:         }
 983:         
 984:         @media print {
 985:             body {
 986:                 background: white;
 987:                 padding: 0;
 988:             }
 989:             .container {
 990:                 box-shadow: none;
 991:             }
 992:         }
 993:     </style>
 994: </head>
 995: <body>
 996:     <div class="container">
 997:         <div class="header">
 998:             <h1>📊 Rapport d'Analyse Vidéo</h1>
 999:             <div class="meta-info">
1000:                 <strong>Projet:</strong> {{ project_name }} | 
1001:                 <strong>Vidéo:</strong> {{ video_name }}<br>
1002:                 <strong>Généré le:</strong> {{ generated_at }}
1003:                 {% if archive_probe_source.metadata.provenance == 'archives' %}
1004:                 <br><span class="badge badge-archive">📦 Données archivées</span>
1005:                 {% endif %}
1006:             </div>
1007:         </div>
1008:         
1009:         <div class="section">
1010:             <h2 class="section-title">📹 Métadonnées Vidéo</h2>
1011:             <div class="stats-grid">
1012:                 <div class="stat-card">
1013:                     <div class="stat-label">Durée</div>
1014:                     <div class="stat-value">{{ statistics.video.duration_seconds|format_duration }}</div>
1015:                 </div>
1016:                 <div class="stat-card">
1017:                     <div class="stat-label">Frames Totales</div>
1018:                     <div class="stat-value">{{ statistics.video.total_frames|int }}</div>
1019:                 </div>
1020:                 <div class="stat-card">
1021:                     <div class="stat-label">FPS</div>
1022:                     <div class="stat-value">{{ statistics.video.fps|round(2) }} <span class="stat-unit">fps</span></div>
1023:                 </div>
1024:             </div>
1025:         </div>
1026:         
1027:         {% if scenes.available %}
1028:         <div class="section">
1029:             <h2 class="section-title">✂️ Analyse des Scènes</h2>
1030:             <span class="badge badge-success">{{ statistics.scenes.total_count }} scènes détectées</span>
1031:             {% if archive_probe_source.scenes.provenance == 'archives' %}
1032:             <span class="badge badge-archive">📦 Données archivées</span>
1033:             {% endif %}
1034:             
1035:             <div class="stats-grid" style="margin-top: 20px;">
1036:                 <div class="stat-card">
1037:                     <div class="stat-label">Durée Moyenne</div>
1038:                     <div class="stat-value">{{ statistics.scenes.average_duration|round(1) }} <span class="stat-unit">s</span></div>
1039:                 </div>
1040:                 <div class="stat-card">
1041:                     <div class="stat-label">Scène la Plus Courte</div>
1042:                     <div class="stat-value">{{ statistics.scenes.shortest_duration|round(1) }} <span class="stat-unit">s</span></div>
1043:                 </div>
1044:                 <div class="stat-card">
1045:                     <div class="stat-label">Scène la Plus Longue</div>
1046:                     <div class="stat-value">{{ statistics.scenes.longest_duration|round(1) }} <span class="stat-unit">s</span></div>
1047:                 </div>
1048:             </div>
1049:         </div>
1050:         {% endif %}
1051:         
1052:         {% if audio.available %}
1053:         <div class="section">
1054:             <h2 class="section-title">🔊 Analyse Audio</h2>
1055:             <span class="badge badge-info">{{ statistics.audio.total_segments }} segments de parole</span>
1056:             <span class="badge badge-info">{{ statistics.audio.unique_speakers }} locuteurs</span>
1057:             {% if archive_probe_source.audio.provenance == 'archives' %}
1058:             <span class="badge badge-archive">📦 Données archivées</span>
1059:             {% endif %}
1060:             
1061:             <div class="stats-grid" style="margin-top: 20px;">
1062:                 <div class="stat-card">
1063:                     <div class="stat-label">Durée Totale de Parole</div>
1064:                     <div class="stat-value">{{ statistics.audio.total_speech_duration|format_duration }}</div>
1065:                 </div>
1066:                 <div class="stat-card">
1067:                     <div class="stat-label">Couverture Audio</div>
1068:                     <div class="stat-value">{{ statistics.audio.speech_coverage_percent|format_percentage }}</div>
1069:                     <div class="progress-bar">
1070:                         <div class="progress-fill" style="width: {{ statistics.audio.speech_coverage_percent }}%">
1071:                             {{ statistics.audio.speech_coverage_percent|format_percentage }}
1072:                         </div>
1073:                     </div>
1074:                 </div>
1075:             </div>
1076:         </div>
1077:         {% endif %}
1078:         
1079:         {% if tracking.available %}
1080:         <div class="section">
1081:             <h2 class="section-title">👤 Suivi des Visages</h2>
1082:             <span class="badge badge-warning">{{ statistics.tracking.max_faces_detected }} visages max détectés</span>
1083:             {% if archive_probe_source.tracking.provenance == 'archives' %}
1084:             <span class="badge badge-archive">📦 Données archivées</span>
1085:             {% endif %}
1086:             
1087:             <div class="stats-grid" style="margin-top: 20px;">
1088:                 <div class="stat-card">
1089:                     <div class="stat-label">Frames avec Visages</div>
1090:                     <div class="stat-value">{{ statistics.tracking.frames_with_faces|int }}</div>
1091:                 </div>
1092:                 <div class="stat-card">
1093:                     <div class="stat-label">Frames avec Parole Détectée</div>
1094:                     <div class="stat-value">{{ statistics.tracking.frames_with_speaking|int }}</div>
1095:                 </div>
1096:                 <div class="stat-card">
1097:                     <div class="stat-label">Couverture Visages</div>
1098:                     <div class="stat-value">{{ statistics.tracking.face_coverage_percent|format_percentage }}</div>
1099:                     <div class="progress-bar">
1100:                         <div class="progress-fill" style="width: {{ statistics.tracking.face_coverage_percent }}%">
1101:                             {{ statistics.tracking.face_coverage_percent|format_percentage }}
1102:                         </div>
1103:                     </div>
1104:                 </div>
1105:             </div>
1106:         </div>
1107:         {% endif %}
1108:         
1109:         <div class="footer">
1110:             <p>Rapport généré par MediaPipe Workflow Analysis System</p>
1111:             <p>{{ generated_at }}</p>
1112:         </div>
1113:     </div>
1114: </body>
1115: </html>"""
1116:         
1117:         with open(template_path, 'w', encoding='utf-8') as f:
1118:             f.write(default_template)
1119:         
1120:         logger.info(f"Created default report template at {template_path}")
1121: 
1122:     @staticmethod
1123:     def _create_default_project_template():
1124:         """Create a default consolidated project report template if none exists."""
1125:         template_dir = config.BASE_PATH_SCRIPTS / "templates" / "reports"
1126:         template_dir.mkdir(parents=True, exist_ok=True)
1127: 
1128:         template_path = template_dir / "project_report.html"
1129:         if template_path.exists():
1130:             return
1131: 
1132:         default_template = """<!DOCTYPE html>
1133: <html lang=\"fr\">
1134: <head>
1135:     <meta charset=\"UTF-8\">
1136:     <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">
1137:     <title>Rapport de Projet - {{ project_name }}</title>
1138:     <style>
1139:         body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, sans-serif; background:#f5f5f5; color:#333; padding:20px; }
1140:         .container { max-width: 1200px; margin:0 auto; background:#fff; border-radius:8px; box-shadow:0 2px 10px rgba(0,0,0,0.1); padding:40px; }
1141:         .header { border-bottom:3px solid #007bff; padding-bottom:16px; margin-bottom:24px; }
1142:         .header h1 { color:#007bff; margin:0 0 6px; }
1143:         .section { margin:28px 0; }
1144:         .section-title { color:#007bff; border-left:4px solid #007bff; padding-left:12px; font-size:22px; margin-bottom:12px; }
1145:         .grid { display:grid; grid-template-columns: repeat(auto-fit, minmax(260px,1fr)); gap:16px; }
1146:         .card { background:#f8f9fa; border-left:4px solid #007bff; border-radius:8px; padding:16px; }
1147:         .badge { display:inline-block; padding:4px 10px; border-radius:12px; font-size:12px; font-weight:600; margin-right:8px; }
1148:         .badge-info { background:#17a2b8; color:#fff; }
1149:         .table { width:100%; border-collapse:collapse; }
1150:         .table th, .table td { border-bottom:1px solid #e9ecef; padding:10px; text-align:left; }
1151:         .table th { background:#f1f3f5; }
1152:         .muted { color:#666; font-size:13px; }
1153:     </style>
1154: </head>
1155: <body>
1156: <div class=\"container\">
1157:   <div class=\"header\">
1158:     <h1>📁 Rapport Consolidé du Projet</h1>
1159:     <div class=\"muted\">Projet: <strong>{{ project_name }}</strong> · Généré le {{ generated_at }}</div>
1160:   </div>
1161: 
1162:   <div class=\"section\">
1163:     <h2 class=\"section-title\">📊 Statistiques Globales</h2>
1164:     <div class=\"grid\">
1165:       <div class=\"card\"><div class=\"muted\">Durée Totale</div><div style=\"font-size:28px;color:#007bff;\">{{ consolidated.video.duration_seconds|format_duration }}</div></div>
1166:       <div class=\"card\"><div class=\"muted\">Frames Totales</div><div style=\"font-size:28px;color:#007bff;\">{{ consolidated.video.total_frames|int }}</div></div>
1167:       <div class=\"card\"><div class=\"muted\">FPS Moyen</div><div style=\"font-size:28px;color:#007bff;\">{{ consolidated.video.fps|round(2) }}</div></div>
1168:       <div class=\"card\"><div class=\"muted\">Scènes Totales</div><div style=\"font-size:28px;color:#007bff;\">{{ consolidated.scenes.total_count|int }}</div></div>
1169:       <div class=\"card\"><div class=\"muted\">Segments de Parole</div><div style=\"font-size:28px;color:#007bff;\">{{ consolidated.audio.total_segments|int }}</div></div>
1170:       <div class=\"card\"><div class=\"muted\">Locuteurs Uniques</div><div style=\"font-size:28px;color:#007bff;\">{{ consolidated.audio.unique_speakers|int }}</div></div>
1171:       <div class=\"card\"><div class=\"muted\">Couverture Parole</div><div style=\"font-size:28px;color:#007bff;\">{{ consolidated.audio.speech_coverage_percent|format_percentage }}</div></div>
1172:       <div class=\"card\"><div class=\"muted\">Frames avec Visages</div><div style=\"font-size:28px;color:#007bff;\">{{ consolidated.tracking.frames_with_faces|int }}</div></div>
1173:     </div>
1174:   </div>
1175: 
1176:   <div class=\"section\">
1177:     <h2 class=\"section-title\">🎬 Détails par Vidéo</h2>
1178:     <table class=\"table\">
1179:       <thead>
1180:         <tr>
1181:           <th>Vidéo</th>
1182:           <th>Durée</th>
1183:           <th>Scènes</th>
1184:           <th>Locuteurs</th>
1185:           <th>Segments</th>
1186:           <th>Faces (max)</th>
1187:           <th>Couverture Visages</th>
1188:           <th>Statut</th>
1189:         </tr>
1190:       </thead>
1191:       <tbody>
1192:         {% for v in videos %}
1193:         <tr>
1194:           <td>{{ v.video_name }}</td>
1195:           {% if v.error %}
1196:             <td colspan=\"6\" class=\"muted\">—</td>
1197:             <td><span class=\"badge badge-info\">Erreur</span> {{ v.error }}</td>
1198:           {% else %}
1199:             <td>{{ v.statistics.video.duration_seconds|format_duration }}</td>
1200:             <td>{{ v.statistics.scenes.total_count|int }}</td>
1201:             <td>{{ v.metadata.get('speaker_count', 0) or v.statistics.audio.unique_speakers|int }}</td>
1202:             <td>{{ v.statistics.audio.total_segments|int }}</td>
1203:             <td>{{ v.statistics.tracking.max_faces_detected|int }}</td>
1204:             <td>{{ v.statistics.tracking.face_coverage_percent|format_percentage }}</td>
1205:             <td><span class=\"badge badge-info\">OK</span></td>
1206:           {% endif %}
1207:         </tr>
1208:         {% endfor %}
1209:       </tbody>
1210:     </table>
1211:   </div>
1212: 
1213:   <div class=\"section\" style=\"text-align:center;color:#666;font-size:13px;\">
1214:     Rapport généré par MediaPipe Workflow Analysis System · {{ generated_at }}
1215:   </div>
1216: </div>
1217: </body>
1218: </html>"""
1219: 
1220:         with open(template_path, "w", encoding="utf-8") as f:
1221:             f.write(default_template)
1222:         logger.info(f"Created default project report template at {template_path}")
```

## File: services/results_archiver.py
```python
  1: """
  2: Results Archiver Service
  3: Ensures persistence of analysis artifacts (scenes CSV, audio JSON, tracking JSON)
  4: across workflow executions (e.g., when step 7 cleans/rebuilds project dirs).
  5: 
  6: Architecture:
  7: - Service-layer only. No routes. Import and use from other services (e.g., VisualizationService).
  8: - Archives structure: {ARCHIVES_DIR}/{project_name}/{video_hash}/
  9:   - {video_stem}_scenes.csv
 10:   - {video_stem}_audio.json
 11:   - {video_stem}_tracking.json
 12:   - metadata.json (source path, created_at)
 13: 
 14: Key design choices:
 15: - Index by strong content hash of the video file to avoid false positives on same names.
 16: - Preserve original filenames (with video_stem prefix) for readability.
 17: - Provide resilient lookup: prefer exact-hash directory, fallback to any existing artifacts by stem.
 18: """
 19: from __future__ import annotations
 20: 
 21: import hashlib
 22: import json
 23: import shutil
 24: from dataclasses import dataclass
 25: from pathlib import Path
 26: from typing import Optional, Tuple
 27: from datetime import datetime, timezone
 28: 
 29: from config.settings import config
 30: import logging
 31: 
 32: logger = logging.getLogger(__name__)
 33: 
 34: SCENES_SUFFIX = "_scenes.csv"
 35: AUDIO_SUFFIX = "_audio.json"
 36: TRACKING_SUFFIX = "_tracking.json"
 37: VIDEO_METADATA_NAME = "video_metadata.json"
 38: 
 39: 
 40: @dataclass
 41: class ArchivePaths:
 42:     project_dir: Path
 43:     video_hash_dir: Path
 44: 
 45: 
 46: class ResultsArchiver:
 47:     """Service that persists and retrieves analysis artifacts for videos."""
 48: 
 49:     # In-process cache that maps base project names to the timestamped archive dir created
 50:     _PROJECT_ARCHIVE_DIRS: dict[str, Path] = {}
 51: 
 52:     @staticmethod
 53:     def _format_timestamp(now: Optional[datetime] = None) -> str:
 54:         """Return a safe timestamp string for directory names: YYYY-MM-DD_HH-MM-SS."""
 55:         dt = now or datetime.now(timezone.utc)
 56:         # Use local-like format without timezone symbols for fs safety
 57:         return dt.strftime("%Y-%m-%d_%H-%M-%S")
 58: 
 59:     @staticmethod
 60:     def _list_matching_project_dirs(base_name: str) -> list[Path]:
 61:         """List archive project directories matching a base project name.
 62: 
 63:         Matches directories named exactly `base_name` or starting with `base_name + ' '`. Sorted descending by name.
 64:         """
 65:         root = config.ARCHIVES_DIR
 66:         if not root.exists():
 67:             return []
 68:         candidates: list[Path] = []
 69:         prefix = f"{base_name} "
 70:         try:
 71:             for d in root.iterdir():
 72:                 if not d.is_dir():
 73:                     continue
 74:                 n = d.name
 75:                 if n == base_name or n.startswith(prefix):
 76:                     candidates.append(d)
 77:         except Exception:
 78:             return []
 79:         # Sort newest-first by name (timestamp suffix sorts lexicographically correctly)
 80:         candidates.sort(key=lambda p: p.name, reverse=True)
 81:         return candidates
 82: 
 83:     @classmethod
 84:     def _get_or_create_archive_project_dir(cls, base_name: str) -> Path:
 85:         """Get or create (once per process) the timestamp-suffixed archive project directory.
 86: 
 87:         Ensures subsequent writes in this process for the same base project go to the same directory.
 88:         """
 89:         if base_name in cls._PROJECT_ARCHIVE_DIRS:
 90:             return cls._PROJECT_ARCHIVE_DIRS[base_name]
 91:         ts = ResultsArchiver._format_timestamp()
 92:         proj_dir = config.ARCHIVES_DIR / f"{base_name} {ts}"
 93:         proj_dir.mkdir(parents=True, exist_ok=True)
 94:         cls._PROJECT_ARCHIVE_DIRS[base_name] = proj_dir
 95:         return proj_dir
 96: 
 97:     @staticmethod
 98:     def compute_video_hash(video_path: Path, chunk_size: int = 1024 * 1024) -> Optional[str]:
 99:         """Compute SHA256 hash of a video file.
100:         Args:
101:             video_path: Absolute path to video file.
102:             chunk_size: Read chunk size.
103:         Returns:
104:             Hex digest string or None on failure.
105:         """
106:         try:
107:             h = hashlib.sha256()
108:             with open(video_path, 'rb') as f:
109:                 while True:
110:                     chunk = f.read(chunk_size)
111:                     if not chunk:
112:                         break
113:                     h.update(chunk)
114:             return h.hexdigest()
115:         except Exception as e:
116:             logger.warning(f"Failed to compute hash for {video_path}: {e}")
117:             return None
118: 
119:     @staticmethod
120:     def archive_project_analysis(project_name: str) -> dict:
121:         """Scan the project directory for analysis artifacts and persist them to archives.
122: 
123:         For each video found under PROJECTS_DIR/<project_name>, attempts to copy
124:         any matching *_scenes.csv, *_audio.json, *_tracking.json into the archives
125:         hashed directory.
126: 
127:         Returns:
128:             dict summary: {"project_name": str, "processed": int, "copied": int, "details": [...]}
129:         """
130:         details = []
131:         copied_count = 0
132:         processed = 0
133:         try:
134:             base_path = getattr(config, 'PROJECTS_DIR', None) or (config.BASE_PATH_SCRIPTS / "projets_extraits")
135:             project_path = base_path / project_name
136:             if not project_path.exists():
137:                 return {"error": f"Project folder not found: {project_path}"}
138: 
139:             # Enumerate candidate video files (stems) and search artifacts alongside
140:             video_extensions = ('.mp4', '.mov', '.avi', '.mkv', '.webm')
141:             video_files = []
142:             for ext in video_extensions:
143:                 video_files.extend(project_path.rglob(f"*{ext}"))
144: 
145:             for v in video_files:
146:                 processed += 1
147:                 stem = v.stem
148:                 scenes = next(project_path.rglob(f"{stem}{SCENES_SUFFIX}"), None)
149:                 if scenes is None:
150:                     # Fallback: plain CSV (e.g., '<stem>.csv')
151:                     scenes = next(project_path.rglob(f"{stem}.csv"), None)
152:                 audio = next(project_path.rglob(f"{stem}{AUDIO_SUFFIX}"), None)
153:                 tracking = next(project_path.rglob(f"{stem}{TRACKING_SUFFIX}"), None)
154:                 if tracking is None:
155:                     # Fallback: plain JSON (e.g., '<stem>.json')
156:                     tracking = next(project_path.rglob(f"{stem}.json"), None)
157: 
158:                 archived_dir = ResultsArchiver.archive_analysis_files(
159:                     project_name, v, scenes_file=scenes, audio_file=audio, tracking_file=tracking
160:                 )
161:                 copied = {
162:                     "scenes": bool(scenes),
163:                     "audio": bool(audio),
164:                     "tracking": bool(tracking),
165:                     "archived": bool(archived_dir),
166:                 }
167:                 if archived_dir and any([scenes, audio, tracking]):
168:                     copied_count += 1
169:                 details.append({
170:                     "video": str(v.relative_to(project_path)),
171:                     "copied": copied,
172:                     "archive_dir": str(archived_dir) if archived_dir else None,
173:                 })
174: 
175:             return {
176:                 "project_name": project_name,
177:                 "processed": processed,
178:                 "copied": copied_count,
179:                 "details": details,
180:             }
181:         except Exception as e:
182:             logger.error(f"Error archiving project analysis for {project_name}: {e}")
183:             return {"error": str(e)}
184: 
185:     @staticmethod
186:     def save_video_metadata(project_name: str, video_path: Path, metadata: dict) -> Optional[Path]:
187:         """Persist computed video metadata into the archive folder.
188:         Writes video_metadata.json alongside metadata.json with a created_at.
189:         """
190:         try:
191:             video_hash = ResultsArchiver.compute_video_hash(video_path)
192:             if not video_hash:
193:                 return None
194:             # Write into the unique (timestamped) archive project directory
195:             ap = ResultsArchiver.get_archive_paths(project_name, video_hash, create=True)
196:             ap.video_hash_dir.mkdir(parents=True, exist_ok=True)
197:             payload = {
198:                 "created_at": datetime.now(timezone.utc).isoformat(),
199:                 "metadata": metadata,
200:             }
201:             out_path = ap.video_hash_dir / VIDEO_METADATA_NAME
202:             with open(out_path, "w", encoding="utf-8") as f:
203:                 json.dump(payload, f, ensure_ascii=False, indent=2)
204:             return out_path
205:         except Exception as e:
206:             logger.warning(f"Failed to save video metadata archive: {e}")
207:             return None
208: 
209:     @staticmethod
210:     def load_video_metadata(project_name: str, video_path: Path) -> Optional[dict]:
211:         """Load archived video metadata if present. Returns dict with keys
212:         {created_at: str, metadata: {...}} or None.
213:         """
214:         try:
215:             video_hash = ResultsArchiver.compute_video_hash(video_path)
216:             # If hash not computable (video missing), try any matching stem folder
217:             if video_hash:
218:                 ap = ResultsArchiver.get_archive_paths(project_name, video_hash, create=False)
219:                 p = ap.video_hash_dir / VIDEO_METADATA_NAME
220:                 if p.exists():
221:                     with open(p, "r", encoding="utf-8") as f:
222:                         return json.load(f)
223:             # Fallback by stem across matching suffixed project dirs
224:             matches = ResultsArchiver._list_matching_project_dirs(project_name)
225:             if not matches:
226:                 return None
227:             stem = video_path.stem
228:             for arch_proj in matches:
229:                 for meta_path in arch_proj.rglob(VIDEO_METADATA_NAME):
230:                     try:
231:                         # ensure sibling files match stem
232:                         sibling = meta_path.parent
233:                         any_match = any(
234:                             q.stem.startswith(stem) and (
235:                                 q.name.endswith(SCENES_SUFFIX) or q.name.endswith(AUDIO_SUFFIX) or q.name.endswith(TRACKING_SUFFIX)
236:                             ) for q in sibling.iterdir()
237:                         )
238:                         if any_match:
239:                             with open(meta_path, "r", encoding="utf-8") as f:
240:                                 return json.load(f)
241:                     except Exception:
242:                         continue
243:         except Exception:
244:             pass
245:         return None
246: 
247:     @staticmethod
248:     def get_archive_paths(project_name: str, video_hash: str, create: bool = False) -> ArchivePaths:
249:         """Resolve archive paths for a project/video hash.
250: 
251:         - When create=True, use or create the timestamp-suffixed archive dir for this process.
252:         - When create=False, resolve the most recent matching archive dir (exact or suffixed), fallback to base name.
253:         """
254:         if create:
255:             project_dir = ResultsArchiver._get_or_create_archive_project_dir(project_name)
256:         else:
257:             matches = ResultsArchiver._list_matching_project_dirs(project_name)
258:             project_dir = matches[0] if matches else (config.ARCHIVES_DIR / project_name)
259:         video_hash_dir = project_dir / video_hash
260:         return ArchivePaths(project_dir=project_dir, video_hash_dir=video_hash_dir)
261: 
262:     @staticmethod
263:     def archive_analysis_files(
264:         project_name: str,
265:         video_path: Path,
266:         scenes_file: Optional[Path] = None,
267:         audio_file: Optional[Path] = None,
268:         tracking_file: Optional[Path] = None,
269:     ) -> Optional[Path]:
270:         """Copy available analysis files into the archive, keyed by video hash.
271:         Returns the archive directory on success or None if unsupported.
272:         """
273:         try:
274:             video_hash = ResultsArchiver.compute_video_hash(video_path)
275:             if not video_hash:
276:                 return None
277:             ap = ResultsArchiver.get_archive_paths(project_name, video_hash, create=True)
278:             ap.video_hash_dir.mkdir(parents=True, exist_ok=True)
279:             video_stem = video_path.stem
280: 
281:             def _copy(src: Optional[Path], suffix: str):
282:                 if src and src.exists():
283:                     dst = ap.video_hash_dir / f"{video_stem}{suffix}"
284:                     try:
285:                         shutil.copy2(src, dst)
286:                         return str(dst)
287:                     except Exception as e:
288:                         logger.warning(f"Failed to archive {src} -> {dst}: {e}")
289:                 return None
290: 
291:             copied = {
292:                 "scenes": _copy(scenes_file, SCENES_SUFFIX),
293:                 "audio": _copy(audio_file, AUDIO_SUFFIX),
294:                 "tracking": _copy(tracking_file, TRACKING_SUFFIX),
295:             }
296: 
297:             # metadata
298:             meta = {
299:                 "video_path": str(video_path),
300:                 "video_stem": video_stem,
301:                 "video_hash": video_hash,
302:                 "created_at": datetime.now(timezone.utc).isoformat(),
303:                 "copied": copied,
304:             }
305:             with open(ap.video_hash_dir / "metadata.json", "w", encoding="utf-8") as f:
306:                 json.dump(meta, f, ensure_ascii=False, indent=2)
307: 
308:             return ap.video_hash_dir
309:         except Exception as e:
310:             logger.error(f"Error archiving analysis files: {e}")
311:             return None
312: 
313:     @staticmethod
314:     def find_analysis_file(
315:         project_name: str,
316:         video_path: Path,
317:         suffix: str,
318:     ) -> Optional[Path]:
319:         """Resolve an analysis file from archives for this video.
320:         Prefers exact hash dir; falls back to any file matching the stem in project archives.
321:         """
322:         try:
323:             video_hash = ResultsArchiver.compute_video_hash(video_path)
324:             video_stem = video_path.stem
325: 
326:             # Try exact-hash directory
327:             if video_hash:
328:                 ap = ResultsArchiver.get_archive_paths(project_name, video_hash)
329:                 candidate = ap.video_hash_dir / f"{video_stem}{suffix}"
330:                 if candidate.exists():
331:                     return candidate
332: 
333:             # Fallback: search by stem within project archive
334:             matches = ResultsArchiver._list_matching_project_dirs(project_name)
335:             for project_arch in matches:
336:                 for p in project_arch.rglob(f"{video_stem}{suffix}"):
337:                     return p
338:         except Exception:
339:             pass
340:         return None
341: 
342:     @staticmethod
343:     def project_has_analysis(project_name: str) -> Tuple[bool, bool, bool]:
344:         """Check if archives contain any analysis for this project.
345:         Returns tuple (has_scenes, has_audio, has_tracking).
346:         """
347:         matches = ResultsArchiver._list_matching_project_dirs(project_name)
348:         if not matches:
349:             return (False, False, False)
350:         has_scenes = False
351:         has_audio = False
352:         has_tracking = False
353:         for base in matches:
354:             if not has_scenes:
355:                 has_scenes = any(base.rglob(f"*{SCENES_SUFFIX}"))
356:             if not has_audio:
357:                 has_audio = any(base.rglob(f"*{AUDIO_SUFFIX}"))
358:             if not has_tracking:
359:                 has_tracking = any(base.rglob(f"*{TRACKING_SUFFIX}"))
360:             if has_scenes and has_audio and has_tracking:
361:                 break
362:         return (has_scenes, has_audio, has_tracking)
```

## File: services/visualization_service.py
```python
  1: """
  2: Visualization Service
  3: Aggregates and processes data from workflow steps for timeline visualization.
  4: Handles scene detection (Step 3), audio analysis (Step 4), and video tracking (Step 5).
  5: """
  6: 
  7: import logging
  8: import json
  9: import csv
 10: import subprocess
 11: import re
 12: from pathlib import Path
 13: from typing import Dict, Any, List, Optional, Tuple
 14: from datetime import datetime, timezone
 15: from config.settings import config
 16: from services.results_archiver import ResultsArchiver, SCENES_SUFFIX, AUDIO_SUFFIX, TRACKING_SUFFIX
 17: 
 18: logger = logging.getLogger(__name__)
 19: 
 20: class VisualizationService:
 21:     """
 22:     Service for aggregating and processing workflow results for visualization.
 23:     Combines data from multiple steps to create unified timeline data.
 24:     """
 25:     
 26:     @staticmethod
 27:     def get_available_projects() -> Dict[str, Any]:
 28:         """
 29:         Scan for completed projects with visualization data.
 30:         
 31:         Returns:
 32:             Dictionary with available projects:
 33:             {
 34:                 "projects": [
 35:                     {
 36:                         "name": str,
 37:                         "path": str,
 38:                         "videos": [str],
 39:                         "has_scenes": bool,
 40:                         "has_audio": bool,
 41:                         "has_tracking": bool
 42:                     }
 43:                 ],
 44:                 "count": int
 45:             }
 46:         """
 47:         try:
 48:             # Use centralized configuration to resolve project root and archives
 49:             base_path = getattr(config, 'PROJECTS_DIR', None) or (config.BASE_PATH_SCRIPTS / "projets_extraits")
 50:             archives_root = getattr(config, 'ARCHIVES_DIR', None) or (config.BASE_PATH_SCRIPTS / "archives")
 51:             projects_index: Dict[str, Dict[str, Any]] = {}
 52: 
 53:             def _parse_archive_name(name: str) -> Tuple[str, Optional[str]]:
 54:                 """Return (base_name, archive_timestamp) if name matches '<base> YYYY-MM-DD_HH-MM-SS'.
 55:                 archive_timestamp is in 'YYYY-MM-DD HH:MM:SS' human-readable format.
 56:                 """
 57:                 try:
 58:                     m = re.match(r"^(.*) (\d{4}-\d{2}-\d{2})_(\d{2}-\d{2}-\d{2})$", name)
 59:                     if not m:
 60:                         return name, None
 61:                     base = m.group(1)
 62:                     date_part = m.group(2)
 63:                     time_part = m.group(3).replace('-', ':')
 64:                     return base, f"{date_part} {time_part}"
 65:                 except Exception:
 66:                     return name, None
 67:             
 68:             # Scan current projects directory (may be empty after Step 7)
 69:             if base_path.exists():
 70:                 for project_dir in base_path.iterdir():
 71:                     if not project_dir.is_dir():
 72:                         continue
 73:                     base_name, ts = _parse_archive_name(project_dir.name)
 74:                     projects_index[project_dir.name] = {
 75:                         "name": project_dir.name,
 76:                         "path": str(project_dir),
 77:                         "videos": [],
 78:                         "has_scenes": False,
 79:                         "has_audio": False,
 80:                         "has_tracking": False,
 81:                         "video_count": 0,
 82:                         "source": "projects",
 83:                         "display_base": base_name,
 84:                         "archive_timestamp": ts,
 85:                     }
 86: 
 87:             # Populate details for projects directory
 88:             for name, proj in list(projects_index.items()):
 89:                 project_dir = Path(proj["path"])
 90:                 # Find videos in project
 91:                 videos: List[str] = []
 92:                 video_extensions = ('.mp4', '.mov', '.avi', '.mkv', '.webm')
 93:                 for ext in video_extensions:
 94:                     videos.extend([str(v.relative_to(project_dir)) for v in project_dir.rglob(f'*{ext}')])
 95:                 has_scenes = any(project_dir.rglob('*_scenes.csv'))
 96:                 has_audio = any(project_dir.rglob('*_audio.json'))
 97:                 has_tracking = any(project_dir.rglob('*_tracking.json'))
 98:                 arch_s, arch_a, arch_t = ResultsArchiver.project_has_analysis(name)
 99:                 proj.update({
100:                     "videos": videos,
101:                     "has_scenes": has_scenes or arch_s,
102:                     "has_audio": has_audio or arch_a,
103:                     "has_tracking": has_tracking or arch_t,
104:                     "video_count": len(videos)
105:                 })
106: 
107:             # Also include archived-only projects (no current dir present)
108:             if archives_root.exists():
109:                 for arch_proj_dir in archives_root.iterdir():
110:                     if not arch_proj_dir.is_dir():
111:                         continue
112:                     name = arch_proj_dir.name
113:                     base_name, ts = _parse_archive_name(name)
114:                     if name not in projects_index:
115:                         # Derive original filenames (with extensions when possible) from archived metadata
116:                         stems: set[str] = set()
117:                         videos_map: dict[str, str] = {}
118:                         # Prefer metadata.json inside each video hash directory (contains original video_path)
119:                         for meta_path in arch_proj_dir.rglob('metadata.json'):
120:                             try:
121:                                 with open(meta_path, 'r', encoding='utf-8') as mf:
122:                                     meta = json.load(mf)
123:                                 stem = str(meta.get('video_stem') or '').strip()
124:                                 video_path_str = meta.get('video_path')
125:                                 if not stem:
126:                                     # Fallback: infer stem from sibling analysis files
127:                                     sibling_files = list(meta_path.parent.glob('*'))
128:                                     for q in sibling_files:
129:                                         qn = q.name
130:                                         if qn.endswith(SCENES_SUFFIX):
131:                                             stem = q.stem.replace('_scenes', '')
132:                                             break
133:                                         if qn.endswith(AUDIO_SUFFIX):
134:                                             stem = q.stem.replace('_audio', '')
135:                                             break
136:                                         if qn.endswith(TRACKING_SUFFIX):
137:                                             stem = q.stem.replace('_tracking', '')
138:                                             break
139:                                 if stem:
140:                                     if isinstance(video_path_str, str) and video_path_str:
141:                                         try:
142:                                             videos_map[stem] = Path(video_path_str).name
143:                                         except Exception:
144:                                             videos_map[stem] = stem
145:                                     else:
146:                                         videos_map[stem] = stem
147:                             except Exception:
148:                                 continue
149:                         # Also scan artifacts in case some entries miss metadata; fill gaps with stems
150:                         for p in arch_proj_dir.rglob(f'*{SCENES_SUFFIX}'):
151:                             stems.add(p.stem.replace('_scenes', ''))
152:                         for p in arch_proj_dir.rglob(f'*{AUDIO_SUFFIX}'):
153:                             stems.add(p.stem.replace('_audio', ''))
154:                         for p in arch_proj_dir.rglob(f'*{TRACKING_SUFFIX}'):
155:                             stems.add(p.stem.replace('_tracking', ''))
156:                         for s in stems:
157:                             videos_map.setdefault(s, s)
158:                         projects_index[name] = {
159:                             "name": name,
160:                             "path": str(base_path / name),  # expected original path
161:                             # Use best-known filename (with extension) when available
162:                             "videos": sorted(list(videos_map.values())),
163:                             "has_scenes": any(arch_proj_dir.rglob('*_scenes.csv')),
164:                             "has_audio": any(arch_proj_dir.rglob('*_audio.json')),
165:                             "has_tracking": any(arch_proj_dir.rglob('*_tracking.json')),
166:                             "video_count": len(videos_map),
167:                             "source": "archives",
168:                             "display_base": base_name,
169:                             "archive_timestamp": ts,
170:                         }
171:                     else:
172:                         # If same name was present from projects/, enrich with timestamp if any
173:                         proj = projects_index[name]
174:                         proj.setdefault("display_base", base_name)
175:                         proj.setdefault("archive_timestamp", ts)
176: 
177:             projects = list(projects_index.values())
178:             # Sort by name
179:             projects.sort(key=lambda p: p["name"])
180:             
181:             return {
182:                 "projects": projects,
183:                 "count": len(projects),
184:                 "timestamp": datetime.now(timezone.utc).isoformat()
185:             }
186:             
187:         except Exception as e:
188:             logger.error(f"Error scanning projects: {e}")
189:             return {
190:                 "projects": [],
191:                 "count": 0,
192:                 "error": str(e)
193:             }
194:     
195:     @staticmethod
196:     def get_project_timeline(project_name: str, video_name: str) -> Dict[str, Any]:
197:         """
198:         Get aggregated timeline data for a specific video in a project.
199:         
200:         Args:
201:             project_name: Name of the project directory
202:             video_name: Name of the video file
203:             
204:         Returns:
205:             Timeline data with scenes, audio, and tracking information
206:         """
207:         try:
208:             # Use centralized configuration to resolve project root
209:             base_path = getattr(config, 'PROJECTS_DIR', None) or (config.BASE_PATH_SCRIPTS / "projets_extraits")
210:             project_path = base_path / project_name
211:             project_exists = project_path.exists()
212:             
213:             # video_name may be a relative path or an archive stem when the project no longer exists
214:             video_path = project_path / video_name
215:             video_exists = video_path.exists()
216:             
217:             # If project/video do not exist (post Step 7), infer stem and continue with archives only
218:             video_stem = (video_path.stem if video_exists else Path(video_name).stem)
219:             
220:             # Load data (project dir first, then archives fallback)
221:             scenes_data = VisualizationService._load_scenes_data(project_name, project_path, video_path, video_stem)
222:             audio_data = VisualizationService._load_audio_data(project_name, project_path, video_path, video_stem)
223:             tracking_data = VisualizationService._load_tracking_data(project_name, project_path, video_path, video_stem)
224: 
225:             # Persist any fresh analysis files found in the project into archives
226:             try:
227:                 scenes_src = scenes_data.get("source_path") if scenes_data.get("available") else None
228:                 audio_src = audio_data.get("source_path") if audio_data.get("available") else None
229:                 tracking_src = tracking_data.get("source_path") if tracking_data.get("available") else None
230:                 # Only archive from project-origin sources
231:                 def _is_project_src(p: Optional[str]) -> Optional[Path]:
232:                     if not p:
233:                         return None
234:                     pp = Path(p)
235:                     try:
236:                         if project_exists:
237:                             pp.relative_to(project_path)
238:                         return pp
239:                     except Exception:
240:                         return None
241:                 ResultsArchiver.archive_analysis_files(
242:                     project_name,
243:                     video_path,
244:                     scenes_file=_is_project_src(scenes_src),
245:                     audio_file=_is_project_src(audio_src),
246:                     tracking_file=_is_project_src(tracking_src),
247:                 )
248:             except Exception:
249:                 pass
250:             
251:             # Get video metadata
252:             metadata = VisualizationService._get_video_metadata(
253:                 video_path, scenes_data, audio_data, tracking_data
254:             )
255: 
256:             # Save video metadata to archives when we can compute it from an existing video
257:             try:
258:                 if video_exists and metadata:
259:                     ResultsArchiver.save_video_metadata(project_name, video_path, metadata)
260:             except Exception:
261:                 pass
262: 
263:             # Try to load archived video metadata to extract created_at
264:             metadata_archive_created_at = None
265:             try:
266:                 vm = ResultsArchiver.load_video_metadata(project_name, video_path)
267:                 if vm and isinstance(vm, dict):
268:                     metadata_archive_created_at = vm.get('created_at')
269:             except Exception:
270:                 pass
271:             
272:             return {
273:                 "project_name": project_name,
274:                 "video_name": video_name,
275:                 "metadata": metadata,
276:                 "scenes": scenes_data,
277:                 "audio": audio_data,
278:                 "tracking": tracking_data,
279:                 "archive_probe_source": {
280:                     "metadata": {
281:                         "provenance": "project" if video_exists else "archives",
282:                         "created_at": metadata_archive_created_at
283:                     },
284:                     "scenes": {
285:                         "provenance": scenes_data.get("provenance", "unknown"),
286:                         "created_at": scenes_data.get("archive_created_at")
287:                     },
288:                     "audio": {
289:                         "provenance": audio_data.get("provenance", "unknown"),
290:                         "created_at": audio_data.get("archive_created_at")
291:                     },
292:                     "tracking": {
293:                         "provenance": tracking_data.get("provenance", "unknown"),
294:                         "created_at": tracking_data.get("archive_created_at")
295:                     }
296:                 },
297:                 "timestamp": datetime.now(timezone.utc).isoformat()
298:             }
299:             
300:         except Exception as e:
301:             logger.error(f"Error loading timeline for {project_name}/{video_name}: {e}")
302:             return {"error": str(e)}
303: 
304:     @staticmethod
305:     def get_project_diagnostics(project_name: str) -> Dict[str, Any]:
306:         """Return per-video diagnostics for a project: availability and provenance of analysis data.
307: 
308:         Args:
309:             project_name: Project name
310: 
311:         Returns:
312:             Dict with list of videos and their data availability/provenance and key metrics.
313:         """
314:         try:
315:             projects = VisualizationService.get_available_projects()
316:             proj = next((p for p in projects.get("projects", []) if p.get("name") == project_name), None)
317:             if not proj:
318:                 return {"error": f"Project '{project_name}' not found"}
319: 
320:             results = []
321:             for video in proj.get("videos", []):
322:                 tl = VisualizationService.get_project_timeline(project_name, video)
323:                 if "error" in tl:
324:                     results.append({
325:                         "video_name": video,
326:                         "error": tl.get("error"),
327:                     })
328:                     continue
329:                 md = tl.get("metadata", {})
330:                 sc = tl.get("scenes", {})
331:                 au = tl.get("audio", {})
332:                 tr = tl.get("tracking", {})
333:                 ap = tl.get("archive_probe_source", {})
334:                 results.append({
335:                     "video_name": video,
336:                     "availability": {
337:                         "metadata": bool(md),
338:                         "scenes": sc.get("available", False),
339:                         "audio": au.get("available", False),
340:                         "tracking": tr.get("available", False),
341:                     },
342:                     "provenance": {
343:                         "scenes": sc.get("provenance"),
344:                         "audio": au.get("provenance"),
345:                         "tracking": tr.get("provenance"),
346:                         "metadata": (ap.get("metadata") or {}).get("provenance"),
347:                     },
348:                     "metrics": {
349:                         "fps": md.get("fps"),
350:                         "total_frames": md.get("total_frames"),
351:                         "duration_seconds": md.get("duration_seconds"),
352:                         "scene_count": md.get("scene_count"),
353:                         "speech_segment_count": md.get("speech_segment_count"),
354:                         "face_coverage": md.get("face_coverage"),
355:                     }
356:                 })
357: 
358:             return {
359:                 "project_name": project_name,
360:                 "count": len(results),
361:                 "videos": results,
362:                 "timestamp": datetime.now(timezone.utc).isoformat(),
363:             }
364:         except Exception as e:
365:             logger.error(f"Diagnostics error for project {project_name}: {e}")
366:             return {"error": str(e)}
367:     
368:     @staticmethod
369:     def _load_scenes_data(project_name: str, project_path: Path, video_path: Path, video_stem: str) -> Dict[str, Any]:
370:         """Load scene detection data from CSV."""
371:         # Search recursively to support nested outputs
372:         scenes_file = next(project_path.rglob(f"{video_stem}{SCENES_SUFFIX}"), None)
373:         # Fallback: some pipelines save scenes as plain '<stem>.csv'
374:         if scenes_file is None:
375:             scenes_file = next(project_path.rglob(f"{video_stem}.csv"), None)
376:         
377:         provenance = "project"
378:         archive_created_at = None
379:         # Fallback to archives
380:         if scenes_file is None or not scenes_file.exists():
381:             scenes_file = ResultsArchiver.find_analysis_file(project_name, video_path, SCENES_SUFFIX)
382:             if scenes_file is None:
383:                 return {"available": False, "scenes": []}
384:             provenance, archive_created_at = VisualizationService._provenance_info(Path(scenes_file))
385:         else:
386:             provenance = "project"
387:         
388:         try:
389:             scenes = []
390:             with open(scenes_file, 'r', encoding='utf-8') as f:
391:                 reader = csv.DictReader(f)
392:                 for row in reader:
393:                     scenes.append({
394:                         "scene_number": int(row.get("Scene Number", 0)),
395:                         "start_frame": int(row.get("Start Frame", 0)),
396:                         "end_frame": int(row.get("End Frame", 0)),
397:                         "start_timecode": row.get("Start Timecode", "00:00:00.000"),
398:                         "end_timecode": row.get("End Timecode", "00:00:00.000"),
399:                         "start_time_seconds": VisualizationService._timecode_to_seconds(
400:                             row.get("Start Timecode", "00:00:00.000")
401:                         ),
402:                         "end_time_seconds": VisualizationService._timecode_to_seconds(
403:                             row.get("End Timecode", "00:00:00.000")
404:                         )
405:                     })
406:             
407:             return {
408:                 "available": True,
409:                 "count": len(scenes),
410:                 "scenes": scenes,
411:                 "source_path": str(scenes_file),
412:                 "provenance": provenance,
413:                 "archive_created_at": archive_created_at
414:             }
415:             
416:         except Exception as e:
417:             logger.error(f"Error loading scenes data: {e}")
418:             return {"available": False, "error": str(e), "scenes": []}
419:     
420:     @staticmethod
421:     def _load_audio_data(project_name: str, project_path: Path, video_path: Path, video_stem: str) -> Dict[str, Any]:
422:         """Load audio analysis data from JSON."""
423:         # Search recursively to support nested outputs
424:         audio_file = next(project_path.rglob(f"{video_stem}{AUDIO_SUFFIX}"), None)
425:         
426:         provenance = "project"
427:         archive_created_at = None
428:         # Fallback to archives
429:         if audio_file is None or not audio_file.exists():
430:             audio_file = ResultsArchiver.find_analysis_file(project_name, video_path, AUDIO_SUFFIX)
431:             if audio_file is None:
432:                 return {"available": False, "segments": []}
433:             provenance, archive_created_at = VisualizationService._provenance_info(Path(audio_file))
434:         else:
435:             provenance = "project"
436:         
437:         try:
438:             with open(audio_file, 'r', encoding='utf-8') as f:
439:                 data = json.load(f)
440:             
441:             # Extract speech segments
442:             segments = []
443:             fps = data.get("fps", 25.0)
444:             max_frame_seen = 0
445:             
446:             # Group consecutive speech frames
447:             current_segment = None
448:             
449:             for frame_data in data.get("frames_analysis", []):
450:                 frame = frame_data.get("frame", 0)
451:                 audio_info = frame_data.get("audio_info", {})
452:                 is_speech = audio_info.get("is_speech_present", False)
453:                 speakers = audio_info.get("active_speaker_labels", [])
454:                 time_sec = audio_info.get("timecode_sec", (frame - 1) / fps)
455:                 if isinstance(frame, (int, float)):
456:                     try:
457:                         max_frame_seen = max(max_frame_seen, int(frame))
458:                     except Exception:
459:                         pass
460:                 
461:                 if is_speech:
462:                     if current_segment is None:
463:                         current_segment = {
464:                             "start_frame": frame,
465:                             "start_time": time_sec,
466:                             "speakers": set(speakers)
467:                         }
468:                     else:
469:                         current_segment["speakers"].update(speakers)
470:                 else:
471:                     if current_segment is not None:
472:                         # End current segment
473:                         segments.append({
474:                             "start_frame": current_segment["start_frame"],
475:                             "end_frame": frame - 1,
476:                             "start_time": current_segment["start_time"],
477:                             "end_time": time_sec,
478:                             "speakers": sorted(list(current_segment["speakers"])),
479:                             "speaker_count": len(current_segment["speakers"])
480:                         })
481:                         current_segment = None
482:             
483:             # Close last segment if open
484:             if current_segment is not None:
485:                 last_frame = data.get("total_frames") or max_frame_seen
486:                 segments.append({
487:                     "start_frame": current_segment["start_frame"],
488:                     "end_frame": last_frame,
489:                     "start_time": current_segment["start_time"],
490:                     "end_time": last_frame / fps,
491:                     "speakers": sorted(list(current_segment["speakers"])),
492:                     "speaker_count": len(current_segment["speakers"])
493:                 })
494:             
495:             # Get unique speakers
496:             all_speakers = set()
497:             for seg in segments:
498:                 all_speakers.update(seg["speakers"])
499:             
500:             total_frames = data.get("total_frames") or max_frame_seen
501:             if not total_frames and segments:
502:                 try:
503:                     total_frames = max(int(s.get("end_frame", 0)) for s in segments)
504:                 except Exception:
505:                     total_frames = 0
506: 
507:             return {
508:                 "available": True,
509:                 "fps": fps,
510:                 "total_frames": int(total_frames or 0),
511:                 "segments": segments,
512:                 "segment_count": len(segments),
513:                 "unique_speakers": sorted(list(all_speakers)),
514:                 "speaker_count": len(all_speakers),
515:                 "source_path": str(audio_file),
516:                 "provenance": provenance,
517:                 "archive_created_at": archive_created_at
518:             }
519:             
520:         except Exception as e:
521:             logger.error(f"Error loading audio data: {e}")
522:             return {"available": False, "error": str(e), "segments": []}
523:     
524:     @staticmethod
525:     def _load_tracking_data(project_name: str, project_path: Path, video_path: Path, video_stem: str) -> Dict[str, Any]:
526:         """Load video tracking data from JSON."""
527:         # Search recursively to support nested outputs
528:         tracking_file = next(project_path.rglob(f"{video_stem}{TRACKING_SUFFIX}"), None)
529:         # Fallback: some pipelines save tracking as plain '<stem>.json'
530:         if tracking_file is None:
531:             tracking_file = next(project_path.rglob(f"{video_stem}.json"), None)
532:         
533:         provenance = "project"
534:         archive_created_at = None
535:         # Fallback to archives
536:         if tracking_file is None or not tracking_file.exists():
537:             tracking_file = ResultsArchiver.find_analysis_file(project_name, video_path, TRACKING_SUFFIX)
538:             if tracking_file is None:
539:                 return {"available": False, "summary": {}}
540:             provenance, archive_created_at = VisualizationService._provenance_info(Path(tracking_file))
541:         else:
542:             provenance = "project"
543:         
544:         try:
545:             with open(tracking_file, 'r', encoding='utf-8') as f:
546:                 data = json.load(f)
547:             
548:             # Calculate summary statistics
549:             total_frames = data.get("total_frames", 0)
550:             fps = data.get("fps", 25.0)
551:             frames_with_faces = 0
552:             frames_with_speaking = 0
553:             max_faces = 0
554:             max_frame_seen = 0
555:             
556:             for frame_data in data.get("frames_analysis", []):
557:                 frame = frame_data.get("frame")
558:                 if isinstance(frame, (int, float)):
559:                     try:
560:                         max_frame_seen = max(max_frame_seen, int(frame))
561:                     except Exception:
562:                         pass
563: 
564:                 # Support two schemas:
565:                 # 1) faces_data: [{ is_speaking: bool, ... }]
566:                 # 2) tracked_objects: [{ label: "face", active_speakers: [...], ... }]
567:                 faces_list = frame_data.get("faces_data")
568:                 if faces_list is None:
569:                     tracked = frame_data.get("tracked_objects", []) or []
570:                     faces_list = [o for o in tracked if str(o.get("label", "")).lower() == "face"]
571: 
572:                 face_count = len(faces_list)
573:                 if face_count > 0:
574:                     frames_with_faces += 1
575:                     max_faces = max(max_faces, face_count)
576: 
577:                     # Speaking detection across schemas
578:                     speaking = any(
579:                         (
580:                             # legacy schema
581:                             (isinstance(f, dict) and f.get("is_speaking", False)) or
582:                             # tracked_objects schema: any active_speakers present
583:                             (isinstance(f, dict) and bool(f.get("active_speakers")))
584:                         )
585:                         for f in faces_list
586:                     )
587:                     if speaking:
588:                         frames_with_speaking += 1
589:             
590:             if (not total_frames or total_frames == 0) and max_frame_seen:
591:                 total_frames = max_frame_seen
592:             coverage_percent = (frames_with_faces / total_frames * 100) if total_frames > 0 else 0
593:             
594:             return {
595:                 "available": True,
596:                 "fps": fps,
597:                 "total_frames": total_frames,
598:                 "summary": {
599:                     "frames_with_faces": frames_with_faces,
600:                     "frames_with_speaking": frames_with_speaking,
601:                     "max_faces_detected": max_faces,
602:                     "face_coverage_percent": round(coverage_percent, 2)
603:                 },
604:                 "source_path": str(tracking_file),
605:                 "provenance": provenance,
606:                 "archive_created_at": archive_created_at
607:             }
608:             
609:         except Exception as e:
610:             logger.error(f"Error loading tracking data: {e}")
611:             return {"available": False, "error": str(e), "summary": {}}
612: 
613:     @staticmethod
614:     def _provenance_info(file_path: Path) -> (str, Optional[str]):
615:         """Return (provenance, created_at) for a given analysis file.
616:         If file is under ARCHIVES_DIR, provenance='archives' and created_at read from sibling metadata.json.
617:         Otherwise provenance='project', created_at=None.
618:         """
619:         try:
620:             arch_root = config.ARCHIVES_DIR.resolve()
621:             fp = file_path.resolve()
622:             try:
623:                 fp.relative_to(arch_root)
624:                 # archived
625:                 meta_path = fp.parent / 'metadata.json'
626:                 if meta_path.exists():
627:                     with open(meta_path, 'r', encoding='utf-8') as mf:
628:                         meta = json.load(mf)
629:                         return 'archives', meta.get('created_at')
630:                 return 'archives', None
631:             except Exception:
632:                 return 'project', None
633:         except Exception as e:
634:             logger.error(f"Error getting provenance info: {e}")
635:             return 'unknown', None
636:     
637:     @staticmethod
638:     def _get_video_metadata(
639:         video_path: Path, 
640:         scenes_data: Dict, 
641:         audio_data: Dict, 
642:         tracking_data: Dict
643:     ) -> Dict[str, Any]:
644:         """Extract video metadata and combine with analysis data."""
645:         try:
646:             # Prefer FPS/frames from analysis data when present
647:             fps = audio_data.get("fps") or tracking_data.get("fps")
648:             total_frames = audio_data.get("total_frames") or tracking_data.get("total_frames")
649: 
650:             duration_seconds = 0.0
651: 
652:             # Derive duration from scenes if available
653:             if scenes_data.get("available") and scenes_data.get("scenes"):
654:                 try:
655:                     duration_seconds = max(s.get("end_time_seconds", 0) for s in scenes_data["scenes"]) or 0.0
656:                 except Exception:
657:                     pass
658: 
659:             # If duration or fps/frames still missing, probe the video file directly (if it exists)
660:             if (not fps or not total_frames or duration_seconds == 0.0) and video_path.exists():
661:                 probed = VisualizationService._probe_video_file(video_path)
662:                 if probed:
663:                     probed_fps = probed.get("fps")
664:                     probed_duration = probed.get("duration")
665:                     if not fps and probed_fps:
666:                         fps = probed_fps
667:                     if duration_seconds == 0.0 and probed_duration:
668:                         duration_seconds = probed_duration
669:                     if not total_frames and fps and duration_seconds:
670:                         total_frames = int(round(fps * duration_seconds))
671: 
672:             # Final fallbacks
673:             if not fps:
674:                 fps = 25.0
675:             if not total_frames and duration_seconds and fps:
676:                 total_frames = int(round(fps * duration_seconds))
677:             if not duration_seconds and total_frames and fps:
678:                 duration_seconds = total_frames / fps
679:             
680:             return {
681:                 "filename": video_path.name if video_path.name else "(archive)",
682:                 "fps": fps,
683:                 "total_frames": total_frames,
684:                 "duration_seconds": round(duration_seconds, 2),
685:                 "duration_formatted": VisualizationService._seconds_to_timecode(duration_seconds),
686:                 "has_scenes": scenes_data.get("available", False),
687:                 "has_audio": audio_data.get("available", False),
688:                 "has_tracking": tracking_data.get("available", False),
689:                 "scene_count": scenes_data.get("count", 0),
690:                 "speech_segment_count": audio_data.get("segment_count", 0),
691:                 "face_coverage": tracking_data.get("summary", {}).get("face_coverage_percent", 0)
692:             }
693:             
694:         except Exception as e:
695:             logger.error(f"Error extracting metadata: {e}")
696:             return {}
697: 
698:     @staticmethod
699:     def _probe_video_file(video_path: Path) -> Optional[Dict[str, float]]:
700:         """Probe a video file using ffprobe (fallback to OpenCV) to get fps and duration.
701:         Returns a dict like {"fps": float|None, "duration": float|None} or None on failure.
702:         """
703:         try:
704:             # Try ffprobe first
705:             cmd = [
706:                 'ffprobe', '-v', 'error',
707:                 '-select_streams', 'v:0',
708:                 '-show_entries', 'stream=avg_frame_rate,duration',
709:                 '-of', 'json', str(video_path)
710:             ]
711:             result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, timeout=10)
712:             if result.returncode == 0 and result.stdout:
713:                 data = json.loads(result.stdout)
714:                 streams = data.get('streams') or []
715:                 if streams:
716:                     stream = streams[0]
717:                     fps = None
718:                     afr = stream.get('avg_frame_rate')
719:                     if afr and afr != '0/0':
720:                         try:
721:                             num, den = afr.split('/')
722:                             num, den = float(num), float(den)
723:                             if den != 0:
724:                                 fps = num / den
725:                         except Exception:
726:                             pass
727:                     duration = None
728:                     try:
729:                         duration_val = stream.get('duration')
730:                         if duration_val is not None:
731:                             duration = float(duration_val)
732:                     except Exception:
733:                         pass
734:                     return {"fps": fps, "duration": duration}
735:         except Exception:
736:             pass
737:         # Fallback to OpenCV if available
738:         try:
739:             import cv2  # type: ignore
740:             cap = cv2.VideoCapture(str(video_path))
741:             if cap.isOpened():
742:                 fps = cap.get(cv2.CAP_PROP_FPS) or None
743:                 frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT) or None
744:                 cap.release()
745:                 duration = None
746:                 if fps and frame_count:
747:                     duration = float(frame_count) / float(fps)
748:                 return {"fps": float(fps) if fps else None, "duration": duration}
749:         except Exception:
750:             pass
751:         return None
752:     
753:     @staticmethod
754:     def _timecode_to_seconds(timecode: str) -> float:
755:         """Convert timecode (HH:MM:SS.mmm) to seconds."""
756:         try:
757:             parts = timecode.split(':')
758:             if len(parts) != 3:
759:                 return 0.0
760:             
761:             hours = int(parts[0])
762:             minutes = int(parts[1])
763:             seconds = float(parts[2])
764:             
765:             return hours * 3600 + minutes * 60 + seconds
766:             
767:         except Exception:
768:             return 0.0
769:     
770:     @staticmethod
771:     def _seconds_to_timecode(seconds: float) -> str:
772:         """Convert seconds to timecode (HH:MM:SS.mmm)."""
773:         try:
774:             hours = int(seconds // 3600)
775:             minutes = int((seconds % 3600) // 60)
776:             secs = seconds % 60
777:             
778:             return f"{hours:02d}:{minutes:02d}:{secs:06.3f}"
779:             
780:         except Exception:
781:             return "00:00:00.000"
```

## File: services/webhook_service.py
```python
  1: """
  2: WebhookService
  3: Provides a simple external JSON-based source of download links.
  4: Respects project standards: service-layer only, robust error handling, caching, and status reporting.
  5: 
  6: Public methods:
  7: - fetch_records(): List[Dict[str, str]] of {url, timestamp, source, url_type}
  8: - get_service_status(): Dict[str, Any]
  9: """
 10: from __future__ import annotations
 11: 
 12: from datetime import datetime
 13: import logging
 14: import time
 15: from typing import Any, Dict, List, Optional
 16: 
 17: import requests
 18: 
 19: from config.settings import config
 20: 
 21: logger = logging.getLogger(__name__)
 22: 
 23: # In-memory cache
 24: _cache_data: Optional[List[Dict[str, str]]] = None
 25: _cache_fetched_at: float = 0.0
 26: _last_error: Optional[str] = None
 27: _last_status: Dict[str, Any] = {
 28:     "available": False,
 29:     "last_fetch_ts": None,
 30:     "error": None,
 31:     "records": 0,
 32: }
 33: 
 34: 
 35: def _classify_url_type(url: str) -> str:
 36:     try:
 37:         u = (url or "").lower()
 38:         if "fromsmash.com" in u:
 39:             return "fromsmash"
 40:         if "swisstransfer.com" in u:
 41:             return "swisstransfer"
 42:         if "dropbox.com" in u or "dl.dropboxusercontent.com" in u:
 43:             return "dropbox"
 44:         # R2/Worker proxy URLs for Dropbox (example: https://<host>.workers.dev/dropbox/<...>/file)
 45:         if "/dropbox/" in u and ("workers.dev" in u or "worker" in u):
 46:             return "dropbox"
 47:         return "external"
 48:     except Exception:
 49:         return "external"
 50: 
 51: 
 52: def _normalize_timestamp(ts: Optional[str]) -> Optional[str]:
 53:     """Best-effort normalization: accept ISO or 'YYYY-MM-DD HH:MM:SS'.
 54:     Leave unchanged if parsing is uncertain; CSVService will also handle.
 55:     """
 56:     if not ts:
 57:         return None
 58:     try:
 59:         dt = datetime.fromisoformat(str(ts).replace('Z', '+00:00'))
 60:         if not dt.tzinfo:
 61:             return str(ts)
 62:         return dt.astimezone().strftime('%Y-%m-%d %H:%M:%S')
 63:     except Exception:
 64:         return str(ts)
 65: 
 66: 
 67: def _validate_and_format(items: Any) -> List[Dict[str, str]]:
 68:     """Validate raw JSON and return standardized rows.
 69: 
 70:     Expected input example:
 71:     [
 72:         {"url": "https://...", "timestamp": "2025-10-17T12:34:13+0200", "source": "webhook"}
 73:     ]
 74:     """
 75:     rows: List[Dict[str, str]] = []
 76:     if not isinstance(items, list):
 77:         return rows
 78:     for it in items:
 79:         try:
 80:             if not isinstance(it, dict):
 81:                 continue
 82:             # New webhook schema: pre-normalized urls + metadata
 83:             source_url = str(it.get("source_url") or "").strip()
 84:             r2_url = str(it.get("r2_url") or "").strip()
 85:             original_filename = str(it.get("original_filename") or "").strip()
 86:             provider = str(it.get("provider") or "").strip()
 87:             created_at = _normalize_timestamp(it.get("created_at"))
 88: 
 89:             if r2_url or source_url:
 90:                 preferred_url = r2_url or source_url
 91:                 fallback_url = source_url if (r2_url and source_url and source_url != r2_url) else ""
 92:                 url_type = (provider.lower() if provider else _classify_url_type(preferred_url))
 93:                 row_new: Dict[str, str] = {
 94:                     "url": preferred_url,
 95:                     "fallback_url": fallback_url,
 96:                     "original_filename": original_filename,
 97:                     "provider": provider,
 98:                     "timestamp": created_at or "",
 99:                     "source": "webhook",
100:                     "url_type": url_type,
101:                 }
102:                 rows.append(row_new)
103:                 continue
104: 
105:             # Legacy webhook schema
106:             url = str(it.get("url") or "").strip()
107:             if not url:
108:                 continue
109:             ts = _normalize_timestamp(it.get("timestamp"))
110:             src = str(it.get("source") or "webhook").strip() or "webhook"
111:             url_type = str(it.get("url_type") or "").strip() or _classify_url_type(url)
112:             row_legacy: Dict[str, str] = {
113:                 "url": url,
114:                 "timestamp": ts or "",
115:                 "source": src,
116:                 "url_type": url_type,
117:             }
118:             rows.append(row_legacy)
119:         except Exception as e:
120:             logger.debug(f"WebhookService: skipping invalid item due to error: {e}")
121:             continue
122:     return rows
123: 
124: 
125: def fetch_records() -> Optional[List[Dict[str, str]]]:
126:     """Fetch and cache webhook JSON records.
127: 
128:     Returns None on hard error; returns [] if no data.
129:     Respects WEBHOOK_CACHE_TTL for caching.
130:     """
131:     global _cache_data, _cache_fetched_at, _last_error, _last_status
132:     now = time.time()
133: 
134:     # Use cache if within TTL
135:     if _cache_data is not None and (now - _cache_fetched_at) < max(0, config.WEBHOOK_CACHE_TTL):
136:         return _cache_data
137: 
138:     url = config.WEBHOOK_JSON_URL
139:     timeout = max(1, config.WEBHOOK_TIMEOUT)
140: 
141:     # Simple retry: up to 2 retries (total 3 attempts) with small backoff
142:     attempts = 3
143:     backoff = 1.0
144:     last_exc: Optional[Exception] = None
145: 
146:     for attempt in range(1, attempts + 1):
147:         try:
148:             resp = requests.get(url, timeout=timeout)
149:             resp.raise_for_status()
150:             data = resp.json()
151:             rows = _validate_and_format(data)
152:             _cache_data = rows
153:             _cache_fetched_at = now
154:             _last_error = None
155:             _last_status.update({
156:                 "available": True,
157:                 "last_fetch_ts": time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(now)),
158:                 "error": None,
159:                 "records": len(rows),
160:             })
161:             return rows
162:         except Exception as e:
163:             last_exc = e
164:             _last_error = str(e)
165:             _last_status.update({
166:                 "available": False,
167:                 "last_fetch_ts": time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(now)),
168:                 "error": _last_error,
169:             })
170:             if attempt < attempts:
171:                 time.sleep(backoff)
172:                 backoff *= 2
173:             else:
174:                 logger.error(f"WebhookService: failed to fetch webhook JSON after {attempts} attempts: {e}")
175:                 return None
176: 
177: 
178: def get_service_status() -> Dict[str, Any]:
179:     """Return last known status for observability."""
180:     # Shallow copy to avoid mutation
181:     return dict(_last_status)
```

## File: services/workflow_service.py
```python
  1: import logging
  2: import os
  3: import threading
  4: import time
  5: from collections import deque
  6: from datetime import datetime, timezone
  7: from typing import Dict, Any, List, Optional, Tuple
  8: from pathlib import Path
  9: 
 10: from config.settings import config
 11: from services.workflow_state import get_workflow_state
 12: 
 13: logger = logging.getLogger(__name__)
 14: logger.setLevel(logging.DEBUG)
 15: 
 16: 
 17: class WorkflowService:
 18:     """
 19:     Centralized service for workflow management.
 20: 
 21:     This service follows the Service & Blueprint Architecture pattern by:
 22:     - Using app_new.py as the single source of truth for state
 23:     - Providing clean, stateless methods for workflow operations
 24:     - Implementing proper error handling and fallback mechanisms
 25:     - Maintaining thread-safe operations through proper state access
 26:     """
 27: 
 28:     _initialized = False
 29:     _initialization_lock = threading.Lock()
 30: 
 31:     @staticmethod
 32:     def initialize(commands_config: Dict[str, Any]) -> None:
 33:         """
 34:         Initialize the workflow service.
 35: 
 36:         This method primarily serves as a confirmation that the service is loaded,
 37:         as state is managed via WorkflowState singleton.
 38: 
 39:         Args:
 40:             commands_config: Dict[str, Any] - Dictionary of step configurations (informational only)
 41:         """
 42:         with WorkflowService._initialization_lock:
 43:             if WorkflowService._initialized:
 44:                 logger.debug("[WorkflowService] Already initialized, skipping")
 45:                 return
 46: 
 47:             try:
 48:                 workflow_state = get_workflow_state()
 49:                 if workflow_state:
 50:                     WorkflowService._initialized = True
 51:                     logger.info(f"Workflow service initialized, linked to WorkflowState singleton with {len(commands_config)} steps.")
 52:                 else:
 53:                     raise RuntimeError("WorkflowState singleton not available")
 54:             except Exception as e:
 55:                 logger.error(f"Workflow service initialization failed: {e}")
 56:                 raise RuntimeError("WorkflowService cannot initialize without access to WorkflowState") from e
 57: 
 58:     @staticmethod
 59:     def _get_workflow_state():
 60:         return get_workflow_state()
 61:     
 62:     @staticmethod
 63:     def get_step_status(step_key: str, include_logs: bool = False) -> Dict[str, Any]:
 64:         """
 65:         Get current status of a workflow step from the single source of truth.
 66: 
 67:         Args:
 68:             step_key: Step identifier
 69:             include_logs: Whether to include log entries
 70: 
 71:         Returns:
 72:             Step status dictionary
 73: 
 74:         Raises:
 75:             ValueError: If step_key is not found
 76:             RuntimeError: If workflow state is not available
 77:         """
 78:         workflow_state = WorkflowService._get_workflow_state()
 79: 
 80:         if not workflow_state:
 81:             raise RuntimeError("WorkflowService cannot access workflow state")
 82: 
 83:         info = workflow_state.get_step_info(step_key)
 84:         if not info:
 85:             raise ValueError(f"Step '{step_key}' not found or not initialized")
 86: 
 87:         current_sequence_state = workflow_state.is_sequence_running()
 88: 
 89:         from config.workflow_commands import WorkflowCommandsConfig
 90:         config_instance = WorkflowCommandsConfig()
 91:         step_display_name = config_instance.get_step_display_name(step_key) or step_key
 92: 
 93:         result = {
 94:             "step": step_key,
 95:             "display_name": step_display_name,
 96:             "status": info['status'],
 97:             "return_code": info['return_code'],
 98:             "progress_current": info['progress_current'],
 99:             "progress_total": info['progress_total'],
100:             "progress_current_fractional": info.get('progress_current_fractional'),
101:             "progress_text": info['progress_text'],
102:             "duration_str": info.get('duration_str', None),
103:             "is_any_sequence_running": current_sequence_state
104:         }
105: 
106:         if include_logs:
107:             result["log"] = info.get('log', [])
108: 
109:         return result
110: 
111:     @staticmethod
112:     def get_step_log_file(step_key: str, log_index: int) -> Dict[str, Any]:
113:         from config.workflow_commands import WorkflowCommandsConfig
114: 
115:         config_instance = WorkflowCommandsConfig()
116:         step_config = config_instance.get_step_config(step_key)
117:         if not step_config:
118:             raise ValueError("Step not found")
119: 
120:         specific_logs_config = step_config.get("specific_logs", [])
121:         if not (0 <= log_index < len(specific_logs_config)):
122:             raise ValueError("Log index out of range")
123: 
124:         log_conf = specific_logs_config[log_index]
125:         log_type = log_conf["type"]
126:         num_lines = log_conf.get("lines", 50)
127: 
128:         content = ""
129:         error_msg = None
130:         processed_path_str = "N/A"
131: 
132:         def _read_tail(path: Path, lines: int) -> str:
133:             with open(path, 'r', encoding='utf-8', errors='ignore') as f:
134:                 tail = deque(f, maxlen=lines)
135:             if not tail:
136:                 return "Fichier vide."
137:             return ''.join(tail)
138: 
139:         if log_type == "file":
140:             log_path = Path(log_conf["path"])
141:             processed_path_str = str(log_path)
142:             if not log_path.exists():
143:                 error_msg = "Fichier non trouvé"
144:                 content = f"Fichier non trouvé: {log_path}"
145:             else:
146:                 try:
147:                     content = _read_tail(log_path, int(num_lines))
148:                 except Exception as e:
149:                     error_msg = f"Erreur de lecture: {e}"
150:                     content = f"Impossible de lire le fichier: {e}"
151: 
152:         elif log_type == "directory_latest":
153:             log_dir = Path(log_conf["path"])
154:             pattern = log_conf.get("pattern", "*.log")
155:             processed_path_str = f"{log_dir}/{pattern}"
156: 
157:             if not (log_dir.exists() and log_dir.is_dir()):
158:                 error_msg = "Répertoire non trouvé"
159:                 content = f"Répertoire non trouvé: {log_dir}"
160:                 processed_path_str = str(log_dir)
161:             else:
162:                 try:
163:                     matching_files = [p for p in log_dir.glob(pattern) if p.is_file()]
164:                     if not matching_files:
165:                         error_msg = "Aucun fichier trouvé"
166:                         content = f"Aucun fichier correspondant au pattern '{pattern}' dans {log_dir}"
167:                     else:
168:                         latest_path = max(matching_files, key=lambda p: p.stat().st_mtime)
169:                         processed_path_str = str(latest_path)
170:                         try:
171:                             content = _read_tail(latest_path, int(num_lines))
172:                         except Exception as e:
173:                             error_msg = f"Erreur de lecture: {e}"
174:                             content = f"Impossible de lire le fichier: {e}"
175:                 except Exception as e:
176:                     error_msg = f"Erreur de recherche: {e}"
177:                     content = f"Erreur lors de la recherche de fichiers: {e}"
178: 
179:         else:
180:             error_msg = f"Type de log non supporté: {log_type}"
181:             content = f"Type de log '{log_type}' non supporté"
182: 
183:         return {
184:             "content": content,
185:             "error": error_msg,
186:             "path": processed_path_str,
187:             "type": log_type,
188:             "lines_requested": num_lines,
189:         }
190:     
191:     @staticmethod
192:     def run_step(step_key: str) -> Dict[str, str]:
193:         """
194:         Execute a single workflow step.
195: 
196:         Args:
197:             step_key: Step identifier
198: 
199:         Returns:
200:             Result dictionary with status and message
201:         """
202:                 
203:         workflow_state = WorkflowService._get_workflow_state()
204:         if not workflow_state:
205:             return {"status": "error", "message": "WorkflowState not available."}
206: 
207:                 
208:         from config.workflow_commands import WorkflowCommandsConfig
209:         config_instance = WorkflowCommandsConfig()
210:         
211:                 
212:         if not config_instance.validate_step_key(step_key):
213:             return {"status": "error", "message": "Étape inconnue"}
214:         
215:         step_display_name = config_instance.get_step_display_name(step_key)
216: 
217:                 
218:         projects_dir = config.BASE_PATH_SCRIPTS / 'projets_extraits'
219:         projects_dir.mkdir(parents=True, exist_ok=True)
220: 
221:                 
222:         if workflow_state.is_sequence_running():
223:             return {
224:                 "status": "error",
225:                 "message": "Une séquence de workflow est en cours. Veuillez attendre."
226:             }
227: 
228:                 
229:         if workflow_state.is_step_running(step_key):
230:             return {
231:                 "status": "error",
232:                 "message": f"'{step_display_name}' est déjà en cours."
233:             }
234: 
235:                 
236:         import sys
237:         if 'app_new' not in sys.modules:
238:             return {"status": "error", "message": "Execution context not available"}
239:         
240:         app_new = sys.modules['app_new']
241:         if not hasattr(app_new, 'run_process_async'):
242:             logger.error("run_process_async not available in app_new")
243:             return {"status": "error", "message": "Execution function not available"}
244: 
245:         try:
246:             thread = threading.Thread(
247:                 target=app_new.run_process_async,
248:                 args=(step_key,)
249:             )
250:             thread.daemon = True
251:             thread.start()
252:         except Exception as e:
253:             logger.error(f"Error starting step execution thread: {e}")
254:             return {"status": "error", "message": f"Failed to start step execution: {str(e)}"}
255:         
256:         return {
257:             "status": "initiated",
258:             "message": f"Lancement de: {step_display_name}"
259:         }
260:     
261:     @staticmethod
262:     def run_custom_sequence(steps: List[str]) -> Dict[str, str]:
263:         """
264:         Execute a custom sequence of workflow steps.
265: 
266:         Args:
267:             steps: List of step identifiers
268: 
269:         Returns:
270:             Result dictionary with status and message
271:         """
272:                 
273:         workflow_state = WorkflowService._get_workflow_state()
274:         if not workflow_state:
275:             return {"status": "error", "message": "WorkflowState not available."}
276: 
277:         from config.workflow_commands import WorkflowCommandsConfig
278:         config_instance = WorkflowCommandsConfig()
279: 
280:                 
281:         if workflow_state.is_sequence_running():
282:             return {
283:                 "status": "error",
284:                 "message": "Une autre séquence de workflow est déjà en cours."
285:             }
286: 
287:                 
288:         for step_key in steps:
289:             if not config_instance.validate_step_key(step_key):
290:                 return {"status": "error", "message": f"Étape inconnue : {step_key}"}
291: 
292:                 
293:         import sys
294:         if 'app_new' not in sys.modules:
295:             return {"status": "error", "message": "Execution context not available"}
296:         
297:         app_new = sys.modules['app_new']
298:         if not hasattr(app_new, 'execute_step_sequence_worker'):
299:             logger.error("execute_step_sequence_worker not available in app_new")
300:             return {"status": "error", "message": "Sequence execution function not available"}
301: 
302:         try:
303:             thread = threading.Thread(
304:                 target=app_new.execute_step_sequence_worker,
305:                 args=(steps, "Custom")
306:             )
307:             thread.daemon = True
308:             thread.start()
309:         except Exception as e:
310:             logger.error(f"Error starting sequence execution thread: {e}")
311:             return {"status": "error", "message": f"Failed to start sequence execution: {str(e)}"}
312: 
313:         return {
314:             "status": "initiated",
315:             "message": f"Séquence personnalisée lancée avec {len(steps)} étapes."
316:         }
317: 
318:     @staticmethod
319:     def stop_step(step_key: str) -> Dict[str, str]:
320:         """
321:         Stop a running workflow step.
322: 
323:         Args:
324:             step_key: Step identifier
325: 
326:         Returns:
327:             Result dictionary with status and message
328: 
329:         Raises:
330:             ValueError: If step_key is not found
331:             RuntimeError: If workflow state is not available
332:         """
333:         workflow_state = WorkflowService._get_workflow_state()
334:         if not workflow_state:
335:             raise RuntimeError("WorkflowService cannot access workflow state")
336: 
337:                 
338:         info = workflow_state.get_step_info(step_key)
339:         if not info:
340:             raise ValueError(f"Step '{step_key}' not found")
341: 
342:         if info['status'] not in ['running', 'starting']:
343:             return {
344:                 "status": "error",
345:                 "message": f"Step '{step_key}' is not running"
346:             }
347: 
348:                 
349:         process = workflow_state.get_step_process(step_key)
350:         
351:         if process:
352:             try:
353:                 process.terminate()
354:                                 
355:                 workflow_state.update_step_info(
356:                     step_key,
357:                     status='stopped',
358:                     return_code=-1
359:                 )
360: 
361:                 return {
362:                     "status": "success",
363:                     "message": f"Step '{step_key}' stopped successfully"
364:                 }
365:             except Exception as e:
366:                 logger.error(f"Error stopping step {step_key}: {e}")
367:                 return {
368:                     "status": "error",
369:                     "message": f"Failed to stop step: {str(e)}"
370:                 }
371:         else:
372:             return {
373:                 "status": "error",
374:                 "message": f"No process found for step '{step_key}'"
375:             }
376:     
377:     @staticmethod
378:     def get_sequence_status() -> Dict[str, Any]:
379:         """
380:         Get current sequence execution status.
381:         
382:         Returns:
383:             Sequence status dictionary
384:         """
385:         workflow_state = WorkflowService._get_workflow_state()
386:         if not workflow_state:
387:             return {
388:                 "is_running": False,
389:                 "current_step": None,
390:                 "progress": {"current": 0, "total": 0},
391:                 "last_outcome": {"status": "unknown"},
392:                 "error": "Unable to access workflow state"
393:             }
394:         
395:                 
396:         is_running = workflow_state.is_sequence_running()
397:         
398:                 
399:         current_step = None
400:         all_steps_info = workflow_state.get_all_steps_info()
401:         for step_key, info in all_steps_info.items():
402:             if info['status'] in ['running', 'starting']:
403:                 current_step = step_key
404:                 break
405:         
406:                 
407:         total_steps = len(all_steps_info)
408:         completed_steps = sum(
409:             1 for info in all_steps_info.values()
410:             if info['status'] == 'completed'
411:         )
412:         
413:         return {
414:             "is_running": is_running,
415:             "current_step": current_step,
416:             "progress": {
417:                 "current": completed_steps,
418:                 "total": total_steps
419:             },
420:             "last_outcome": workflow_state.get_sequence_outcome()
421:         }
422:     
423:     @staticmethod
424:     def stop_sequence() -> Dict[str, str]:
425:         """
426:         Stop the currently running sequence.
427:         
428:         Returns:
429:             Result dictionary with status and message
430:         """
431:         workflow_state = WorkflowService._get_workflow_state()
432:         if not workflow_state:
433:             return {
434:                 "status": "error",
435:                 "message": "Unable to access workflow state"
436:             }
437:         
438:                 
439:         if not workflow_state.is_sequence_running():
440:             return {
441:                 "status": "error",
442:                 "message": "Aucune séquence en cours d'exécution"
443:             }
444:         
445:                 
446:         workflow_state.complete_sequence(
447:             success=False,
448:             message="Sequence manually stopped by user"
449:         )
450:         
451:                 
452:         stopped_steps = []
453:         all_steps_info = workflow_state.get_all_steps_info()
454:         for step_key, info in all_steps_info.items():
455:             if info['status'] in ['running', 'starting']:
456:                 try:
457:                     process = workflow_state.get_step_process(step_key)
458:                     if process:
459:                         process.terminate()
460:                         workflow_state.update_step_info(
461:                             step_key,
462:                             status='stopped',
463:                             return_code=-1
464:                         )
465:                         stopped_steps.append(step_key)
466:                 except Exception as e:
467:                     logger.error(f"Error stopping step {step_key}: {e}")
468:         
469:         return {
470:             "status": "success",
471:             "message": f"Sequence stopped. Stopped steps: {', '.join(stopped_steps) if stopped_steps else 'none'}"
472:         }
473:     
474:     @staticmethod
475:     def get_current_workflow_status_summary() -> Dict[str, Any]:
476:         """
477:         Get workflow status summary for remote servers.
478: 
479:         Returns:
480:             Workflow status summary
481:         """
482:         workflow_state = WorkflowService._get_workflow_state()
483:         if not workflow_state:
484:             return {
485:                 "is_sequence_running": False,
486:                 "steps": {},
487:                 "last_sequence_outcome": {"status": "unknown"},
488:                 "timestamp": datetime.now(timezone.utc).isoformat(),
489:                 "error": "Unable to access workflow state"
490:             }
491: 
492:         try:
493:                     
494:             sequence_running = workflow_state.is_sequence_running()
495: 
496:                         
497:             steps_summary = {}
498:             all_steps_info = workflow_state.get_all_steps_info()
499:             for step_key, info in all_steps_info.items():
500:                 steps_summary[step_key] = {
501:                     "status": info['status'],
502:                     "progress_current": info['progress_current'],
503:                     "progress_total": info['progress_total'],
504:                     "return_code": info['return_code']
505:                 }
506: 
507:             return {
508:                 "is_sequence_running": sequence_running,
509:                 "steps": steps_summary,
510:                 "last_sequence_outcome": workflow_state.get_sequence_outcome(),
511:                 "timestamp": datetime.now(timezone.utc).isoformat()
512:             }
513:         except Exception as e:
514:             logger.error(f"Error getting workflow status summary: {e}")
515:             return {
516:                 "is_sequence_running": False,
517:                 "steps": {},
518:                 "last_sequence_outcome": {"status": "error"},
519:                 "timestamp": datetime.now(timezone.utc).isoformat(),
520:                 "error": f"Failed to get status: {str(e)}"
521:             }
522:     
523:     @staticmethod
524:     def prepare_step_execution(step_key: str, process_info: Dict, commands_config: Dict) -> Tuple[List[str], str]:
525:         """Prepare step configuration and command for execution.
526:         
527:         Args:
528:             step_key: Step identifier
529:             process_info: Process information dictionary
530:             commands_config: Commands configuration dictionary
531:             
532:         Returns:
533:             Tuple of (command_list, working_directory)
534:         """
535:         step_config = commands_config[step_key]
536:         
537:                 
538:         process_info['status'] = 'starting'
539:         process_info['log'].clear()
540:         process_info['log'].append(f"--- Lancement de: {step_config['display_name']} ---\n")
541:         process_info['return_code'] = None
542:         process_info['progress_current'] = 0
543:         process_info['progress_total'] = 0
544:         process_info['progress_text'] = ''
545:         process_info['start_time_epoch'] = time.time()
546:         
547:                 
548:         cmd_list = [str(c) for c in step_config['cmd']]
549:         cwd = step_config['cwd']
550:         
551:         logger.info(f"{step_key} preparation complete: {step_config['display_name']}")
552:         
553:         return cmd_list, cwd
554:     
555:     @staticmethod
556:     def prepare_tracking_step(base_path: Path, keyword: str, subdir: str) -> Optional[List[str]]:
557:         """Prepare tracking step (STEP5) by finding videos to process.
558:         
559:         Args:
560:             base_path: Base path for video search
561:             keyword: Keyword filter for videos
562:             subdir: Subdirectory filter
563:             
564:         Returns:
565:             List of video paths to process, or None if no videos found
566:         """
567:         from services.filesystem_service import FilesystemService
568:         
569:         logger.info("STEP5: Searching for videos requiring tracking...")
570:         
571:         videos_to_process = FilesystemService.find_videos_for_tracking(
572:             base_path,
573:             keyword,
574:             subdir
575:         )
576:         
577:         if not videos_to_process:
578:             logger.info("STEP5: No videos to process (all have existing JSON)")
579:             return None
580:         
581:         logger.info(f"STEP5: Found {len(videos_to_process)} videos needing tracking")
582:         return videos_to_process
583:     
584:     @staticmethod
585:     def create_tracking_temp_file(videos: List[str]) -> Path:
586:         """Create temporary file with list of videos for tracking.
587:         
588:         Writes a JSON array of video paths directly (not wrapped in an object).
589:         This format is expected by run_tracking_manager.py.
590:         
591:         Args:
592:             videos: List of video file paths
593:             
594:         Returns:
595:             Path to temporary file containing JSON array of video paths
596:         """
597:         import tempfile
598:         import json
599:         
600:         temp_fd, temp_path = tempfile.mkstemp(suffix='.json', prefix='tracking_videos_')
601:         temp_file = Path(temp_path)
602:         
603:         try:
604:             with open(temp_file, 'w', encoding='utf-8') as f:
605:                 json.dump(videos, f, indent=2)
606:             
607:             os.close(temp_fd)
608:             logger.info(f"Created tracking temp file: {temp_file}")
609:             return temp_file
610:             
611:         except Exception as e:
612:             os.close(temp_fd)
613:             logger.error(f"Failed to create tracking temp file: {e}")
614:             raise
615:     
616:     @staticmethod
617:     def calculate_step_duration(start_time_epoch: Optional[float]) -> str:
618:         """Calculate duration string for a step.
619:         
620:         Args:
621:             start_time_epoch: Start time as epoch timestamp
622:             
623:         Returns:
624:             Duration string (e.g., "2m 30s")
625:         """
626:         if start_time_epoch is None:
627:             return "N/A"
628:         
629:         try:
630:             elapsed = time.time() - start_time_epoch
631:             if elapsed < 60:
632:                 return f"{int(elapsed)}s"
633:             elif elapsed < 3600:
634:                 minutes = int(elapsed // 60)
635:                 seconds = int(elapsed % 60)
636:                 return f"{minutes}m {seconds}s"
637:             else:
638:                 hours = int(elapsed // 3600)
639:                 minutes = int((elapsed % 3600) // 60)
640:                 return f"{hours}h {minutes}m"
641:         except Exception as e:
642:             logger.warning(f"Failed to calculate duration: {e}")
643:             return "N/A"
644:     
645:     @staticmethod
646:     def format_duration_seconds(total_seconds: float) -> str:
647:         """Format duration in seconds to human-readable string.
648:         
649:         Args:
650:             total_seconds: Duration in seconds
651:             
652:         Returns:
653:             Formatted duration string
654:         """
655:         if total_seconds < 0:
656:             return "N/A"
657:         
658:         try:
659:             if total_seconds < 60:
660:                 return f"{int(total_seconds)}s"
661:             elif total_seconds < 3600:
662:                 minutes = int(total_seconds // 60)
663:                 seconds = int(total_seconds % 60)
664:                 return f"{minutes}m {seconds}s"
665:             else:
666:                 hours = int(total_seconds // 3600)
667:                 minutes = int((total_seconds % 3600) // 60)
668:                 return f"{hours}h {minutes}m"
669:         except Exception:
670:             return "N/A"
```

## File: static/css/components/csv-workflow-prompt.css
```css
  1: /**
  2:  * CSV Workflow Prompt Styles
  3:  * Engaging and playful styles for the CSV download completion prompt
  4:  */
  5: 
  6: /* Main prompt container */
  7: .csv-workflow-prompt .popup-content {
  8:     max-width: 600px;
  9:     min-width: 500px;
 10:     border-radius: 16px;
 11:     background: linear-gradient(135deg, #ffffff 0%, #f8f9fa 100%);
 12:     border: 2px solid var(--accent-primary);
 13:     box-shadow: 0 20px 40px rgba(0, 0, 0, 0.15);
 14:     animation: csvPromptSlideIn 0.4s cubic-bezier(0.34, 1.56, 0.64, 1);
 15:     position: relative;
 16:     /* Ensure solid, opaque background for readability */
 17:     opacity: 1;
 18: }
 19: 
 20: /* Close button */
 21: .csv-workflow-prompt .popup-close-button {
 22:     position: absolute;
 23:     top: 16px;
 24:     right: 16px;
 25:     background: none;
 26:     border: none;
 27:     font-size: 24px;
 28:     color: #6c757d;
 29:     cursor: pointer;
 30:     width: 32px;
 31:     height: 32px;
 32:     display: flex;
 33:     align-items: center;
 34:     justify-content: center;
 35:     border-radius: 50%;
 36:     transition: all 0.2s ease;
 37: }
 38: 
 39: .csv-workflow-prompt .popup-close-button:hover {
 40:     background: #f8f9fa;
 41:     color: #495057;
 42:     transform: scale(1.1);
 43: }
 44: 
 45: /* Title */
 46: .csv-workflow-prompt .popup-title {
 47:     margin: 0 0 16px 0;
 48:     padding: 24px 24px 0 24px;
 49:     font-size: 20px;
 50:     font-weight: 600;
 51:     color: #212529;
 52:     text-align: center;
 53: }
 54: 
 55: /* Slide-in animation */
 56: @keyframes csvPromptSlideIn {
 57:     0% {
 58:         opacity: 0;
 59:         transform: translateY(-30px) scale(0.9);
 60:     }
 61:     100% {
 62:         opacity: 1;
 63:         transform: translateY(0) scale(1);
 64:     }
 65: }
 66: 
 67: /* Prompt content container */
 68: .csv-workflow-prompt-content {
 69:     padding: 0;
 70: }
 71: 
 72: /* Header section */
 73: .prompt-header {
 74:     display: flex;
 75:     align-items: center;
 76:     gap: 16px;
 77:     padding: 24px 24px 16px 24px;
 78:     border-bottom: 1px solid #e0e0e0;
 79:     background: linear-gradient(90deg, rgba(121, 134, 203, 0.1) 0%, rgba(255, 255, 255, 0.95) 100%);
 80:     /* Ensure solid background for text readability */
 81:     backdrop-filter: blur(10px);
 82: }
 83: 
 84: .download-icon {
 85:     font-size: 48px;
 86:     animation: csvIconBounce 2s ease-in-out infinite;
 87: }
 88: 
 89: @keyframes csvIconBounce {
 90:     0%, 100% { transform: translateY(0); }
 91:     50% { transform: translateY(-8px); }
 92: }
 93: 
 94: .download-info {
 95:     flex: 1;
 96: }
 97: 
 98: .download-title {
 99:     margin: 0 0 4px 0;
100:     font-size: 18px;
101:     font-weight: 600;
102:     color: #212529;
103:     word-break: break-word;
104: }
105: 
106: .download-subtitle {
107:     margin: 0;
108:     font-size: 14px;
109:     color: #6c757d;
110:     opacity: 1;
111: }
112: 
113: /* Message section */
114: .prompt-message {
115:     padding: 20px 24px;
116: }
117: 
118: .main-message {
119:     margin: 0 0 20px 0;
120:     font-size: 16px;
121:     line-height: 1.5;
122:     color: #495057;
123:     text-align: center;
124: }
125: 
126: /* Workflow preview */
127: .workflow-preview {
128:     background: #f8f9fa;
129:     border-radius: 12px;
130:     padding: 16px;
131:     border: 1px solid #e0e0e0;
132:     /* Ensure solid background for content readability */
133:     opacity: 1;
134: }
135: 
136: .workflow-steps {
137:     display: flex;
138:     align-items: center;
139:     justify-content: center;
140:     flex-wrap: wrap;
141:     gap: 8px;
142: }
143: 
144: .step-badge {
145:     background: var(--accent-primary);
146:     color: white;
147:     padding: 6px 12px;
148:     border-radius: 20px;
149:     font-size: 12px;
150:     font-weight: 500;
151:     white-space: nowrap;
152:     animation: csvStepPulse 3s ease-in-out infinite;
153: }
154: 
155: .step-badge:nth-child(odd) {
156:     animation-delay: 0.5s;
157: }
158: 
159: @keyframes csvStepPulse {
160:     0%, 100% { transform: scale(1); opacity: 1; }
161:     50% { transform: scale(1.05); opacity: 0.9; }
162: }
163: 
164: .step-arrow {
165:     color: var(--accent-primary);
166:     font-weight: bold;
167:     font-size: 14px;
168: }
169: 
170: /* Actions section */
171: .prompt-actions {
172:     display: flex;
173:     gap: 12px;
174:     padding: 20px 24px 16px 24px;
175:     justify-content: center;
176: }
177: 
178: .workflow-launch-btn {
179:     background: linear-gradient(135deg, var(--accent-primary) 0%, var(--accent-secondary) 100%);
180:     color: white;
181:     border: none;
182:     padding: 14px 24px;
183:     border-radius: 12px;
184:     font-size: 16px;
185:     font-weight: 600;
186:     cursor: pointer;
187:     display: flex;
188:     align-items: center;
189:     gap: 8px;
190:     transition: all 0.3s ease;
191:     box-shadow: 0 4px 12px rgba(var(--accent-primary-rgb), 0.3);
192:     position: relative;
193:     overflow: hidden;
194: }
195: 
196: .workflow-launch-btn:hover {
197:     transform: translateY(-2px);
198:     box-shadow: 0 6px 20px rgba(var(--accent-primary-rgb), 0.4);
199: }
200: 
201: .workflow-launch-btn:active {
202:     transform: translateY(0);
203: }
204: 
205: .workflow-launch-btn::before {
206:     content: '';
207:     position: absolute;
208:     top: 0;
209:     left: -100%;
210:     width: 100%;
211:     height: 100%;
212:     background: linear-gradient(90deg, transparent, rgba(255, 255, 255, 0.2), transparent);
213:     transition: left 0.5s;
214: }
215: 
216: .workflow-launch-btn:hover::before {
217:     left: 100%;
218: }
219: 
220: .workflow-dismiss-btn {
221:     background: #ffffff;
222:     color: #6c757d;
223:     border: 2px solid #e0e0e0;
224:     padding: 12px 20px;
225:     border-radius: 12px;
226:     font-size: 14px;
227:     font-weight: 500;
228:     cursor: pointer;
229:     display: flex;
230:     align-items: center;
231:     gap: 6px;
232:     transition: all 0.3s ease;
233:     /* Ensure solid background for button readability */
234:     opacity: 1;
235: }
236: 
237: .workflow-dismiss-btn:hover {
238:     background: #f8f9fa;
239:     border-color: var(--accent-primary);
240:     color: #495057;
241: }
242: 
243: .btn-icon {
244:     font-size: 16px;
245: }
246: 
247: .btn-text {
248:     white-space: nowrap;
249: }
250: 
251: /* Footer section */
252: .prompt-footer {
253:     padding: 16px 24px 24px 24px;
254:     border-top: 1px solid #e0e0e0;
255:     background: #f8f9fa;
256:     border-radius: 0 0 14px 14px;
257:     /* Ensure solid background for footer readability */
258:     opacity: 1;
259: }
260: 
261: .footer-note {
262:     margin: 0;
263:     font-size: 13px;
264:     color: #6c757d;
265:     text-align: center;
266:     line-height: 1.4;
267: }
268: 
269: /* Responsive design */
270: @media (max-width: 768px) {
271:     .csv-workflow-prompt .popup-content {
272:         min-width: 90vw;
273:         max-width: 95vw;
274:         margin: 20px;
275:     }
276:     
277:     .prompt-header {
278:         flex-direction: column;
279:         text-align: center;
280:         gap: 12px;
281:     }
282:     
283:     .workflow-steps {
284:         flex-direction: column;
285:         gap: 12px;
286:     }
287:     
288:     .step-arrow {
289:         transform: rotate(90deg);
290:     }
291:     
292:     .prompt-actions {
293:         flex-direction: column;
294:         gap: 12px;
295:     }
296:     
297:     .workflow-launch-btn,
298:     .workflow-dismiss-btn {
299:         width: 100%;
300:         justify-content: center;
301:     }
302: }
303: 
304: /* Dark mode adjustments */
305: @media (prefers-color-scheme: dark) {
306:     .csv-workflow-prompt .popup-content {
307:         background: linear-gradient(135deg, #2c2c3e 0%, #1a1a1a 100%);
308:         border-color: var(--accent-primary);
309:         color: #e0e0e0;
310:     }
311: 
312:     .prompt-header {
313:         background: linear-gradient(90deg, rgba(121, 134, 203, 0.15) 0%, rgba(44, 44, 62, 0.95) 100%);
314:         border-bottom-color: #404040;
315:     }
316: 
317:     .workflow-preview {
318:         background: #3a3a4e;
319:         border-color: #404040;
320:     }
321: 
322:     .prompt-footer {
323:         background: #3a3a4e;
324:         border-top-color: #404040;
325:     }
326: 
327:     .workflow-dismiss-btn {
328:         background: #3a3a4e;
329:         color: #a0a0b0;
330:         border-color: #404040;
331:     }
332: 
333:     .workflow-dismiss-btn:hover {
334:         background: #4a4a5e;
335:         color: #e0e0e0;
336:     }
337: 
338:     .download-title {
339:         color: #e0e0e0;
340:     }
341: 
342:     .download-subtitle {
343:         color: #a0a0b0;
344:     }
345: 
346:     .main-message {
347:         color: #e0e0e0;
348:     }
349: 
350:     .footer-note {
351:         color: #a0a0b0;
352:     }
353: }
354: 
355: /* Accessibility improvements */
356: .workflow-launch-btn:focus,
357: .workflow-dismiss-btn:focus {
358:     outline: 2px solid var(--accent-primary);
359:     outline-offset: 2px;
360: }
361: 
362: /* High contrast mode support */
363: @media (prefers-contrast: high) {
364:     .csv-workflow-prompt .popup-content {
365:         border-width: 3px;
366:     }
367:     
368:     .step-badge {
369:         border: 2px solid currentColor;
370:     }
371:     
372:     .workflow-launch-btn,
373:     .workflow-dismiss-btn {
374:         border-width: 2px;
375:     }
376: }
```

## File: static/css/components/downloads.css
```css
  1: /* Styles pour la nouvelle section des téléchargements locaux */
  2: .local-downloads-section {
  3:     background-color: var(--bg-card);
  4:     border: 1px solid var(--border-color);
  5:     padding: 25px;
  6:     margin: 25px auto;
  7:     border-radius: 12px;
  8:     box-shadow: 0 5px 15px rgba(0,0,0,0.2);
  9:     width: 100%;
 10:     max-width: 900px;
 11:     /* Animated visibility (GPU-friendly). Visible by default to avoid empty space before JS runs */
 12:     opacity: 1;
 13:     transform: none;
 14:     transition: opacity 0.32s cubic-bezier(0.4, 0, 0.2, 1),
 15:                 transform 0.32s cubic-bezier(0.4, 0, 0.2, 1),
 16:                 box-shadow 0.32s ease;
 17:     will-change: opacity, transform;
 18: }
 19: 
 20: .local-downloads-section[data-visible="false"] {
 21:     opacity: 0;
 22:     transform: translateX(24px) scale(0.985);
 23: }
 24: 
 25: .local-downloads-section[data-visible="true"] {
 26:     opacity: 1;
 27:     transform: translateX(0) scale(1);
 28:     box-shadow: 0 10px 30px rgba(0,0,0,0.26);
 29: }
 30: 
 31: @media (prefers-reduced-motion: reduce) {
 32:     .local-downloads-section {
 33:         transition: none !important;
 34:         transform: none !important;
 35:     }
 36: }
 37: 
 38: .local-downloads-section h2 {
 39:     color: var(--accent-secondary);
 40:     margin-top: 0;
 41:     border-bottom: 1px solid var(--border-color);
 42:     padding-bottom: 10px;
 43:     margin-bottom: 20px;
 44:     font-size: 1.5em;
 45:     display: flex;
 46:     align-items: center;
 47:     font-weight: 500;
 48: }
 49: .local-downloads-section h2 .section-icon {
 50:     margin-right: 12px;
 51:     font-size: 1.6em;
 52: }
 53: 
 54: .local-downloads-list-container {
 55:     max-height: 350px;
 56:     overflow-y: auto;
 57:     padding-right: 10px;
 58: }
 59: 
 60: .status-list { 
 61:     list-style-type: none;
 62:     padding-left: 0;
 63:     margin:0;
 64: }
 65: 
 66: .status-list li {
 67:     padding: 10px 5px;
 68:     border-bottom: 1px solid var(--border-color-translucent);
 69:     font-size: 0.95em;
 70:     display: flex;
 71:     flex-wrap: wrap;
 72:     gap: 5px 10px;
 73: }
 74: .status-list li:last-child {
 75:     border-bottom: none;
 76: }
 77: .status-list li.placeholder {
 78:     color: var(--text-secondary);
 79:     font-style: italic;
 80:     justify-content: center;
 81: }
 82: 
 83: .status-list li .timestamp {
 84:     color: var(--text-secondary);
 85:     font-size: 0.9em;
 86:     margin-right: 8px;
 87:     flex-shrink: 0;
 88: }
 89: .status-list li .filename {
 90:     font-weight: 500;
 91:     color: var(--text-primary);
 92:     margin-right: 8px;
 93:     overflow: hidden;
 94:     text-overflow: ellipsis;
 95:     white-space: nowrap;
 96:     max-width: 300px;
 97: }
 98: .status-list li .status-text {
 99:     font-weight: bold;
100: }
101: .status-list li .progress-percentage {
102:     color: var(--blue);
103:     font-weight: bold;
104: }
105: .status-list li .message {
106:     font-size: 0.85em;
107:     color: var(--text-secondary);
108:     font-style: italic;
109:     max-width: 300px;
110:     overflow: hidden;
111:     text-overflow: ellipsis;
112:     white-space: nowrap;
113: }
114: 
115: .download-status-pending .status-text { color: var(--blue); }
116: .download-status-starting .status-text { color: var(--blue); }
117: .download-status-downloading .status-text { color: var(--yellow); }
118: .download-status-completed .status-text { color: var(--green); }
119: .download-status-failed .status-text { color: var(--red); }
120: 
121: 
122: /* --- Unified Controls: Downloads Toggle Button --- */
123: .downloads-toggle {
124:     margin-left: 6px;
125:     padding: 6px 10px;
126:     border-radius: 8px;
127:     border: 1px solid var(--border-color);
128:     background: var(--bg-muted);
129:     color: var(--text-primary);
130:     cursor: pointer;
131:     transition: background-color 0.2s ease, color 0.2s ease, border-color 0.2s ease;
132: }
133: .downloads-toggle[aria-pressed="true"] {
134:     background: var(--accent-primary-light);
135:     border-color: var(--accent-primary);
136: }
137: .downloads-toggle--hidden {
138:     opacity: 0.85;
139: }
140: .downloads-toggle--alert {
141:     background: var(--red-translucent, rgba(220, 53, 69, 0.12));
142:     border-color: var(--red, #dc3545);
143:     color: var(--red, #dc3545);
144: }
```

## File: static/css/components/notifications.css
```css
 1: /* --- Notification Area Styles --- */
 2: #notifications-area {
 3:     position: fixed;
 4:     top: calc(var(--topbar-height) + 10px);
 5:     right: 20px;
 6:     z-index: 1200;
 7:     width: 300px;
 8:     max-height: 90vh;
 9:     overflow-y: auto;
10:     display: flex;
11:     flex-direction: column;
12:     gap: 8px;
13: }
14: 
15: .notification {
16:     background-color: var(--bg-card);
17:     color: var(--text-primary);
18:     padding: 15px;
19:     margin-bottom: 10px;
20:     border-radius: 8px;
21:     box-shadow: 0 2px 10px rgba(0,0,0,0.3);
22:     font-size: 0.9em;
23:     opacity: 0.95;
24:     border-left: 5px solid var(--blue);
25: }
26: 
27: .notification.error {
28:     border-left-color: var(--red);
29: }
30: .notification.success {
31:     border-left-color: var(--green);
32: }
33: .notification.warning {
34:     border-left-color: var(--yellow);
35:     color: #333;
36: }
37: /* --- End Notification Area Styles --- */
```

## File: static/css/components/popups.css
```css
  1: .popup-overlay {
  2:     display: none; 
  3:     position: fixed;
  4:     top: 0;
  5:     left: 0;
  6:     width: 100%;
  7:     height: 100%;
  8:     background: rgba(0,0,0,0.7);
  9:     justify-content: center;
 10:     align-items: center;
 11:     z-index: 1000;
 12:     /* Transitions for smooth show/hide when data-visible toggles */
 13:     opacity: 0;
 14:     transform: scale(0.98);
 15:     transition: opacity 0.25s cubic-bezier(0.4, 0, 0.2, 1),
 16:                 transform 0.25s cubic-bezier(0.4, 0, 0.2, 1);
 17:     will-change: opacity, transform;
 18: }
 19: 
 20: /* Respect users who prefer reduced motion */
 21: @media (prefers-reduced-motion: reduce) {
 22:     .popup-overlay {
 23:         transition: none !important;
 24:         transform: none !important;
 25:     }
 26:     .popup-content {
 27:         animation: none !important;
 28:     }
 29: }
 30: .popup-content {
 31:     background: var(--bg-card);
 32:     padding: 30px;
 33:     border-radius: 10px;
 34:     min-width: 300px;
 35:     max-width: 500px;
 36:     box-shadow: 0 0 20px rgba(0,0,0,0.5);
 37:     color: var(--text-primary);
 38:     /* Subtle entrance animation */
 39:     animation: modalSlideIn 0.3s cubic-bezier(0.34, 1.56, 0.64, 1);
 40: }
 41: .popup-content h3 {
 42:     color: var(--accent-primary);
 43:     margin-top: 0;
 44:     text-align: center;
 45: }
 46: .popup-list {
 47:     list-style: none;
 48:     padding: 0;
 49:     margin-bottom: 20px;
 50: }
 51: .popup-list li {
 52:     padding: 8px 0;
 53:     border-bottom: 1px solid var(--border-color);
 54: }
 55: .popup-list li:last-child {
 56:     border-bottom: none;
 57: }
 58: .popup-list li .order-prefix {
 59:     font-weight: bold;
 60:     color: var(--orange);
 61:     margin-right: 10px;
 62: }
 63: .status-icon {
 64:     margin-right: 8px;
 65: }
 66: .summary-popup li .duration {
 67:     font-size: 0.9em;
 68:     color: var(--text-secondary);
 69:     margin-left: 10px;
 70: }
 71: .popup-buttons {
 72:     display: flex;
 73:     justify-content: space-around;
 74:     margin-top: 20px;
 75: }
 76: .popup-buttons button {
 77:     padding: 10px 20px;
 78:     border: none;
 79:     border-radius: 5px;
 80:     cursor: pointer;
 81:     font-weight: bold;
 82: }
 83: .popup-confirm-button { background-color: var(--green); color: white; }
 84: .popup-cancel-button, #close-summary-popup { background-color: var(--red); color: white; }
 85: 
 86: #close-summary-popup {
 87:      display: block;
 88:      margin: 20px auto 0 auto;
 89: }
 90: #close-summary-popup:hover { background-color: #d9534f; }
 91: .popup-confirm-button:hover { background-color: #5cb85c; }
 92: 
 93: /* Keyboard accessibility: focus visible ring */
 94: .popup-buttons button:focus-visible,
 95: .popup-confirm-button:focus-visible,
 96: .popup-cancel-button:focus-visible,
 97: #close-summary-popup:focus-visible {
 98:     outline: none;
 99:     box-shadow: 0 0 0 3px color-mix(in oklab, var(--accent-primary) 35%, transparent);
100: }
101: 
102: /* Visible state driven by data attribute for smoother transitions */
103: .popup-overlay[data-visible="true"] {
104:     opacity: 1;
105:     transform: scale(1);
106: }
107: 
108: @keyframes modalSlideIn {
109:     from {
110:         opacity: 0;
111:         transform: translateY(-16px) scale(0.97);
112:     }
113:     to {
114:         opacity: 1;
115:         transform: translateY(0) scale(1);
116:     }
117: }
```

## File: static/css/components/widgets.css
```css
  1: /* --- Generic loader & button loading state --- */
  2: .loading-spinner {
  3:     display: inline-block;
  4:     width: 16px;
  5:     height: 16px;
  6:     border: 2px solid var(--border-color);
  7:     border-top-color: var(--accent-primary);
  8:     border-radius: 50%;
  9:     animation: spin 0.8s linear infinite;
 10:     vertical-align: middle;
 11:     margin-left: 8px;
 12: }
 13: 
 14: button[data-loading="true"] {
 15:     pointer-events: none;
 16:     opacity: 0.75;
 17: }
 18: 
 19: button[data-loading="true"]::before {
 20:     content: '';
 21:     display: inline-block;
 22:     width: 14px;
 23:     height: 14px;
 24:     margin-right: 8px;
 25:     border: 2px solid currentColor;
 26:     border-top-color: transparent;
 27:     border-radius: 50%;
 28:     animation: spin 0.8s linear infinite;
 29:     vertical-align: -2px;
 30: }
 31: 
 32: /* --- Styles pour le Widget de Monitoring Système --- */
 33: .system-monitor-widget {
 34:     position: fixed;
 35:     top: 24px;
 36:     right: 24px;
 37:     width: 260px;
 38:     background-color: var(--bg-card);
 39:     border: 1px solid var(--border-color);
 40:     border-radius: 10px;
 41:     padding: 15px;
 42:     box-shadow: 0 5px 20px rgba(0,0,0,0.3);
 43:     z-index: 1000;
 44:     font-size: 0.9em;
 45:     color: var(--text-secondary);
 46:     transition: opacity 0.3s ease, transform 0.3s ease, height 0.25s ease, width 0.25s ease, padding 0.25s ease, right 0.25s ease, top 0.25s ease;
 47: }
 48: 
 49: .workflow-wrapper.logs-active + .logs-column + #system-monitor-widget,
 50: .workflow-wrapper.logs-active ~ #system-monitor-widget {
 51:     transform: translateX(-12px);
 52: }
 53: 
 54: .monitor-header {
 55:     display: flex;
 56:     align-items: center;
 57:     margin-bottom: 15px;
 58:     padding-bottom: 8px;
 59:     border-bottom: 1px solid var(--border-color);
 60: }
 61: 
 62: .monitor-icon {
 63:     font-size: 1.5em;
 64:     margin-right: 10px;
 65: }
 66: 
 67: .monitor-title {
 68:     font-weight: 600;
 69:     color: var(--accent-primary);
 70: }
 71: 
 72: .monitor-close {
 73:     margin-left: auto;
 74:     background: transparent;
 75:     border: 0;
 76:     color: var(--text-secondary);
 77:     font-size: 1.2em;
 78:     line-height: 1;
 79:     cursor: pointer;
 80: }
 81: 
 82: .monitor-close:hover,
 83: .monitor-close:focus {
 84:     color: var(--text-primary);
 85: }
 86: 
 87: /* Keyboard accessibility: focus ring for widget controls */
 88: .monitor-close:focus-visible,
 89: .upload-button:focus-visible,
 90: .btn-like-switch:focus-visible {
 91:     outline: none;
 92:     box-shadow: 0 0 0 3px color-mix(in oklab, var(--accent-primary) 35%, transparent);
 93:     border-color: var(--accent-primary);
 94: }
 95: 
 96: .monitor-item {
 97:     display: flex;
 98:     align-items: center;
 99:     margin-bottom: 10px;
100: }
101: 
102: .monitor-label {
103:     width: 35px; /* Pour aligner les barres */
104:     font-weight: 500;
105: }
106: 
107: .monitor-bar-container {
108:     flex-grow: 1;
109:     height: 16px;
110:     background-color: var(--border-color);
111:     border-radius: 8px;
112:     padding: 2px;
113:     margin: 0 8px;
114: }
115: 
116: .monitor-bar {
117:     height: 100%;
118:     width: 0%;
119:     border-radius: 6px;
120:     transition: width 0.5s ease-out, background-color 0.5s ease;
121:     background-color: var(--green);
122: }
123: 
124: .monitor-value {
125:     width: 50px; /* Pour aligner */
126:     text-align: right;
127:     font-weight: bold;
128:     font-family: 'SFMono-Regular', Consolas, monospace;
129: }
130: 
131: .monitor-details {
132:     font-size: 0.8em;
133:     text-align: center;
134:     color: var(--text-secondary);
135:     margin-top: 5px;
136: }
137: 
138: /* Compact/minimized mode */
139: .system-monitor-widget.minimized {
140:     width: auto; /* auto-size to content */
141:     min-width: 220px;
142:     max-width: 360px; /* cap to avoid overflow */
143:     padding: 8px 10px;
144: }
145: 
146: .system-monitor-widget.minimized .monitor-header {
147:     margin-bottom: 6px;
148:     padding-bottom: 4px;
149: }
150: 
151: .system-monitor-widget.minimized .monitor-item,
152: .system-monitor-widget.minimized #ram-monitor-details,
153: .system-monitor-widget.minimized #gpu-monitor-section,
154: .system-monitor-widget.minimized #gpu-monitor-error {
155:     display: none !important;
156: }
157: 
158: .monitor-compact-line {
159:     display: flex;
160:     align-items: center;
161:     justify-content: flex-start;
162:     gap: 2px;
163:     font-family: 'SFMono-Regular', Consolas, monospace;
164:     font-size: 0.62em; /* tighter */
165:     letter-spacing: -0.04em; /* tighter */
166:     white-space: nowrap;
167:     overflow: hidden;
168:     text-overflow: clip; /* prefer full display within max-width */
169:     color: var(--text-primary);
170: }
171: 
172: /* Styles pour les couleurs des barres en fonction de l'utilisation */
173: #cpu-monitor-bar[data-usage-level="high"],
174: #ram-monitor-bar[data-usage-level="high"] {
175:     background-color: var(--red);
176: }
177: 
178: #cpu-monitor-bar[data-usage-level="medium"],
179: #ram-monitor-bar[data-usage-level="medium"] {
180:     background-color: var(--yellow);
181: }
182: 
183: #cpu-monitor-bar[data-usage-level="low"],
184: #ram-monitor-bar[data-usage-level="low"] {
185:     background-color: var(--green);
186: }
187: 
188: /* --- Styles pour le Widget de Contrôle Auto-Scroll --- */
189: .auto-scroll-widget {
190:     position: static;
191:     width: 100%;
192:     background-color: var(--bg-secondary);
193:     border: 1px solid var(--border-color);
194:     border-radius: 8px;
195:     padding: 12px;
196:     box-shadow: none;
197:     font-size: 0.9em;
198:     display: flex;
199:     flex-direction: column;
200:     gap: 8px;
201: }
202: 
203: .auto-scroll-header {
204:     display: flex;
205:     align-items: center;
206:     margin-bottom: 8px;
207:     font-weight: 600;
208:     color: var(--text-primary);
209: }
210: 
211: .auto-scroll-icon {
212:     margin-right: 6px;
213:     font-size: 1.1em;
214: }
215: 
216: .auto-scroll-title {
217:     font-size: 0.9em;
218: }
219: 
220: .auto-scroll-control {
221:     display: flex;
222:     align-items: center;
223:     justify-content: space-between;
224: }
225: 
226: .auto-scroll-switch {
227:     position: relative;
228:     display: inline-block;
229:     width: 44px;
230:     height: 24px;
231: }
232: 
233: .auto-scroll-switch input {
234:     opacity: 0;
235:     width: 0;
236:     height: 0;
237: }
238: 
239: .auto-scroll-slider {
240:     position: absolute;
241:     cursor: pointer;
242:     top: 0;
243:     left: 0;
244:     right: 0;
245:     bottom: 0;
246:     background-color: var(--border-color);
247:     transition: 0.3s;
248:     border-radius: 24px;
249: }
250: 
251: .auto-scroll-slider:before {
252:     position: absolute;
253:     content: "";
254:     height: 18px;
255:     width: 18px;
256:     left: 3px;
257:     bottom: 3px;
258:     background-color: white;
259:     transition: 0.3s;
260:     border-radius: 50%;
261: }
262: 
263: input:checked + .auto-scroll-slider {
264:     background-color: var(--accent-primary);
265: }
266: 
267: input:checked + .auto-scroll-slider:before {
268:     transform: translateX(20px);
269: }
270: 
271: .auto-scroll-status {
272:     font-size: 0.8em;
273:     color: var(--text-secondary);
274:     margin-left: 8px;
275: }
276: 
277: /* --- Styles pour le Widget de Contrôle Sonore --- */
278: .sound-control-widget {
279:     position: static;
280:     width: 100%;
281:     background-color: var(--bg-secondary);
282:     border: 1px solid var(--border-color);
283:     border-radius: 8px;
284:     padding: 12px;
285:     box-shadow: none;
286:     font-size: 0.9em;
287:     display: flex;
288:     flex-direction: column;
289:     gap: 8px;
290: }
291: 
292: 
293: .sound-control-header {
294:     display: flex;
295:     align-items: center;
296:     margin-bottom: 8px;
297:     font-weight: 600;
298:     color: var(--text-primary);
299: }
300: 
301: .sound-control-icon {
302:     margin-right: 6px;
303:     font-size: 1.1em;
304: }
305: 
306: .sound-control-title {
307:     font-size: 0.9em;
308: }
309: 
310: .sound-control-controls {
311:     display: flex;
312:     align-items: center;
313:     justify-content: space-between;
314: }
315: 
316: .sound-control-switch {
317:     position: relative;
318:     display: inline-block;
319:     width: 44px;
320:     height: 24px;
321: }
322: 
323: .sound-control-switch input {
324:     opacity: 0;
325:     width: 0;
326:     height: 0;
327: }
328: 
329: .sound-control-slider {
330:     position: absolute;
331:     cursor: pointer;
332:     top: 0;
333:     left: 0;
334:     right: 0;
335:     bottom: 0;
336:     background-color: var(--border-color);
337:     transition: 0.3s;
338:     border-radius: 24px;
339: }
340: 
341: .sound-control-slider:before {
342:     position: absolute;
343:     content: "";
344:     height: 18px;
345:     width: 18px;
346:     left: 3px;
347:     bottom: 3px;
348:     background-color: white;
349:     transition: 0.3s;
350:     border-radius: 50%;
351: }
352: 
353: input:checked + .sound-control-slider {
354:     background-color: var(--accent-primary);
355: }
356: 
357: input:checked + .sound-control-slider:before {
358:     transform: translateX(20px);
359: }
360: 
361: .sound-control-status {
362:     font-size: 0.8em;
363:     color: var(--text-secondary);
364:     margin-left: 8px;
365: }
366: 
367: /* ===== Overrides for topbar integration (no floating panels) ===== */
```

## File: static/css/features/reports.css
```css
  1: /**
  2:  * Reports Generator Styles
  3:  * Visual report generation feature
  4:  */
  5: 
  6: /* Report Modal */
  7: .report-overlay {
  8:     display: none;
  9:     position: fixed;
 10:     top: 0;
 11:     left: 0;
 12:     right: 0;
 13:     bottom: 0;
 14:     background: rgba(0, 0, 0, 0.7);
 15:     backdrop-filter: blur(5px);
 16:     z-index: 9999;
 17:     align-items: center;
 18:     justify-content: center;
 19:     opacity: 0;
 20:     transition: opacity 0.3s ease;
 21: }
 22: 
 23: .report-overlay[data-visible="true"] {
 24:     opacity: 1;
 25: }
 26: 
 27: .report-modal {
 28:     background: var(--card-bg);
 29:     border-radius: 12px;
 30:     box-shadow: 0 10px 40px rgba(0, 0, 0, 0.3);
 31:     width: 90%;
 32:     max-width: 1200px;
 33:     max-height: 90vh;
 34:     display: flex;
 35:     flex-direction: column;
 36:     transform: scale(0.95);
 37:     transition: transform 0.3s ease;
 38: }
 39: 
 40: .report-overlay[data-visible="true"] .report-modal {
 41:     transform: scale(1);
 42: }
 43: 
 44: /* Report Header */
 45: .report-header {
 46:     padding: 20px 30px;
 47:     border-bottom: 2px solid var(--border-color);
 48:     display: flex;
 49:     align-items: center;
 50:     justify-content: space-between;
 51: }
 52: 
 53: .report-title {
 54:     display: flex;
 55:     align-items: center;
 56:     gap: 12px;
 57:     font-size: 24px;
 58:     font-weight: 600;
 59:     color: var(--text-primary);
 60: }
 61: 
 62: .report-close {
 63:     background: none;
 64:     border: none;
 65:     font-size: 28px;
 66:     cursor: pointer;
 67:     color: var(--text-secondary);
 68:     transition: all 0.2s ease;
 69:     padding: 5px 10px;
 70:     border-radius: 6px;
 71: }
 72: 
 73: .report-close:hover {
 74:     background: var(--hover-bg);
 75:     color: var(--text-primary);
 76: }
 77: 
 78: /* Report Body */
 79: .report-body {
 80:     flex: 1;
 81:     overflow-y: auto;
 82:     padding: 30px;
 83: }
 84: 
 85: /* Report Selectors */
 86: .report-selectors {
 87:     display: grid;
 88:     grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
 89:     gap: 20px;
 90:     margin-bottom: 30px;
 91: }
 92: 
 93: .report-selector-group {
 94:     display: flex;
 95:     flex-direction: column;
 96:     gap: 8px;
 97: }
 98: 
 99: .report-selector-label {
100:     font-size: 14px;
101:     font-weight: 600;
102:     color: var(--text-secondary);
103:     text-transform: uppercase;
104:     letter-spacing: 0.5px;
105: }
106: 
107: .report-selector {
108:     padding: 12px 16px;
109:     border: 2px solid var(--border-color);
110:     border-radius: 8px;
111:     background: var(--input-bg);
112:     color: var(--text-primary);
113:     font-size: 16px;
114:     cursor: pointer;
115:     transition: all 0.2s ease;
116: }
117: 
118: .report-selector:hover:not(:disabled) {
119:     border-color: var(--accent-color);
120: }
121: 
122: .report-selector:focus {
123:     outline: none;
124:     border-color: var(--accent-color);
125:     box-shadow: 0 0 0 3px rgba(var(--accent-rgb), 0.1);
126: }
127: 
128: .report-selector:disabled {
129:     opacity: 0.5;
130:     cursor: not-allowed;
131: }
132: 
133: /* Report Options */
134: .report-options {
135:     background: var(--section-bg);
136:     padding: 20px;
137:     border-radius: 8px;
138:     margin-bottom: 30px;
139: }
140: 
141: .report-options-title {
142:     font-size: 16px;
143:     font-weight: 600;
144:     margin-bottom: 15px;
145:     color: var(--text-primary);
146: }
147: 
148: .report-format-group {
149:     display: flex;
150:     gap: 15px;
151:     flex-wrap: wrap;
152: }
153: 
154: .report-format-option {
155:     display: flex;
156:     align-items: center;
157:     gap: 8px;
158:     padding: 10px 16px;
159:     border: 2px solid var(--border-color);
160:     border-radius: 8px;
161:     cursor: pointer;
162:     transition: all 0.2s ease;
163:     background: var(--card-bg);
164: }
165: 
166: .report-format-option:hover {
167:     border-color: var(--accent-color);
168:     background: var(--hover-bg);
169: }
170: 
171: .report-format-option input[type="radio"] {
172:     cursor: pointer;
173: }
174: 
175: .report-format-option.selected {
176:     border-color: var(--accent-color);
177:     background: rgba(var(--accent-rgb), 0.1);
178: }
179: 
180: /* Report Actions */
181: .report-actions {
182:     display: flex;
183:     gap: 15px;
184:     justify-content: flex-end;
185:     margin-bottom: 30px;
186: }
187: 
188: .report-button {
189:     padding: 12px 24px;
190:     border: none;
191:     border-radius: 8px;
192:     font-size: 16px;
193:     font-weight: 600;
194:     cursor: pointer;
195:     transition: all 0.2s ease;
196:     display: flex;
197:     align-items: center;
198:     gap: 8px;
199: }
200: 
201: .report-button-primary {
202:     background: var(--accent-color);
203:     color: white;
204: }
205: 
206: .report-button-primary:hover:not(:disabled) {
207:     background: var(--accent-hover);
208:     transform: translateY(-2px);
209:     box-shadow: 0 4px 12px rgba(var(--accent-rgb), 0.3);
210: }
211: 
212: .report-button-secondary {
213:     background: var(--section-bg);
214:     color: var(--text-primary);
215:     border: 2px solid var(--border-color);
216: }
217: 
218: .report-button-secondary:hover:not(:disabled) {
219:     background: var(--hover-bg);
220:     border-color: var(--accent-color);
221: }
222: 
223: .report-button:disabled {
224:     opacity: 0.5;
225:     cursor: not-allowed;
226:     transform: none;
227: }
228: 
229: /* Report Preview */
230: .report-preview {
231:     border: 2px solid var(--border-color);
232:     border-radius: 8px;
233:     background: white;
234:     min-height: 400px;
235:     overflow: hidden;
236: }
237: 
238: .report-preview-iframe {
239:     width: 100%;
240:     height: 600px;
241:     border: none;
242: }
243: 
244: .report-preview-empty {
245:     display: flex;
246:     flex-direction: column;
247:     align-items: center;
248:     justify-content: center;
249:     padding: 60px;
250:     color: var(--text-secondary);
251: }
252: 
253: .report-preview-icon {
254:     font-size: 64px;
255:     margin-bottom: 20px;
256:     opacity: 0.5;
257: }
258: 
259: .report-preview-message {
260:     font-size: 18px;
261:     font-weight: 600;
262:     margin-bottom: 10px;
263: }
264: 
265: .report-preview-detail {
266:     font-size: 14px;
267:     opacity: 0.7;
268: }
269: 
270: /* Loading State */
271: .report-loading {
272:     display: flex;
273:     flex-direction: column;
274:     align-items: center;
275:     justify-content: center;
276:     padding: 60px;
277: }
278: 
279: .report-loading-spinner {
280:     width: 50px;
281:     height: 50px;
282:     border: 4px solid var(--border-color);
283:     border-top-color: var(--accent-color);
284:     border-radius: 50%;
285:     animation: report-spin 1s linear infinite;
286: }
287: 
288: @keyframes report-spin {
289:     to {
290:         transform: rotate(360deg);
291:     }
292: }
293: 
294: .report-loading-text {
295:     margin-top: 20px;
296:     font-size: 16px;
297:     color: var(--text-secondary);
298: }
299: 
300: /* Download Info */
301: .report-download-info {
302:     background: var(--success-bg);
303:     border: 2px solid var(--success-color);
304:     border-radius: 8px;
305:     padding: 15px 20px;
306:     margin-top: 20px;
307:     display: flex;
308:     align-items: center;
309:     gap: 12px;
310:     color: var(--success-text);
311: }
312: 
313: .report-download-icon {
314:     font-size: 24px;
315: }
316: 
317: /* Responsive */
318: @media (max-width: 768px) {
319:     .report-modal {
320:         width: 95%;
321:         max-height: 95vh;
322:     }
323:     
324:     .report-header {
325:         padding: 15px 20px;
326:     }
327:     
328:     .report-body {
329:         padding: 20px;
330:     }
331:     
332:     .report-selectors {
333:         grid-template-columns: 1fr;
334:     }
335:     
336:     .report-actions {
337:         flex-direction: column;
338:     }
339:     
340:     .report-button {
341:         width: 100%;
342:         justify-content: center;
343:     }
344: }
```

## File: static/css/features/responsive.css
```css
  1: /* Ajustements Responsive */
  2: @media (max-width: 768px) {
  3:     body {
  4:       padding-top: 70px; 
  5:       padding-left: 10px;
  6:       padding-right: 10px;
  7:     }
  8:     #clear-cache-global-button {
  9:         top:10px; left:10px;
 10:         padding: 8px 12px;
 11:         font-size: 0.8em;
 12:     }
 13:     /* Unified controls section mobile styles */
 14:     .unified-controls-section {
 15:         margin: 20px auto 30px auto;
 16:         padding: 20px;
 17:         max-width: 95%;
 18:     }
 19: 
 20:     .auto-mode-container {
 21:         flex-direction: column;
 22:         gap: 8px;
 23:         padding: 10px;
 24:     }
 25: 
 26:     .workflow-controls {
 27:         flex-direction: column;
 28:         gap: 12px;
 29:     }
 30: 
 31:     /* Enhanced mobile styling for workflow control buttons */
 32:     .workflow-controls button {
 33:         width: 100%;
 34:         padding: 14px 20px;
 35:         font-size: 1em;
 36:         min-width: unset;
 37:         border-radius: 25px;
 38:         margin: 0;
 39:     }
 40: 
 41:     #run-all-steps-button, #run-custom-sequence-button, #clear-custom-sequence-button {
 42:         box-shadow: 0 3px 10px rgba(0, 0, 0, 0.15);
 43:     }
 44: 
 45:     #run-all-steps-button:hover, #run-custom-sequence-button:hover, #clear-custom-sequence-button:hover {
 46:         transform: translateY(-2px);
 47:         box-shadow: 0 4px 15px rgba(0, 0, 0, 0.2);
 48:     }
 49:     .auto-mode-label { margin-right: 0;}
 50: 
 51:     .local-downloads-section {
 52:         max-width: 95%;
 53:         padding: 15px;
 54:     }
 55:     .local-downloads-section h2 {
 56:         font-size: 1.3em;
 57:     }
 58:     .local-downloads-section h2 .section-icon {
 59:         font-size: 1.4em;
 60:     }
 61:     .status-list li .filename,
 62:     .status-list li .message {
 63:         max-width: 100%;
 64:         white-space: normal;
 65:     }
 66: 
 67:     .system-monitor-widget {
 68:         display: none; /* Cacher le widget sur les petits écrans pour ne pas surcharger */
 69:     }
 70: 
 71:     .auto-scroll-widget {
 72:         width: 160px;
 73:         font-size: 0.8em;
 74:         padding: 8px;
 75:     }
 76: 
 77:     .auto-scroll-switch {
 78:         width: 36px;
 79:         height: 20px;
 80:     }
 81: 
 82:     .auto-scroll-slider:before {
 83:         height: 14px;
 84:         width: 14px;
 85:         left: 3px;
 86:         bottom: 3px;
 87:     }
 88: 
 89:     input:checked + .auto-scroll-slider:before {
 90:         transform: translateX(16px);
 91:     }
 92: 
 93:     .sound-control-widget {
 94:         width: 160px;
 95:         font-size: 0.8em;
 96:         padding: 8px;
 97:         top: 70px; /* Positioned below auto-scroll widget on smaller screens */
 98:         left: 15px; /* Aligned with auto-scroll widget */
 99:     }
100: 
101:     .sound-control-switch {
102:         width: 36px;
103:         height: 20px;
104:     }
105: 
106:     .sound-control-slider:before {
107:         height: 14px;
108:         width: 14px;
109:         left: 3px;
110:         bottom: 3px;
111:     }
112: 
113:     input:checked + .sound-control-slider:before {
114:         transform: translateX(16px);
115:     }
116: }
```

## File: static/css/features/stats-dashboard.css
```css
  1: /* =============================================================================
  2:    STATS DASHBOARD - Workflow MediaPipe v4.0
  3:    Performance statistics dashboard with charts and metrics
  4:    ============================================================================= */
  5: 
  6: /* Stats Dashboard Modal */
  7: .stats-dashboard-overlay {
  8:     position: fixed;
  9:     top: 0;
 10:     left: 0;
 11:     right: 0;
 12:     bottom: 0;
 13:     background: rgba(0, 0, 0, 0.7);
 14:     backdrop-filter: blur(5px);
 15:     display: flex;
 16:     align-items: center;
 17:     justify-content: center;
 18:     z-index: 10000;
 19:     padding: 20px;
 20:     opacity: 0;
 21:     transition: opacity 0.3s ease;
 22: }
 23: 
 24: .stats-dashboard-overlay[data-visible="true"] {
 25:     opacity: 1;
 26: }
 27: 
 28: .stats-dashboard-content {
 29:     background: var(--bg-card);
 30:     border: 2px solid var(--border-color);
 31:     border-radius: 12px;
 32:     max-width: 1200px;
 33:     width: 100%;
 34:     max-height: 90vh;
 35:     overflow: hidden;
 36:     display: flex;
 37:     flex-direction: column;
 38:     box-shadow: 0 10px 40px rgba(0, 0, 0, 0.3);
 39:     transform: scale(0.95);
 40:     transition: transform 0.3s ease;
 41: }
 42: 
 43: .stats-dashboard-overlay[data-visible="true"] .stats-dashboard-content {
 44:     transform: scale(1);
 45: }
 46: 
 47: /* Dashboard Header */
 48: .stats-dashboard-header {
 49:     display: flex;
 50:     justify-content: space-between;
 51:     align-items: center;
 52:     padding: 20px 24px;
 53:     border-bottom: 2px solid var(--border-color);
 54:     background: linear-gradient(135deg, var(--bg-secondary) 0%, var(--bg-card) 100%);
 55: }
 56: 
 57: .stats-dashboard-title {
 58:     display: flex;
 59:     align-items: center;
 60:     gap: 12px;
 61:     font-size: 24px;
 62:     font-weight: 600;
 63:     color: var(--accent-primary);
 64: }
 65: 
 66: .stats-dashboard-title::before {
 67:     content: '📊';
 68:     font-size: 28px;
 69: }
 70: 
 71: .stats-dashboard-close {
 72:     background: none;
 73:     border: none;
 74:     font-size: 32px;
 75:     color: var(--text-secondary);
 76:     cursor: pointer;
 77:     padding: 4px 12px;
 78:     line-height: 1;
 79:     transition: all 0.2s ease;
 80:     border-radius: 4px;
 81: }
 82: 
 83: .stats-dashboard-close:hover {
 84:     background: var(--bg-tertiary);
 85:     color: var(--text-primary);
 86: }
 87: 
 88: /* Dashboard Body */
 89: .stats-dashboard-body {
 90:     overflow-y: auto;
 91:     padding: 24px;
 92:     flex: 1;
 93: }
 94: 
 95: /* Summary Cards */
 96: .stats-summary-grid {
 97:     display: grid;
 98:     grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
 99:     gap: 16px;
100:     margin-bottom: 24px;
101: }
102: 
103: .stats-summary-card {
104:     background: var(--bg-secondary);
105:     border: 1px solid var(--border-color);
106:     border-radius: 8px;
107:     padding: 20px;
108:     transition: all 0.3s ease;
109: }
110: 
111: .stats-summary-card:hover {
112:     border-color: var(--accent-primary);
113:     box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
114:     transform: translateY(-2px);
115: }
116: 
117: .stats-summary-card-label {
118:     font-size: 13px;
119:     color: var(--text-secondary);
120:     text-transform: uppercase;
121:     letter-spacing: 0.5px;
122:     margin-bottom: 8px;
123:     font-weight: 500;
124: }
125: 
126: .stats-summary-card-value {
127:     font-size: 32px;
128:     font-weight: 700;
129:     color: var(--text-primary);
130:     line-height: 1.2;
131: }
132: 
133: .stats-summary-card-unit {
134:     font-size: 16px;
135:     color: var(--text-secondary);
136:     margin-left: 4px;
137: }
138: 
139: .stats-summary-card-subtitle {
140:     font-size: 12px;
141:     color: var(--text-muted);
142:     margin-top: 8px;
143: }
144: 
145: /* Icon colors for different card types */
146: .stats-summary-card.api-calls { border-left: 4px solid var(--blue); }
147: .stats-summary-card.errors { border-left: 4px solid var(--red); }
148: .stats-summary-card.response-time { border-left: 4px solid var(--green); }
149: .stats-summary-card.error-rate { border-left: 4px solid var(--orange); }
150: 
151: /* Chart Sections */
152: .stats-chart-section {
153:     background: var(--bg-secondary);
154:     border: 1px solid var(--border-color);
155:     border-radius: 8px;
156:     padding: 20px;
157:     margin-bottom: 20px;
158: }
159: 
160: .stats-chart-header {
161:     display: flex;
162:     justify-content: space-between;
163:     align-items: center;
164:     margin-bottom: 16px;
165:     padding-bottom: 12px;
166:     border-bottom: 1px solid var(--border-color);
167: }
168: 
169: .stats-chart-title {
170:     font-size: 18px;
171:     font-weight: 600;
172:     color: var(--text-primary);
173:     display: flex;
174:     align-items: center;
175:     gap: 8px;
176: }
177: 
178: .stats-chart-controls {
179:     display: flex;
180:     gap: 8px;
181: }
182: 
183: .stats-chart-button {
184:     background: var(--bg-tertiary);
185:     border: 1px solid var(--border-color);
186:     color: var(--text-secondary);
187:     padding: 6px 12px;
188:     border-radius: 4px;
189:     font-size: 13px;
190:     cursor: pointer;
191:     transition: all 0.2s ease;
192: }
193: 
194: .stats-chart-button:hover {
195:     background: var(--bg-card);
196:     border-color: var(--accent-primary);
197:     color: var(--accent-primary);
198: }
199: 
200: .stats-chart-button.active {
201:     background: var(--accent-primary);
202:     border-color: var(--accent-primary);
203:     color: white;
204: }
205: 
206: .stats-chart-container {
207:     position: relative;
208:     height: 300px;
209:     margin-top: 12px;
210: }
211: 
212: .stats-chart-canvas {
213:     max-height: 300px;
214: }
215: 
216: /* Loading State */
217: .stats-loading {
218:     display: flex;
219:     flex-direction: column;
220:     align-items: center;
221:     justify-content: center;
222:     padding: 60px 20px;
223:     color: var(--text-secondary);
224: }
225: 
226: .stats-loading-spinner {
227:     width: 48px;
228:     height: 48px;
229:     border: 4px solid var(--bg-tertiary);
230:     border-top-color: var(--accent-primary);
231:     border-radius: 50%;
232:     animation: stats-spin 1s linear infinite;
233:     margin-bottom: 16px;
234: }
235: 
236: @keyframes stats-spin {
237:     to { transform: rotate(360deg); }
238: }
239: 
240: .stats-loading-text {
241:     font-size: 16px;
242: }
243: 
244: /* Error State */
245: .stats-error {
246:     background: rgba(239, 83, 80, 0.1);
247:     border: 1px solid var(--red);
248:     border-radius: 8px;
249:     padding: 20px;
250:     color: var(--red);
251:     text-align: center;
252: }
253: 
254: .stats-error-icon {
255:     font-size: 48px;
256:     margin-bottom: 12px;
257: }
258: 
259: .stats-error-message {
260:     font-size: 16px;
261:     margin-bottom: 8px;
262: }
263: 
264: .stats-error-detail {
265:     font-size: 13px;
266:     color: var(--text-secondary);
267: }
268: 
269: /* Empty State */
270: .stats-empty {
271:     text-align: center;
272:     padding: 60px 20px;
273:     color: var(--text-secondary);
274: }
275: 
276: .stats-empty-icon {
277:     font-size: 64px;
278:     margin-bottom: 16px;
279:     opacity: 0.5;
280: }
281: 
282: .stats-empty-message {
283:     font-size: 18px;
284:     margin-bottom: 8px;
285: }
286: 
287: .stats-empty-detail {
288:     font-size: 14px;
289: }
290: 
291: /* Alerts Section */
292: .stats-alerts-list {
293:     max-height: 200px;
294:     overflow-y: auto;
295: }
296: 
297: .stats-alert-item {
298:     background: var(--bg-card);
299:     border-left: 4px solid var(--orange);
300:     padding: 12px 16px;
301:     margin-bottom: 8px;
302:     border-radius: 4px;
303:     display: flex;
304:     align-items: flex-start;
305:     gap: 12px;
306: }
307: 
308: .stats-alert-item.severity-warning {
309:     border-left-color: var(--orange);
310: }
311: 
312: .stats-alert-item.severity-error {
313:     border-left-color: var(--red);
314: }
315: 
316: .stats-alert-icon {
317:     font-size: 20px;
318:     flex-shrink: 0;
319: }
320: 
321: .stats-alert-content {
322:     flex: 1;
323: }
324: 
325: .stats-alert-message {
326:     font-size: 14px;
327:     color: var(--text-primary);
328:     margin-bottom: 4px;
329: }
330: 
331: .stats-alert-timestamp {
332:     font-size: 12px;
333:     color: var(--text-muted);
334: }
335: 
336: /* Step History Table */
337: .stats-step-table {
338:     width: 100%;
339:     border-collapse: collapse;
340: }
341: 
342: .stats-step-table th,
343: .stats-step-table td {
344:     padding: 12px;
345:     text-align: left;
346:     border-bottom: 1px solid var(--border-color);
347: }
348: 
349: .stats-step-table th {
350:     background: var(--bg-tertiary);
351:     color: var(--text-secondary);
352:     font-weight: 600;
353:     font-size: 13px;
354:     text-transform: uppercase;
355:     letter-spacing: 0.5px;
356: }
357: 
358: .stats-step-table td {
359:     color: var(--text-primary);
360:     font-size: 14px;
361: }
362: 
363: .stats-step-table tr:hover td {
364:     background: var(--bg-tertiary);
365: }
366: 
367: .stats-step-name {
368:     font-weight: 500;
369:     color: var(--accent-primary);
370: }
371: 
372: /* Dashboard Button */
373: .stats-dashboard-button {
374:     background: var(--bg-secondary);
375:     border: 1px solid var(--border-color);
376:     color: var(--text-primary);
377:     padding: 8px 16px;
378:     border-radius: 6px;
379:     cursor: pointer;
380:     transition: all 0.3s ease;
381:     font-size: 14px;
382:     font-weight: 500;
383:     display: inline-flex;
384:     align-items: center;
385:     gap: 8px;
386: }
387: 
388: .stats-dashboard-button::before {
389:     content: '📊';
390:     font-size: 16px;
391: }
392: 
393: .stats-dashboard-button:hover {
394:     background: var(--bg-tertiary);
395:     border-color: var(--accent-primary);
396:     color: var(--accent-primary);
397:     box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
398: }
399: 
400: /* Responsive Design */
401: @media (max-width: 768px) {
402:     .stats-dashboard-content {
403:         max-width: 100%;
404:         max-height: 95vh;
405:         border-radius: 8px;
406:     }
407:     
408:     .stats-summary-grid {
409:         grid-template-columns: 1fr;
410:     }
411:     
412:     .stats-chart-container {
413:         height: 250px;
414:     }
415:     
416:     .stats-dashboard-header {
417:         padding: 16px;
418:     }
419:     
420:     .stats-dashboard-body {
421:         padding: 16px;
422:     }
423:     
424:     .stats-chart-controls {
425:         flex-wrap: wrap;
426:     }
427: }
428: 
429: /* Scrollbar Styling */
430: .stats-dashboard-body::-webkit-scrollbar {
431:     width: 8px;
432: }
433: 
434: .stats-dashboard-body::-webkit-scrollbar-track {
435:     background: var(--bg-tertiary);
436:     border-radius: 4px;
437: }
438: 
439: .stats-dashboard-body::-webkit-scrollbar-thumb {
440:     background: var(--border-color);
441:     border-radius: 4px;
442: }
443: 
444: .stats-dashboard-body::-webkit-scrollbar-thumb:hover {
445:     background: var(--accent-primary);
446: }
447: 
448: /* Refresh Button Animation */
449: .stats-refresh-button {
450:     position: relative;
451: }
452: 
453: .stats-refresh-button.refreshing::before {
454:     animation: stats-spin 1s linear infinite;
455: }
```

## File: static/css/utils/animations.css
```css
 1: @keyframes pulseStatus {
 2:     0%, 100% { opacity: 1; }
 3:     50% { opacity: 0.6; }
 4: }
 5: 
 6: /* Shine pass for active progress bar fill */
 7: @keyframes progressShine {
 8:     0% { left: -100%; }
 9:     100% { left: 100%; }
10: }
11: 
12: /* Subtle pulse for processing filename text */
13: @keyframes textPulse {
14:     0%, 100% { opacity: 1; }
15:     50% { opacity: 0.7; }
16: }
17: 
18: /* Generic spinner */
19: @keyframes spin {
20:     0% { transform: rotate(0deg); }
21:     100% { transform: rotate(360deg); }
22: }
23: 
24: /* Micro-interactions for step cards */
25: @keyframes subtleOpacityPulse {
26:     0%, 100% { opacity: 0.22; }
27:     50% { opacity: 0.42; }
28: }
29: 
30: @keyframes cardBreath {
31:     0%, 100% { transform: scale(1); }
32:     50% { transform: scale(1.01); }
33: }
34: 
35: /* Brief highlight flash for newly opened sections/titles */
36: @keyframes highlightFlash {
37:     0% { box-shadow: 0 0 0 0 color-mix(in oklab, var(--accent-primary) 55%, transparent); }
38:     100% { box-shadow: 0 0 0 10px transparent; }
39: }
40: 
41: .section-focus-highlight {
42:     animation: highlightFlash 0.35s ease-out;
43:     border-radius: 6px;
44: }
45: 
46: @media (prefers-reduced-motion: reduce) {
47:     .section-focus-highlight { animation: none; }
48: }
```

## File: static/css/themes.css
```css
  1: /* =============================================================================
  2:    THEME SYSTEM - Workflow MediaPipe v4.0
  3:    Dynamic theme definitions with smooth transitions
  4:    ============================================================================= */
  5: 
  6: /* Base theme transition for smooth switching */
  7: :root {
  8:     transition: background-color 0.3s ease, color 0.3s ease;
  9: }
 10: 
 11: body {
 12:     transition: background-color 0.3s ease, color 0.3s ease;
 13: }
 14: 
 15: /* Apply transitions to all theme-dependent elements */
 16: .card-like-section,
 17: .step,
 18: .popup-content,
 19: .unified-controls-section,
 20: button,
 21: input,
 22: select,
 23: textarea {
 24:     transition: background-color 0.3s ease, 
 25:                 color 0.3s ease, 
 26:                 border-color 0.3s ease,
 27:                 box-shadow 0.3s ease;
 28: }
 29: 
 30: /* =============================================================================
 31:    THEME 1: DARK PRO (Default - Current)
 32:    ============================================================================= */
 33: [data-theme="dark-pro"],
 34: :root:not([data-theme]) {
 35:     --bg-dark: #1e1e2f;
 36:     --bg-card: #2c2c3e;
 37:     --bg-secondary: #2c2c3e;
 38:     --bg-tertiary: #3a3a4e;
 39:     --text-primary: #e0e0e0;
 40:     --text-secondary: #a0a0b0;
 41:     --text-muted: #707080;
 42:     --accent-primary: #7986cb;
 43:     --accent-secondary: #ff8a65;
 44:     --border-color: #39394d;
 45:     --green: #66bb6a;
 46:     --red: #ef5350;
 47:     --yellow: #ffee58;
 48:     --blue: #42a5f5;
 49:     --orange: #ffb74d;
 50:     --log-bg: #161625;
 51:     --log-text: #c0c0d0;
 52:     --border-color-translucent: #39394d88;
 53: }
 54: 
 55: /* =============================================================================
 56:    THEME 2: LIGHT MODE (Professional Light)
 57:    ============================================================================= */
 58: [data-theme="light-mode"] {
 59:     --bg-dark: #f5f7fa;
 60:     --bg-card: #ffffff;
 61:     --bg-secondary: #ffffff;
 62:     --bg-tertiary: #e8eef3;
 63:     --text-primary: #2c3e50;
 64:     --text-secondary: #546e7a;
 65:     --text-muted: #90a4ae;
 66:     --accent-primary: #5c6bc0;
 67:     --accent-secondary: #ff7043;
 68:     --border-color: #cfd8dc;
 69:     --green: #43a047;
 70:     --red: #e53935;
 71:     --yellow: #fbc02d;
 72:     --blue: #1e88e5;
 73:     --orange: #fb8c00;
 74:     --log-bg: #f9fafb;
 75:     --log-text: #37474f;
 76:     --border-color-translucent: #cfd8dc88;
 77: }
 78: 
 79: /* =============================================================================
 80:    THEME 3: PASTEL ZEN (Soft and Calming)
 81:    ============================================================================= */
 82: [data-theme="pastel-zen"] {
 83:     --bg-dark: #faf3f3;
 84:     --bg-card: #fff5f5;
 85:     --bg-secondary: #ffe8f0;
 86:     --bg-tertiary: #f3e8ff;
 87:     --text-primary: #4a4a5a;
 88:     --text-secondary: #7a7a8a;
 89:     --text-muted: #aaaaba;
 90:     --accent-primary: #b39ddb;
 91:     --accent-secondary: #f48fb1;
 92:     --border-color: #e1d5e7;
 93:     --green: #81c784;
 94:     --red: #ef9a9a;
 95:     --yellow: #fff59d;
 96:     --blue: #90caf9;
 97:     --orange: #ffcc80;
 98:     --log-bg: #fef7ff;
 99:     --log-text: #5a5a6a;
100:     --border-color-translucent: #e1d5e788;
101: }
102: 
103: /* =============================================================================
104:    THEME 4: NEON CYBERPUNK (Vibrant and Futuristic)
105:    ============================================================================= */
106: [data-theme="neon-cyberpunk"] {
107:     --bg-dark: #0a0e27;
108:     --bg-card: #1a1f3a;
109:     --bg-secondary: #1a1f3a;
110:     --bg-tertiary: #2a2f4a;
111:     --text-primary: #e0f7ff;
112:     --text-secondary: #a0d7ff;
113:     --text-muted: #6097bf;
114:     --accent-primary: #00d9ff;
115:     --accent-secondary: #ff006e;
116:     --border-color: #00ffff;
117:     --green: #00ff88;
118:     --red: #ff0066;
119:     --yellow: #ffff00;
120:     --blue: #00d4ff;
121:     --orange: #ff9500;
122:     --log-bg: #050816;
123:     --log-text: #b0e7ff;
124:     --border-color-translucent: #00ffff44;
125: }
126: 
127: /* Neon glow effects for cyberpunk theme */
128: [data-theme="neon-cyberpunk"] button:hover {
129:     box-shadow: 0 0 15px var(--accent-primary);
130: }
131: 
132: [data-theme="neon-cyberpunk"] .step.running {
133:     box-shadow: 0 0 20px var(--blue);
134: }
135: 
136: [data-theme="neon-cyberpunk"] .step.completed {
137:     box-shadow: 0 0 20px var(--green);
138: }
139: 
140: [data-theme="neon-cyberpunk"] .step.error {
141:     box-shadow: 0 0 20px var(--red);
142: }
143: 
144: /* =============================================================================
145:    THEME 5: FOREST NIGHT (Natural and Earthy)
146:    ============================================================================= */
147: [data-theme="forest-night"] {
148:     --bg-dark: #1a2421;
149:     --bg-card: #243530;
150:     --bg-secondary: #2a3f3a;
151:     --bg-tertiary: #344a44;
152:     --text-primary: #d4e8d4;
153:     --text-secondary: #a4c8a4;
154:     --text-muted: #748874;
155:     --accent-primary: #66bb6a;
156:     --accent-secondary: #ffb74d;
157:     --border-color: #3d5a4d;
158:     --green: #81c784;
159:     --red: #e57373;
160:     --yellow: #fff176;
161:     --blue: #64b5f6;
162:     --orange: #ffb74d;
163:     --log-bg: #151f1a;
164:     --log-text: #c4d8c4;
165:     --border-color-translucent: #3d5a4d88;
166: }
167: 
168: /* =============================================================================
169:    THEME 6: OCEAN DEPTH (Deep Blue Tones)
170:    ============================================================================= */
171: [data-theme="ocean-depth"] {
172:     --bg-dark: #0d1b2a;
173:     --bg-card: #1b263b;
174:     --bg-secondary: #1b263b;
175:     --bg-tertiary: #2a3f5f;
176:     --text-primary: #e0f4ff;
177:     --text-secondary: #a8dadc;
178:     --text-muted: #6d9bb5;
179:     --accent-primary: #457b9d;
180:     --accent-secondary: #f1faee;
181:     --border-color: #264653;
182:     --green: #52b788;
183:     --red: #ef476f;
184:     --yellow: #ffd166;
185:     --blue: #118ab2;
186:     --orange: #06d6a0;
187:     --log-bg: #081420;
188:     --log-text: #d0e4f0;
189:     --border-color-translucent: #26465388;
190: }
191: 
192: /* =============================================================================
193:    THEME SELECTOR STYLING
194:    ============================================================================= */
195: .theme-selector-container {
196:     display: inline-flex;
197:     align-items: center;
198:     gap: 8px;
199:     margin-left: 16px;
200:     padding: 8px 12px;
201:     background: var(--bg-card);
202:     border: 1px solid var(--border-color);
203:     border-radius: 6px;
204:     transition: all 0.3s ease;
205: }
206: 
207: .theme-selector-container:hover {
208:     border-color: var(--accent-primary);
209:     box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
210: }
211: 
212: .theme-selector-label {
213:     font-size: 14px;
214:     color: var(--text-secondary);
215:     font-weight: 500;
216:     white-space: nowrap;
217: }
218: 
219: .theme-selector {
220:     background: var(--bg-secondary);
221:     color: var(--text-primary);
222:     border: 1px solid var(--border-color);
223:     border-radius: 4px;
224:     padding: 6px 10px;
225:     font-size: 14px;
226:     cursor: pointer;
227:     outline: none;
228:     transition: all 0.2s ease;
229:     min-width: 140px;
230: }
231: 
232: .theme-selector:hover {
233:     border-color: var(--accent-primary);
234:     background: var(--bg-tertiary);
235: }
236: 
237: .theme-selector:focus {
238:     border-color: var(--accent-primary);
239:     box-shadow: 0 0 0 3px rgba(121, 134, 203, 0.3);
240: }
241: 
242: .theme-selector option {
243:     background: var(--bg-card);
244:     color: var(--text-primary);
245:     padding: 8px;
246: }
247: 
248: /* Icon in theme selector */
249: .theme-selector-container::before {
250:     content: '🎨';
251:     font-size: 18px;
252: }
253: 
254: /* =============================================================================
255:    RESPONSIVE ADJUSTMENTS
256:    ============================================================================= */
257: @media (max-width: 768px) {
258:     .theme-selector-container {
259:         margin-left: 0;
260:         margin-top: 8px;
261:         width: 100%;
262:         justify-content: space-between;
263:     }
264:     
265:     .theme-selector {
266:         flex: 1;
267:     }
268: }
```

## File: static/test-exports/fetchWithLoadingState.js
```javascript
 1: // Standalone test export for fetchWithLoadingState (no optional chaining, no external deps)
 2: export async function fetchWithLoadingState(url, options = {}, buttonElOrId = null) {
 3:   let btn = null;
 4:   if (typeof buttonElOrId === 'string' && typeof document !== 'undefined') {
 5:     btn = document.getElementById(buttonElOrId);
 6:   } else if (buttonElOrId && buttonElOrId.nodeType === 1) {
 7:     btn = buttonElOrId;
 8:   }
 9:   try {
10:     if (btn) {
11:       btn.setAttribute('data-loading', 'true');
12:       btn.disabled = true;
13:     }
14:     const response = await fetch(url, options);
15:     let data = {};
16:     try { data = await response.json(); } catch (e) { data = {}; }
17:     if (!response.ok) {
18:       throw new Error((data && data.message) || ('Erreur HTTP ' + response.status));
19:     }
20:     return data;
21:   } finally {
22:     if (btn) {
23:       btn.removeAttribute('data-loading');
24:       btn.disabled = false;
25:     }
26:   }
27: }
```

## File: static/utils/DOMBatcher.js
```javascript
  1: /**
  2:  * DOM Update Batcher
  3:  * Batches DOM updates using requestAnimationFrame for optimal performance.
  4:  */
  5: 
  6: class DOMUpdateBatcher {
  7:     constructor() {
  8:         this.pendingUpdates = new Map();
  9:         this.rafId = null;
 10:         this.isDestroyed = false;
 11:         this.updateCount = 0;
 12:         this.lastFlushTime = 0;
 13:         
 14:         this.stats = {
 15:             totalUpdates: 0,
 16:             batchedUpdates: 0,
 17:             averageBatchSize: 0,
 18:             lastBatchSize: 0
 19:         };
 20:         
 21:         console.debug('[DOMBatcher] Initialized');
 22:     }
 23:     
 24:     /**
 25:      * Schedule a DOM update to be batched.
 26:      * @param {string} key - Unique key for the update (prevents duplicates)
 27:      * @param {Function} updateFn - Function that performs DOM updates
 28:      * @param {number} [priority] - Update priority (lower = higher priority)
 29:      */
 30:     scheduleUpdate(key, updateFn, priority = 0) {
 31:         if (this.isDestroyed) {
 32:             console.warn('[DOMBatcher] Attempted to schedule update on destroyed batcher');
 33:             return;
 34:         }
 35:         
 36:         if (typeof updateFn !== 'function') {
 37:             console.error('[DOMBatcher] Update function must be a function');
 38:             return;
 39:         }
 40:         
 41:         this.pendingUpdates.set(key, {
 42:             updateFn,
 43:             priority,
 44:             timestamp: performance.now()
 45:         });
 46:         
 47:         this.updateCount++;
 48:         
 49:         if (!this.rafId) {
 50:             this.rafId = requestAnimationFrame(() => {
 51:                 this.flushUpdates();
 52:             });
 53:         }
 54:     }
 55:     
 56:     /**
 57:      * Schedule multiple related updates as a group.
 58:      * @param {string} groupKey - Key for the update group
 59:      * @param {Object} updates - Object with update keys and functions
 60:      * @param {number} [priority] - Priority for the entire group
 61:      */
 62:     scheduleUpdateGroup(groupKey, updates, priority = 0) {
 63:         Object.entries(updates).forEach(([key, updateFn]) => {
 64:             this.scheduleUpdate(`${groupKey}:${key}`, updateFn, priority);
 65:         });
 66:     }
 67:     
 68:     /**
 69:      * Schedule a high-priority update that should be processed first.
 70:      * @param {string} key - Unique key for the update
 71:      * @param {Function} updateFn - Function that performs DOM updates
 72:      */
 73:     scheduleHighPriorityUpdate(key, updateFn) {
 74:         this.scheduleUpdate(key, updateFn, -1);
 75:     }
 76:     
 77:     /**
 78:      * Cancel a scheduled update.
 79:      * @param {string} key - Key of the update to cancel
 80:      * @returns {boolean} True if update was cancelled
 81:      */
 82:     cancelUpdate(key) {
 83:         return this.pendingUpdates.delete(key);
 84:     }
 85:     
 86:     /**
 87:      * Cancel all updates matching a pattern.
 88:      * @param {RegExp|string} pattern - Pattern to match against keys
 89:      * @returns {number} Number of cancelled updates
 90:      */
 91:     cancelUpdatesMatching(pattern) {
 92:         let cancelled = 0;
 93:         const regex = pattern instanceof RegExp ? pattern : new RegExp(pattern);
 94:         
 95:         for (const key of this.pendingUpdates.keys()) {
 96:             if (regex.test(key)) {
 97:                 this.pendingUpdates.delete(key);
 98:                 cancelled++;
 99:             }
100:         }
101:         
102:         return cancelled;
103:     }
104:     
105:     /**
106:      * Force immediate flush of all pending updates.
107:      */
108:     flushUpdates() {
109:         if (this.isDestroyed || this.pendingUpdates.size === 0) {
110:             this.rafId = null;
111:             return;
112:         }
113:         
114:         const startTime = performance.now();
115:         const batchSize = this.pendingUpdates.size;
116:         
117:         try {
118:             const sortedUpdates = Array.from(this.pendingUpdates.entries())
119:                 .sort(([, a], [, b]) => a.priority - b.priority);
120:             
121:             for (const [key, { updateFn }] of sortedUpdates) {
122:                 try {
123:                     updateFn();
124:                 } catch (error) {
125:                     console.error(`[DOMBatcher] Update error for key "${key}":`, error);
126:                 }
127:             }
128:             
129:             this.stats.totalUpdates += batchSize;
130:             this.stats.batchedUpdates++;
131:             this.stats.lastBatchSize = batchSize;
132:             this.stats.averageBatchSize = this.stats.totalUpdates / this.stats.batchedUpdates;
133:             
134:             const flushTime = performance.now() - startTime;
135:             this.lastFlushTime = flushTime;
136:             
137:             if (flushTime > 16) { // More than one frame
138:                 console.warn(`[DOMBatcher] Slow batch update: ${flushTime.toFixed(2)}ms for ${batchSize} updates`);
139:             }
140:             
141:         } catch (error) {
142:             console.error('[DOMBatcher] Flush error:', error);
143:         } finally {
144:             this.pendingUpdates.clear();
145:             this.rafId = null;
146:         }
147:     }
148:     
149:     /**
150:      * Get current statistics about batching performance.
151:      * @returns {Object} Statistics object
152:      */
153:     getStats() {
154:         return {
155:             ...this.stats,
156:             pendingUpdates: this.pendingUpdates.size,
157:             isScheduled: this.rafId !== null,
158:             lastFlushTime: this.lastFlushTime,
159:             updateCount: this.updateCount
160:         };
161:     }
162:     
163:     /**
164:      * Reset statistics.
165:      */
166:     resetStats() {
167:         this.stats = {
168:             totalUpdates: 0,
169:             batchedUpdates: 0,
170:             averageBatchSize: 0,
171:             lastBatchSize: 0
172:         };
173:         this.updateCount = 0;
174:         this.lastFlushTime = 0;
175:         
176:         console.debug('[DOMBatcher] Statistics reset');
177:     }
178:     
179:     /**
180:      * Check if there are pending updates.
181:      * @returns {boolean} True if updates are pending
182:      */
183:     hasPendingUpdates() {
184:         return this.pendingUpdates.size > 0;
185:     }
186:     
187:     /**
188:      * Get list of pending update keys.
189:      * @returns {string[]} Array of pending update keys
190:      */
191:     getPendingUpdateKeys() {
192:         return Array.from(this.pendingUpdates.keys());
193:     }
194:     
195:     /**
196:      * Destroy the batcher and cleanup resources.
197:      */
198:     destroy() {
199:         if (this.rafId) {
200:             cancelAnimationFrame(this.rafId);
201:             this.rafId = null;
202:         }
203:         
204:         this.pendingUpdates.clear();
205:         this.isDestroyed = true;
206:         
207:         console.debug('[DOMBatcher] Destroyed');
208:     }
209: }
210: 
211: /**
212:  * Performance-optimized DOM update utilities.
213:  */
214: class DOMUpdateUtils {
215:     /**
216:      * Batch update multiple element properties.
217:      * @param {HTMLElement} element - Target element
218:      * @param {Object} properties - Properties to update
219:      */
220:     static updateElementProperties(element, properties) {
221:         if (!element) return;
222:         
223:         if (properties.style) {
224:             Object.assign(element.style, properties.style);
225:         }
226:         
227:         if (properties.attributes) {
228:             Object.entries(properties.attributes).forEach(([attr, value]) => {
229:                 if (value === null || value === undefined) {
230:                     element.removeAttribute(attr);
231:                 } else {
232:                     element.setAttribute(attr, value);
233:                 }
234:             });
235:         }
236:         
237:         Object.entries(properties).forEach(([prop, value]) => {
238:             if (prop !== 'style' && prop !== 'attributes' && prop in element) {
239:                 element[prop] = value;
240:             }
241:         });
242:     }
243:     
244:     /**
245:      * Efficiently update text content with HTML escaping.
246:      * @param {HTMLElement} element - Target element
247:      * @param {string} text - Text content
248:      * @param {boolean} [escapeHtml] - Whether to escape HTML
249:      */
250:     static updateTextContent(element, text, escapeHtml = true) {
251:         if (!element) return;
252:         
253:         const content = escapeHtml ? DOMUpdateUtils.escapeHtml(text) : text;
254:         
255:         if (escapeHtml) {
256:             element.textContent = content;
257:         } else {
258:             element.innerHTML = content;
259:         }
260:     }
261:     
262:     /**
263:      * Escape HTML characters.
264:      * @param {string} text - Text to escape
265:      * @returns {string} Escaped text
266:      */
267:     static escapeHtml(text) {
268:         const div = document.createElement('div');
269:         div.textContent = text;
270:         return div.innerHTML;
271:     }
272:     
273:     /**
274:      * Update progress bar with animation.
275:      * @param {HTMLElement} progressBar - Progress bar element
276:      * @param {number} percentage - Progress percentage (0-100)
277:      * @param {boolean} [animate] - Whether to animate the change
278:      */
279:     static updateProgressBar(progressBar, percentage, animate = true) {
280:         if (!progressBar) return;
281:         
282:         const clampedPercentage = Math.max(0, Math.min(100, percentage));
283:         
284:         if (animate) {
285:             progressBar.style.transition = 'width 0.3s ease-in-out';
286:         } else {
287:             progressBar.style.transition = 'none';
288:         }
289:         
290:         progressBar.style.width = `${clampedPercentage}%`;
291:         progressBar.setAttribute('aria-valuenow', clampedPercentage);
292:         
293:         const textElement = progressBar.querySelector('.progress-text');
294:         if (textElement) {
295:             textElement.textContent = `${Math.round(clampedPercentage)}%`;
296:         }
297:     }
298:     
299:     /**
300:      * Update element visibility with optional animation.
301:      * @param {HTMLElement} element - Target element
302:      * @param {boolean} visible - Whether element should be visible
303:      * @param {string} [animation] - Animation type ('fade', 'slide', 'none')
304:      */
305:     static updateVisibility(element, visible, animation = 'none') {
306:         if (!element) return;
307:         
308:         if (animation === 'fade') {
309:             element.style.transition = 'opacity 0.3s ease-in-out';
310:             element.style.opacity = visible ? '1' : '0';
311:             element.style.display = visible ? '' : 'none';
312:         } else if (animation === 'slide') {
313:             element.style.transition = 'max-height 0.3s ease-in-out';
314:             element.style.maxHeight = visible ? '1000px' : '0';
315:             element.style.overflow = 'hidden';
316:         } else {
317:             element.style.display = visible ? '' : 'none';
318:         }
319:     }
320: }
321: 
322: // Create and export singleton instance
323: export const domBatcher = new DOMUpdateBatcher();
324: 
325: // Cleanup on page unload
326: window.addEventListener('beforeunload', () => {
327:     domBatcher.destroy();
328: });
329: 
330: // Export utilities
331: export { DOMUpdateUtils };
332: 
333: // Development helpers
334: if (window.location.hostname === 'localhost' || window.location.hostname === '127.0.0.1') {
335: window.domBatcher = domBatcher;
336:     
337:     const originalFlush = domBatcher.flushUpdates;
338:     domBatcher.flushUpdates = function() {
339:         const startTime = performance.now();
340:         originalFlush.call(this);
341:         const duration = performance.now() - startTime;
342:         
343:         if (duration > 16) {
344:             console.warn(`[DOMBatcher] Performance warning: ${duration.toFixed(2)}ms flush time`);
345:         }
346:     };
347: }
348: 
349: export default domBatcher;
```

## File: static/utils/ErrorHandler.js
```javascript
  1: /**
  2:  * Comprehensive error handling utility for workflow_mediapipe frontend.
  3:  * 
  4:  * This module provides centralized error handling with user feedback,
  5:  * exponential backoff for repeated errors, and proper error reporting.
  6:  */
  7: 
  8: class ErrorHandler {
  9:     constructor() {
 10:         this.consecutiveErrors = new Map();
 11:         this.errorHistory = [];
 12:         this.maxHistorySize = 100;
 13:         this.notificationTimeouts = new Map();
 14:         
 15:         this._bindGlobalErrorHandlers();
 16:         
 17:         console.debug('ErrorHandler initialized');
 18:     }
 19: 
 20:     /**
 21:      * Handle polling errors with exponential backoff and user feedback.
 22:      * 
 23:      * @param {string} operation - Name of the operation that failed
 24:      * @param {Error} error - The error that occurred
 25:      * @param {Object} context - Additional context information
 26:      * @returns {number} - Delay in milliseconds before retry (0 = no delay)
 27:      */
 28:     async handlePollingError(operation, error, context = {}) {
 29:         const errorKey = `${operation}_${context.stepKey || 'global'}`;
 30:         const count = this.consecutiveErrors.get(errorKey) || 0;
 31:         const newCount = count + 1;
 32:         
 33:         this.consecutiveErrors.set(errorKey, newCount);
 34:         
 35:         console.error(`Polling error in ${operation} (attempt ${newCount}):`, error);
 36:         
 37:         this._addToHistory({
 38:             operation,
 39:             error: error.message || error.toString(),
 40:             context,
 41:             count: newCount,
 42:             timestamp: new Date().toISOString()
 43:         });
 44:         
 45:         if (newCount >= 3) {
 46:             this._showErrorNotification(
 47:                 operation,
 48:                 `Unable to update ${operation}. Retrying...`,
 49:                 'warning',
 50:                 context.elementId
 51:             );
 52:         }
 53:         
 54:         if (newCount >= 5) {
 55:             this._showErrorNotification(
 56:                 operation,
 57:                 `${operation} is experiencing persistent issues. Please check your connection.`,
 58:                 'error',
 59:                 context.elementId
 60:             );
 61:         }
 62:         
 63:         let delay = 0;
 64:         if (newCount >= 3) {
 65:             delay = Math.min(30000, 2000 * Math.pow(2, newCount - 3));
 66:         }
 67:         
 68:         this._dispatchErrorEvent(operation, error, newCount, delay);
 69:         
 70:         return delay;
 71:     }
 72: 
 73:     /**
 74:      * Clear error state for a successful operation.
 75:      * 
 76:      * @param {string} operation - Name of the operation that succeeded
 77:      * @param {Object} context - Additional context information
 78:      */
 79:     clearErrors(operation, context = {}) {
 80:         const errorKey = `${operation}_${context.stepKey || 'global'}`;
 81:         const hadErrors = this.consecutiveErrors.has(errorKey);
 82:         
 83:         this.consecutiveErrors.delete(errorKey);
 84:         
 85:         if (hadErrors) {
 86:             console.debug(`Cleared error state for ${operation}`);
 87:             this._clearErrorNotification(operation);
 88:             
 89:             if (context.elementId) {
 90:                 this.clearErrorState(context.elementId);
 91:             }
 92:         }
 93:     }
 94: 
 95:     /**
 96:      * Handle API errors with proper user feedback.
 97:      * 
 98:      * @param {string} endpoint - API endpoint that failed
 99:      * @param {Error} error - The error that occurred
100:      * @param {Object} context - Additional context information
101:      */
102:     handleApiError(endpoint, error, context = {}) {
103:         console.error(`API error for ${endpoint}:`, error);
104:         
105:         this._addToHistory({
106:             type: 'api',
107:             endpoint,
108:             error: error.message || error.toString(),
109:             context,
110:             timestamp: new Date().toISOString()
111:         });
112:         
113:         let message = 'An unexpected error occurred';
114:         let type = 'error';
115:         
116:         if (error.name === 'TypeError' && error.message.includes('fetch')) {
117:             message = 'Network connection error. Please check your internet connection.';
118:             type = 'warning';
119:         } else if (error.message.includes('401')) {
120:             message = 'Authentication error. Please refresh the page.';
121:             type = 'error';
122:         } else if (error.message.includes('404')) {
123:             message = 'Service not found. Please contact support.';
124:             type = 'error';
125:         } else if (error.message.includes('500')) {
126:             message = 'Server error. Please try again later.';
127:             type = 'error';
128:         } else if (error.message) {
129:             message = error.message;
130:         }
131:         
132:         this._showErrorNotification(
133:             `api-${endpoint}`,
134:             message,
135:             type
136:         );
137:         
138:         this._dispatchErrorEvent(`api-${endpoint}`, error, 1, 0);
139:     }
140: 
141:     /**
142:      * Show error state on a specific UI element.
143:      * 
144:      * @param {string} elementId - ID of the element to show error state
145:      * @param {string} message - Error message to display
146:      */
147:     showErrorState(elementId, message) {
148:         const element = document.getElementById(elementId);
149:         if (!element) {
150:             console.warn(`Element not found for error state: ${elementId}`);
151:             return;
152:         }
153:         
154:         element.classList.add('error-state');
155:         
156:         let errorElement = element.querySelector('.error-message');
157:         if (!errorElement) {
158:             errorElement = document.createElement('div');
159:             errorElement.className = 'error-message';
160:             element.appendChild(errorElement);
161:         }
162:         
163:         errorElement.textContent = message;
164:         errorElement.style.display = 'block';
165:         
166:         console.debug(`Showing error state for ${elementId}: ${message}`);
167:     }
168: 
169:     /**
170:      * Clear error state from a specific UI element.
171:      * 
172:      * @param {string} elementId - ID of the element to clear error state
173:      */
174:     clearErrorState(elementId) {
175:         const element = document.getElementById(elementId);
176:         if (!element) {
177:             return;
178:         }
179:         
180:         element.classList.remove('error-state');
181:         
182:         // Hide error message
183:         const errorElement = element.querySelector('.error-message');
184:         if (errorElement) {
185:             errorElement.style.display = 'none';
186:         }
187:         
188:         console.debug(`Cleared error state for ${elementId}`);
189:     }
190: 
191:     /**
192:      * Get error statistics for monitoring.
193:      * 
194:      * @returns {Object} - Error statistics
195:      */
196:     getErrorStats() {
197:         const now = Date.now();
198:         const recentErrors = this.errorHistory.filter(
199:             error => (now - new Date(error.timestamp).getTime()) < 300000
200:         );
201:         
202:         return {
203:             totalErrors: this.errorHistory.length,
204:             recentErrors: recentErrors.length,
205:             activeErrorOperations: Array.from(this.consecutiveErrors.keys()),
206:             errorsByOperation: this._groupErrorsByOperation(recentErrors)
207:         };
208:     }
209: 
210:     /**
211:      * Clear all error history and state.
212:      */
213:     clearAllErrors() {
214:         this.consecutiveErrors.clear();
215:         this.errorHistory = [];
216:         
217:         this.notificationTimeouts.forEach((timeout, key) => {
218:             clearTimeout(timeout);
219:             this._clearErrorNotification(key);
220:         });
221:         this.notificationTimeouts.clear();
222:         
223:         console.debug('Cleared all error state');
224:     }
225: 
226:     /**
227:      * Add error to history with size limit.
228:      * @private
229:      */
230:     _addToHistory(errorInfo) {
231:         this.errorHistory.push(errorInfo);
232:         
233:         // Maintain history size limit
234:         if (this.errorHistory.length > this.maxHistorySize) {
235:             this.errorHistory.shift();
236:         }
237:     }
238: 
239:     /**
240:      * Show error notification with deduplication.
241:      * @private
242:      */
243:     _showErrorNotification(key, message, type = 'error', elementId = null) {
244:         const existingTimeout = this.notificationTimeouts.get(key);
245:         if (existingTimeout) {
246:             clearTimeout(existingTimeout);
247:         }
248:         
249:         if (typeof window.showNotification === 'function') {
250:             window.showNotification(message, type, 5000);
251:         } else {
252:             console.warn('showNotification function not available');
253:         }
254:         
255:         if (elementId) {
256:             this.showErrorState(elementId, message);
257:         }
258:         
259:         const timeout = setTimeout(() => {
260:             this._clearErrorNotification(key);
261:         }, 10000);
262:         
263:         this.notificationTimeouts.set(key, timeout);
264:     }
265: 
266:     /**
267:      * Clear error notification.
268:      * @private
269:      */
270:     _clearErrorNotification(key) {
271:         const timeout = this.notificationTimeouts.get(key);
272:         if (timeout) {
273:             clearTimeout(timeout);
274:             this.notificationTimeouts.delete(key);
275:         }
276:     }
277: 
278:     /**
279:      * Dispatch custom error event.
280:      * @private
281:      */
282:     _dispatchErrorEvent(operation, error, count, delay) {
283:         const event = new CustomEvent('applicationError', {
284:             detail: {
285:                 operation,
286:                 error: error.message || error.toString(),
287:                 count,
288:                 delay,
289:                 timestamp: new Date().toISOString()
290:             }
291:         });
292:         
293:         window.dispatchEvent(event);
294:     }
295: 
296:     /**
297:      * Group errors by operation for statistics.
298:      * @private
299:      */
300:     _groupErrorsByOperation(errors) {
301:         const grouped = {};
302:         
303:         errors.forEach(error => {
304:             const operation = error.operation || error.endpoint || 'unknown';
305:             if (!grouped[operation]) {
306:                 grouped[operation] = 0;
307:             }
308:             grouped[operation]++;
309:         });
310:         
311:         return grouped;
312:     }
313: 
314:     /**
315:      * Bind global error handlers.
316:      * @private
317:      */
318:     _bindGlobalErrorHandlers() {
319:         window.addEventListener('unhandledrejection', (event) => {
320:             console.error('Unhandled promise rejection:', event.reason);
321:             this.handleApiError('unhandled-promise', event.reason);
322:         });
323:         
324:         window.addEventListener('error', (event) => {
325:             console.error('Global JavaScript error:', event.error);
326:             this._addToHistory({
327:                 type: 'javascript',
328:                 error: event.error?.message || event.message,
329:                 filename: event.filename,
330:                 lineno: event.lineno,
331:                 colno: event.colno,
332:                 timestamp: new Date().toISOString()
333:             });
334:         });
335:     }
336: }
337: 
338: // Create and export global error handler instance
339: const errorHandler = new ErrorHandler();
340: 
341: // Export for use in other modules
342: export { ErrorHandler, errorHandler };
343: 
344: // Also make available globally for legacy code
345: window.errorHandler = errorHandler;
```

## File: static/utils/PerformanceMonitor.js
```javascript
  1: /**
  2:  * Frontend Performance Monitor
  3:  * Monitors and reports frontend performance metrics.
  4:  */
  5: 
  6: import { performanceOptimizer } from './PerformanceOptimizer.js';
  7: import { domBatcher } from './DOMBatcher.js';
  8: import { pollingManager } from './PollingManager.js';
  9: import { errorHandler } from './ErrorHandler.js';
 10: 
 11: class PerformanceMonitor {
 12:     constructor() {
 13:         this.isMonitoring = false;
 14:         this.metrics = {
 15:             pageLoad: null,
 16:             apiCalls: [],
 17:             domUpdates: [],
 18:             memoryUsage: [],
 19:             userInteractions: []
 20:         };
 21:         this.observers = new Map();
 22:         this.startTime = performance.now();
 23:         
 24:         console.debug('[PerformanceMonitor] Initialized');
 25:     }
 26:     
 27:     /**
 28:      * Start performance monitoring.
 29:      */
 30:     startMonitoring() {
 31:         if (this.isMonitoring) {
 32:             console.warn('[PerformanceMonitor] Already monitoring');
 33:             return;
 34:         }
 35:         
 36:         this.isMonitoring = true;
 37:         
 38:         // Monitor page load performance
 39:         this.monitorPageLoad();
 40:         
 41:         // Monitor API calls
 42:         this.monitorApiCalls();
 43:         
 44:         // Monitor DOM updates
 45:         this.monitorDomUpdates();
 46:         
 47:         // Monitor memory usage
 48:         this.monitorMemoryUsage();
 49:         
 50:         // Monitor user interactions
 51:         this.monitorUserInteractions();
 52:         
 53:         // Monitor long tasks
 54:         this.monitorLongTasks();
 55:         
 56:         // Start periodic reporting
 57:         this.startPeriodicReporting();
 58:         
 59:         console.info('[PerformanceMonitor] Monitoring started');
 60:     }
 61:     
 62:     /**
 63:      * Stop performance monitoring.
 64:      */
 65:     stopMonitoring() {
 66:         if (!this.isMonitoring) return;
 67:         
 68:         this.isMonitoring = false;
 69:         
 70:         // Disconnect all observers
 71:         this.observers.forEach(observer => {
 72:             if (observer.disconnect) observer.disconnect();
 73:         });
 74:         this.observers.clear();
 75:         
 76:         // Stop periodic reporting
 77:         if (this.reportingInterval) {
 78:             clearInterval(this.reportingInterval);
 79:             this.reportingInterval = null;
 80:         }
 81:         
 82:         console.info('[PerformanceMonitor] Monitoring stopped');
 83:     }
 84:     
 85:     /**
 86:      * Monitor page load performance.
 87:      */
 88:     monitorPageLoad() {
 89:         if (document.readyState === 'complete') {
 90:             this.recordPageLoadMetrics();
 91:         } else {
 92:             window.addEventListener('load', () => {
 93:                 this.recordPageLoadMetrics();
 94:             });
 95:         }
 96:     }
 97:     
 98:     /**
 99:      * Record page load metrics.
100:      */
101:     recordPageLoadMetrics() {
102:         const navigation = performance.getEntriesByType('navigation')[0];
103:         if (!navigation) return;
104:         
105:         this.metrics.pageLoad = {
106:             domContentLoaded: navigation.domContentLoadedEventEnd - navigation.domContentLoadedEventStart,
107:             loadComplete: navigation.loadEventEnd - navigation.loadEventStart,
108:             domInteractive: navigation.domInteractive - navigation.navigationStart,
109:             firstPaint: this.getFirstPaint(),
110:             firstContentfulPaint: this.getFirstContentfulPaint(),
111:             timestamp: Date.now()
112:         };
113:         
114:         console.debug('[PerformanceMonitor] Page load metrics recorded:', this.metrics.pageLoad);
115:     }
116:     
117:     /**
118:      * Get First Paint timing.
119:      * @returns {number|null} First Paint time or null
120:      */
121:     getFirstPaint() {
122:         const paintEntries = performance.getEntriesByType('paint');
123:         const firstPaint = paintEntries.find(entry => entry.name === 'first-paint');
124:         return firstPaint ? firstPaint.startTime : null;
125:     }
126:     
127:     /**
128:      * Get First Contentful Paint timing.
129:      * @returns {number|null} First Contentful Paint time or null
130:      */
131:     getFirstContentfulPaint() {
132:         const paintEntries = performance.getEntriesByType('paint');
133:         const fcp = paintEntries.find(entry => entry.name === 'first-contentful-paint');
134:         return fcp ? fcp.startTime : null;
135:     }
136:     
137:     /**
138:      * Monitor API calls by wrapping fetch.
139:      */
140:     monitorApiCalls() {
141:         const originalFetch = window.fetch;
142:         
143:         window.fetch = async (...args) => {
144:             const startTime = performance.now();
145:             const url = args[0];
146:             
147:             try {
148:                 const response = await originalFetch(...args);
149:                 const duration = performance.now() - startTime;
150:                 
151:                 this.recordApiCall({
152:                     url,
153:                     method: args[1]?.method || 'GET',
154:                     status: response.status,
155:                     duration,
156:                     success: response.ok,
157:                     timestamp: Date.now()
158:                 });
159:                 
160:                 return response;
161:             } catch (error) {
162:                 const duration = performance.now() - startTime;
163:                 
164:                 this.recordApiCall({
165:                     url,
166:                     method: args[1]?.method || 'GET',
167:                     status: 0,
168:                     duration,
169:                     success: false,
170:                     error: error.message,
171:                     timestamp: Date.now()
172:                 });
173:                 
174:                 throw error;
175:             }
176:         };
177:     }
178:     
179:     /**
180:      * Record API call metrics.
181:      * @param {Object} metric - API call metric
182:      */
183:     recordApiCall(metric) {
184:         this.metrics.apiCalls.push(metric);
185:         
186:         // Keep only last 50 API calls
187:         if (this.metrics.apiCalls.length > 50) {
188:             this.metrics.apiCalls.shift();
189:         }
190:         
191:         // Log slow API calls
192:         if (metric.duration > 1000) {
193:             console.warn(`[PerformanceMonitor] Slow API call: ${metric.url} took ${metric.duration.toFixed(2)}ms`);
194:         }
195:     }
196:     
197:     /**
198:      * Monitor DOM updates using MutationObserver.
199:      */
200:     monitorDomUpdates() {
201:         if (!window.MutationObserver) return;
202:         
203:         const observer = new MutationObserver((mutations) => {
204:             const updateCount = mutations.length;
205:             const timestamp = Date.now();
206:             
207:             this.recordDomUpdate({
208:                 mutationCount: updateCount,
209:                 timestamp
210:             });
211:         });
212:         
213:         observer.observe(document.body, {
214:             childList: true,
215:             subtree: true,
216:             attributes: true,
217:             attributeOldValue: false,
218:             characterData: true,
219:             characterDataOldValue: false
220:         });
221:         
222:         this.observers.set('domUpdates', observer);
223:     }
224:     
225:     /**
226:      * Record DOM update metrics.
227:      * @param {Object} metric - DOM update metric
228:      */
229:     recordDomUpdate(metric) {
230:         this.metrics.domUpdates.push(metric);
231:         
232:         // Keep only last 100 DOM updates
233:         if (this.metrics.domUpdates.length > 100) {
234:             this.metrics.domUpdates.shift();
235:         }
236:     }
237:     
238:     /**
239:      * Monitor memory usage periodically.
240:      */
241:     monitorMemoryUsage() {
242:         if (!performance.memory) return;
243:         
244:         const recordMemory = () => {
245:             if (!this.isMonitoring) return;
246:             
247:             const memory = performance.memory;
248:             this.metrics.memoryUsage.push({
249:                 usedJSHeapSize: memory.usedJSHeapSize,
250:                 totalJSHeapSize: memory.totalJSHeapSize,
251:                 jsHeapSizeLimit: memory.jsHeapSizeLimit,
252:                 usagePercent: (memory.usedJSHeapSize / memory.jsHeapSizeLimit) * 100,
253:                 timestamp: Date.now()
254:             });
255:             
256:             // Keep only last 20 memory measurements
257:             if (this.metrics.memoryUsage.length > 20) {
258:                 this.metrics.memoryUsage.shift();
259:             }
260:             
261:             // Check for memory leaks
262:             const currentUsage = memory.usedJSHeapSize / memory.jsHeapSizeLimit;
263:             if (currentUsage > 0.8) {
264:                 console.warn(`[PerformanceMonitor] High memory usage: ${(currentUsage * 100).toFixed(1)}%`);
265:             }
266:         };
267:         
268:         // Record immediately and then every 10 seconds
269:         recordMemory();
270:         const memoryInterval = setInterval(recordMemory, 10000);
271:         this.observers.set('memoryUsage', { disconnect: () => clearInterval(memoryInterval) });
272:     }
273:     
274:     /**
275:      * Monitor user interactions.
276:      */
277:     monitorUserInteractions() {
278:         const interactionTypes = ['click', 'keydown', 'scroll', 'resize'];
279:         
280:         interactionTypes.forEach(type => {
281:             const handler = performanceOptimizer.throttle(`interaction_${type}`, (event) => {
282:                 this.recordUserInteraction({
283:                     type,
284:                     target: event.target.tagName,
285:                     timestamp: Date.now()
286:                 });
287:             }, 100);
288:             
289:             document.addEventListener(type, handler, { passive: true });
290:             
291:             this.observers.set(`interaction_${type}`, {
292:                 disconnect: () => document.removeEventListener(type, handler)
293:             });
294:         });
295:     }
296:     
297:     /**
298:      * Record user interaction metrics.
299:      * @param {Object} metric - User interaction metric
300:      */
301:     recordUserInteraction(metric) {
302:         this.metrics.userInteractions.push(metric);
303:         
304:         // Keep only last 50 interactions
305:         if (this.metrics.userInteractions.length > 50) {
306:             this.metrics.userInteractions.shift();
307:         }
308:     }
309:     
310:     /**
311:      * Monitor long tasks using PerformanceObserver.
312:      */
313:     monitorLongTasks() {
314:         if (!window.PerformanceObserver) return;
315:         
316:         try {
317:             const observer = new PerformanceObserver((list) => {
318:                 for (const entry of list.getEntries()) {
319:                     if (entry.duration > 50) { // Tasks longer than 50ms
320:                         console.warn(`[PerformanceMonitor] Long task detected: ${entry.duration.toFixed(2)}ms`);
321:                         
322:                         this.recordLongTask({
323:                             duration: entry.duration,
324:                             startTime: entry.startTime,
325:                             timestamp: Date.now()
326:                         });
327:                     }
328:                 }
329:             });
330:             
331:             observer.observe({ entryTypes: ['longtask'] });
332:             this.observers.set('longTasks', observer);
333:             
334:         } catch (error) {
335:             console.debug('[PerformanceMonitor] Long task monitoring not supported');
336:         }
337:     }
338:     
339:     /**
340:      * Record long task metrics.
341:      * @param {Object} metric - Long task metric
342:      */
343:     recordLongTask(metric) {
344:         if (!this.metrics.longTasks) {
345:             this.metrics.longTasks = [];
346:         }
347:         
348:         this.metrics.longTasks.push(metric);
349:         
350:         // Keep only last 20 long tasks
351:         if (this.metrics.longTasks.length > 20) {
352:             this.metrics.longTasks.shift();
353:         }
354:     }
355:     
356:     /**
357:      * Start periodic performance reporting.
358:      */
359:     startPeriodicReporting() {
360:         this.reportingInterval = setInterval(() => {
361:             this.sendPerformanceReport();
362:         }, 60000); // Every minute
363:     }
364:     
365:     /**
366:      * Send performance report to backend.
367:      */
368:     async sendPerformanceReport() {
369:         if (!this.isMonitoring) return;
370:         
371:         try {
372:             const report = this.generatePerformanceReport();
373:             
374:             // Send to backend performance API
375:             await fetch('/api/performance/frontend', {
376:                 method: 'POST',
377:                 headers: {
378:                     'Content-Type': 'application/json'
379:                 },
380:                 body: JSON.stringify(report)
381:             });
382:             
383:         } catch (error) {
384:             console.debug('[PerformanceMonitor] Failed to send performance report:', error);
385:         }
386:     }
387:     
388:     /**
389:      * Generate comprehensive performance report.
390:      * @returns {Object} Performance report
391:      */
392:     generatePerformanceReport() {
393:         const optimizerStats = performanceOptimizer.getPerformanceStats();
394:         const batcherStats = domBatcher.getStats();
395:         const pollingStats = pollingManager.getStats();
396:         const errorStats = errorHandler.getStats();
397:         
398:         return {
399:             timestamp: Date.now(),
400:             uptime: performance.now() - this.startTime,
401:             pageLoad: this.metrics.pageLoad,
402:             apiCalls: this.getApiCallSummary(),
403:             domUpdates: this.getDomUpdateSummary(),
404:             memoryUsage: this.getMemoryUsageSummary(),
405:             userInteractions: this.getUserInteractionSummary(),
406:             longTasks: this.metrics.longTasks || [],
407:             components: {
408:                 optimizer: optimizerStats,
409:                 batcher: batcherStats,
410:                 polling: pollingStats,
411:                 errorHandler: errorStats
412:             }
413:         };
414:     }
415:     
416:     /**
417:      * Get API call summary.
418:      * @returns {Object} API call summary
419:      */
420:     getApiCallSummary() {
421:         const calls = this.metrics.apiCalls;
422:         if (calls.length === 0) return { count: 0 };
423:         
424:         const durations = calls.map(c => c.duration);
425:         const errors = calls.filter(c => !c.success);
426:         
427:         return {
428:             count: calls.length,
429:             averageDuration: durations.reduce((a, b) => a + b, 0) / durations.length,
430:             maxDuration: Math.max(...durations),
431:             errorRate: errors.length / calls.length,
432:             slowCalls: calls.filter(c => c.duration > 1000).length
433:         };
434:     }
435:     
436:     /**
437:      * Get DOM update summary.
438:      * @returns {Object} DOM update summary
439:      */
440:     getDomUpdateSummary() {
441:         const updates = this.metrics.domUpdates;
442:         return {
443:             count: updates.length,
444:             totalMutations: updates.reduce((sum, u) => sum + u.mutationCount, 0)
445:         };
446:     }
447:     
448:     /**
449:      * Get memory usage summary.
450:      * @returns {Object} Memory usage summary
451:      */
452:     getMemoryUsageSummary() {
453:         const usage = this.metrics.memoryUsage;
454:         if (usage.length === 0) return { available: false };
455:         
456:         const latest = usage[usage.length - 1];
457:         const usagePercents = usage.map(u => u.usagePercent);
458:         
459:         return {
460:             current: latest,
461:             average: usagePercents.reduce((a, b) => a + b, 0) / usagePercents.length,
462:             peak: Math.max(...usagePercents)
463:         };
464:     }
465:     
466:     /**
467:      * Get user interaction summary.
468:      * @returns {Object} User interaction summary
469:      */
470:     getUserInteractionSummary() {
471:         const interactions = this.metrics.userInteractions;
472:         const byType = {};
473:         
474:         interactions.forEach(interaction => {
475:             byType[interaction.type] = (byType[interaction.type] || 0) + 1;
476:         });
477:         
478:         return {
479:             total: interactions.length,
480:             byType
481:         };
482:     }
483:     
484:     /**
485:      * Get current performance metrics.
486:      * @returns {Object} Current metrics
487:      */
488:     getMetrics() {
489:         return this.generatePerformanceReport();
490:     }
491:     
492:     /**
493:      * Reset all metrics.
494:      */
495:     resetMetrics() {
496:         this.metrics = {
497:             pageLoad: null,
498:             apiCalls: [],
499:             domUpdates: [],
500:             memoryUsage: [],
501:             userInteractions: []
502:         };
503:         
504:         console.debug('[PerformanceMonitor] Metrics reset');
505:     }
506:     
507:     /**
508:      * Destroy the monitor and cleanup resources.
509:      */
510:     destroy() {
511:         this.stopMonitoring();
512:         this.resetMetrics();
513:         
514:         console.debug('[PerformanceMonitor] Destroyed');
515:     }
516: }
517: 
518: // Create and export singleton instance
519: export const performanceMonitor = new PerformanceMonitor();
520: 
521: // Auto-start monitoring when DOM is ready
522: if (document.readyState === 'loading') {
523:     document.addEventListener('DOMContentLoaded', () => {
524:         performanceMonitor.startMonitoring();
525:     });
526: } else {
527:     performanceMonitor.startMonitoring();
528: }
529: 
530: // Cleanup on page unload
531: window.addEventListener('beforeunload', () => {
532:     performanceMonitor.destroy();
533: });
534: 
535: // Development helpers
536: if (window.location.hostname === 'localhost' || window.location.hostname === '127.0.0.1') {
537:     window.performanceMonitor = performanceMonitor; // Expose for debugging
538: }
539: 
540: export default performanceMonitor;
```

## File: static/utils/PerformanceOptimizer.js
```javascript
  1: /**
  2:  * Performance Optimizer
  3:  * Provides debouncing, throttling, and performance monitoring utilities.
  4:  */
  5: 
  6: class PerformanceOptimizer {
  7:     constructor() {
  8:         this.debounceTimers = new Map();
  9:         this.throttleTimers = new Map();
 10:         this.performanceMetrics = {
 11:             apiCalls: [],
 12:             domUpdates: [],
 13:             errors: []
 14:         };
 15:         this.isDestroyed = false;
 16:         
 17:         console.debug('[PerformanceOptimizer] Initialized');
 18:     }
 19:     
 20:     /**
 21:      * Debounce a function call.
 22:      * @param {string} key - Unique key for the debounced function
 23:      * @param {Function} func - Function to debounce
 24:      * @param {number} delay - Delay in milliseconds
 25:      * @param {boolean} [immediate] - Execute immediately on first call
 26:      * @returns {Function} Debounced function
 27:      */
 28:     debounce(key, func, delay = 300, immediate = false) {
 29:         return (...args) => {
 30:             if (this.isDestroyed) return;
 31:             
 32:             const callNow = immediate && !this.debounceTimers.has(key);
 33:             
 34:             // Clear existing timer
 35:             if (this.debounceTimers.has(key)) {
 36:                 clearTimeout(this.debounceTimers.get(key));
 37:             }
 38:             
 39:             // Set new timer
 40:             const timerId = setTimeout(() => {
 41:                 this.debounceTimers.delete(key);
 42:                 if (!immediate) func.apply(this, args);
 43:             }, delay);
 44:             
 45:             this.debounceTimers.set(key, timerId);
 46:             
 47:             // Execute immediately if requested
 48:             if (callNow) func.apply(this, args);
 49:         };
 50:     }
 51:     
 52:     /**
 53:      * Throttle a function call.
 54:      * @param {string} key - Unique key for the throttled function
 55:      * @param {Function} func - Function to throttle
 56:      * @param {number} interval - Minimum interval between calls in milliseconds
 57:      * @returns {Function} Throttled function
 58:      */
 59:     throttle(key, func, interval = 100) {
 60:         return (...args) => {
 61:             if (this.isDestroyed) return;
 62:             
 63:             if (!this.throttleTimers.has(key)) {
 64:                 func.apply(this, args);
 65:                 
 66:                 this.throttleTimers.set(key, setTimeout(() => {
 67:                     this.throttleTimers.delete(key);
 68:                 }, interval));
 69:             }
 70:         };
 71:     }
 72:     
 73:     /**
 74:      * Create a debounced update function for DOM elements.
 75:      * @param {string} key - Unique key
 76:      * @param {Function} updateFn - Update function
 77:      * @param {number} [delay] - Debounce delay
 78:      * @returns {Function} Debounced update function
 79:      */
 80:     debouncedUpdate(key, updateFn, delay = 100) {
 81:         return this.debounce(`update_${key}`, updateFn, delay);
 82:     }
 83:     
 84:     /**
 85:      * Create a throttled scroll handler.
 86:      * @param {string} key - Unique key
 87:      * @param {Function} scrollFn - Scroll handler function
 88:      * @param {number} [interval] - Throttle interval
 89:      * @returns {Function} Throttled scroll handler
 90:      */
 91:     throttledScroll(key, scrollFn, interval = 16) { // ~60fps
 92:         return this.throttle(`scroll_${key}`, scrollFn, interval);
 93:     }
 94:     
 95:     /**
 96:      * Create a throttled resize handler.
 97:      * @param {string} key - Unique key
 98:      * @param {Function} resizeFn - Resize handler function
 99:      * @param {number} [interval] - Throttle interval
100:      * @returns {Function} Throttled resize handler
101:      */
102:     throttledResize(key, resizeFn, interval = 100) {
103:         return this.throttle(`resize_${key}`, resizeFn, interval);
104:     }
105:     
106:     /**
107:      * Measure and record API call performance.
108:      * @param {string} endpoint - API endpoint name
109:      * @param {Function} apiCall - Function that makes the API call
110:      * @returns {Promise} Promise that resolves with API call result
111:      */
112:     async measureApiCall(endpoint, apiCall) {
113:         const startTime = performance.now();
114:         let success = true;
115:         let error = null;
116:         
117:         try {
118:             const result = await apiCall();
119:             return result;
120:         } catch (err) {
121:             success = false;
122:             error = err;
123:             throw err;
124:         } finally {
125:             const duration = performance.now() - startTime;
126:             
127:             this.recordApiMetric({
128:                 endpoint,
129:                 duration,
130:                 success,
131:                 error: error ? error.message : null,
132:                 timestamp: Date.now()
133:             });
134:         }
135:     }
136:     
137:     /**
138:      * Measure DOM update performance.
139:      * @param {string} operation - Operation name
140:      * @param {Function} updateFn - DOM update function
141:      * @returns {*} Result of update function
142:      */
143:     measureDomUpdate(operation, updateFn) {
144:         const startTime = performance.now();
145:         
146:         try {
147:             const result = updateFn();
148:             const duration = performance.now() - startTime;
149:             
150:             this.recordDomMetric({
151:                 operation,
152:                 duration,
153:                 success: true,
154:                 timestamp: Date.now()
155:             });
156:             
157:             return result;
158:         } catch (error) {
159:             const duration = performance.now() - startTime;
160:             
161:             this.recordDomMetric({
162:                 operation,
163:                 duration,
164:                 success: false,
165:                 error: error.message,
166:                 timestamp: Date.now()
167:             });
168:             
169:             throw error;
170:         }
171:     }
172:     
173:     /**
174:      * Record API performance metric.
175:      * @param {Object} metric - Metric data
176:      */
177:     recordApiMetric(metric) {
178:         this.performanceMetrics.apiCalls.push(metric);
179:         
180:         // Keep only last 100 metrics
181:         if (this.performanceMetrics.apiCalls.length > 100) {
182:             this.performanceMetrics.apiCalls.shift();
183:         }
184:         
185:         // Log slow API calls
186:         if (metric.duration > 1000) {
187:             console.warn(`[PerformanceOptimizer] Slow API call: ${metric.endpoint} took ${metric.duration.toFixed(2)}ms`);
188:         }
189:     }
190:     
191:     /**
192:      * Record DOM update performance metric.
193:      * @param {Object} metric - Metric data
194:      */
195:     recordDomMetric(metric) {
196:         this.performanceMetrics.domUpdates.push(metric);
197:         
198:         // Keep only last 100 metrics
199:         if (this.performanceMetrics.domUpdates.length > 100) {
200:             this.performanceMetrics.domUpdates.shift();
201:         }
202:         
203:         // Log slow DOM updates
204:         if (metric.duration > 16) { // More than one frame
205:             console.warn(`[PerformanceOptimizer] Slow DOM update: ${metric.operation} took ${metric.duration.toFixed(2)}ms`);
206:         }
207:     }
208:     
209:     /**
210:      * Get performance statistics.
211:      * @returns {Object} Performance statistics
212:      */
213:     getPerformanceStats() {
214:         const apiCalls = this.performanceMetrics.apiCalls;
215:         const domUpdates = this.performanceMetrics.domUpdates;
216:         
217:         const apiStats = this.calculateStats(apiCalls.map(m => m.duration));
218:         const domStats = this.calculateStats(domUpdates.map(m => m.duration));
219:         
220:         return {
221:             api: {
222:                 ...apiStats,
223:                 totalCalls: apiCalls.length,
224:                 errorRate: apiCalls.filter(m => !m.success).length / apiCalls.length,
225:                 slowCalls: apiCalls.filter(m => m.duration > 1000).length
226:             },
227:             dom: {
228:                 ...domStats,
229:                 totalUpdates: domUpdates.length,
230:                 errorRate: domUpdates.filter(m => !m.success).length / domUpdates.length,
231:                 slowUpdates: domUpdates.filter(m => m.duration > 16).length
232:             },
233:             memory: this.getMemoryStats()
234:         };
235:     }
236:     
237:     /**
238:      * Calculate basic statistics for an array of numbers.
239:      * @param {number[]} values - Array of values
240:      * @returns {Object} Statistics object
241:      */
242:     calculateStats(values) {
243:         if (values.length === 0) {
244:             return { avg: 0, min: 0, max: 0, median: 0 };
245:         }
246:         
247:         const sorted = [...values].sort((a, b) => a - b);
248:         const sum = values.reduce((a, b) => a + b, 0);
249:         
250:         return {
251:             avg: sum / values.length,
252:             min: sorted[0],
253:             max: sorted[sorted.length - 1],
254:             median: sorted[Math.floor(sorted.length / 2)]
255:         };
256:     }
257:     
258:     /**
259:      * Get memory usage statistics (if available).
260:      * @returns {Object} Memory statistics
261:      */
262:     getMemoryStats() {
263:         if (performance.memory) {
264:             return {
265:                 usedJSHeapSize: performance.memory.usedJSHeapSize,
266:                 totalJSHeapSize: performance.memory.totalJSHeapSize,
267:                 jsHeapSizeLimit: performance.memory.jsHeapSizeLimit,
268:                 usagePercent: (performance.memory.usedJSHeapSize / performance.memory.jsHeapSizeLimit) * 100
269:             };
270:         }
271:         
272:         return { available: false };
273:     }
274:     
275:     /**
276:      * Clear all timers and reset metrics.
277:      */
278:     reset() {
279:         // Clear all debounce timers
280:         this.debounceTimers.forEach(timerId => clearTimeout(timerId));
281:         this.debounceTimers.clear();
282:         
283:         // Clear all throttle timers
284:         this.throttleTimers.forEach(timerId => clearTimeout(timerId));
285:         this.throttleTimers.clear();
286:         
287:         // Reset metrics
288:         this.performanceMetrics = {
289:             apiCalls: [],
290:             domUpdates: [],
291:             errors: []
292:         };
293:         
294:         console.debug('[PerformanceOptimizer] Reset completed');
295:     }
296:     
297:     /**
298:      * Get current timer counts.
299:      * @returns {Object} Timer counts
300:      */
301:     getTimerCounts() {
302:         return {
303:             debounceTimers: this.debounceTimers.size,
304:             throttleTimers: this.throttleTimers.size,
305:             totalTimers: this.debounceTimers.size + this.throttleTimers.size
306:         };
307:     }
308:     
309:     /**
310:      * Cancel specific timer.
311:      * @param {string} key - Timer key
312:      * @param {string} type - Timer type ('debounce' or 'throttle')
313:      * @returns {boolean} True if timer was cancelled
314:      */
315:     cancelTimer(key, type) {
316:         const timers = type === 'debounce' ? this.debounceTimers : this.throttleTimers;
317:         
318:         if (timers.has(key)) {
319:             clearTimeout(timers.get(key));
320:             timers.delete(key);
321:             return true;
322:         }
323:         
324:         return false;
325:     }
326:     
327:     /**
328:      * Destroy the optimizer and cleanup resources.
329:      */
330:     destroy() {
331:         this.reset();
332:         this.isDestroyed = true;
333:         
334:         console.debug('[PerformanceOptimizer] Destroyed');
335:     }
336: }
337: 
338: // Create and export singleton instance
339: export const performanceOptimizer = new PerformanceOptimizer();
340: 
341: // Cleanup on page unload
342: window.addEventListener('beforeunload', () => {
343:     performanceOptimizer.destroy();
344: });
345: 
346: // Development helpers
347: if (window.location.hostname === 'localhost' || window.location.hostname === '127.0.0.1') {
348:     window.performanceOptimizer = performanceOptimizer; // Expose for debugging
349:     
350:     // Log performance stats periodically in development
351:     setInterval(() => {
352:         const stats = performanceOptimizer.getPerformanceStats();
353:         if (stats.api.totalCalls > 0 || stats.dom.totalUpdates > 0) {
354:             console.debug('[PerformanceOptimizer] Stats:', stats);
355:         }
356:     }, 30000); // Every 30 seconds
357: }
358: 
359: export default performanceOptimizer;
```

## File: static/constants.js
```javascript
 1: export const POLLING_INTERVAL = 2000; // Increased from 500ms to 2000ms to reduce logging
 2: 
 3: // --- MODIFICATION: La liste des étapes est mise à jour pour correspondre au backend ---
 4: export const defaultSequenceableStepsKeys = [
 5:     "STEP1",
 6:     "STEP2",
 7:     "STEP3",
 8:     "STEP4",
 9:     "STEP5",
10:     "STEP6",
11:     "STEP7"
12: ];
```

## File: static/csvDownloadMonitor.js
```javascript
  1: /**
  2:  * CSV Download Monitor
  3:  * Monitors CSV download completions and triggers auto-workflow prompts
  4:  */
  5: 
  6: import { appState } from './state/AppState.js';
  7: import { soundEvents } from './soundManager.js';
  8: import { showCSVWorkflowPrompt } from './csvWorkflowPrompt.js';
  9: 
 10: /**
 11:  * CSV download monitoring state
 12:  */
 13: let previousDownloads = new Map();
 14: let isMonitoringEnabled = true;
 15: const processedDownloads = new Set();
 16: 
 17: /**
 18:  * Handle download status updates from the polling system
 19:  * @param {Array} newDownloads - New download list from API
 20:  * @param {Array} oldDownloads - Previous download list
 21:  */
 22: const handleDownloadStatusUpdate = (newDownloads, oldDownloads) => {
 23:     if (!isMonitoringEnabled) {
 24:         console.log('[CSV_MONITOR] Monitoring disabled, skipping update');
 25:         return;
 26:     }
 27: 
 28:     if (!Array.isArray(newDownloads)) {
 29:         console.warn('[CSV_MONITOR] Invalid data format - expected array, got:', typeof newDownloads, newDownloads);
 30:         return;
 31:     }
 32: 
 33:     try {
 34:         const csvDownloads = newDownloads.filter(download => isCSVTriggeredDownload(download));
 35: 
 36:         if (csvDownloads.length > 0) {
 37:             console.log('[CSV_MONITOR] CSV downloads found:', csvDownloads.length, csvDownloads.map(d => ({ id: d.id, status: d.status })));
 38:         }
 39: 
 40:         const newlyCompletedDownloads = detectNewlyCompletedCSVDownloads(newDownloads);
 41:         console.log('[CSV_MONITOR] Newly completed CSV downloads:', newlyCompletedDownloads.length);
 42: 
 43:         newlyCompletedDownloads.forEach(download => {
 44:             console.log('[CSV_MONITOR] Processing completion for:', download.id, download.filename);
 45:             handleCSVDownloadCompletion(download);
 46:         });
 47: 
 48:         updateDownloadTrackingState(newDownloads);
 49: 
 50:     } catch (error) {
 51:         console.error('[CSV_MONITOR] Error handling download status update:', error);
 52:     }
 53: };
 54: 
 55: /**
 56:  * Detect newly completed CSV downloads
 57:  * @param {Array} currentDownloads - Current download list
 58:  * @returns {Array} Newly completed CSV downloads
 59:  */
 60: function detectNewlyCompletedCSVDownloads(currentDownloads) {
 61:     const newlyCompleted = [];
 62: 
 63:     currentDownloads.forEach(download => {
 64:         const downloadId = download.id;
 65:         const isCSVDownload = isCSVTriggeredDownload(download);
 66:         const isCompleted = download.status === 'completed';
 67:         
 68:         if (isCSVDownload && isCompleted) {
 69:             const previousDownload = previousDownloads.get(downloadId);
 70:             const alreadyProcessed = processedDownloads.has(downloadId);
 71: 
 72:             if (!alreadyProcessed && (!previousDownload || previousDownload.status !== 'completed')) {
 73:                 newlyCompleted.push(download);
 74:                 console.log('[CSV_MONITOR] Detected newly completed CSV download:', {
 75:                     id: downloadId,
 76:                     filename: download.filename,
 77:                     previousStatus: previousDownload?.status || 'unknown',
 78:                     alreadyProcessed: false
 79:                 });
 80:             } else if (alreadyProcessed) {
 81:                 console.log('[CSV_MONITOR] Skipping already processed download:', downloadId);
 82:             }
 83:         }
 84:     });
 85: 
 86:     return newlyCompleted;
 87: }
 88: 
 89: /**
 90:  * Check if a download was triggered by CSV monitoring
 91:  * @param {Object} download - Download object
 92:  * @returns {boolean} True if CSV-triggered
 93:  */
 94: function isCSVTriggeredDownload(download) {
 95:     return (
 96:         download.id && download.id.startsWith('csv_') ||
 97:         download.csv_timestamp ||
 98:         (download.message && download.message.includes('CSV'))
 99:     );
100: }
101: 
102: /**
103:  * Handle a completed CSV download
104:  * @param {Object} download - Completed download object
105:  */
106: function handleCSVDownloadCompletion(download) {
107:     console.log('[CSV_MONITOR] Processing completed CSV download:', download.filename);
108: 
109:     try {
110:         processedDownloads.add(download.id);
111:         console.log('[CSV_MONITOR] Marked download as processed:', download.id);
112: 
113:         soundEvents.csvDownloadCompletion();
114: 
115:         showCSVWorkflowPrompt(download);
116: 
117:         appState.setState({
118:             lastCSVDownloadCompletion: {
119:                 downloadId: download.id,
120:                 filename: download.filename,
121:                 timestamp: new Date().toISOString(),
122:                 promptShown: true
123:             }
124:         }, 'csv_download_completed');
125: 
126:     } catch (error) {
127:         console.error('[CSV_MONITOR] Error handling CSV download completion:', error);
128:         processedDownloads.add(download.id);
129:     }
130: }
131: 
132: /**
133:  * Update our internal tracking state
134:  * @param {Array} currentDownloads - Current download list
135:  */
136: function updateDownloadTrackingState(currentDownloads) {
137: 
138:     currentDownloads.forEach(download => {
139:         if (download.id) {
140:             previousDownloads.set(download.id, {
141:                 id: download.id,
142:                 status: download.status,
143:                 filename: download.filename,
144:                 timestamp: new Date().toISOString()
145:             });
146:         }
147:     });
148: 
149:     const oneHourAgo = Date.now() - (60 * 60 * 1000);
150:     for (const [downloadId, downloadInfo] of previousDownloads.entries()) {
151:         const downloadTime = new Date(downloadInfo.timestamp).getTime();
152:         if (downloadTime < oneHourAgo) {
153:             previousDownloads.delete(downloadId);
154:             processedDownloads.delete(downloadId);
155:         }
156:     }
157: }
158: 
159: /**
160:  * Initialize CSV download monitoring
161:  */
162: export function initializeCSVDownloadMonitor() {
163:     console.log('[CSV_MONITOR] CSV download completion monitor initializing...');
164: 
165:     console.log('[CSV_MONITOR] handleDownloadStatusUpdate type:', typeof handleDownloadStatusUpdate);
166:     console.log('[CSV_MONITOR] handleDownloadStatusUpdate function:', handleDownloadStatusUpdate);
167: 
168:     if (typeof handleDownloadStatusUpdate !== 'function') {
169:         console.error('[CSV_MONITOR] ERROR: handleDownloadStatusUpdate is not a function!');
170:         return;
171:     }
172: 
173:     try {
174:         appState.subscribeToProperty('csvDownloads', handleDownloadStatusUpdate);
175:         console.log('[CSV_MONITOR] Successfully subscribed to csvDownloads state changes');
176:     } catch (error) {
177:         console.error('[CSV_MONITOR] Failed to subscribe to appState:', error);
178:         return;
179:     }
180: 
181:     console.log('[CSV_MONITOR] CSV download completion monitor initialized successfully');
182: }
183: 
184: /**
185:  * Enable or disable CSV download monitoring
186:  * @param {boolean} enabled - Whether monitoring should be enabled
187:  */
188: export function setCSVMonitoringEnabled(enabled) {
189:     isMonitoringEnabled = enabled;
190:     console.log(`[CSV_MONITOR] CSV download monitoring ${enabled ? 'enabled' : 'disabled'}`);
191: }
192: 
193: /**
194:  * Get current monitoring state
195:  * @returns {boolean} Whether monitoring is enabled
196:  */
197: export function isCSVMonitoringEnabled() {
198:     return isMonitoringEnabled;
199: }
200: 
201: /**
202:  * Get statistics about monitored downloads
203:  * @returns {Object} Monitoring statistics
204:  */
205: export function getMonitoringStats() {
206:     return {
207:         isEnabled: isMonitoringEnabled,
208:         trackedDownloads: previousDownloads.size,
209:         lastUpdate: new Date().toISOString()
210:     };
211: }
```

## File: static/csvWorkflowPrompt.js
```javascript
  1: /**
  2:  * CSV Workflow Prompt
  3:  * Engaging popup that prompts users to process completed CSV downloads
  4:  */
  5: 
  6: import { openPopupUI, closePopupUI } from './popupManager.js';
  7: import { soundEvents } from './soundManager.js';
  8: import { appState } from './state/AppState.js';
  9: import { runStepSequence } from './sequenceManager.js';
 10: import { defaultSequenceableStepsKeys } from './constants.js';
 11: import { showNotification } from './utils.js';
 12: import { DOMUpdateUtils } from './utils/DOMBatcher.js';
 13: 
 14: // Global variables to track popup state and prevent duplicates
 15: let currentCSVPopup = null;
 16: const shownPopups = new Set(); // Track downloads that have already shown popups
 17: const POPUP_COOLDOWN_MS = 5000; // 5 second cooldown between popups
 18: let lastPopupTime = 0;
 19: 
 20: // Helper: detect Dropbox hostnames
 21: function isDropboxUrl(rawUrl) {
 22:     try {
 23:         const u = new URL(String(rawUrl || '').trim());
 24:         const host = (u.hostname || '').toLowerCase();
 25:         return host === 'dropbox.com' || host === 'www.dropbox.com' || host === 'dl.dropboxusercontent.com';
 26:     } catch {
 27:         return false;
 28:     }
 29: }
 30: 
 31: // Helper: detect Dropbox proxy URLs (R2/Worker) like https://<host>.workers.dev/dropbox/<...>/file
 32: function isDropboxProxyUrl(rawUrl) {
 33:     try {
 34:         const u = new URL(String(rawUrl || '').trim());
 35:         const host = (u.hostname || '').toLowerCase();
 36:         const path = (u.pathname || '').toLowerCase();
 37:         return (host.includes('workers.dev') || host.includes('worker')) && path.includes('/dropbox/');
 38:     } catch {
 39:         return false;
 40:     }
 41: }
 42: 
 43: function isDropboxLikeDownload(download) {
 44:     try {
 45:         const isManualOpen = Boolean(download && download.manual_open);
 46:         if (isManualOpen) return false;
 47:         const urlStr = (download && (download.original_url || download.url)) || '';
 48:         const urlType = (download && String(download.url_type || '').toLowerCase()) || '';
 49:         return urlType === 'dropbox' || isDropboxUrl(urlStr) || isDropboxProxyUrl(urlStr);
 50:     } catch {
 51:         return false;
 52:     }
 53: }
 54: 
 55: /**
 56:  * Show the CSV workflow prompt popup
 57:  * @param {Object} download - Completed download object
 58:  */
 59: export function showCSVWorkflowPrompt(download) {
 60:     console.log('[CSV_WORKFLOW_PROMPT] Showing workflow prompt for:', download.filename);
 61: 
 62:     if (!isDropboxLikeDownload(download)) {
 63:         console.log('[CSV_WORKFLOW_PROMPT] Ignoring non-Dropbox download:', download && download.id);
 64:         return;
 65:     }
 66: 
 67:     // Rate limiting: Check if we've already shown a popup for this download
 68:     if (shownPopups.has(download.id)) {
 69:         console.log('[CSV_WORKFLOW_PROMPT] Popup already shown for download:', download.id);
 70:         return;
 71:     }
 72: 
 73:     // Rate limiting: Check cooldown period
 74:     const now = Date.now();
 75:     if (now - lastPopupTime < POPUP_COOLDOWN_MS) {
 76:         console.log('[CSV_WORKFLOW_PROMPT] Popup cooldown active, skipping:', download.id);
 77:         return;
 78:     }
 79: 
 80:     // Close any existing popup first
 81:     if (currentCSVPopup) {
 82:         console.log('[CSV_WORKFLOW_PROMPT] Closing existing popup before showing new one');
 83:         closeCSVWorkflowPrompt();
 84:     }
 85: 
 86:     // Mark this download as having shown a popup
 87:     shownPopups.add(download.id);
 88:     lastPopupTime = now;
 89: 
 90:     // Create the popup overlay
 91:     currentCSVPopup = createPopupOverlay(download);
 92: 
 93:     // Add to document body
 94:     document.body.appendChild(currentCSVPopup);
 95: 
 96:     // Show the popup
 97:     openPopupUI(currentCSVPopup);
 98: 
 99:     // Add event listeners
100:     setupPromptEventListeners(download);
101: 
102:     // Play notification sound
103:     soundEvents.workflowCompletion();
104: }
105: 
106: /**
107:  * Create the popup overlay element
108:  * @param {Object} download - Completed download object
109:  * @returns {HTMLElement} Popup overlay element
110:  */
111: function createPopupOverlay(download) {
112:     const overlay = document.createElement('div');
113:     overlay.className = 'popup-overlay csv-workflow-prompt';
114:     overlay.style.display = 'none';
115:     overlay.setAttribute('role', 'dialog');
116:     overlay.setAttribute('aria-modal', 'true');
117: 
118:     const popupContent = document.createElement('div');
119:     popupContent.className = 'popup-content';
120: 
121:     // Add close button
122:     const closeButton = document.createElement('button');
123:     closeButton.className = 'popup-close-button';
124:     closeButton.innerHTML = '×';
125:     closeButton.setAttribute('aria-label', 'Fermer');
126:     closeButton.onclick = () => closeCSVWorkflowPrompt();
127: 
128:     // Add title (conditional for FromSmash)
129:     const title = document.createElement('h2');
130:     title.className = 'popup-title';
131:     const titleId = `csv-workflow-prompt-title-${Date.now()}`;
132:     title.id = titleId;
133:     overlay.setAttribute('aria-labelledby', titleId);
134:     const urlStr = (download && (download.original_url || download.url)) || '';
135:     const isFromSmash = (download && (download.url_type === 'fromsmash' || urlStr.toLowerCase().includes('fromsmash.com')));
136:     const isSwissTransfer = (download && (download.url_type === 'swisstransfer' || urlStr.toLowerCase().includes('swisstransfer.com')));
137:     const isDropbox = isDropboxLikeDownload(download);
138:     title.textContent = (!isDropbox) ? '🚀 Nouveau lien disponible !' : '🎉 Téléchargement Terminé !';
139: 
140:     // Add main content
141:     const contentDiv = document.createElement('div');
142:     contentDiv.innerHTML = createWorkflowPromptContent(download);
143: 
144:     // Assemble popup
145:     popupContent.appendChild(closeButton);
146:     popupContent.appendChild(title);
147:     popupContent.appendChild(contentDiv);
148:     overlay.appendChild(popupContent);
149: 
150:     return overlay;
151: }
152: 
153: /**
154:  * Create the HTML content for the workflow prompt
155:  * @param {Object} download - Completed download object
156:  * @returns {string} HTML content
157:  */
158: function createWorkflowPromptContent(download) {
159:     const filename = download.filename || 'Fichier téléchargé';
160:     const downloadTime = download.display_timestamp || 'maintenant';
161:     const urlStr = (download && (download.original_url || download.url || ''));
162:     const isFromSmash = (download && (download.url_type === 'fromsmash' || urlStr.toLowerCase().includes('fromsmash.com')));
163:     const isSwissTransfer = (download && (download.url_type === 'swisstransfer' || urlStr.toLowerCase().includes('swisstransfer.com')));
164:     const isDropbox = isDropboxLikeDownload(download);
165:     const isDropboxByTypeOrUrl = (
166:         (download && String(download.url_type || '').toLowerCase() === 'dropbox')
167:         || isDropboxUrl(urlStr)
168:         || isDropboxProxyUrl(urlStr)
169:     );
170:     
171:     if (!isDropbox) {
172:         const safeUrl = sanitizeExternalUrl(download.original_url || download.url || '');
173:         const providerLabel = isFromSmash ? 'FromSmash' : (isSwissTransfer ? 'SwissTransfer' : (isDropboxByTypeOrUrl ? 'Dropbox' : 'Lien Externe'));
174:         const hiddenId = isFromSmash ? 'csv-fromsmash-hidden-link' : (isSwissTransfer ? 'csv-swisstransfer-hidden-link' : 'csv-external-hidden-link');
175:         const openBtnId = isFromSmash ? 'csv-open-fromsmash-btn' : (isSwissTransfer ? 'csv-open-swisstransfer-btn' : 'csv-open-external-btn');
176:         const mainMsg = isFromSmash
177:             ? '🚀 Un nouveau lien FromSmash est disponible. Cliquez sur « Ouvrir et télécharger » pour l’ouvrir dans un nouvel onglet et démarrer le téléchargement.'
178:             : (isSwissTransfer
179:                 ? '🚀 Un nouveau lien SwissTransfer est disponible. Cliquez sur « Ouvrir et télécharger » pour l’ouvrir dans un nouvel onglet et démarrer le téléchargement.'
180:                 : (isDropboxByTypeOrUrl
181:                     ? '🚀 Un nouveau lien Dropbox est disponible. Cliquez sur « Ouvrir et télécharger » pour l’ouvrir dans un nouvel onglet et démarrer le téléchargement.'
182:                     : '🚀 Un nouveau lien externe est disponible. Cliquez sur « Ouvrir manuellement » pour l’ouvrir dans un nouvel onglet.'));
183:         return `
184:         <div class="csv-workflow-prompt-content">
185:             <div class="prompt-header">
186:                 <div class="download-icon">🔗</div>
187:                 <div class="download-info">
188:                     <h3 class="download-title">${DOMUpdateUtils.escapeHtml(filename)}</h3>
189:                     <p class="download-subtitle">Reçu à ${downloadTime}</p>
190:                 </div>
191:             </div>
192:             
193:             <div class="prompt-message">
194:                 <p class="main-message">${mainMsg}</p>
195:                 ${!safeUrl ? '<p class="warning">⚠️ Domaine non autorisé. L’ouverture automatique est bloquée pour des raisons de sécurité.</p>' : ''}
196:             </div>
197:             
198:             <div class="prompt-actions">
199:                 <button id="${openBtnId}" class="btn-primary" ${!safeUrl ? 'disabled' : ''}>
200:                     <span class="btn-icon">🌐</span>
201:                     <span class="btn-text">${isFromSmash || isSwissTransfer || isDropboxByTypeOrUrl ? 'Ouvrir et télécharger' : 'Ouvrir manuellement'}</span>
202:                 </button>
203:                 <button id="csv-workflow-dismiss-btn" class="btn-secondary workflow-dismiss-btn">
204:                     <span class="btn-icon">⏭️</span>
205:                     <span class="btn-text">Plus tard</span>
206:                 </button>
207:             </div>
208:             
209:             <div class="prompt-footer">
210:                 <p class="footer-note">
211:                     💡 Aucun traitement automatique ne sera lancé pour ce lien (${providerLabel}). Revenez lancer le workflow une fois le fichier téléchargé et disponible localement.
212:                 </p>
213:             </div>
214:             
215:             <a id="${hiddenId}" href="${safeUrl || '#'}" target="_blank" rel="noopener noreferrer" style="display:none;">open</a>
216:         </div>
217:         `;
218:     }
219:     
220:     return `
221:         <div class="csv-workflow-prompt-content">
222:             <div class="prompt-header">
223:                 <div class="download-icon">📥</div>
224:                 <div class="download-info">
225:                     <h3 class="download-title">${DOMUpdateUtils.escapeHtml(filename)}</h3>
226:                     <p class="download-subtitle">Téléchargé à ${downloadTime}</p>
227:                 </div>
228:             </div>
229:             
230:             <div class="prompt-message">
231:                 <p class="main-message">
232:                     🚀 Votre fichier est prêt ! Voulez-vous lancer le workflow complet 
233:                     pour traiter automatiquement ce contenu ?
234:                 </p>
235:                 <div class="workflow-preview">
236:                     <div class="workflow-steps">
237:                         <span class="step-badge">1️⃣ Extraction</span>
238:                         <span class="step-arrow">→</span>
239:                         <span class="step-badge">2️⃣ Analyse</span>
240:                         <span class="step-arrow">→</span>
241:                         <span class="step-badge">3️⃣ Détection</span>
242:                         <span class="step-arrow">→</span>
243:                         <span class="step-badge">4️⃣ Audio</span>
244:                         <span class="step-arrow">→</span>
245:                         <span class="step-badge">5️⃣ Tracking</span>
246:                         <span class="step-arrow">→</span>
247:                         <span class="step-badge">6️⃣ Finalisation</span>
248:                     </div>
249:                 </div>
250:             </div>
251:             
252:             <div class="prompt-actions">
253:                 <button id="csv-workflow-launch-btn" class="btn-primary workflow-launch-btn">
254:                     <span class="btn-icon">✨</span>
255:                     <span class="btn-text">Oui, Lancer le Workflow !</span>
256:                 </button>
257:                 <button id="csv-workflow-dismiss-btn" class="btn-secondary workflow-dismiss-btn">
258:                     <span class="btn-icon">⏭️</span>
259:                     <span class="btn-text">Plus tard</span>
260:                 </button>
261:             </div>
262:             
263:             <div class="prompt-footer">
264:                 <p class="footer-note">
265:                     💡 Le workflow traitera automatiquement votre fichier selon la séquence complète (étapes 1-6)
266:                 </p>
267:             </div>
268:         </div>
269:     `;
270: }
271: 
272: /**
273:  * Close the CSV workflow prompt popup
274:  */
275: function closeCSVWorkflowPrompt() {
276:     if (currentCSVPopup) {
277:         closePopupUI(currentCSVPopup);
278:         document.body.removeChild(currentCSVPopup);
279:         currentCSVPopup = null;
280:     }
281: }
282: 
283: /**
284:  * Setup event listeners for the prompt buttons
285:  * @param {Object} download - Download object
286:  */
287: function setupPromptEventListeners(download) {
288:     const urlStr = (download && (download.original_url || download.url || ''));
289:     const isFromSmash = (download && (download.url_type === 'fromsmash' || urlStr.toLowerCase().includes('fromsmash.com')));
290:     const isSwissTransfer = (download && (download.url_type === 'swisstransfer' || urlStr.toLowerCase().includes('swisstransfer.com')));
291:     const isDropbox = isDropboxLikeDownload(download);
292:     const dismissBtn = document.getElementById('csv-workflow-dismiss-btn');
293: 
294:     if (dismissBtn) {
295:         dismissBtn.addEventListener('click', () => handleWorkflowDismiss(download));
296:     }
297: 
298:     if (!isDropbox) {
299:         const btnId = isFromSmash ? 'csv-open-fromsmash-btn' : (isSwissTransfer ? 'csv-open-swisstransfer-btn' : 'csv-open-external-btn');
300:         const openBtn = document.getElementById(btnId);
301:         if (openBtn) {
302:             openBtn.addEventListener('click', () => openExternalLink(download, { isFromSmash, isSwissTransfer }));
303:         }
304:     } else {
305:         const launchBtn = document.getElementById('csv-workflow-launch-btn');
306:         if (launchBtn) {
307:             launchBtn.addEventListener('click', () => handleWorkflowLaunch(download));
308:         }
309:     }
310: }
311: 
312: /**
313:  * Open FromSmash link in a new tab with basic URL sanitization
314:  * @param {Object} download
315:  */
316: function openExternalLink(download, { isFromSmash = false, isSwissTransfer = false } = {}) {
317:     try {
318:         const rawUrl = (download && (download.original_url || download.url)) || '';
319:         const safeUrl = sanitizeExternalUrl(rawUrl);
320:         if (!safeUrl) {
321:             showNotification('Lien invalide ou domaine non autorisé.', 'error');
322:             return;
323:         }
324: 
325:         // Close the popup first
326:         closeCSVWorkflowPrompt();
327: 
328:         // Attempt to open via hidden anchor to respect browser policies
329:         const hiddenId = isFromSmash ? 'csv-fromsmash-hidden-link' : (isSwissTransfer ? 'csv-swisstransfer-hidden-link' : 'csv-external-hidden-link');
330:         const hiddenA = document.getElementById(hiddenId);
331:         if (hiddenA && hiddenA.href) {
332:             hiddenA.click();
333:         } else {
334:             window.open(safeUrl, '_blank', 'noopener');
335:         }
336: 
337:         const provider = isFromSmash ? 'FromSmash' : (isSwissTransfer ? 'SwissTransfer' : 'externe');
338:         showNotification(`Ouverture du lien ${provider} dans un nouvel onglet...`, 'success');
339:         soundEvents.workflowStart();
340:     } catch (e) {
341:         console.error('[CSV_WORKFLOW_PROMPT] Failed to open external link:', e);
342:         showNotification("Impossible d'ouvrir le lien.", 'error');
343:         soundEvents.errorEvent();
344:     }
345: }
346: 
347: /**
348:  * Open SwissTransfer link in a new tab with basic URL sanitization
349:  * @param {Object} download
350:  */
351: function openSwissTransferLink(download) {
352:     try {
353:         const rawUrl = (download && (download.original_url || download.url)) || '';
354:         const safeUrl = sanitizeExternalUrl(rawUrl);
355:         if (!safeUrl) {
356:             showNotification("Lien invalide ou non supporté.", 'error');
357:             return;
358:         }
359: 
360:         // Close the popup first
361:         closeCSVWorkflowPrompt();
362: 
363:         // Attempt to open via hidden anchor to respect browser policies
364:         const hiddenA = document.getElementById('csv-swisstransfer-hidden-link');
365:         if (hiddenA && hiddenA.href) {
366:             hiddenA.click();
367:         } else {
368:             window.open(safeUrl, '_blank', 'noopener');
369:         }
370: 
371:         showNotification("Ouverture du lien SwissTransfer dans un nouvel onglet...", 'success');
372:         soundEvents.workflowStart();
373:     } catch (e) {
374:         console.error('[CSV_WORKFLOW_PROMPT] Failed to open SwissTransfer link:', e);
375:         showNotification("Impossible d'ouvrir le lien.", 'error');
376:         soundEvents.errorEvent();
377:     }
378: }
379: 
380: /**
381:  * Sanitize external URL (very basic allowlist)
382:  * @param {string} url
383:  * @returns {string|null}
384:  */
385: function sanitizeExternalUrl(url) {
386:     if (typeof url !== 'string') return null;
387:     const trimmed = url.trim();
388:     if (!trimmed) return null;
389:     try {
390:         const u = new URL(trimmed);
391:         const hostname = (u.hostname || '').toLowerCase();
392:         // Allow any valid HTTP(S) URL. Non-Dropbox links are opened manually in a new tab.
393:         if (!['http:', 'https:'].includes(u.protocol)) {
394:             return null;
395:         }
396:         return u.toString();
397:     } catch {
398:         return null;
399:     }
400: }
401: 
402: /**
403:  * Handle workflow launch button click
404:  * @param {Object} download - Download object
405:  */
406: async function handleWorkflowLaunch(download) {
407:     console.log('[CSV_WORKFLOW_PROMPT] User chose to launch workflow for:', download.filename);
408:     
409:     try {
410:         // Close the popup first
411:         closeCSVWorkflowPrompt();
412:         
413:         // Check if a sequence is already running
414:         if (appState.getStateProperty('isAnySequenceRunning')) {
415:             showNotification("Une séquence est déjà en cours. Veuillez attendre qu'elle se termine.", 'warning');
416:             return;
417:         }
418:         
419:         // Play workflow start sound
420:         soundEvents.workflowStart();
421:         
422:         // Show success notification
423:         showNotification(`🚀 Lancement du workflow pour "${download.filename}"`, 'success');
424:         
425:         // Launch the complete workflow sequence
426:         await runStepSequence(defaultSequenceableStepsKeys, "Séquence CSV Auto");
427:         
428:         // Update app state to track the auto-launch
429:         appState.setState({
430:             lastCSVAutoLaunch: {
431:                 downloadId: download.id,
432:                 filename: download.filename,
433:                 timestamp: new Date().toISOString(),
434:                 sequenceType: 'CSV Auto'
435:             }
436:         }, 'csv_workflow_auto_launched');
437:         
438:     } catch (error) {
439:         console.error('[CSV_WORKFLOW_PROMPT] Error launching workflow:', error);
440:         showNotification("Erreur lors du lancement du workflow. Veuillez réessayer.", 'error');
441:         soundEvents.errorEvent();
442:     }
443: }
444: 
445: /**
446:  * Handle workflow dismiss button click
447:  * @param {Object} download - Download object
448:  */
449: function handleWorkflowDismiss(download) {
450:     console.log('[CSV_WORKFLOW_PROMPT] User dismissed workflow prompt for:', download.filename);
451:     
452:     // Close the popup
453:     closeCSVWorkflowPrompt();
454:     
455:     // Show a gentle notification
456:     showNotification("Workflow reporté. Vous pouvez le lancer manuellement quand vous le souhaitez.", 'info');
457:     
458:     // Update app state to track the dismissal
459:     appState.setState({
460:         lastCSVPromptDismissal: {
461:             downloadId: download.id,
462:             filename: download.filename,
463:             timestamp: new Date().toISOString()
464:         }
465:     }, 'csv_workflow_prompt_dismissed');
466: }
467: 
468: // Using DOMUpdateUtils.escapeHtml from DOMBatcher for XSS safety (project standard)
```

## File: static/reportViewer.js
```javascript
  1: /**
  2:  * Report Viewer - Visual Analysis Report Generator
  3:  * Workflow MediaPipe v4.0
  4:  * 
  5:  * Generates comprehensive HTML reports with statistics and infographics
  6:  */
  7: 
  8: import { domBatcher } from './utils/DOMBatcher.js';
  9: 
 10: class ReportViewer {
 11:     constructor() {
 12:         this.overlay = null;
 13:         this.isVisible = false;
 14:         this.projects = [];
 15:         this.currentReport = null;
 16:         this.selectedFormat = 'html';
 17:         this.availableMonths = new Set();
 18:         this.prevFocusEl = null;
 19:         this._keydownHandler = null;
 20:     }
 21: 
 22:     _getFocusableElements(container) {
 23:         if (!container || typeof container.querySelectorAll !== 'function') return [];
 24:         const selectors = [
 25:             'a[href]',
 26:             'button:not([disabled])',
 27:             'textarea:not([disabled])',
 28:             'input:not([disabled])',
 29:             'select:not([disabled])',
 30:             '[tabindex]:not([tabindex="-1"])'
 31:         ];
 32:         return Array.from(container.querySelectorAll(selectors.join(',')))
 33:             .filter(el => el && typeof el.focus === 'function' && (el.offsetParent !== null || (typeof el.getAttribute === 'function' && el.getAttribute('aria-hidden') !== 'true')));
 34:     }
 35: 
 36:     _enableModalFocusTrap() {
 37:         const overlay = this.overlay;
 38:         if (!overlay) return;
 39: 
 40:         try {
 41:             const currentFocused = document.activeElement;
 42:             if (currentFocused && currentFocused !== document.body && currentFocused !== document.documentElement && typeof currentFocused.focus === 'function') {
 43:                 this.prevFocusEl = currentFocused;
 44:             } else {
 45:                 this.prevFocusEl = null;
 46:             }
 47:         } catch {
 48:             this.prevFocusEl = null;
 49:         }
 50: 
 51:         if (typeof overlay.getAttribute === 'function' && overlay.getAttribute('tabindex') === null && typeof overlay.setAttribute === 'function') {
 52:             overlay.setAttribute('tabindex', '-1');
 53:         }
 54: 
 55:         const focusables = this._getFocusableElements(overlay);
 56:         const first = focusables[0] || overlay;
 57:         if (first && typeof first.focus === 'function') {
 58:             first.focus();
 59:         }
 60: 
 61:         this._keydownHandler = (e) => {
 62:             if (e.key === 'Escape') {
 63:                 e.preventDefault();
 64:                 this.close();
 65:                 return;
 66:             }
 67:             if (e.key === 'Tab') {
 68:                 const focusEls = this._getFocusableElements(overlay);
 69:                 if (focusEls.length === 0) {
 70:                     e.preventDefault();
 71:                     overlay.focus && overlay.focus();
 72:                     return;
 73:                 }
 74:                 const currentIndex = focusEls.indexOf(document.activeElement);
 75:                 let nextIndex = currentIndex;
 76:                 if (e.shiftKey) {
 77:                     nextIndex = currentIndex <= 0 ? focusEls.length - 1 : currentIndex - 1;
 78:                 } else {
 79:                     nextIndex = currentIndex === focusEls.length - 1 ? 0 : currentIndex + 1;
 80:                 }
 81:                 e.preventDefault();
 82:                 focusEls[nextIndex].focus();
 83:             }
 84:         };
 85: 
 86:         overlay.addEventListener('keydown', this._keydownHandler);
 87:     }
 88: 
 89:     _disableModalFocusTrap() {
 90:         const overlay = this.overlay;
 91:         if (overlay && this._keydownHandler) {
 92:             overlay.removeEventListener('keydown', this._keydownHandler);
 93:         }
 94:         this._keydownHandler = null;
 95:         const prev = this.prevFocusEl;
 96:         this.prevFocusEl = null;
 97: 
 98:         if (prev && typeof prev.focus === 'function') {
 99:             try {
100:                 prev.focus();
101:             } catch (_) {}
102:         }
103:     }
104: 
105:     /**
106:      * Generate monthly archive report
107:      */
108:     async generateMonthlyReport() {
109:         try {
110:             const monthInput = document.getElementById('report-month-select');
111:             const previewContainer = document.getElementById('report-preview-container');
112:             if (!monthInput) return;
113: 
114:             const month = (monthInput.value || '').trim();
115:             if (!month) {
116:                 this.showError('Veuillez sélectionner un mois au format YYYY-MM.');
117:                 return;
118:             }
119: 
120:             this.showLoading(previewContainer);
121: 
122:             const response = await fetch('/api/reports/generate/monthly', {
123:                 method: 'POST',
124:                 headers: { 'Content-Type': 'application/json' },
125:                 body: JSON.stringify({ month })
126:             });
127: 
128:             if (!response.ok) {
129:                 let backendMsg = '';
130:                 try {
131:                     const err = await response.json();
132:                     backendMsg = err && err.error ? ` — ${err.error}` : '';
133:                 } catch {}
134:                 const message = `HTTP ${response.status}: ${response.statusText}${backendMsg}`;
135:                 throw new Error(message);
136:             }
137: 
138:             const data = await response.json();
139:             if (data.error) {
140:                 throw new Error(data.error);
141:             }
142: 
143:             // Render the monthly HTML into the preview container
144:             this.currentReport = { ...data, project_name: null, video_name: null };
145:             this.renderReport({ html: data.html, format: 'html' }, previewContainer);
146: 
147:             // Enable download button
148:             const downloadButton = document.getElementById('report-download-button');
149:             if (downloadButton) downloadButton.disabled = false;
150: 
151:         } catch (error) {
152:             console.error('[ReportViewer] Error generating monthly report:', error);
153:             let hint = '';
154:             if (this.availableMonths && this.availableMonths.size > 0) {
155:                 hint = `\nMois disponibles: ${Array.from(this.availableMonths).sort().join(', ')}`;
156:             }
157:             this.showError('Erreur de génération du rapport mensuel: ' + error.message + hint);
158:         }
159:     }
160: 
161:     /**
162:      * Parse a project name that may contain a timestamp suffix
163:      * Pattern: "<base> YYYY-MM-DD_HH-MM-SS"
164:      * Returns { baseName, timestampRaw, timestampPretty }
165:      */
166:     parseArchiveName(name) {
167:         try {
168:             const m = name.match(/^(.*) (\d{4}-\d{2}-\d{2}_)\b(\d{2}-\d{2}-\d{2})$/);
169:             if (!m) {
170:                 return { baseName: name, timestampRaw: null, timestampPretty: null };
171:             }
172:             const base = m[1];
173:             const datePart = m[2].slice(0, -1); // remove trailing underscore
174:             const timePart = m[3].replace(/-/g, ':');
175:             const pretty = `${datePart} ${timePart}`; // YYYY-MM-DD HH:MM:SS
176:             return { baseName: base, timestampRaw: `${datePart}_${m[3]}`, timestampPretty: pretty };
177:         } catch {
178:             return { baseName: name, timestampRaw: null, timestampPretty: null };
179:         }
180:     }
181: 
182:     /**
183:      * Initialize the report viewer
184:      */
185:     init() {
186:         console.log('[ReportViewer] Initializing...');
187:         
188:         this.overlay = document.getElementById('report-overlay');
189:         
190:         if (!this.overlay) {
191:             console.warn('[ReportViewer] Report overlay not found');
192:             return;
193:         }
194: 
195:         this.setupEventHandlers();
196:         
197:         console.log('[ReportViewer] Initialized successfully');
198:     }
199: 
200:     /**
201:      * Setup event handlers
202:      */
203:     setupEventHandlers() {
204:         // Open button
205:         const openButton = document.getElementById('open-report-button');
206:         if (openButton) {
207:             openButton.addEventListener('click', () => this.open());
208:         }
209: 
210:         // Close button
211:         const closeButton = document.getElementById('report-close');
212:         if (closeButton) {
213:             closeButton.addEventListener('click', () => this.close());
214:         }
215: 
216:         // Close on overlay click
217:         this.overlay.addEventListener('click', (e) => {
218:             if (e.target === this.overlay) {
219:                 this.close();
220:             }
221:         });
222: 
223:         // Escape key to close
224:         document.addEventListener('keydown', (e) => {
225:             if (e.key === 'Escape' && this.isVisible) {
226:                 this.close();
227:             }
228:         });
229: 
230:         // Generate button
231:         const generateButton = document.getElementById('report-generate-button');
232:         if (generateButton) {
233:             generateButton.addEventListener('click', () => this.generateReport());
234:         }
235: 
236:         // Download button
237:         const downloadButton = document.getElementById('report-download-button');
238:         if (downloadButton) {
239:             downloadButton.addEventListener('click', () => this.downloadReport());
240:         }
241: 
242:         // Project selector change
243:         const projectSelect = document.getElementById('report-project-select');
244:         if (projectSelect) {
245:             projectSelect.addEventListener('change', (e) => this.onProjectChange(e.target.value));
246:         }
247: 
248:         // No format radios anymore (HTML-only)
249: 
250:         // Optional: project-only checkbox to generate consolidated project report
251:         const projectOnly = document.getElementById('report-project-only');
252:         if (projectOnly) {
253:             projectOnly.addEventListener('change', () => this.updateGenerateButtonState());
254:         }
255: 
256:         // Monthly report button
257:         const monthlyBtn = document.getElementById('report-generate-monthly-button');
258:         if (monthlyBtn) {
259:             monthlyBtn.addEventListener('click', () => this.generateMonthlyReport());
260:         }
261: 
262:         // Analyze uploaded monthly report
263:         const analyzeBtn = document.getElementById('report-analyze-upload-button');
264:         const uploadInput = document.getElementById('monthly-report-upload');
265:         const resultEl = document.getElementById('report-analyze-result');
266:         if (analyzeBtn && uploadInput && resultEl) {
267:             analyzeBtn.addEventListener('click', async () => {
268:                 try {
269:                     if (!uploadInput.files || uploadInput.files.length === 0) {
270:                         resultEl.textContent = 'Veuillez sélectionner un fichier HTML de rapport.';
271:                         resultEl.className = 'form-feedback error';
272:                         return;
273:                     }
274:                     const file = uploadInput.files[0];
275:                     const formData = new FormData();
276:                     formData.append('file', file);
277:                     resultEl.textContent = 'Analyse en cours...';
278:                     resultEl.className = 'form-feedback info';
279: 
280:                     const resp = await fetch('/api/reports/analyze/monthly_upload', {
281:                         method: 'POST',
282:                         body: formData
283:                     });
284:                     if (!resp.ok) {
285:                         let backendMsg = '';
286:                         try { const j = await resp.json(); backendMsg = j && j.error ? ` — ${j.error}` : ''; } catch {}
287:                         throw new Error(`HTTP ${resp.status}: ${resp.statusText}${backendMsg}`);
288:                     }
289:                     const data = await resp.json();
290:                     if (data.error) throw new Error(data.error);
291: 
292:                     const mp4 = data.by_extension?.[".mp4"] ?? 0;
293:                     const other = data.by_extension?.other ?? 0;
294:                     const noext = data.by_extension?.noext ?? 0;
295:                     const parts = [];
296:                     parts.push(`Total listé: ${data.total_listed}`);
297:                     parts.push(`.mp4: ${mp4}`);
298:                     parts.push(`autres: ${other}`);
299:                     parts.push(`sans extension: ${noext}`);
300:                     if (typeof data.projects === 'number') parts.push(`projets: ${data.projects}`);
301:                     if (data.month) parts.push(`mois: ${data.month}`);
302:                     if (data.build_id) parts.push(`build: ${data.build_id}`);
303: 
304:                     resultEl.textContent = parts.join(' · ');
305:                     resultEl.className = 'form-feedback success';
306:                 } catch (err) {
307:                     console.error('[ReportViewer] analyze upload error:', err);
308:                     resultEl.textContent = `Erreur d\'analyse: ${err.message || err}`;
309:                     resultEl.className = 'form-feedback error';
310:                 }
311:             });
312:         }
313:     }
314: 
315:     /**
316:      * Open the report viewer
317:      */
318:     async open() {
319:         console.log('[ReportViewer] Opening report viewer...');
320: 
321:         if (!this.overlay) {
322:             this.overlay = document.getElementById('report-overlay');
323:         }
324:         if (!this.overlay) {
325:             return;
326:         }
327:         
328:         this.isVisible = true;
329:         this.overlay.style.display = 'flex';
330:         this._enableModalFocusTrap();
331:         
332:         // Trigger reflow for animation
333:         setTimeout(() => {
334:             this.overlay.setAttribute('data-visible', 'true');
335:         }, 10);
336: 
337:         // Load projects list
338:         await this.loadProjects();
339:     }
340: 
341:     /**
342:      * Close the report viewer
343:      */
344:     close() {
345:         console.log('[ReportViewer] Closing report viewer...');
346: 
347:         if (!this.overlay) {
348:             this.overlay = document.getElementById('report-overlay');
349:         }
350:         if (!this.overlay) {
351:             this.isVisible = false;
352:             return;
353:         }
354:         
355:         this.isVisible = false;
356:         this.overlay.setAttribute('data-visible', 'false');
357:         
358:         setTimeout(() => {
359:             this.overlay.style.display = 'none';
360:             this._disableModalFocusTrap();
361:         }, 300);
362:     }
363: 
364:     /**
365:      * Load available projects from API
366:      */
367:     async loadProjects() {
368:         try {
369:             const response = await fetch('/api/visualization/projects');
370:             
371:             if (!response.ok) {
372:                 throw new Error(`HTTP ${response.status}: ${response.statusText}`);
373:             }
374: 
375:             const data = await response.json();
376: 
377:             if (data.error) {
378:                 throw new Error(data.error);
379:             }
380: 
381:             this.projects = data.projects || [];
382:             // Build available months set from archive timestamps
383:             this.availableMonths = new Set(
384:                 (this.projects || [])
385:                     .map(p => (p.archive_timestamp || '').slice(0, 7))
386:                     .filter(m => /\d{4}-\d{2}/.test(m))
387:             );
388:             // Prefill month selector to current month if empty
389:             const monthInput = document.getElementById('report-month-select');
390:             if (monthInput && !monthInput.value) {
391:                 const currentMonth = new Date().toISOString().slice(0, 7);
392:                 monthInput.value = currentMonth;
393:                 if (!this.availableMonths.has(monthInput.value) && this.availableMonths.size > 0) {
394:                     // Pick any available month to help the user
395:                     const first = Array.from(this.availableMonths)[0];
396:                     monthInput.value = first;
397:                 }
398:             }
399:             this.renderProjectSelector();
400: 
401:             console.log(`[ReportViewer] Loaded ${this.projects.length} projects`);
402: 
403:         } catch (error) {
404:             console.error('[ReportViewer] Error loading projects:', error);
405:             this.showError('Impossible de charger les projets: ' + error.message);
406:         }
407:     }
408: 
409:     /**
410:      * Update generate button enabled/disabled state based on selections
411:      */
412:     updateGenerateButtonState() {
413:         const projectSelect = document.getElementById('report-project-select');
414:         const videoSelect = document.getElementById('report-video-select');
415:         const generateButton = document.getElementById('report-generate-button');
416:         const projectOnly = document.getElementById('report-project-only');
417: 
418:         if (!generateButton || !projectSelect) return;
419: 
420:         const hasProject = !!projectSelect.value;
421:         const isProjectOnly = !!(projectOnly && projectOnly.checked);
422:         const hasVideo = !!(videoSelect && videoSelect.value);
423: 
424:         generateButton.disabled = !(hasProject && (isProjectOnly || hasVideo));
425:     }
426: 
427:     /**
428:      * Render project selector dropdown
429:      */
430:     renderProjectSelector() {
431:         const projectSelect = document.getElementById('report-project-select');
432:         const videoSelect = document.getElementById('report-video-select');
433:         
434:         if (!projectSelect) return;
435: 
436:         domBatcher.scheduleUpdate('report-projects-select', () => {
437:             // Clear and populate project selector
438:             projectSelect.innerHTML = '<option value="">-- Sélectionner un projet --</option>';
439: 
440:             this.projects.forEach(project => {
441:                 const option = document.createElement('option');
442:                 option.value = project.name;
443:                 // Prefer backend-provided fields; fallback to local parsing
444:                 const baseLabel = project.display_base || this.parseArchiveName(project.name).baseName;
445:                 const tsPretty = project.archive_timestamp || this.parseArchiveName(project.name).timestampPretty;
446:                 const countLabel = `(${project.video_count} vidéo${project.video_count > 1 ? 's' : ''})`;
447:                 const tsLabel = tsPretty ? ` — archivé le ${tsPretty}` : '';
448: 
449:                 // Accessibility: expose full timestamp on hover
450:                 if (tsPretty) {
451:                     option.title = `Archivé le ${tsPretty}`;
452:                 }
453: 
454:                 // Use textContent to avoid HTML injection
455:                 option.textContent = `${baseLabel} ${countLabel}${tsLabel}`;
456:                 projectSelect.appendChild(option);
457:             });
458:         });
459: 
460:         // Reset video selector
461:         if (videoSelect) {
462:             videoSelect.innerHTML = '<option value="">-- Sélectionner une vidéo --</option>';
463:             videoSelect.disabled = true;
464:         }
465: 
466:         // Disable generate button
467:         const generateButton = document.getElementById('report-generate-button');
468:         if (generateButton) {
469:             generateButton.disabled = true;
470:         }
471:     }
472: 
473:     /**
474:      * Handle project selection change
475:      */
476:     onProjectChange(projectName) {
477:         const videoSelect = document.getElementById('report-video-select');
478:         const generateButton = document.getElementById('report-generate-button');
479:         const projectOnly = document.getElementById('report-project-only');
480:         
481:         if (!projectName || !videoSelect) return;
482: 
483:         const project = this.projects.find(p => p.name === projectName);
484:         
485:         if (!project) return;
486: 
487:         // Populate video selector
488:         videoSelect.innerHTML = '<option value="">-- Sélectionner une vidéo --</option>';
489:         
490:         project.videos.forEach(video => {
491:             const option = document.createElement('option');
492:             option.value = video;
493:             option.textContent = video;
494:             videoSelect.appendChild(option);
495:         });
496: 
497:         videoSelect.disabled = false;
498: 
499:         // Enable generate button when video is selected (unless project-only mode)
500:         videoSelect.onchange = () => this.updateGenerateButtonState();
501:         this.updateGenerateButtonState();
502:     }
503: 
504:     /**
505:      * Generate report for selected project/video
506:      */
507:     async generateReport() {
508:         const projectSelect = document.getElementById('report-project-select');
509:         const videoSelect = document.getElementById('report-video-select');
510:         const previewContainer = document.getElementById('report-preview-container');
511:         const projectOnly = document.getElementById('report-project-only');
512:         
513:         if (!projectSelect || !videoSelect) return;
514: 
515:         const projectName = projectSelect.value;
516:         const videoName = videoSelect.value;
517: 
518:         if (!projectName) return;
519: 
520:         try {
521:             this.showLoading(previewContainer);
522: 
523:             const isProjectOnly = !!(projectOnly && projectOnly.checked);
524:             const endpoint = isProjectOnly ? '/api/reports/generate/project' : '/api/reports/generate';
525:             const body = isProjectOnly
526:                 ? { project: projectName, format: 'html' }
527:                 : { project: projectName, video: videoName, format: 'html' };
528: 
529:             const response = await fetch(endpoint, {
530:                 method: 'POST',
531:                 headers: { 'Content-Type': 'application/json' },
532:                 body: JSON.stringify(body)
533:             });
534:             
535:             if (!response.ok) {
536:                 throw new Error(`HTTP ${response.status}: ${response.statusText}`);
537:             }
538: 
539:             const data = await response.json();
540: 
541:             if (data.error) {
542:                 throw new Error(data.error);
543:             }
544: 
545:             this.currentReport = data;
546:             this.renderReport(data, previewContainer);
547: 
548:             console.log('[ReportViewer] Report generated successfully');
549: 
550:         } catch (error) {
551:             console.error('[ReportViewer] Error generating report:', error);
552:             this.showError('Erreur de génération: ' + error.message);
553:         }
554:     }
555: 
556:     /**
557:      * Show loading state
558:      */
559:     showLoading(container) {
560:         domBatcher.scheduleUpdate('report-loading', () => {
561:             container.innerHTML = `
562:                 <div class="report-loading">
563:                     <div class="report-loading-spinner"></div>
564:                     <div class="report-loading-text">Génération du rapport en cours...</div>
565:                 </div>
566:             `;
567:         });
568:     }
569: 
570:     /**
571:      * Show error state
572:      */
573:     showError(message) {
574:         const previewContainer = document.getElementById('report-preview-container');
575:         if (!previewContainer) return;
576: 
577:         domBatcher.scheduleUpdate('report-error', () => {
578:             previewContainer.innerHTML = `
579:                 <div class="report-preview-empty">
580:                     <div class="report-preview-icon">⚠️</div>
581:                     <div class="report-preview-message">Erreur</div>
582:                     <div class="report-preview-detail">${this.escapeHtml(message)}</div>
583:                 </div>
584:             `;
585:         });
586:     }
587: 
588:     /**
589:      * Render generated report
590:      */
591:     renderReport(data, container) {
592:         const { html, format } = data;
593: 
594:         let content = '';
595: 
596:         if (format === 'html' && html) {
597:             // Create sandboxed iframe for HTML preview. We do NOT escape srcdoc to allow proper rendering.
598:             content = `
599:                 <iframe class="report-preview-iframe" id="report-iframe" sandbox="allow-same-origin" srcdoc='${html.replace(/'/g, "&#39;")}'></iframe>
600:                 <div class="report-download-info">
601:                     <div class="report-download-icon">✅</div>
602:                     <div>
603:                         <div style="font-weight: 600;">Rapport généré avec succès</div>
604:                         <div style="font-size: 14px; opacity: 0.9;">Utilisez le bouton "Télécharger" pour sauvegarder</div>
605:                     </div>
606:                 </div>
607:             `;
608:         } else {
609:             content = `
610:                 <div class="report-preview-empty">
611:                     <div class="report-preview-icon">📊</div>
612:                     <div class="report-preview-message">Aucun contenu disponible</div>
613:                 </div>
614:             `;
615:         }
616: 
617:         domBatcher.scheduleUpdate('report-render', () => {
618:             container.innerHTML = content;
619:         });
620: 
621:         // Enable download button
622:         const downloadButton = document.getElementById('report-download-button');
623:         if (downloadButton) {
624:             downloadButton.disabled = false;
625:         }
626:     }
627: 
628:     /**
629:      * Download current report
630:      */
631:     downloadReport() {
632:         if (!this.currentReport) return;
633: 
634:         const { html, format, video_name, project_name } = this.currentReport;
635:         const timestamp = new Date().toISOString().split('T')[0];
636:         const baseName = video_name
637:             ? video_name.replace(/\.[^/.]+$/, "")
638:             : (project_name || 'projet');
639:         
640:         if (format === 'html' && html) {
641:             // Download HTML
642:             const blob = new Blob([html], { type: 'text/html' });
643:             const url = URL.createObjectURL(blob);
644:             const a = document.createElement('a');
645:             a.href = url;
646:             a.download = `rapport_${baseName}_${timestamp}.html`;
647:             document.body.appendChild(a);
648:             a.click();
649:             document.body.removeChild(a);
650:             URL.revokeObjectURL(url);
651:         }
652:     }
653: 
654:     /**
655:      * Update format selection UI
656:      */
657:     updateFormatSelection() {
658:         const formatOptions = document.querySelectorAll('.report-format-option');
659:         formatOptions.forEach(option => {
660:             const radio = option.querySelector('input[type="radio"]');
661:             if (radio && radio.value === this.selectedFormat) {
662:                 option.classList.add('selected');
663:             } else {
664:                 option.classList.remove('selected');
665:             }
666:         });
667:     }
668: 
669:     /**
670:      * Escape HTML to prevent XSS
671:      */
672:     escapeHtml(text) {
673:         const div = document.createElement('div');
674:         div.textContent = text;
675:         return div.innerHTML;
676:     }
677: }
678: 
679: // Create and export singleton instance
680: const reportViewer = new ReportViewer();
681: export { reportViewer };
```

## File: static/soundManager.js
```javascript
  1: // ===== SOUND MANAGER =====
  2: // Manages audio feedback for user interactions and workflow events
  3: 
  4: /**
  5:  * Sound event types and their corresponding audio files
  6:  */
  7: const SOUND_EVENTS = {
  8:     WORKFLOW_START: 'DM_20250703182122_001.mp3',
  9:     CHECKBOX_INTERACTION: 'DM_20250703182143_001.mp3',
 10:     WORKFLOW_COMPLETION: 'DM_20250703182159_001.mp3',
 11:     ERROR_EVENT: 'DM_20250703182219_001.mp3',
 12:     AUTO_MODE_TOGGLE: 'DM_20250703182234_001.mp3',
 13:     STEP_SUCCESS: 'DM_20250703182315_001.mp3',
 14:     CSV_DOWNLOAD_INITIATION: 'DM_20250703194305_001.mp3',
 15:     CSV_DOWNLOAD_COMPLETION: 'DM_20250703182159_001.mp3' // Reuse workflow completion sound
 16: };
 17: 
 18: /**
 19:  * Sound manager state
 20:  */
 21: let soundEnabled = true;
 22: let audioCache = new Map();
 23: let masterVolume = 0.7;
 24: 
 25: /**
 26:  * Initialize the sound manager
 27:  */
 28: export function initializeSoundManager() {
 29:     // Load user preferences from localStorage
 30:     const savedSoundEnabled = localStorage.getItem('soundEnabled');
 31:     if (savedSoundEnabled !== null) {
 32:         soundEnabled = savedSoundEnabled === 'true';
 33:     }
 34: 
 35:     const savedVolume = localStorage.getItem('soundVolume');
 36:     if (savedVolume !== null) {
 37:         masterVolume = parseFloat(savedVolume);
 38:     }
 39: 
 40:     // Preload audio files
 41:     preloadAudioFiles();
 42: 
 43:     console.log('[SOUND] Sound manager initialized', {
 44:         soundEnabled,
 45:         masterVolume,
 46:         availableSounds: Object.keys(SOUND_EVENTS)
 47:     });
 48: }
 49: 
 50: /**
 51:  * Preload all audio files for better performance
 52:  */
 53: function preloadAudioFiles() {
 54:     Object.entries(SOUND_EVENTS).forEach(([eventType, filename]) => {
 55:         try {
 56:             const audio = new Audio(`/sound-design/${filename}`);
 57:             audio.preload = 'auto';
 58:             audio.volume = masterVolume;
 59:             
 60:             // Handle loading events
 61:             audio.addEventListener('canplaythrough', () => {
 62:                 console.log(`[SOUND] Preloaded: ${filename}`);
 63:             });
 64:             
 65:             audio.addEventListener('error', (e) => {
 66:                 console.warn(`[SOUND] Failed to preload: ${filename}`, e);
 67:             });
 68:             
 69:             audioCache.set(eventType, audio);
 70:         } catch (error) {
 71:             console.warn(`[SOUND] Error creating audio object for ${filename}:`, error);
 72:         }
 73:     });
 74: }
 75: 
 76: // Rate limiting for sound events to prevent spam
 77: const soundRateLimit = new Map();
 78: const SOUND_RATE_LIMIT_MS = 1000; // Minimum time between same sound events
 79: 
 80: /**
 81:  * Play a sound for a specific event type with rate limiting and proper cleanup
 82:  * @param {string} eventType - The type of event (from SOUND_EVENTS keys)
 83:  * @param {Object} options - Optional parameters
 84:  * @param {number} options.volume - Volume override (0.0 to 1.0)
 85:  * @param {boolean} options.force - Force play even if sounds are disabled
 86:  */
 87: export function playSound(eventType, options = {}) {
 88:     if (!soundEnabled && !options.force) {
 89:         return;
 90:     }
 91: 
 92:     if (!SOUND_EVENTS[eventType]) {
 93:         console.warn(`[SOUND] Unknown event type: ${eventType}`);
 94:         return;
 95:     }
 96: 
 97:     // Rate limiting - prevent rapid-fire sound events
 98:     const now = Date.now();
 99:     const lastPlayed = soundRateLimit.get(eventType) || 0;
100:     if (now - lastPlayed < SOUND_RATE_LIMIT_MS) {
101:         return; // Skip this sound event to prevent spam
102:     }
103:     soundRateLimit.set(eventType, now);
104: 
105:     try {
106:         const audio = audioCache.get(eventType);
107:         if (!audio) {
108:             console.warn(`[SOUND] Audio not found in cache for event: ${eventType}`);
109:             return;
110:         }
111: 
112:         // Reuse the same audio element instead of cloning to prevent WebMediaPlayer accumulation
113:         audio.currentTime = 0; // Reset to beginning
114:         audio.volume = options.volume !== undefined ? options.volume : masterVolume;
115: 
116:         // Play the sound
117:         const playPromise = audio.play();
118: 
119:         if (playPromise !== undefined) {
120:             playPromise
121:                 .then(() => {
122:                     // Reduced logging - only log success once per audio element
123:                     if (!audio.hasLoggedSuccess) {
124:                         console.log(`[SOUND] Audio system working for ${eventType}`);
125:                         audio.hasLoggedSuccess = true;
126:                     }
127:                 })
128:                 .catch((error) => {
129:                     // Handle autoplay restrictions gracefully with reduced logging
130:                     if (error.name === 'NotAllowedError') {
131:                         if (!audio.hasLoggedAutoplayBlock) {
132:                             console.log(`[SOUND] Autoplay blocked - user interaction required`);
133:                             audio.hasLoggedAutoplayBlock = true;
134:                         }
135:                     } else {
136:                         console.warn(`[SOUND] Error playing ${eventType}:`, error);
137:                     }
138:                 });
139:         }
140:     } catch (error) {
141:         console.warn(`[SOUND] Exception playing sound for ${eventType}:`, error);
142:     }
143: }
144: 
145: /**
146:  * Enable or disable sound effects
147:  * @param {boolean} enabled - Whether sounds should be enabled
148:  */
149: export function setSoundEnabled(enabled) {
150:     soundEnabled = enabled;
151:     localStorage.setItem('soundEnabled', enabled.toString());
152:     console.log(`[SOUND] Sound ${enabled ? 'enabled' : 'disabled'}`);
153: }
154: 
155: /**
156:  * Get current sound enabled state
157:  * @returns {boolean} Whether sounds are enabled
158:  */
159: export function isSoundEnabled() {
160:     return soundEnabled;
161: }
162: 
163: /**
164:  * Set master volume for all sounds
165:  * @param {number} volume - Volume level (0.0 to 1.0)
166:  */
167: export function setMasterVolume(volume) {
168:     masterVolume = Math.max(0, Math.min(1, volume));
169:     localStorage.setItem('soundVolume', masterVolume.toString());
170:     
171:     // Update volume for all cached audio objects
172:     audioCache.forEach((audio) => {
173:         audio.volume = masterVolume;
174:     });
175:     
176:     console.log(`[SOUND] Master volume set to: ${masterVolume}`);
177: }
178: 
179: /**
180:  * Get current master volume
181:  * @returns {number} Current master volume (0.0 to 1.0)
182:  */
183: export function getMasterVolume() {
184:     return masterVolume;
185: }
186: 
187: /**
188:  * Convenience functions for specific event types
189:  */
190: export const soundEvents = {
191:     workflowStart: () => playSound('WORKFLOW_START'),
192:     checkboxInteraction: () => playSound('CHECKBOX_INTERACTION'),
193:     workflowCompletion: () => playSound('WORKFLOW_COMPLETION'),
194:     errorEvent: () => playSound('ERROR_EVENT'),
195:     autoModeToggle: () => playSound('AUTO_MODE_TOGGLE'),
196:     stepSuccess: () => playSound('STEP_SUCCESS'),
197:     csvDownloadInitiation: () => playSound('CSV_DOWNLOAD_INITIATION'),
198:     csvDownloadCompletion: () => playSound('CSV_DOWNLOAD_COMPLETION')
199: };
```

## File: static/stepDetailsPanel.js
```javascript
  1: import { domBatcher } from './utils/DOMBatcher.js';
  2: import { appState } from './state/AppState.js';
  3: 
  4: let lastFocusedStepElement = null;
  5: 
  6: function getEl(id) {
  7:     const el = document.getElementById(id);
  8:     if (!el) {
  9:         console.warn(`[StepDetails] Missing element: #${id}`);
 10:     }
 11:     return el;
 12: }
 13: 
 14: export function refreshStepDetailsPanelIfOpen(stepKey) {
 15:     const open = !!appState.getStateProperty('ui.stepDetailsOpen');
 16:     const selectedStepKey = appState.getStateProperty('ui.selectedStepKey');
 17:     if (!open) return;
 18:     if (!selectedStepKey) return;
 19:     if (stepKey && stepKey !== selectedStepKey) return;
 20:     updatePanelFromStep(selectedStepKey);
 21: }
 22: 
 23: function getStepKeyFromElement(el) {
 24:     if (!el) return null;
 25:     const stepKey = el.dataset ? el.dataset.stepKey : null;
 26:     return stepKey || null;
 27: }
 28: 
 29: function setStepsExpandedState(selectedStepKey, expanded) {
 30:     const steps = document.querySelectorAll('.timeline-step');
 31:     steps.forEach((stepEl) => {
 32:         const stepKey = getStepKeyFromElement(stepEl);
 33:         const isSelected = !!selectedStepKey && stepKey === selectedStepKey;
 34:         stepEl.setAttribute('aria-expanded', expanded && isSelected ? 'true' : 'false');
 35:         if (isSelected && expanded) {
 36:             stepEl.classList.add('is-selected');
 37:         } else {
 38:             stepEl.classList.remove('is-selected');
 39:         }
 40:     });
 41: }
 42: 
 43: function updatePanelFromStep(stepKey) {
 44:     const panel = getEl('step-details-panel');
 45:     if (!panel) return;
 46: 
 47:     const titleEl = getEl('step-details-title');
 48:     const statusEl = getEl('step-details-status');
 49:     const timerEl = getEl('step-details-timer');
 50:     const progressTextEl = getEl('step-details-progress-text');
 51: 
 52:     const runBtn = getEl('step-details-run');
 53:     const cancelBtn = getEl('step-details-cancel');
 54:     const logsBtn = getEl('step-details-open-logs');
 55: 
 56:     const stepCard = document.getElementById(`step-${stepKey}`);
 57:     const stepName = stepCard ? (stepCard.dataset.stepName || stepKey) : stepKey;
 58: 
 59:     const statusBadge = document.getElementById(`status-${stepKey}`);
 60:     const timerSource = document.getElementById(`timer-${stepKey}`);
 61:     const progressTextSource = document.getElementById(`progress-text-${stepKey}`);
 62: 
 63:     const stepRunButton = document.querySelector(`.run-button[data-step="${stepKey}"]`);
 64:     const stepCancelButton = document.querySelector(`.cancel-button[data-step="${stepKey}"]`);
 65: 
 66:     domBatcher.scheduleUpdate(`step-details-update:${stepKey}`, () => {
 67:         if (titleEl) {
 68:             titleEl.textContent = `Détails — ${stepName}`;
 69:         }
 70: 
 71:         if (statusEl) {
 72:             if (statusBadge) {
 73:                 statusEl.textContent = statusBadge.textContent || 'Prêt';
 74:                 statusEl.className = statusBadge.className || 'status-badge status-idle';
 75:             } else {
 76:                 statusEl.textContent = 'Prêt';
 77:                 statusEl.className = 'status-badge status-idle';
 78:             }
 79:         }
 80: 
 81:         if (timerEl) {
 82:             timerEl.textContent = timerSource ? (timerSource.textContent || '') : '';
 83:         }
 84: 
 85:         if (progressTextEl) {
 86:             progressTextEl.textContent = progressTextSource ? (progressTextSource.textContent || '') : '';
 87:         }
 88: 
 89:         if (runBtn) {
 90:             runBtn.disabled = !stepRunButton || !!stepRunButton.disabled;
 91:         }
 92: 
 93:         if (cancelBtn) {
 94:             cancelBtn.disabled = !stepCancelButton || !!stepCancelButton.disabled;
 95:         }
 96: 
 97:         if (logsBtn) {
 98:             logsBtn.disabled = !stepKey;
 99:         }
100: 
101:         panel.dataset.stepKey = stepKey;
102:     });
103: }
104: 
105: function openPanel(stepKey) {
106:     const wrapper = getEl('workflow-wrapper');
107:     const panel = getEl('step-details-panel');
108:     const closeBtn = getEl('close-step-details');
109: 
110:     if (!wrapper || !panel) return;
111: 
112:     lastFocusedStepElement = document.getElementById(`step-${stepKey}`);
113: 
114:     appState.setState({ ui: { stepDetailsOpen: true, selectedStepKey: stepKey } }, 'step_details_open');
115: 
116:     domBatcher.scheduleUpdate('step-details-open', () => {
117:         panel.hidden = false;
118:         wrapper.classList.add('details-active');
119:         setStepsExpandedState(stepKey, true);
120:     });
121: 
122:     updatePanelFromStep(stepKey);
123: 
124:     requestAnimationFrame(() => {
125:         try {
126:             if (closeBtn && typeof closeBtn.focus === 'function') {
127:                 closeBtn.focus();
128:             }
129:         } catch (_) {}
130:     });
131: }
132: 
133: function closePanel() {
134:     const wrapper = getEl('workflow-wrapper');
135:     const panel = getEl('step-details-panel');
136: 
137:     if (!wrapper || !panel) return;
138: 
139:     const selectedStepKey = appState.getStateProperty('ui.selectedStepKey');
140:     appState.setState({ ui: { stepDetailsOpen: false, selectedStepKey: null } }, 'step_details_close');
141: 
142:     domBatcher.scheduleUpdate('step-details-close', () => {
143:         wrapper.classList.remove('details-active');
144:         panel.hidden = true;
145:         setStepsExpandedState(selectedStepKey, false);
146:         panel.removeAttribute('data-step-key');
147:     });
148: 
149:     requestAnimationFrame(() => {
150:         try {
151:             if (lastFocusedStepElement && typeof lastFocusedStepElement.focus === 'function') {
152:                 lastFocusedStepElement.focus();
153:             }
154:         } catch (_) {}
155:         lastFocusedStepElement = null;
156:     });
157: }
158: 
159: function isLogsPanelOpen() {
160:     const wrapper = getEl('workflow-wrapper');
161:     return !!(wrapper && wrapper.classList.contains('logs-active'));
162: }
163: 
164: function attachStepSelectionListeners() {
165:     const steps = document.querySelectorAll('.timeline-step');
166: 
167:     steps.forEach((stepEl) => {
168:         const handleSelect = () => {
169:             if (isLogsPanelOpen()) return;
170:             const stepKey = getStepKeyFromElement(stepEl);
171:             if (!stepKey) return;
172:             openPanel(stepKey);
173:         };
174: 
175:         stepEl.addEventListener('click', (e) => {
176:             if (e && e.target && e.target.closest) {
177:                 const interactive = e.target.closest('button, a, input, select, textarea');
178:                 if (interactive) return;
179:             }
180:             handleSelect();
181:         });
182: 
183:         stepEl.addEventListener('keydown', (e) => {
184:             if (!e) return;
185: 
186:             if (e.target && e.target.closest) {
187:                 const interactive = e.target.closest('button, a, input, select, textarea');
188:                 if (interactive) return;
189:             }
190: 
191:             if (e.key === 'Enter' || e.key === ' ') {
192:                 e.preventDefault();
193:                 handleSelect();
194:             }
195:         });
196:     });
197: }
198: 
199: function attachPanelListeners() {
200:     const closeBtn = getEl('close-step-details');
201:     const runBtn = getEl('step-details-run');
202:     const cancelBtn = getEl('step-details-cancel');
203:     const logsBtn = getEl('step-details-open-logs');
204:     const panel = getEl('step-details-panel');
205: 
206:     if (closeBtn) {
207:         closeBtn.addEventListener('click', () => {
208:             closePanel();
209:         });
210:     }
211: 
212:     if (runBtn) {
213:         runBtn.addEventListener('click', () => {
214:             const stepKey = panel ? panel.dataset.stepKey : null;
215:             if (!stepKey) return;
216:             const btn = document.querySelector(`.run-button[data-step="${stepKey}"]`);
217:             if (btn && typeof btn.click === 'function') {
218:                 btn.click();
219:             }
220:             updatePanelFromStep(stepKey);
221:         });
222:     }
223: 
224:     if (cancelBtn) {
225:         cancelBtn.addEventListener('click', () => {
226:             const stepKey = panel ? panel.dataset.stepKey : null;
227:             if (!stepKey) return;
228:             const btn = document.querySelector(`.cancel-button[data-step="${stepKey}"]`);
229:             if (btn && typeof btn.click === 'function') {
230:                 btn.click();
231:             }
232:             updatePanelFromStep(stepKey);
233:         });
234:     }
235: 
236:     if (logsBtn) {
237:         logsBtn.addEventListener('click', async () => {
238:             const stepKey = panel ? panel.dataset.stepKey : null;
239:             if (!stepKey) return;
240:             closePanel();
241:             try {
242:                 const mod = await import('./uiUpdater.js');
243:                 if (mod && typeof mod.openLogPanelUI === 'function') {
244:                     mod.openLogPanelUI(stepKey, true);
245:                 }
246:             } catch (e) {
247:                 console.error('[StepDetails] Failed to open logs panel:', e);
248:             }
249:         });
250:     }
251: 
252:     document.addEventListener('keydown', (e) => {
253:         if (!e || e.key !== 'Escape') return;
254:         const open = !!appState.getStateProperty('ui.stepDetailsOpen');
255:         if (!open) return;
256:         closePanel();
257:     });
258: }
259: 
260: function attachLogsPanelObserver() {
261:     const wrapper = getEl('workflow-wrapper');
262:     if (!wrapper || typeof MutationObserver === 'undefined') return;
263: 
264:     const observer = new MutationObserver(() => {
265:         if (wrapper.classList.contains('logs-active')) {
266:             const open = !!appState.getStateProperty('ui.stepDetailsOpen');
267:             if (open) {
268:                 closePanel();
269:             }
270:         }
271:     });
272: 
273:     observer.observe(wrapper, { attributes: true, attributeFilter: ['class'] });
274: }
275: 
276: export function initializeStepDetailsPanel() {
277:     attachStepSelectionListeners();
278:     attachPanelListeners();
279:     attachLogsPanelObserver();
280: }
```

## File: static/themeManager.js
```javascript
  1: /**
  2:  * Theme Manager - Workflow MediaPipe v4.0
  3:  * Handles dynamic theme switching with localStorage persistence
  4:  */
  5: 
  6: const THEME_STORAGE_KEY = 'workflow-theme-preference';
  7: 
  8: const THEMES = {
  9:     'dark-pro': {
 10:         name: 'Dark Pro',
 11:         description: 'Professional dark theme (default)'
 12:     },
 13:     'light-mode': {
 14:         name: 'Light Mode',
 15:         description: 'Clean and bright professional theme'
 16:     },
 17:     'pastel-zen': {
 18:         name: 'Pastel Zen',
 19:         description: 'Soft and calming pastel colors'
 20:     },
 21:     'neon-cyberpunk': {
 22:         name: 'Neon Cyberpunk',
 23:         description: 'Vibrant futuristic theme with glows'
 24:     },
 25:     'forest-night': {
 26:         name: 'Forest Night',
 27:         description: 'Natural earthy tones'
 28:     },
 29:     'ocean-depth': {
 30:         name: 'Ocean Depth',
 31:         description: 'Deep blue oceanic theme'
 32:     }
 33: };
 34: 
 35: class ThemeManager {
 36:     constructor() {
 37:         this.currentTheme = this.loadTheme();
 38:         this.themeSelector = null;
 39:     }
 40: 
 41:     /**
 42:      * Initialize the theme system
 43:      */
 44:     init() {
 45:         console.log('[ThemeManager] Initializing theme system...');
 46:         
 47:         // Apply saved theme immediately
 48:         this.applyTheme(this.currentTheme);
 49:         
 50:         // Setup theme selector if it exists
 51:         this.setupThemeSelector();
 52:         
 53:         console.log(`[ThemeManager] Theme system initialized with: ${this.currentTheme}`);
 54:     }
 55: 
 56:     /**
 57:      * Load theme preference from localStorage
 58:      * @returns {string} Theme identifier
 59:      */
 60:     loadTheme() {
 61:         try {
 62:             const savedTheme = localStorage.getItem(THEME_STORAGE_KEY);
 63:             if (savedTheme && THEMES[savedTheme]) {
 64:                 console.log(`[ThemeManager] Loaded saved theme: ${savedTheme}`);
 65:                 return savedTheme;
 66:             }
 67:         } catch (error) {
 68:             console.error('[ThemeManager] Error loading theme from localStorage:', error);
 69:         }
 70:         
 71:         // Default to dark-pro
 72:         return 'dark-pro';
 73:     }
 74: 
 75:     /**
 76:      * Save theme preference to localStorage
 77:      * @param {string} themeId - Theme identifier
 78:      */
 79:     saveTheme(themeId) {
 80:         try {
 81:             localStorage.setItem(THEME_STORAGE_KEY, themeId);
 82:             console.log(`[ThemeManager] Saved theme preference: ${themeId}`);
 83:         } catch (error) {
 84:             console.error('[ThemeManager] Error saving theme to localStorage:', error);
 85:         }
 86:     }
 87: 
 88:     /**
 89:      * Apply a theme to the document
 90:      * @param {string} themeId - Theme identifier
 91:      */
 92:     applyTheme(themeId) {
 93:         if (!THEMES[themeId]) {
 94:             console.warn(`[ThemeManager] Unknown theme: ${themeId}, falling back to dark-pro`);
 95:             themeId = 'dark-pro';
 96:         }
 97: 
 98:         console.log(`[ThemeManager] Applying theme: ${themeId}`);
 99:         
100:         // Apply theme to document root
101:         document.documentElement.setAttribute('data-theme', themeId);
102:         
103:         // Update current theme
104:         this.currentTheme = themeId;
105:         
106:         // Save preference
107:         this.saveTheme(themeId);
108:         
109:         // Update selector if available
110:         if (this.themeSelector) {
111:             this.themeSelector.value = themeId;
112:         }
113: 
114:         // Dispatch custom event for other components
115:         window.dispatchEvent(new CustomEvent('themeChanged', {
116:             detail: { theme: themeId, themeName: THEMES[themeId].name }
117:         }));
118:         
119:         console.log(`[ThemeManager] Theme applied successfully: ${THEMES[themeId].name}`);
120:     }
121: 
122:     /**
123:      * Setup the theme selector dropdown
124:      */
125:     setupThemeSelector() {
126:         this.themeSelector = document.getElementById('theme-selector');
127:         
128:         if (!this.themeSelector) {
129:             console.warn('[ThemeManager] Theme selector element not found');
130:             return;
131:         }
132: 
133:         // Populate options
134:         this.themeSelector.innerHTML = '';
135:         Object.entries(THEMES).forEach(([id, theme]) => {
136:             const option = document.createElement('option');
137:             option.value = id;
138:             option.textContent = theme.name;
139:             option.title = theme.description;
140:             this.themeSelector.appendChild(option);
141:         });
142: 
143:         // Set current selection
144:         this.themeSelector.value = this.currentTheme;
145: 
146:         // Add change event listener
147:         this.themeSelector.addEventListener('change', (e) => {
148:             const selectedTheme = e.target.value;
149:             console.log(`[ThemeManager] User selected theme: ${selectedTheme}`);
150:             this.applyTheme(selectedTheme);
151:         });
152: 
153:         console.log('[ThemeManager] Theme selector setup complete');
154:     }
155: 
156:     /**
157:      * Get current theme info
158:      * @returns {Object} Current theme information
159:      */
160:     getCurrentTheme() {
161:         return {
162:             id: this.currentTheme,
163:             ...THEMES[this.currentTheme]
164:         };
165:     }
166: 
167:     /**
168:      * Get all available themes
169:      * @returns {Object} All themes
170:      */
171:     getAvailableThemes() {
172:         return THEMES;
173:     }
174: }
175: 
176: // Create singleton instance
177: const themeManager = new ThemeManager();
178: 
179: // Export for module usage
180: export { themeManager, THEMES };
181: 
182: // Also expose globally for non-module scripts
183: window.themeManager = themeManager;
```

## File: templates/reports/analysis_report.html
```html
  1: <!DOCTYPE html>
  2: <html lang="fr">
  3: <head>
  4:     <meta charset="UTF-8">
  5:     <meta name="viewport" content="width=device-width, initial-scale=1.0">
  6:     <title>Rapport d'Analyse - {{ video_name }}</title>
  7:     <style>
  8:         * {
  9:             margin: 0;
 10:             padding: 0;
 11:             box-sizing: border-box;
 12:         }
 13:         
 14:         body {
 15:             font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, sans-serif;
 16:             line-height: 1.6;
 17:             color: #333;
 18:             background: #f5f5f5;
 19:             padding: 20px;
 20:         }
 21:         
 22:         .container {
 23:             max-width: 1200px;
 24:             margin: 0 auto;
 25:             background: white;
 26:             padding: 40px;
 27:             border-radius: 8px;
 28:             box-shadow: 0 2px 10px rgba(0,0,0,0.1);
 29:         }
 30:         
 31:         .header {
 32:             border-bottom: 3px solid #007bff;
 33:             padding-bottom: 20px;
 34:             margin-bottom: 30px;
 35:         }
 36:         
 37:         .header h1 {
 38:             color: #007bff;
 39:             margin-bottom: 10px;
 40:         }
 41:         
 42:         .meta-info {
 43:             color: #666;
 44:             font-size: 14px;
 45:         }
 46:         
 47:         .section {
 48:             margin-bottom: 40px;
 49:         }
 50:         
 51:         .section-title {
 52:             color: #007bff;
 53:             border-left: 4px solid #007bff;
 54:             padding-left: 15px;
 55:             margin-bottom: 20px;
 56:             font-size: 24px;
 57:         }
 58:         
 59:         .stats-grid {
 60:             display: grid;
 61:             grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
 62:             gap: 20px;
 63:             margin-bottom: 20px;
 64:         }
 65:         
 66:         .stat-card {
 67:             background: #f8f9fa;
 68:             padding: 20px;
 69:             border-radius: 8px;
 70:             border-left: 4px solid #007bff;
 71:         }
 72:         
 73:         .stat-label {
 74:             color: #666;
 75:             font-size: 12px;
 76:             text-transform: uppercase;
 77:             letter-spacing: 0.5px;
 78:             margin-bottom: 5px;
 79:         }
 80:         
 81:         .stat-value {
 82:             font-size: 28px;
 83:             font-weight: bold;
 84:             color: #007bff;
 85:         }
 86:         
 87:         .stat-unit {
 88:             font-size: 14px;
 89:             color: #666;
 90:             font-weight: normal;
 91:         }
 92:         
 93:         .badge {
 94:             display: inline-block;
 95:             padding: 4px 12px;
 96:             border-radius: 12px;
 97:             font-size: 12px;
 98:             font-weight: 600;
 99:             margin-right: 10px;
100:         }
101:         
102:         .badge-success {
103:             background: #28a745;
104:             color: white;
105:         }
106:         
107:         .badge-warning {
108:             background: #ffc107;
109:             color: #333;
110:         }
111:         
112:         .badge-info {
113:             background: #17a2b8;
114:             color: white;
115:         }
116:         
117:         .badge-archive {
118:             background: #6c757d;
119:             color: white;
120:         }
121:         
122:         .progress-bar {
123:             width: 100%;
124:             height: 30px;
125:             background: #e9ecef;
126:             border-radius: 15px;
127:             overflow: hidden;
128:             margin-top: 10px;
129:         }
130:         
131:         .progress-fill {
132:             height: 100%;
133:             background: linear-gradient(90deg, #007bff, #0056b3);
134:             display: flex;
135:             align-items: center;
136:             justify-content: center;
137:             color: white;
138:             font-weight: bold;
139:             font-size: 14px;
140:         }
141:         
142:         .footer {
143:             margin-top: 40px;
144:             padding-top: 20px;
145:             border-top: 1px solid #dee2e6;
146:             text-align: center;
147:             color: #666;
148:             font-size: 14px;
149:         }
150:         
151:         @media print {
152:             body {
153:                 background: white;
154:                 padding: 0;
155:             }
156:             .container {
157:                 box-shadow: none;
158:             }
159:         }
160:     </style>
161: </head>
162: <body>
163:     <div class="container">
164:         <div class="header">
165:             <h1>📊 Rapport d'Analyse Vidéo</h1>
166:             <div class="meta-info">
167:                 <strong>Projet:</strong> {{ project_name }} | 
168:                 <strong>Vidéo:</strong> {{ video_name }}<br>
169:                 <strong>Généré le:</strong> {{ generated_at }}
170:                 {% if archive_probe_source.metadata.provenance == 'archives' %}
171:                 <br><span class="badge badge-archive">📦 Données archivées</span>
172:                 {% endif %}
173:             </div>
174:         </div>
175:         
176:         <div class="section">
177:             <h2 class="section-title">📹 Métadonnées Vidéo</h2>
178:             <div class="stats-grid">
179:                 <div class="stat-card">
180:                     <div class="stat-label">Durée</div>
181:                     <div class="stat-value">{{ statistics.video.duration_seconds|format_duration }}</div>
182:                 </div>
183:                 <div class="stat-card">
184:                     <div class="stat-label">Frames Totales</div>
185:                     <div class="stat-value">{{ statistics.video.total_frames|int }}</div>
186:                 </div>
187:                 <div class="stat-card">
188:                     <div class="stat-label">FPS</div>
189:                     <div class="stat-value">{{ statistics.video.fps|round(2) }} <span class="stat-unit">fps</span></div>
190:                 </div>
191:             </div>
192:         </div>
193:         
194:         {% if scenes.available %}
195:         <div class="section">
196:             <h2 class="section-title">✂️ Analyse des Scènes</h2>
197:             <span class="badge badge-success">{{ statistics.scenes.total_count }} scènes détectées</span>
198:             {% if archive_probe_source.scenes.provenance == 'archives' %}
199:             <span class="badge badge-archive">📦 Données archivées</span>
200:             {% endif %}
201:             
202:             <div class="stats-grid" style="margin-top: 20px;">
203:                 <div class="stat-card">
204:                     <div class="stat-label">Durée Moyenne</div>
205:                     <div class="stat-value">{{ statistics.scenes.average_duration|round(1) }} <span class="stat-unit">s</span></div>
206:                 </div>
207:                 <div class="stat-card">
208:                     <div class="stat-label">Scène la Plus Courte</div>
209:                     <div class="stat-value">{{ statistics.scenes.shortest_duration|round(1) }} <span class="stat-unit">s</span></div>
210:                 </div>
211:                 <div class="stat-card">
212:                     <div class="stat-label">Scène la Plus Longue</div>
213:                     <div class="stat-value">{{ statistics.scenes.longest_duration|round(1) }} <span class="stat-unit">s</span></div>
214:                 </div>
215:             </div>
216:         </div>
217:         {% endif %}
218:         
219:         {% if audio.available %}
220:         <div class="section">
221:             <h2 class="section-title">🔊 Analyse Audio</h2>
222:             <span class="badge badge-info">{{ statistics.audio.total_segments }} segments de parole</span>
223:             <span class="badge badge-info">{{ statistics.audio.unique_speakers }} locuteurs</span>
224:             {% if archive_probe_source.audio.provenance == 'archives' %}
225:             <span class="badge badge-archive">📦 Données archivées</span>
226:             {% endif %}
227:             
228:             <div class="stats-grid" style="margin-top: 20px;">
229:                 <div class="stat-card">
230:                     <div class="stat-label">Durée Totale de Parole</div>
231:                     <div class="stat-value">{{ statistics.audio.total_speech_duration|format_duration }}</div>
232:                 </div>
233:                 <div class="stat-card">
234:                     <div class="stat-label">Couverture Audio</div>
235:                     <div class="stat-value">{{ statistics.audio.speech_coverage_percent|format_percentage }}</div>
236:                     <div class="progress-bar">
237:                         <div class="progress-fill" style="width: {{ statistics.audio.speech_coverage_percent }}%">
238:                             {{ statistics.audio.speech_coverage_percent|format_percentage }}
239:                         </div>
240:                     </div>
241:                 </div>
242:             </div>
243:         </div>
244:         {% endif %}
245:         
246:         {% if tracking.available %}
247:         <div class="section">
248:             <h2 class="section-title">👤 Suivi des Visages</h2>
249:             <span class="badge badge-warning">{{ statistics.tracking.max_faces_detected }} visages max détectés</span>
250:             {% if archive_probe_source.tracking.provenance == 'archives' %}
251:             <span class="badge badge-archive">📦 Données archivées</span>
252:             {% endif %}
253:             
254:             <div class="stats-grid" style="margin-top: 20px;">
255:                 <div class="stat-card">
256:                     <div class="stat-label">Frames avec Visages</div>
257:                     <div class="stat-value">{{ statistics.tracking.frames_with_faces|int }}</div>
258:                 </div>
259:                 <div class="stat-card">
260:                     <div class="stat-label">Frames avec Parole Détectée</div>
261:                     <div class="stat-value">{{ statistics.tracking.frames_with_speaking|int }}</div>
262:                 </div>
263:                 <div class="stat-card">
264:                     <div class="stat-label">Couverture Visages</div>
265:                     <div class="stat-value">{{ statistics.tracking.face_coverage_percent|format_percentage }}</div>
266:                     <div class="progress-bar">
267:                         <div class="progress-fill" style="width: {{ statistics.tracking.face_coverage_percent }}%">
268:                             {{ statistics.tracking.face_coverage_percent|format_percentage }}
269:                         </div>
270:                     </div>
271:                 </div>
272:             </div>
273:         </div>
274:         {% endif %}
275:         
276:         <div class="footer">
277:             <p>Rapport généré par MediaPipe Workflow Analysis System</p>
278:             <p>{{ generated_at }}</p>
279:         </div>
280:     </div>
281: </body>
282: </html>
```

## File: templates/reports/monthly_archive_report.html
```html
 1: <!doctype html>
 2: <html lang="fr">
 3:   <meta charset="utf-8" />
 4:   <title>Rapport Mensuel Archives - {{ month }}</title>
 5:   <style>
 6:     body { font-family: Arial, sans-serif; margin: 20px; }
 7:     h1 { margin-bottom: 6px; }
 8:     .meta { color: #666; font-size: 12px; margin-bottom: 12px; }
 9:     table { border-collapse: collapse; width: 100%; margin-top: 10px; }
10:     th, td { border: 1px solid #ddd; padding: 8px; font-size: 13px; }
11:     th { background: #fafafa; text-align: left; }
12:     .section { margin-top: 20px; }
13:     ul { margin: 6px 0 12px 18px; }
14:     li { margin: 2px 0; }
15:     .video-names { margin: 4px 0 0 0; }
16:     .video-name { display: block; padding-left: 4px; }
17:   </style>
18:   </head>
19: <body>
20:   <h1>Rapport Mensuel Archives — {{ month }}</h1>
21:   <div class="meta">Généré le {{ generated_at }} · Build {{ build_id }}</div>
22:   <table>
23:     <thead>
24:       <tr><th>Projet</th><th>Horodatage</th><th>Vidéos</th><th>Scènes</th><th>Parole (s)</th><th>Faces (frames)</th></tr>
25:     </thead>
26:     <tbody>
27:       {% for p in projects %}
28:       <tr>
29:         <td>{{ p.display_base }}</td>
30:         <td>{{ p.archive_timestamp or '-' }}</td>
31:         <td style="text-align:right">{{ p.video_count }}</td>
32:         <td style="text-align:right">{{ p.videos | sum(attribute='statistics.scenes.total_count') }}</td>
33:         <td style="text-align:right">{{ p.videos | sum(attribute='statistics.audio.total_speech_duration') | round(1) }}</td>
34:         <td style="text-align:right">{{ p.videos | sum(attribute='statistics.tracking.frames_with_faces') }}</td>
35:       </tr>
36:       {% endfor %}
37:     </tbody>
38:   </table>
39: 
40:   <div class="section">
41:     <h2>Répartition des Durées par Projet</h2>
42:     {% for p in projects %}
43:       <div>
44:         <strong>{{ p.display_base }}</strong> :
45:         <ul>
46:           <li>
47:             moins de 2 minutes : {{ p.duration_counts.lt_2m }}
48:             {% if p.duration_names and p.duration_names.lt_2m %}
49:               : <div class="video-names">
50:                   {% for nm in p.duration_names.lt_2m %}
51:                     <div class="video-name">{{ nm }}</div>
52:                   {% endfor %}
53:                 </div>
54:             {% endif %}
55:           </li>
56:           <li>
57:             entre 2 et 5 minutes : {{ p.duration_counts.between_2_5m }}
58:             {% if p.duration_names and p.duration_names.between_2_5m %}
59:               : <div class="video-names">
60:                   {% for nm in p.duration_names.between_2_5m %}
61:                     <div class="video-name">{{ nm }}</div>
62:                   {% endfor %}
63:                 </div>
64:             {% endif %}
65:           </li>
66:           <li>
67:             plus de 5 minutes : {{ p.duration_counts.gt_5m }}
68:             {% if p.duration_names and p.duration_names.gt_5m %}
69:               : <div class="video-names">
70:                   {% for nm in p.duration_names.gt_5m %}
71:                     <div class="video-name">{{ nm }}</div>
72:                   {% endfor %}
73:                 </div>
74:             {% endif %}
75:           </li>
76:         </ul>
77:       </div>
78:     {% endfor %}
79:   </div>
80: </body>
81: </html>
```

## File: templates/reports/project_report.html
```html
 1: <!DOCTYPE html>
 2: <html lang="fr">
 3: <head>
 4:     <meta charset="UTF-8">
 5:     <meta name="viewport" content="width=device-width, initial-scale=1.0">
 6:     <title>Rapport de Projet - {{ project_name }}</title>
 7:     <style>
 8:         body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, sans-serif; background:#f5f5f5; color:#333; padding:20px; }
 9:         .container { max-width: 1200px; margin:0 auto; background:#fff; border-radius:8px; box-shadow:0 2px 10px rgba(0,0,0,0.1); padding:40px; }
10:         .header { border-bottom:3px solid #007bff; padding-bottom:16px; margin-bottom:24px; }
11:         .header h1 { color:#007bff; margin:0 0 6px; }
12:         .section { margin:28px 0; }
13:         .section-title { color:#007bff; border-left:4px solid #007bff; padding-left:12px; font-size:22px; margin-bottom:12px; }
14:         .grid { display:grid; grid-template-columns: repeat(auto-fit, minmax(260px,1fr)); gap:16px; }
15:         .card { background:#f8f9fa; border-left:4px solid #007bff; border-radius:8px; padding:16px; }
16:         .badge { display:inline-block; padding:4px 10px; border-radius:12px; font-size:12px; font-weight:600; margin-right:8px; }
17:         .badge-info { background:#17a2b8; color:#fff; }
18:         .table { width:100%; border-collapse:collapse; }
19:         .table th, .table td { border-bottom:1px solid #e9ecef; padding:10px; text-align:left; }
20:         .table th { background:#f1f3f5; }
21:         .muted { color:#666; font-size:13px; }
22:     </style>
23: </head>
24: <body>
25: <div class="container">
26:   <div class="header">
27:     <h1>📁 Rapport Consolidé du Projet</h1>
28:     <div class="muted">Projet: <strong>{{ project_name }}</strong> · Généré le {{ generated_at }}</div>
29:   </div>
30: 
31:   <div class="section">
32:     <h2 class="section-title">📊 Statistiques Globales</h2>
33:     <div class="grid">
34:       <div class="card"><div class="muted">Durée Totale</div><div style="font-size:28px;color:#007bff;">{{ consolidated.video.duration_seconds|format_duration }}</div></div>
35:       <div class="card"><div class="muted">Frames Totales</div><div style="font-size:28px;color:#007bff;">{{ consolidated.video.total_frames|int }}</div></div>
36:       <div class="card"><div class="muted">FPS Moyen</div><div style="font-size:28px;color:#007bff;">{{ consolidated.video.fps|round(2) }}</div></div>
37:       <div class="card"><div class="muted">Scènes Totales</div><div style="font-size:28px;color:#007bff;">{{ consolidated.scenes.total_count|int }}</div></div>
38:       <div class="card"><div class="muted">Segments de Parole</div><div style="font-size:28px;color:#007bff;">{{ consolidated.audio.total_segments|int }}</div></div>
39:       <div class="card"><div class="muted">Locuteurs Uniques</div><div style="font-size:28px;color:#007bff;">{{ consolidated.audio.unique_speakers|int }}</div></div>
40:       <div class="card"><div class="muted">Couverture Parole</div><div style="font-size:28px;color:#007bff;">{{ consolidated.audio.speech_coverage_percent|format_percentage }}</div></div>
41:       <div class="card"><div class="muted">Frames avec Visages</div><div style="font-size:28px;color:#007bff;">{{ consolidated.tracking.frames_with_faces|int }}</div></div>
42:     </div>
43:   </div>
44: 
45:   <div class="section">
46:     <h2 class="section-title">🎬 Détails par Vidéo</h2>
47:     <table class="table">
48:       <thead>
49:         <tr>
50:           <th>Vidéo</th>
51:           <th>Durée</th>
52:           <th>Scènes</th>
53:           <th>Locuteurs</th>
54:           <th>Segments</th>
55:           <th>Faces (max)</th>
56:           <th>Couverture Visages</th>
57:           <th>Statut</th>
58:         </tr>
59:       </thead>
60:       <tbody>
61:         {% for v in videos %}
62:         <tr>
63:           <td>{{ v.video_name }}</td>
64:           {% if v.error %}
65:             <td colspan="6" class="muted">—</td>
66:             <td><span class="badge badge-info">Erreur</span> {{ v.error }}</td>
67:           {% else %}
68:             <td>{{ v.statistics.video.duration_seconds|format_duration }}</td>
69:             <td>{{ v.statistics.scenes.total_count|int }}</td>
70:             <td>{{ v.metadata.get('speaker_count', 0) or v.statistics.audio.unique_speakers|int }}</td>
71:             <td>{{ v.statistics.audio.total_segments|int }}</td>
72:             <td>{{ v.statistics.tracking.max_faces_detected|int }}</td>
73:             <td>{{ v.statistics.tracking.face_coverage_percent|format_percentage }}</td>
74:             <td><span class="badge badge-info">OK</span></td>
75:           {% endif %}
76:         </tr>
77:         {% endfor %}
78:       </tbody>
79:     </table>
80:   </div>
81: 
82:   <div class="section" style="text-align:center;color:#666;font-size:13px;">
83:     Rapport généré par MediaPipe Workflow Analysis System · {{ generated_at }}
84:   </div>
85: </div>
86: </body>
87: </html>
```

## File: utils/enhanced_speaking_detection.py
```python
  1: #!/usr/bin/env python3
  2: # -*- coding: utf-8 -*-
  3: """
  4: Enhanced Speaking Detection System
  5: Implements multi-source validation for speaking detection using:
  6: 1. Face blendshapes (jaw movement)
  7: 2. Audio diarization data
  8: 3. CSV segment analysis
  9: 4. Temporal consistency validation
 10: """
 11: 
 12: import json
 13: import csv
 14: import logging
 15: from pathlib import Path
 16: from typing import Dict, List, Optional, Tuple, Any
 17: from dataclasses import dataclass
 18: 
 19: logger = logging.getLogger(__name__)
 20: 
 21: @dataclass
 22: class SegmentInfo:
 23:     """Information about a video segment from CSV data."""
 24:     segment_id: int
 25:     frame_start: int
 26:     frame_end: int
 27:     timecode_start: str
 28:     timecode_end: str
 29: 
 30: @dataclass
 31: class AudioInfo:
 32:     """Audio analysis information for a frame."""
 33:     is_speech_present: bool
 34:     num_speakers: int
 35:     active_speakers: List[str]
 36:     timecode_sec: float
 37: 
 38: @dataclass
 39: class SpeakingDetectionResult:
 40:     """Result of enhanced speaking detection."""
 41:     is_speaking: bool
 42:     confidence: float
 43:     sources: Dict[str, Any]  # Evidence from different sources
 44:     method: str  # Primary detection method used
 45: 
 46: class EnhancedSpeakingDetector:
 47:     """Enhanced speaking detection using multiple data sources."""
 48:     
 49:     def __init__(self, video_path: str, jaw_threshold: float = 0.08):
 50:         """
 51:         Initialize the enhanced speaking detector.
 52:         
 53:         Args:
 54:             video_path: Path to the video file
 55:             jaw_threshold: Threshold for jaw movement detection
 56:         """
 57:         self.video_path = Path(video_path)
 58:         self.jaw_threshold = jaw_threshold
 59:         
 60:         # Load auxiliary data
 61:         self.segments = self._load_csv_segments()
 62:         self.audio_data = self._load_audio_analysis()
 63:         
 64:         # Detection parameters
 65:         self.audio_weight = 0.6  # Weight for audio evidence
 66:         self.visual_weight = 0.4  # Weight for visual evidence
 67:         self.min_confidence_threshold = 0.3
 68:         
 69:         logger.info(f"Enhanced speaking detector initialized for {self.video_path.name}")
 70:         logger.info(f"Loaded {len(self.segments)} segments, audio data: {self.audio_data is not None}")
 71:     
 72:     def _load_csv_segments(self) -> List[SegmentInfo]:
 73:         """Load CSV segmentation data."""
 74:         csv_path = self.video_path.with_suffix('.csv')
 75:         segments = []
 76: 
 77:         if not csv_path.exists():
 78:             logger.warning(f"CSV file not found: {csv_path}")
 79:             return segments
 80: 
 81:         try:
 82:             zero_duration_count = 0
 83:             invalid_duration_count = 0
 84:             with open(csv_path, 'r', encoding='utf-8') as f:
 85:                 reader = csv.DictReader(f)
 86:                 for row in reader:
 87:                     frame_start = int(row['Frame In'])
 88:                     frame_end = int(row['Frame Out'])
 89:                     duration = frame_end - frame_start
 90: 
 91:                     # Check for problematic segments
 92:                     if duration == 0:
 93:                         zero_duration_count += 1
 94:                         logger.warning(f"Zero-duration segment detected: ID {row['No']}, frame {frame_start}")
 95:                     elif duration < 0:
 96:                         invalid_duration_count += 1
 97:                         logger.warning(f"Invalid segment with negative duration: ID {row['No']}, frames {frame_start}-{frame_end}")
 98: 
 99:                     segment = SegmentInfo(
100:                         segment_id=int(row['No']),
101:                         frame_start=frame_start,
102:                         frame_end=frame_end,
103:                         timecode_start=row['Timecode In'],
104:                         timecode_end=row['Timecode Out']
105:                     )
106:                     segments.append(segment)
107: 
108:             logger.info(f"Loaded {len(segments)} segments from {csv_path.name}")
109:             if zero_duration_count > 0:
110:                 logger.warning(f"Found {zero_duration_count} zero-duration segments that will be handled safely")
111:             if invalid_duration_count > 0:
112:                 logger.warning(f"Found {invalid_duration_count} segments with invalid negative durations that will be handled safely")
113: 
114:         except Exception as e:
115:             logger.error(f"Error loading CSV segments: {e}")
116: 
117:         return segments
118:     
119:     def _load_audio_analysis(self) -> Optional[Dict[int, AudioInfo]]:
120:         """Load audio analysis data."""
121:         # Construct audio JSON path: video_name_audio.json
122:         video_stem = self.video_path.stem  # e.g., "AMERICAINES"
123:         audio_json_path = self.video_path.parent / f"{video_stem}_audio.json"
124:         
125:         if not audio_json_path.exists():
126:             logger.warning(f"Audio analysis file not found: {audio_json_path}")
127:             return None
128:         
129:         try:
130:             with open(audio_json_path, 'r', encoding='utf-8') as f:
131:                 data = json.load(f)
132: 
133:             audio_data = {}
134:             warned_schema_once = False
135:             for frame_data in data.get('frames_analysis', []):
136:                 try:
137:                     frame_num = frame_data.get('frame')
138:                     audio_info = frame_data.get('audio_info', {}) or {}
139: 
140:                     # Fallbacks for schema differences
141:                     is_speech_present = bool(audio_info.get('is_speech_present', False))
142:                     if 'num_distinct_speakers_audio' in audio_info:
143:                         num_speakers = int(audio_info.get('num_distinct_speakers_audio', 0) or 0)
144:                     elif 'num_speakers' in audio_info:
145:                         num_speakers = int(audio_info.get('num_speakers', 0) or 0)
146:                     else:
147:                         # derive from list length if available
148:                         num_speakers = len(audio_info.get('active_speaker_labels', []) or [])
149:                         if not warned_schema_once:
150:                             logger.debug("Audio schema using simplified format (active_speaker_labels only); deriving num_speakers from list length")
151:                             warned_schema_once = True
152: 
153:                     active_speakers = list(audio_info.get('active_speaker_labels', []) or [])
154:                     timecode_sec = float(audio_info.get('timecode_sec', 0.0) or 0.0)
155: 
156:                     if frame_num is None:
157:                         continue
158: 
159:                     audio_data[int(frame_num)] = AudioInfo(
160:                         is_speech_present=is_speech_present,
161:                         num_speakers=num_speakers,
162:                         active_speakers=active_speakers,
163:                         timecode_sec=timecode_sec
164:                     )
165:                 except Exception as e:
166:                     logger.warning(f"Skipping malformed audio frame entry: {e}")
167: 
168:             logger.info(f"Loaded audio data for {len(audio_data)} frames")
169:             return audio_data
170:             
171:         except Exception as e:
172:             logger.error(f"Error loading audio analysis: {e}")
173:             return None
174:     
175:     def get_segment_for_frame(self, frame_num: int) -> Optional[SegmentInfo]:
176:         """Get the segment that contains the given frame."""
177:         for segment in self.segments:
178:             if segment.frame_start <= frame_num <= segment.frame_end:
179:                 return segment
180:         return None
181:     
182:     def detect_speaking(self, frame_num: int, blendshapes: Optional[Dict[str, float]] = None,
183:                        source_detector: str = "face_landmarker") -> SpeakingDetectionResult:
184:         """
185:         Enhanced speaking detection using multiple sources.
186:         
187:         Args:
188:             frame_num: Frame number (1-based)
189:             blendshapes: Face blendshapes data (if available)
190:             source_detector: Source of detection ("face_landmarker" or "object_detector")
191:             
192:         Returns:
193:             SpeakingDetectionResult with confidence and evidence
194:         """
195:         sources = {}
196:         confidence_scores = []
197:         
198:         # 1. Audio-based detection (primary source)
199:         audio_confidence = 0.0
200:         if self.audio_data and frame_num in self.audio_data:
201:             audio_info = self.audio_data[frame_num]
202:             if audio_info.is_speech_present:
203:                 # Higher confidence with more speakers
204:                 audio_confidence = min(0.8 + (audio_info.num_speakers * 0.1), 1.0)
205:             sources['audio'] = {
206:                 'is_speech_present': audio_info.is_speech_present,
207:                 'num_speakers': audio_info.num_speakers,
208:                 'active_speakers': audio_info.active_speakers,
209:                 'confidence': audio_confidence
210:             }
211:             confidence_scores.append(audio_confidence * self.audio_weight)
212:         
213:         # 2. Visual-based detection (blendshapes)
214:         visual_confidence = 0.0
215:         if source_detector == "face_landmarker" and blendshapes and "jawOpen" in blendshapes:
216:             jaw_open = blendshapes["jawOpen"]
217:             
218:             # Enhanced jaw movement analysis
219:             if jaw_open > self.jaw_threshold:
220:                 # Scale confidence based on jaw opening amount
221:                 visual_confidence = min(jaw_open / self.jaw_threshold * 0.7, 1.0)
222:                 
223:                 # Additional blendshape analysis for better accuracy
224:                 mouth_indicators = [
225:                     blendshapes.get("mouthOpen", 0.0),
226:                     blendshapes.get("mouthShrugUpper", 0.0),
227:                     blendshapes.get("mouthShrugLower", 0.0)
228:                 ]
229:                 mouth_activity = sum(mouth_indicators) / len(mouth_indicators)
230:                 visual_confidence = min(visual_confidence + mouth_activity * 0.2, 1.0)
231:             
232:             sources['visual'] = {
233:                 'jaw_open': jaw_open,
234:                 'jaw_threshold': self.jaw_threshold,
235:                 'mouth_activity': mouth_activity if 'mouth_activity' in locals() else 0.0,
236:                 'confidence': visual_confidence
237:             }
238:             confidence_scores.append(visual_confidence * self.visual_weight)
239:         
240:         # 3. Segment-based context
241:         segment = self.get_segment_for_frame(frame_num)
242:         if segment:
243:             # Calculate frame position safely, handling zero-duration segments
244:             segment_duration = segment.frame_end - segment.frame_start
245:             if segment_duration > 0:
246:                 frame_position = (frame_num - segment.frame_start) / segment_duration
247:             elif segment_duration == 0:
248:                 # Zero-duration segment: frame is at the single point
249:                 frame_position = 0.0
250:                 logger.debug(f"Zero-duration segment {segment.segment_id} at frame {frame_num}, using position 0.0")
251:             else:
252:                 # Negative duration (invalid segment): treat as zero-duration
253:                 frame_position = 0.0
254:                 logger.warning(f"Invalid segment {segment.segment_id} with negative duration {segment_duration}, using position 0.0")
255: 
256:             sources['segment'] = {
257:                 'segment_id': segment.segment_id,
258:                 'frame_position': frame_position,
259:                 'segment_duration': segment_duration,
260:                 'is_zero_duration': segment_duration == 0
261:             }
262:         
263:         # 4. Calculate final result
264:         if confidence_scores:
265:             final_confidence = sum(confidence_scores)
266:         else:
267:             final_confidence = 0.0
268:         
269:         # Determine speaking status and method
270:         is_speaking = final_confidence > self.min_confidence_threshold
271:         
272:         # Determine primary method
273:         if audio_confidence > visual_confidence:
274:             method = "audio_primary"
275:         elif visual_confidence > 0:
276:             method = "visual_primary"
277:         elif source_detector == "object_detector":
278:             method = "object_detection_fallback"
279:         else:
280:             method = "no_detection"
281:         
282:         return SpeakingDetectionResult(
283:             is_speaking=is_speaking,
284:             confidence=final_confidence,
285:             sources=sources,
286:             method=method
287:         )
288:     
289:     def get_detection_stats(self) -> Dict[str, Any]:
290:         """Get statistics about available detection sources."""
291:         return {
292:             'segments_available': len(self.segments) > 0,
293:             'audio_data_available': self.audio_data is not None,
294:             'total_segments': len(self.segments),
295:             'total_audio_frames': len(self.audio_data) if self.audio_data else 0,
296:             'jaw_threshold': self.jaw_threshold,
297:             'audio_weight': self.audio_weight,
298:             'visual_weight': self.visual_weight
299:         }
```

## File: utils/filename_security.py
```python
  1: #!/usr/bin/env python3
  2: # -*- coding: utf-8 -*-
  3: 
  4: """
  5: Comprehensive filename sanitization and security utilities for archive extraction.
  6: Provides secure filename handling to prevent filesystem attacks and ensure compatibility.
  7: """
  8: 
  9: import os
 10: import re
 11: import unicodedata
 12: import logging
 13: from pathlib import Path
 14: from typing import Tuple, Set, Optional, List
 15: from dataclasses import dataclass
 16: 
 17: logger = logging.getLogger(__name__)
 18: 
 19: WINDOWS_FORBIDDEN_CHARS = set('<>:"|?*')
 20: UNIX_FORBIDDEN_CHARS = set('\x00')  # Null byte
 21: CONTROL_CHARS = set(chr(i) for i in range(32))  # ASCII control characters
 22: WINDOWS_RESERVED_NAMES = {
 23:     'CON', 'PRN', 'AUX', 'NUL',
 24:     'COM1', 'COM2', 'COM3', 'COM4', 'COM5', 'COM6', 'COM7', 'COM8', 'COM9',
 25:     'LPT1', 'LPT2', 'LPT3', 'LPT4', 'LPT5', 'LPT6', 'LPT7', 'LPT8', 'LPT9'
 26: }
 27: 
 28: MAX_FILENAME_LENGTH = 255
 29: MAX_PATH_LENGTH = 4096
 30: MAX_COMPONENT_LENGTH = 255
 31: 
 32: DANGEROUS_PATTERNS = [
 33:     r'\.\.[\\/]',  # Path traversal
 34:     r'^[\\/]',     # Absolute paths
 35:     r'[\\/]\.\.[\\/]',  # Path traversal in middle
 36:     r'[\\/]\.\.$',      # Path traversal at end
 37: ]
 38: 
 39: 
 40: @dataclass
 41: class SanitizationResult:
 42:     """Result of filename sanitization operation."""
 43:     original_name: str
 44:     sanitized_name: str
 45:     was_modified: bool
 46:     security_issues: List[str]
 47:     warnings: List[str]
 48: 
 49: 
 50: class FilenameSanitizer:
 51:     """
 52:     Comprehensive filename sanitizer for secure archive extraction.
 53:     
 54:     Handles:
 55:     - Path traversal prevention
 56:     - Platform-specific forbidden characters
 57:     - Unicode normalization
 58:     - Length limits
 59:     - Reserved names
 60:     - Control character filtering
 61:     """
 62:     
 63:     def __init__(self, max_filename_length: int = MAX_FILENAME_LENGTH):
 64:         """
 65:         Initialize the sanitizer.
 66:         
 67:         Args:
 68:             max_filename_length: Maximum allowed filename length
 69:         """
 70:         self.max_filename_length = max_filename_length
 71:         self.sanitization_stats = {
 72:             'total_processed': 0,
 73:             'modified_count': 0,
 74:             'security_issues_found': 0
 75:         }
 76:     
 77:     def sanitize_archive_member_path(self, member_path: str) -> SanitizationResult:
 78:         """
 79:         Sanitize a complete path from an archive member.
 80:         
 81:         Args:
 82:             member_path: Original path from archive
 83:             
 84:         Returns:
 85:             SanitizationResult with sanitized path and security info
 86:         """
 87:         self.sanitization_stats['total_processed'] += 1
 88:         original_path = member_path
 89:         security_issues = []
 90:         warnings = []
 91:         
 92:         for pattern in DANGEROUS_PATTERNS:
 93:             if re.search(pattern, member_path):
 94:                 security_issues.append(f"Path traversal pattern detected: {pattern}")
 95:         
 96:         member_path = unicodedata.normalize('NFKC', member_path)
 97:         
 98:         path_parts = []
 99:         for part in member_path.split('/'):
100:             if part in ('', '.', '..'):
101:                 security_issues.append(f"Dangerous path component: '{part}'")
102:                 continue
103:             
104:             sanitized_part = self._sanitize_filename_component(part)
105:             if sanitized_part.was_modified:
106:                 warnings.extend(sanitized_part.warnings)
107:                 security_issues.extend(sanitized_part.security_issues)
108:             
109:             if sanitized_part.sanitized_name:  # Only add non-empty parts
110:                 path_parts.append(sanitized_part.sanitized_name)
111:         
112:         sanitized_path = '/'.join(path_parts) if path_parts else 'sanitized_file'
113:         
114:         if len(sanitized_path) > MAX_PATH_LENGTH:
115:             warnings.append(f"Path length {len(sanitized_path)} exceeds limit {MAX_PATH_LENGTH}")
116:             sanitized_path = self._truncate_path(sanitized_path)
117:         
118:         was_modified = original_path != sanitized_path
119:         if was_modified:
120:             self.sanitization_stats['modified_count'] += 1
121:         
122:         if security_issues:
123:             self.sanitization_stats['security_issues_found'] += 1
124:             logger.warning(f"Security issues in path '{original_path}': {security_issues}")
125:         
126:         return SanitizationResult(
127:             original_name=original_path,
128:             sanitized_name=sanitized_path,
129:             was_modified=was_modified,
130:             security_issues=security_issues,
131:             warnings=warnings
132:         )
133:     
134:     def _sanitize_filename_component(self, filename: str) -> SanitizationResult:
135:         """
136:         Sanitize a single filename component.
137:         
138:         Args:
139:             filename: Original filename
140:             
141:         Returns:
142:             SanitizationResult with sanitized filename and security info
143:         """
144:         original_filename = filename
145:         security_issues = []
146:         warnings = []
147:         
148:         for char in CONTROL_CHARS | UNIX_FORBIDDEN_CHARS:
149:             if char in filename:
150:                 security_issues.append(f"Control/null character detected: {repr(char)}")
151:                 filename = filename.replace(char, '')
152:         
153:         for char in WINDOWS_FORBIDDEN_CHARS:
154:             if char in filename:
155:                 warnings.append(f"Windows forbidden character replaced: '{char}'")
156:                 filename = filename.replace(char, '_')
157:         
158:         original_stripped = filename
159:         filename = filename.strip('. ')
160:         if filename != original_stripped:
161:             warnings.append("Removed leading/trailing dots or spaces")
162:         
163:         name_without_ext = Path(filename).stem.upper()
164:         if name_without_ext in WINDOWS_RESERVED_NAMES:
165:             security_issues.append(f"Windows reserved name detected: {name_without_ext}")
166:             filename = f"safe_{filename}"
167:         
168:         if len(filename) > self.max_filename_length:
169:             warnings.append(f"Filename length {len(filename)} exceeds limit {self.max_filename_length}")
170:             filename = self._truncate_filename(filename)
171:         
172:         if not filename or filename in ('.', '..'):
173:             warnings.append("Empty or invalid filename, using fallback")
174:             filename = 'sanitized_file'
175:         
176:         was_modified = original_filename != filename
177:         
178:         return SanitizationResult(
179:             original_name=original_filename,
180:             sanitized_name=filename,
181:             was_modified=was_modified,
182:             security_issues=security_issues,
183:             warnings=warnings
184:         )
185:     
186:     def _truncate_filename(self, filename: str) -> str:
187:         """
188:         Truncate filename while preserving extension.
189:         
190:         Args:
191:             filename: Filename to truncate
192:             
193:         Returns:
194:             Truncated filename
195:         """
196:         if len(filename) <= self.max_filename_length:
197:             return filename
198:         
199:         path_obj = Path(filename)
200:         extension = path_obj.suffix
201:         name_part = path_obj.stem
202:         
203:         max_name_length = self.max_filename_length - len(extension)
204:         
205:         if max_name_length <= 0:
206:             return filename[:self.max_filename_length]
207:         
208:         truncated_name = name_part[:max_name_length]
209:         return truncated_name + extension
210:     
211:     def _truncate_path(self, path: str) -> str:
212:         """
213:         Truncate path while preserving structure.
214:         
215:         Args:
216:             path: Path to truncate
217:             
218:         Returns:
219:             Truncated path
220:         """
221:         if len(path) <= MAX_PATH_LENGTH:
222:             return path
223:         
224:         excess = len(path) - MAX_PATH_LENGTH + 10  # +10 for "..." marker
225:         middle = len(path) // 2
226:         start = middle - excess // 2
227:         end = middle + excess // 2
228:         
229:         return path[:start] + "..." + path[end:]
230:     
231:     def get_stats(self) -> dict:
232:         """Get sanitization statistics."""
233:         return self.sanitization_stats.copy()
234:     
235:     def reset_stats(self):
236:         """Reset sanitization statistics."""
237:         self.sanitization_stats = {
238:             'total_processed': 0,
239:             'modified_count': 0,
240:             'security_issues_found': 0
241:         }
242: 
243: 
244: def validate_extraction_path(member_path: str, base_extraction_dir: Path) -> bool:
245:     """
246:     Validate that an extraction path is safe relative to base directory.
247:     
248:     Args:
249:         member_path: Path from archive member
250:         base_extraction_dir: Base directory for extraction
251:         
252:     Returns:
253:         True if path is safe, False otherwise
254:     """
255:     try:
256:         full_path = (base_extraction_dir / member_path).resolve()
257:         base_resolved = base_extraction_dir.resolve()
258:         
259:         try:
260:             full_path.relative_to(base_resolved)
261:             return True
262:         except ValueError:
263:             logger.warning(f"Path outside base directory: {member_path}")
264:             return False
265:             
266:     except Exception as e:
267:         logger.error(f"Error validating extraction path '{member_path}': {e}")
268:         return False
269: 
270: 
271: def sanitize_filename(filename: str, max_length: int = MAX_FILENAME_LENGTH) -> str:
272:     """
273:     Simple filename sanitization function.
274:     
275:     Args:
276:         filename: Original filename
277:         max_length: Maximum allowed length
278:         
279:     Returns:
280:         Sanitized filename
281:     """
282:     sanitizer = FilenameSanitizer(max_length)
283:     result = sanitizer._sanitize_filename_component(filename)
284:     return result.sanitized_name
```

## File: utils/mediapipe_asset_helper.py
```python
  1: import os
  2: import shutil
  3: import time
  4: import uuid
  5: import mediapipe as mp
  6: import sys
  7: 
  8: def _get_mediapipe_assets_dir():
  9:     """
 10:     Finds the 'assets' directory within the installed MediaPipe package.
 11:     This is the most reliable way to locate it.
 12:     """
 13:     try:
 14:         # The __file__ attribute of the mediapipe package gives us the path to its __init__.py
 15:         mp_package_path = os.path.dirname(mp.__file__)
 16:         assets_path = os.path.join(mp_package_path, 'tasks', 'python', 'vision', 'pybind', 'assets')
 17:         
 18:         # A fallback path seen in some installations
 19:         if not os.path.exists(assets_path):
 20:             assets_path = os.path.join(mp_package_path, 'tasks', 'python', 'assets')
 21:             
 22:         if os.path.exists(assets_path):
 23:             return assets_path
 24:         else:
 25:             # If all else fails, create a local assets dir as a last resort
 26:             local_assets = os.path.join(os.getcwd(), 'mediapipe_assets')
 27:             os.makedirs(local_assets, exist_ok=True)
 28:             print(f"WARNING: MediaPipe assets directory not found. Using local fallback: {local_assets}", file=sys.stderr)
 29:             return local_assets
 30:             
 31:     except Exception as e:
 32:         print(f"ERROR: Could not determine MediaPipe assets path: {e}", file=sys.stderr)
 33:         return None
 34: 
 35: def create_mediapipe_asset_copy(original_model_path):
 36:     """
 37:     Copies a model file to the MediaPipe assets directory to ensure it can be loaded.
 38:     MediaPipe tasks often require models to be in this specific 'assets' folder.
 39:     
 40:     Args:
 41:         original_model_path (str): The absolute path to the model file.
 42: 
 43:     Returns:
 44:         dict: A dictionary with path information, or None on failure.
 45:               {'asset_path': '...', 'asset_name': '...'}
 46:     """
 47:     assets_dir = _get_mediapipe_assets_dir()
 48:     if not assets_dir:
 49:         print("ERROR: Cannot create model copy because MediaPipe assets directory was not found.", file=sys.stderr)
 50:         return None
 51: 
 52:     if not os.path.exists(original_model_path):
 53:         print(f"ERROR: Original model file does not exist: {original_model_path}", file=sys.stderr)
 54:         return None
 55:         
 56:     try:
 57:         original_filename = os.path.basename(original_model_path)
 58:         # Create a unique name to avoid conflicts if multiple processes run
 59:         unique_suffix = uuid.uuid4().hex[:8]
 60:         temp_filename = f"temp_{unique_suffix}_{original_filename}"
 61:         
 62:         destination_path = os.path.join(assets_dir, temp_filename)
 63:         
 64:         # Copy the file
 65:         shutil.copy2(original_model_path, destination_path)
 66:         
 67:         print(f"INFO: Model '{original_filename}' copied to MediaPipe assets as '{temp_filename}' for processing.", file=sys.stderr)
 68:         
 69:         return {
 70:             "asset_path": os.path.abspath(destination_path),
 71:             "asset_name": temp_filename
 72:         }
 73:     except Exception as e:
 74:         print(f"ERROR: Failed to copy model to MediaPipe assets directory: {e}", file=sys.stderr)
 75:         return None
 76: 
 77: def cleanup_mediapipe_temp_assets(max_age_hours=1):
 78:     """
 79:     Deletes temporary model files from the MediaPipe assets directory that are older
 80:     than a specified age. This prevents the folder from filling up with old temp files.
 81: 
 82:     Args:
 83:         max_age_hours (int): The maximum age of a file in hours to be kept.
 84:     
 85:     Returns:
 86:         tuple: (number_of_files_deleted, total_space_freed_in_bytes)
 87:     """
 88:     assets_dir = _get_mediapipe_assets_dir()
 89:     if not assets_dir:
 90:         print("WARNING: Cannot clean up assets, directory not found.", file=sys.stderr)
 91:         return 0, 0
 92:         
 93:     now = time.time()
 94:     max_age_seconds = max_age_hours * 3600
 95:     deleted_count = 0
 96:     freed_space = 0
 97:     
 98:     try:
 99:         for filename in os.listdir(assets_dir):
100:             if filename.startswith("temp_"):
101:                 file_path = os.path.join(assets_dir, filename)
102:                 try:
103:                     if os.path.isfile(file_path):
104:                         file_age = now - os.path.getmtime(file_path)
105:                         if file_age > max_age_seconds:
106:                             file_size = os.path.getsize(file_path)
107:                             os.remove(file_path)
108:                             deleted_count += 1
109:                             freed_space += file_size
110:                             print(f"CLEANUP: Removed old temp asset: {filename}", file=sys.stderr)
111:                 except Exception as e_inner:
112:                     # This can happen if another process deletes the file in the meantime
113:                     print(f"WARNING: Could not process asset '{filename}' during cleanup: {e_inner}", file=sys.stderr)
114: 
115:     except Exception as e:
116:         print(f"ERROR: An error occurred during asset cleanup: {e}", file=sys.stderr)
117:         
118:     return deleted_count, freed_space
```

## File: utils/performance.py
```python
  1: """
  2: Performance Utilities
  3: Comprehensive performance optimization utilities for backend operations.
  4: """
  5: 
  6: import time
  7: import logging
  8: import threading
  9: from functools import lru_cache, wraps
 10: from typing import Dict, Any, Optional, Callable, List
 11: from contextlib import contextmanager
 12: from concurrent.futures import ThreadPoolExecutor, as_completed
 13: from pathlib import Path
 14: 
 15: logger = logging.getLogger(__name__)
 16: 
 17: # Performance tracking
 18: performance_stats = {
 19:     "function_calls": {},
 20:     "cache_hits": 0,
 21:     "cache_misses": 0,
 22:     "parallel_operations": 0
 23: }
 24: stats_lock = threading.Lock()
 25: 
 26: 
 27: def track_performance(func_name: str = None):
 28:     """
 29:     Decorator to track function performance.
 30:     
 31:     Args:
 32:         func_name: Optional custom name for the function
 33:         
 34:     Returns:
 35:         Decorator function
 36:     """
 37:     def decorator(func: Callable) -> Callable:
 38:         name = func_name or f"{func.__module__}.{func.__name__}"
 39:         
 40:         @wraps(func)
 41:         def wrapper(*args, **kwargs):
 42:             start_time = time.perf_counter()
 43:             
 44:             try:
 45:                 result = func(*args, **kwargs)
 46:                 success = True
 47:                 error = None
 48:             except Exception as e:
 49:                 success = False
 50:                 error = str(e)
 51:                 raise
 52:             finally:
 53:                 duration = (time.perf_counter() - start_time) * 1000  # Convert to ms
 54:                 
 55:                 with stats_lock:
 56:                     if name not in performance_stats["function_calls"]:
 57:                         performance_stats["function_calls"][name] = {
 58:                             "total_time": 0,
 59:                             "calls": 0,
 60:                             "errors": 0,
 61:                             "avg_time": 0
 62:                         }
 63:                     
 64:                     stats = performance_stats["function_calls"][name]
 65:                     stats["total_time"] += duration
 66:                     stats["calls"] += 1
 67:                     if not success:
 68:                         stats["errors"] += 1
 69:                     stats["avg_time"] = stats["total_time"] / stats["calls"]
 70:                 
 71:                 # Log slow operations
 72:                 if duration > 1000:  # More than 1 second
 73:                     logger.warning(f"Slow operation: {name} took {duration:.2f}ms")
 74:             
 75:             return result
 76:         
 77:         return wrapper
 78:     return decorator
 79: 
 80: 
 81: @contextmanager
 82: def profile_section(section_name: str):
 83:     """
 84:     Context manager for profiling code sections.
 85:     
 86:     Args:
 87:         section_name: Name of the section being profiled
 88:         
 89:     Usage:
 90:         with profile_section("video_processing"):
 91:             # Code to profile
 92:             process_video()
 93:     """
 94:     start_time = time.perf_counter()
 95:     try:
 96:         yield
 97:     finally:
 98:         elapsed_time = (time.perf_counter() - start_time) * 1000  # Convert to ms
 99:         
100:         with stats_lock:
101:             if section_name not in performance_stats["function_calls"]:
102:                 performance_stats["function_calls"][section_name] = {
103:                     "total_time": 0,
104:                     "calls": 0,
105:                     "errors": 0,
106:                     "avg_time": 0
107:                 }
108:             
109:             stats = performance_stats["function_calls"][section_name]
110:             stats["total_time"] += elapsed_time
111:             stats["calls"] += 1
112:             stats["avg_time"] = stats["total_time"] / stats["calls"]
113: 
114: 
115: @lru_cache(maxsize=128)
116: def get_video_metadata_cached(video_path: str) -> Dict[str, Any]:
117:     """
118:     Get video metadata with caching to avoid repeated file reads.
119:     
120:     Args:
121:         video_path: Path to video file
122:         
123:     Returns:
124:         Video metadata dictionary
125:     """
126:     try:
127:         import cv2
128:         
129:         with stats_lock:
130:             performance_stats["cache_misses"] += 1
131:         
132:         cap = cv2.VideoCapture(video_path)
133:         try:
134:             if not cap.isOpened():
135:                 raise ValueError(f"Cannot open video: {video_path}")
136:             
137:             metadata = {
138:                 'frame_count': int(cap.get(cv2.CAP_PROP_FRAME_COUNT)),
139:                 'fps': cap.get(cv2.CAP_PROP_FPS),
140:                 'width': int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),
141:                 'height': int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)),
142:                 'duration_seconds': None
143:             }
144:             
145:             # Calculate duration
146:             if metadata['fps'] > 0 and metadata['frame_count'] > 0:
147:                 metadata['duration_seconds'] = metadata['frame_count'] / metadata['fps']
148:             
149:             return metadata
150:         finally:
151:             cap.release()
152:             
153:     except Exception as e:
154:         logger.error(f"Video metadata error for {video_path}: {e}")
155:         return {
156:             'frame_count': 0,
157:             'fps': 0,
158:             'width': 0,
159:             'height': 0,
160:             'duration_seconds': None,
161:             'error': str(e)
162:         }
163: 
164: 
165: def cached_with_stats(maxsize: int = 128, typed: bool = False):
166:     """
167:     LRU cache decorator with statistics tracking.
168:     
169:     Args:
170:         maxsize: Maximum cache size
171:         typed: Whether to consider argument types
172:         
173:     Returns:
174:         Decorator function
175:     """
176:     def decorator(func: Callable) -> Callable:
177:         cached_func = lru_cache(maxsize=maxsize, typed=typed)(func)
178:         
179:         @wraps(func)
180:         def wrapper(*args, **kwargs):
181:             # Check if result is in cache
182:             cache_info = cached_func.cache_info()
183:             initial_hits = cache_info.hits
184:             
185:             result = cached_func(*args, **kwargs)
186:             
187:             # Update statistics
188:             new_cache_info = cached_func.cache_info()
189:             if new_cache_info.hits > initial_hits:
190:                 with stats_lock:
191:                     performance_stats["cache_hits"] += 1
192:             else:
193:                 with stats_lock:
194:                     performance_stats["cache_misses"] += 1
195:             
196:             return result
197:         
198:         # Expose cache methods
199:         wrapper.cache_info = cached_func.cache_info
200:         wrapper.cache_clear = cached_func.cache_clear
201:         
202:         return wrapper
203:     
204:     return decorator
205: 
206: 
207: @track_performance("parallel_video_processing")
208: def process_videos_parallel(video_list: List[Path], 
209:                           process_func: Callable,
210:                           max_workers: int = 4,
211:                           **kwargs) -> List[bool]:
212:     """
213:     Process multiple videos with controlled parallelism.
214:     
215:     Args:
216:         video_list: List of video file paths
217:         process_func: Function to process each video
218:         max_workers: Maximum number of parallel workers
219:         **kwargs: Additional arguments for process_func
220:         
221:     Returns:
222:         List of success/failure results
223:     """
224:     if not video_list:
225:         return []
226:     
227:     results = []
228:     
229:     with stats_lock:
230:         performance_stats["parallel_operations"] += 1
231:     
232:     with ThreadPoolExecutor(max_workers=max_workers) as executor:
233:         # Submit all tasks
234:         future_to_video = {
235:             executor.submit(process_func, video, **kwargs): video
236:             for video in video_list
237:         }
238:         
239:         # Collect results as they complete
240:         for future in as_completed(future_to_video):
241:             video = future_to_video[future]
242:             try:
243:                 result = future.result()
244:                 results.append(result)
245:                 logger.info(f"Completed processing: {video.name}")
246:             except Exception as e:
247:                 logger.error(f"Failed processing {video.name}: {e}")
248:                 results.append(False)
249:     
250:     return results
251: 
252: 
253: @track_performance("batch_file_operations")
254: def batch_file_operations(file_paths: List[Path], 
255:                          operation: Callable,
256:                          batch_size: int = 10,
257:                          **kwargs) -> List[Any]:
258:     """
259:     Process files in batches for memory efficiency.
260:     
261:     Args:
262:         file_paths: List of file paths to process
263:         operation: Function to apply to each file
264:         batch_size: Number of files to process in each batch
265:         **kwargs: Additional arguments for operation
266:         
267:     Returns:
268:         List of operation results
269:     """
270:     results = []
271:     
272:     for i in range(0, len(file_paths), batch_size):
273:         batch = file_paths[i:i + batch_size]
274:         
275:         with profile_section(f"batch_{i//batch_size + 1}"):
276:             batch_results = []
277:             for file_path in batch:
278:                 try:
279:                     result = operation(file_path, **kwargs)
280:                     batch_results.append(result)
281:                 except Exception as e:
282:                     logger.error(f"Batch operation failed for {file_path}: {e}")
283:                     batch_results.append(None)
284:             
285:             results.extend(batch_results)
286:     
287:     return results
288: 
289: 
290: class PerformanceTimer:
291:     """
292:     Context manager for timing operations with automatic logging.
293:     """
294:     
295:     def __init__(self, operation_name: str, log_threshold_ms: float = 100.0):
296:         """
297:         Initialize performance timer.
298:         
299:         Args:
300:             operation_name: Name of the operation being timed
301:             log_threshold_ms: Log warning if operation takes longer than this
302:         """
303:         self.operation_name = operation_name
304:         self.log_threshold_ms = log_threshold_ms
305:         self.start_time = None
306:         self.end_time = None
307:     
308:     def __enter__(self):
309:         self.start_time = time.perf_counter()
310:         return self
311:     
312:     def __exit__(self, exc_type, exc_val, exc_tb):
313:         self.end_time = time.perf_counter()
314:         duration_ms = (self.end_time - self.start_time) * 1000
315:         
316:         if duration_ms > self.log_threshold_ms:
317:             logger.warning(f"Slow operation: {self.operation_name} took {duration_ms:.2f}ms")
318:         else:
319:             logger.debug(f"Operation: {self.operation_name} took {duration_ms:.2f}ms")
320:     
321:     @property
322:     def duration_ms(self) -> Optional[float]:
323:         """Get operation duration in milliseconds."""
324:         if self.start_time and self.end_time:
325:             return (self.end_time - self.start_time) * 1000
326:         return None
327: 
328: 
329: def optimize_memory_usage():
330:     """
331:     Perform memory optimization operations.
332:     """
333:     import gc
334:     
335:     # Clear function caches
336:     get_video_metadata_cached.cache_clear()
337:     
338:     # Force garbage collection
339:     collected = gc.collect()
340:     
341:     logger.info(f"Memory optimization: collected {collected} objects")
342: 
343: 
344: def get_performance_stats() -> Dict[str, Any]:
345:     """
346:     Get comprehensive performance statistics.
347:     
348:     Returns:
349:         Performance statistics dictionary
350:     """
351:     with stats_lock:
352:         stats = {
353:             "function_calls": dict(performance_stats["function_calls"]),
354:             "cache_stats": {
355:                 "hits": performance_stats["cache_hits"],
356:                 "misses": performance_stats["cache_misses"],
357:                 "hit_rate": (
358:                     performance_stats["cache_hits"] / 
359:                     (performance_stats["cache_hits"] + performance_stats["cache_misses"])
360:                     if (performance_stats["cache_hits"] + performance_stats["cache_misses"]) > 0
361:                     else 0
362:                 )
363:             },
364:             "parallel_operations": performance_stats["parallel_operations"]
365:         }
366:     
367:     # Add cache info for cached functions
368:     try:
369:         stats["video_metadata_cache"] = {
370:             "info": get_video_metadata_cached.cache_info()._asdict()
371:         }
372:     except AttributeError:
373:         pass
374:     
375:     return stats
376: 
377: 
378: def reset_performance_stats():
379:     """Reset all performance statistics."""
380:     global performance_stats
381:     
382:     with stats_lock:
383:         performance_stats = {
384:             "function_calls": {},
385:             "cache_hits": 0,
386:             "cache_misses": 0,
387:             "parallel_operations": 0
388:         }
389:     
390:     # Clear function caches
391:     get_video_metadata_cached.cache_clear()
392:     
393:     logger.info("Performance statistics reset")
394: 
395: 
396: def print_performance_summary(logger_fn: Callable = logger.info):
397:     """
398:     Print a formatted summary of performance statistics.
399:     
400:     Args:
401:         logger_fn: Function to use for logging output
402:     """
403:     stats = get_performance_stats()
404:     
405:     logger_fn("\n--- PERFORMANCE SUMMARY ---")
406:     
407:     # Function call statistics
408:     if stats["function_calls"]:
409:         logger_fn("Function Call Statistics:")
410:         sorted_functions = sorted(
411:             stats["function_calls"].items(),
412:             key=lambda x: x[1]["total_time"],
413:             reverse=True
414:         )
415:         
416:         for func_name, func_stats in sorted_functions[:10]:  # Top 10
417:             logger_fn(
418:                 f"  {func_name}: {func_stats['calls']} calls, "
419:                 f"{func_stats['total_time']:.2f}ms total, "
420:                 f"{func_stats['avg_time']:.2f}ms avg"
421:             )
422:     
423:     # Cache statistics
424:     cache_stats = stats["cache_stats"]
425:     logger_fn(f"Cache Statistics: {cache_stats['hit_rate']:.1%} hit rate "
426:              f"({cache_stats['hits']} hits, {cache_stats['misses']} misses)")
427:     
428:     # Parallel operations
429:     logger_fn(f"Parallel Operations: {stats['parallel_operations']}")
430:     
431:     logger_fn("--- END PERFORMANCE SUMMARY ---\n")
432: 
433: 
434: # Utility functions for common performance patterns
435: 
436: def memoize_with_ttl(ttl_seconds: int = 300):
437:     """
438:     Memoization decorator with time-to-live.
439:     
440:     Args:
441:         ttl_seconds: Time to live for cached results
442:         
443:     Returns:
444:         Decorator function
445:     """
446:     def decorator(func: Callable) -> Callable:
447:         cache = {}
448:         
449:         @wraps(func)
450:         def wrapper(*args, **kwargs):
451:             key = str(args) + str(sorted(kwargs.items()))
452:             current_time = time.time()
453:             
454:             # Check if result is cached and not expired
455:             if key in cache:
456:                 result, timestamp = cache[key]
457:                 if current_time - timestamp < ttl_seconds:
458:                     with stats_lock:
459:                         performance_stats["cache_hits"] += 1
460:                     return result
461:                 else:
462:                     del cache[key]
463:             
464:             # Compute and cache result
465:             result = func(*args, **kwargs)
466:             cache[key] = (result, current_time)
467:             
468:             with stats_lock:
469:                 performance_stats["cache_misses"] += 1
470:             
471:             return result
472:         
473:         wrapper.cache_clear = lambda: cache.clear()
474:         wrapper.cache_info = lambda: {"size": len(cache)}
475:         
476:         return wrapper
477:     
478:     return decorator
```

## File: utils/resource_manager.py
```python
  1: """
  2: Resource management utilities for workflow_mediapipe.
  3: 
  4: This module provides context managers and utilities for proper resource cleanup,
  5: preventing memory leaks and ensuring resources are properly released.
  6: """
  7: 
  8: import logging
  9: import cv2
 10: import tempfile
 11: from pathlib import Path
 12: from typing import Optional, List, Any, Union
 13: from contextlib import contextmanager
 14: 
 15: logger = logging.getLogger(__name__)
 16: 
 17: 
 18: class VideoResourceManager:
 19:     """
 20:     Context manager for safe video capture resource management.
 21:     
 22:     Ensures that cv2.VideoCapture objects are properly released
 23:     even if exceptions occur during processing.
 24:     """
 25:     
 26:     def __init__(self, video_path: Union[str, Path]):
 27:         """
 28:         Initialize video resource manager.
 29:         
 30:         Args:
 31:             video_path: Path to the video file to open
 32:         """
 33:         self.video_path = Path(video_path)
 34:         self.capture: Optional[cv2.VideoCapture] = None
 35:         self.is_opened = False
 36:         
 37:     def __enter__(self) -> cv2.VideoCapture:
 38:         """
 39:         Enter the context manager and open video capture.
 40:         
 41:         Returns:
 42:             cv2.VideoCapture: Opened video capture object
 43:             
 44:         Raises:
 45:             ValueError: If video file cannot be opened
 46:             FileNotFoundError: If video file doesn't exist
 47:         """
 48:         if not self.video_path.exists():
 49:             raise FileNotFoundError(f"Video file not found: {self.video_path}")
 50:         
 51:         try:
 52:             self.capture = cv2.VideoCapture(str(self.video_path))
 53:             
 54:             if not self.capture.isOpened():
 55:                 raise ValueError(f"Cannot open video file: {self.video_path}")
 56:             
 57:             self.is_opened = True
 58:             logger.debug(f"Video capture opened successfully: {self.video_path.name}")
 59:             
 60:             return self.capture
 61:             
 62:         except Exception as e:
 63:             # Ensure cleanup if opening fails
 64:             if self.capture:
 65:                 self.capture.release()
 66:             logger.error(f"Failed to open video capture for {self.video_path}: {e}")
 67:             raise
 68:     
 69:     def __exit__(self, exc_type, exc_val, exc_tb):
 70:         """
 71:         Exit the context manager and release video capture.
 72:         
 73:         Args:
 74:             exc_type: Exception type if an exception occurred
 75:             exc_val: Exception value if an exception occurred
 76:             exc_tb: Exception traceback if an exception occurred
 77:         """
 78:         if self.capture and self.is_opened:
 79:             try:
 80:                 self.capture.release()
 81:                 logger.debug(f"Video capture released: {self.video_path.name}")
 82:             except Exception as e:
 83:                 logger.error(f"Error releasing video capture for {self.video_path}: {e}")
 84:             finally:
 85:                 self.capture = None
 86:                 self.is_opened = False
 87:         
 88:         # Log any exceptions that occurred during processing
 89:         if exc_type:
 90:             logger.error(f"Exception in video processing for {self.video_path}: {exc_val}")
 91:         
 92:         # Don't suppress exceptions
 93:         return False
 94: 
 95: 
 96: class TempFileManager:
 97:     """
 98:     Context manager for temporary file management.
 99:     
100:     Ensures temporary files are cleaned up even if exceptions occur.
101:     """
102:     
103:     def __init__(self, suffix: str = "", prefix: str = "workflow_", dir: Optional[Path] = None):
104:         """
105:         Initialize temporary file manager.
106:         
107:         Args:
108:             suffix: File suffix/extension
109:             prefix: File prefix
110:             dir: Directory to create temp file in (defaults to system temp)
111:         """
112:         self.suffix = suffix
113:         self.prefix = prefix
114:         self.dir = dir
115:         self.temp_files: List[Path] = []
116:         
117:     def create_temp_file(self, suffix: Optional[str] = None, prefix: Optional[str] = None) -> Path:
118:         """
119:         Create a temporary file and track it for cleanup.
120:         
121:         Args:
122:             suffix: Override default suffix
123:             prefix: Override default prefix
124:             
125:         Returns:
126:             Path: Path to the created temporary file
127:         """
128:         actual_suffix = suffix or self.suffix
129:         actual_prefix = prefix or self.prefix
130:         
131:         # Create temporary file
132:         fd, temp_path = tempfile.mkstemp(
133:             suffix=actual_suffix,
134:             prefix=actual_prefix,
135:             dir=self.dir
136:         )
137:         
138:         # Close the file descriptor (we just need the path)
139:         import os
140:         os.close(fd)
141:         
142:         temp_path = Path(temp_path)
143:         self.temp_files.append(temp_path)
144:         
145:         logger.debug(f"Created temporary file: {temp_path}")
146:         return temp_path
147:     
148:     def __enter__(self):
149:         """Enter the context manager."""
150:         return self
151:     
152:     def __exit__(self, exc_type, exc_val, exc_tb):
153:         """Exit the context manager and cleanup temporary files."""
154:         self.cleanup()
155:         
156:         # Don't suppress exceptions
157:         return False
158:     
159:     def cleanup(self):
160:         """Clean up all tracked temporary files."""
161:         for temp_file in self.temp_files:
162:             try:
163:                 if temp_file.exists():
164:                     temp_file.unlink()
165:                     logger.debug(f"Cleaned up temporary file: {temp_file}")
166:             except Exception as e:
167:                 logger.error(f"Failed to cleanup temporary file {temp_file}: {e}")
168:         
169:         self.temp_files.clear()
170: 
171: 
172: @contextmanager
173: def safe_video_processing(video_path: Union[str, Path], cleanup_temp_files: bool = True):
174:     """
175:     Context manager for safe video processing with automatic resource cleanup.
176:     
177:     Args:
178:         video_path: Path to video file
179:         cleanup_temp_files: Whether to automatically cleanup temporary files
180:         
181:     Yields:
182:         tuple: (video_capture, temp_file_manager)
183:         
184:     Example:
185:         with safe_video_processing("video.mp4") as (cap, temp_mgr):
186:             # Process video frames
187:             while cap.isOpened():
188:                 ret, frame = cap.read()
189:                 if not ret:
190:                     break
191:                 
192:                 # Create temporary files if needed
193:                 temp_output = temp_mgr.create_temp_file(suffix=".json")
194:                 
195:                 # Process frame...
196:     """
197:     video_manager = VideoResourceManager(video_path)
198:     temp_manager = TempFileManager() if cleanup_temp_files else None
199:     
200:     try:
201:         with video_manager as capture:
202:             if temp_manager:
203:                 with temp_manager:
204:                     yield capture, temp_manager
205:             else:
206:                 yield capture, None
207:                 
208:     except Exception as e:
209:         logger.error(f"Error in safe video processing for {video_path}: {e}")
210:         raise
211: 
212: 
213: def get_video_metadata(video_path: Union[str, Path]) -> dict:
214:     """
215:     Safely get video metadata with proper resource cleanup.
216:     
217:     Args:
218:         video_path: Path to video file
219:         
220:     Returns:
221:         dict: Video metadata including frame count, fps, width, height
222:         
223:     Raises:
224:         ValueError: If video cannot be opened
225:         FileNotFoundError: If video file doesn't exist
226:     """
227:     with VideoResourceManager(video_path) as capture:
228:         metadata = {
229:             'frame_count': int(capture.get(cv2.CAP_PROP_FRAME_COUNT)),
230:             'fps': capture.get(cv2.CAP_PROP_FPS),
231:             'width': int(capture.get(cv2.CAP_PROP_FRAME_WIDTH)),
232:             'height': int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT)),
233:             'duration_seconds': None
234:         }
235:         
236:         # Calculate duration if fps is available
237:         if metadata['fps'] > 0:
238:             metadata['duration_seconds'] = metadata['frame_count'] / metadata['fps']
239:         
240:         logger.debug(f"Retrieved metadata for {Path(video_path).name}: {metadata}")
241:         return metadata
242: 
243: 
244: class ResourceTracker:
245:     """
246:     Track and monitor resource usage during processing.
247:     
248:     Useful for debugging resource leaks and monitoring performance.
249:     """
250:     
251:     def __init__(self):
252:         """Initialize resource tracker."""
253:         self.active_resources = {}
254:         self.resource_counter = 0
255:     
256:     def register_resource(self, resource: Any, resource_type: str, description: str = "") -> str:
257:         """
258:         Register a resource for tracking.
259:         
260:         Args:
261:             resource: The resource object to track
262:             resource_type: Type of resource (e.g., 'video_capture', 'file_handle')
263:             description: Optional description
264:             
265:         Returns:
266:             str: Resource ID for later reference
267:         """
268:         resource_id = f"{resource_type}_{self.resource_counter}"
269:         self.resource_counter += 1
270:         
271:         self.active_resources[resource_id] = {
272:             'resource': resource,
273:             'type': resource_type,
274:             'description': description,
275:             'created_at': logger.handlers[0].formatter.formatTime(logger.makeRecord(
276:                 '', 0, '', 0, '', (), None
277:             )) if logger.handlers else 'unknown'
278:         }
279:         
280:         logger.debug(f"Registered resource {resource_id}: {resource_type} - {description}")
281:         return resource_id
282:     
283:     def unregister_resource(self, resource_id: str):
284:         """
285:         Unregister a resource.
286:         
287:         Args:
288:             resource_id: ID of resource to unregister
289:         """
290:         if resource_id in self.active_resources:
291:             resource_info = self.active_resources.pop(resource_id)
292:             logger.debug(f"Unregistered resource {resource_id}: {resource_info['type']}")
293:         else:
294:             logger.warning(f"Attempted to unregister unknown resource: {resource_id}")
295:     
296:     def get_active_resources(self) -> dict:
297:         """
298:         Get information about currently active resources.
299:         
300:         Returns:
301:             dict: Information about active resources
302:         """
303:         return {
304:             'count': len(self.active_resources),
305:             'resources': {rid: {k: v for k, v in info.items() if k != 'resource'} 
306:                          for rid, info in self.active_resources.items()}
307:         }
308:     
309:     def cleanup_all(self):
310:         """
311:         Attempt to cleanup all tracked resources.
312:         
313:         This is a last-resort cleanup method.
314:         """
315:         logger.warning(f"Emergency cleanup of {len(self.active_resources)} resources")
316:         
317:         for resource_id, resource_info in list(self.active_resources.items()):
318:             try:
319:                 resource = resource_info['resource']
320:                 resource_type = resource_info['type']
321:                 
322:                 # Attempt cleanup based on resource type
323:                 if resource_type == 'video_capture' and hasattr(resource, 'release'):
324:                     resource.release()
325:                 elif resource_type == 'file_handle' and hasattr(resource, 'close'):
326:                     resource.close()
327:                 elif hasattr(resource, '__exit__'):
328:                     resource.__exit__(None, None, None)
329:                 
330:                 self.unregister_resource(resource_id)
331:                 
332:             except Exception as e:
333:                 logger.error(f"Failed to cleanup resource {resource_id}: {e}")
334: 
335: 
336: # Global resource tracker instance
337: resource_tracker = ResourceTracker()
```

## File: utils/simple_profiling.py
```python
 1: import time
 2: from collections import defaultdict
 3: from contextlib import contextmanager
 4: 
 5: # Dictionnaire global pour stocker les statistiques de profilage
 6: PROFILING_STATS = defaultdict(lambda: {"total_time": 0, "calls": 0})
 7: 
 8: 
 9: @contextmanager
10: def profile_section(section_name):
11:     """Un context manager pour profiler une section de code."""
12:     start_time = time.perf_counter()
13:     try:
14:         yield
15:     finally:
16:         elapsed_time = (time.perf_counter() - start_time) * 1000  # en ms
17:         PROFILING_STATS[section_name]["total_time"] += elapsed_time
18:         PROFILING_STATS[section_name]["calls"] += 1
19: 
20: 
21: def reset_profiling_stats():
22:     """Réinitialise les statistiques de profilage."""
23:     global PROFILING_STATS
24:     PROFILING_STATS = defaultdict(lambda: {"total_time": 0, "calls": 0})
25: 
26: 
27: # Créer un alias pour maintenir la compatibilité avec le worker
28: reset_profiling = reset_profiling_stats
29: 
30: 
31: def get_profiling_stats_dict():
32:     """Retourne une copie des statistiques actuelles."""
33:     return {k: dict(v) for k, v in PROFILING_STATS.items()}
34: 
35: 
36: def print_profiling_stats(logger_fn=print):
37:     """Affiche un résumé formaté des statistiques de profilage."""
38:     if not PROFILING_STATS:
39:         logger_fn("Aucune donnée de profilage collectée.")
40:         return
41: 
42:     logger_fn("\n--- PROFILING RESULTS (Detailed) ---")
43: 
44:     # Calcul du temps total pour le pourcentage
45:     total_time_all_sections = sum(
46:         stats["total_time"] for stats in PROFILING_STATS.values()
47:     )
48:     if total_time_all_sections == 0:
49:         logger_fn(
50:             "Temps total de profilage est zéro. Impossible de calculer les pourcentages."
51:         )
52:         total_time_all_sections = 1  # Pour éviter la division par zéro
53: 
54:     # Tri par temps total décroissant
55:     sorted_stats = sorted(
56:         PROFILING_STATS.items(), key=lambda item: item[1]["total_time"], reverse=True
57:     )
58: 
59:     logger_fn(
60:         f"{'Section':<30} | {'Total Time (ms)':>15} | {'Calls':>10} | {'Avg Time (ms)':>15} | {'% of Total':>10}"
61:     )
62:     logger_fn("-" * 85)
63: 
64:     for name, stats in sorted_stats:
65:         total_ms = stats["total_time"]
66:         calls = stats["calls"]
67:         avg_ms = total_ms / calls if calls > 0 else 0
68:         percentage = (total_ms / total_time_all_sections) * 100
69: 
70:         logger_fn(
71:             f"{name:<30} | {total_ms:>15.2f} | {calls:>10} | {avg_ms:>15.4f} | {percentage:>9.2f}%"
72:         )
73:     logger_fn("-" * 85)
```

## File: utils/tracking_optimizations.py
```python
  1: # Fichier : tracking_optimizations.py
  2: 
  3: import os
  4: import numpy as np
  5: from scipy.spatial import KDTree
  6: import sys
  7: from typing import Dict, List, Tuple, Any, Optional
  8: 
  9: 
 10: def _filter_blendshapes_for_export(blendshapes: Any) -> Any:
 11:     profile = os.environ.get("STEP5_BLENDSHAPES_PROFILE", "full").strip().lower()
 12:     if not blendshapes or not isinstance(blendshapes, dict):
 13:         return blendshapes
 14: 
 15:     if profile in {"", "full", "all"}:
 16:         return blendshapes
 17: 
 18:     if profile in {"none", "off", "0", "false", "no"}:
 19:         return None
 20: 
 21:     if profile == "mouth":
 22:         filtered = {
 23:             k: v
 24:             for k, v in blendshapes.items()
 25:             if k.startswith("mouth") or k.startswith("jaw")
 26:         }
 27:         include_tongue = os.environ.get("STEP5_BLENDSHAPES_INCLUDE_TONGUE", "0").strip().lower() in {
 28:             "1",
 29:             "true",
 30:             "yes",
 31:         }
 32:         if include_tongue and "tongueOut" in blendshapes:
 33:             filtered["tongueOut"] = blendshapes["tongueOut"]
 34:         return filtered or None
 35: 
 36:     if profile == "mediapipe":
 37:         filtered = dict(blendshapes)
 38:         filtered.pop("tongueOut", None)
 39:         filtered.setdefault("_neutral", 0.0)
 40:         return filtered or None
 41: 
 42:     if profile == "custom":
 43:         keys_raw = os.environ.get("STEP5_BLENDSHAPES_EXPORT_KEYS", "").strip()
 44:         if not keys_raw:
 45:             return blendshapes
 46:         keys = [k.strip() for k in keys_raw.split(",") if k.strip()]
 47:         filtered = {k: blendshapes[k] for k in keys if k in blendshapes}
 48:         return filtered or None
 49: 
 50:     return blendshapes
 51: 
 52: 
 53: def get_next_id(counter_ref, prefix="obj_"):
 54:     """Incrémente et retourne un nouvel ID."""
 55:     counter_ref["value"] += 1
 56:     return f"{prefix}{counter_ref['value']}"
 57: 
 58: 
 59: def apply_tracking_and_management(
 60:     active_objects,
 61:     current_detections,
 62:     next_id_counter,
 63:     distance_threshold,
 64:     frames_unseen_to_deregister,
 65:     speaking_detection_jaw_open_threshold=0.08,
 66:     enhanced_speaking_detector=None,
 67:     current_frame_num=None,
 68: ):
 69:     """
 70:     Gère le cycle de vie complet du tracking pour une frame en utilisant KDTree.
 71:     Version optimisée et centralisée pour toute l'application.
 72:     """
 73:     # --- Phase 1: Incrémentation des 'frames_unseen' pour tous les objets actifs ---
 74:     for obj_id in active_objects:
 75:         active_objects[obj_id]["frames_unseen"] += 1
 76: 
 77:     matched_detection_indices = set()
 78: 
 79:     # --- Phase 2: Appariement si des objets actifs ET des détections existent ---
 80:     if active_objects and current_detections:
 81:         active_obj_ids = list(active_objects.keys())
 82:         tracked_centroids = np.array([obj["centroid"] for obj in active_objects.values()])
 83:         detection_centroids = np.array([det["centroid"] for det in current_detections])
 84: 
 85:         kdtree = KDTree(tracked_centroids)
 86:         distances, indices = kdtree.query(detection_centroids, k=1)
 87: 
 88:         potential_matches = sorted(zip(distances, indices, range(len(current_detections))))
 89:         
 90:         matched_tracked_indices = set()
 91: 
 92:         for dist, tracked_idx, det_idx in potential_matches:
 93:             if (
 94:                 dist > distance_threshold
 95:                 or det_idx in matched_detection_indices
 96:                 or tracked_idx in matched_tracked_indices
 97:             ):
 98:                 continue
 99: 
100:             obj_id = active_obj_ids[tracked_idx]
101:             matched_det = current_detections[det_idx]
102: 
103:             active_objects[obj_id].update({**matched_det, "frames_unseen": 0})
104:             
105:             matched_detection_indices.add(det_idx)
106:             matched_tracked_indices.add(tracked_idx)
107: 
108:     # --- Phase 3: Enregistrement des nouvelles détections non appariées ---
109:     # Ce bloc est maintenant exécuté que active_objects soit vide ou non.
110:     if current_detections:
111:         for i, det_info in enumerate(current_detections):
112:             if i not in matched_detection_indices:
113:                 new_id = get_next_id(next_id_counter)
114:                 active_objects[new_id] = {
115:                     "id": new_id,
116:                     **det_info,
117:                     "frames_unseen": 0,
118:                 }
119: 
120:     # --- Phase 4: Préparation de la sortie et Nettoyage ---
121:     output_objects_for_json = []
122:     ids_to_remove = []
123: 
124:     for obj_id, obj_data in active_objects.items():
125:         if obj_data["frames_unseen"] > frames_unseen_to_deregister:
126:             ids_to_remove.append(obj_id)
127:             continue
128: 
129:         if obj_data["frames_unseen"] == 0:
130:             confidence_val = obj_data.get("confidence")
131:             output_obj = {
132:                 "id": obj_id,
133:                 "centroid_x": obj_data["centroid"][0],
134:                 "centroid_y": obj_data["centroid"][1],
135:                 "bbox_xmin": obj_data["bbox"][0],
136:                 "bbox_xmax": obj_data["bbox"][0] + obj_data["bbox"][2],
137:                 "source": obj_data.get("source_detector", "unknown"),
138:                 "label": obj_data.get("label", ""),
139:                 "confidence": (
140:                     round(float(confidence_val), 3) if confidence_val is not None else 0.0
141:                 ),
142:                 "blendshapes": _filter_blendshapes_for_export(obj_data.get("blendshapes")),
143:             }
144: 
145:             # Always include bbox size for face_landmarker before speaking detection branching
146:             try:
147:                 bbox_tuple = obj_data.get("bbox")
148:                 if bbox_tuple and len(bbox_tuple) >= 4:
149:                     output_obj["bbox_ymin"] = int(bbox_tuple[1])
150:                     output_obj["bbox_ymax"] = int(bbox_tuple[1] + bbox_tuple[3])
151:                     output_obj["bbox_width"] = int(bbox_tuple[2])
152:                     output_obj["bbox_height"] = int(bbox_tuple[3])
153:             except Exception:
154:                 # Non-blocking if bbox is missing or malformed
155:                 pass
156: 
157:             # Enhanced speaking detection
158:             if enhanced_speaking_detector and current_frame_num:
159:                 # Use enhanced multi-source speaking detection
160:                 detection_result = enhanced_speaking_detector.detect_speaking(
161:                     frame_num=current_frame_num,
162:                     blendshapes=output_obj.get("blendshapes"),
163:                     source_detector=output_obj["source"]
164:                 )
165: 
166:                 output_obj["is_speaking"] = detection_result.is_speaking
167:                 output_obj["speaking_confidence"] = detection_result.confidence
168:                 output_obj["speaking_method"] = detection_result.method
169:                 output_obj["speaking_sources"] = detection_result.sources
170: 
171:             elif output_obj["source"] == "face_landmarker":
172:                 # Fallback to original jaw-based detection
173:                 if output_obj["blendshapes"] and "jawOpen" in output_obj["blendshapes"]:
174:                     jaw_open_score = output_obj["blendshapes"].get("jawOpen", 0.0)
175:                     output_obj["is_speaking"] = (
176:                         jaw_open_score > speaking_detection_jaw_open_threshold
177:                     )
178:                     output_obj["speaking_confidence"] = min(jaw_open_score / speaking_detection_jaw_open_threshold, 1.0)
179:                     output_obj["speaking_method"] = "jaw_threshold_fallback"
180:                 else:
181:                     # S'assurer que la clé existe pour les visages, même sans blendshapes
182:                     output_obj["is_speaking"] = False
183:                     output_obj["speaking_confidence"] = 0.0
184:                     output_obj["speaking_method"] = "no_blendshapes"
185:             else:
186:                 # For object detection, try enhanced detection or set defaults
187:                 if enhanced_speaking_detector and current_frame_num:
188:                     detection_result = enhanced_speaking_detector.detect_speaking(
189:                         frame_num=current_frame_num,
190:                         blendshapes=None,
191:                         source_detector=output_obj["source"]
192:                     )
193:                     output_obj["is_speaking"] = detection_result.is_speaking
194:                     output_obj["speaking_confidence"] = detection_result.confidence
195:                     output_obj["speaking_method"] = detection_result.method
196:                     output_obj["speaking_sources"] = detection_result.sources
197:                 else:
198:                     # Remove speaking-related fields for objects without enhanced detection
199:                     output_obj.pop("blendshapes", None)
200:                     output_obj.pop("is_speaking", None)
201:                     output_obj.pop("speaking_confidence", None)
202:                     output_obj.pop("speaking_method", None)
203:                     output_obj.pop("speaking_sources", None)
204: 
205:             # Optional exports for face engines (not present for object detector)
206:             # Controlled by STEP5_EXPORT_VERBOSE_FIELDS to reduce JSON size
207:             try:
208:                 if output_obj.get("source") == "face_landmarker":
209:                     export_verbose = os.environ.get("STEP5_EXPORT_VERBOSE_FIELDS", "false").strip().lower()
210:                     should_export_verbose = export_verbose in {"true", "1", "yes", "on", "all"}
211:                     
212:                     if should_export_verbose:
213:                         landmarks_val = obj_data.get("landmarks")
214:                         if landmarks_val:
215:                             output_obj["landmarks"] = landmarks_val
216: 
217:                         eos_val = obj_data.get("eos")
218:                         if eos_val and isinstance(eos_val, dict):
219:                             output_obj["eos"] = eos_val
220:             except Exception:
221:                 pass
222: 
223:             output_objects_for_json.append(output_obj)
224: 
225:     for obj_id in ids_to_remove:
226:         del active_objects[obj_id]
227: 
228:     output_objects_for_json.sort(key=lambda o: o.get("centroid_x", 0))
229: 
230:     return output_objects_for_json
```

## File: utils/transnetv2_library.py
```python
  1: import os
  2: import numpy as np
  3: import tensorflow as tf
  4: 
  5: 
  6: class TransNetV2:
  7: 
  8:     def __init__(self, model_dir=None):
  9:         if model_dir is None:
 10:             model_dir = os.path.join(os.path.dirname(__file__), "transnetv2-weights/")
 11:             if not os.path.isdir(model_dir):
 12:                 raise FileNotFoundError(f"[TransNetV2] ERROR: {model_dir} is not a directory.")
 13:             else:
 14:                 print(f"[TransNetV2] Using weights from {model_dir}.")
 15: 
 16:         self._input_size = (27, 48, 3)
 17:         try:
 18:             self._model = tf.saved_model.load(model_dir)
 19:         except OSError as exc:
 20:             raise IOError(f"[TransNetV2] It seems that files in {model_dir} are corrupted or missing. "
 21:                           f"Re-download them manually and retry. For more info, see: "
 22:                           f"https://github.com/soCzech/TransNetV2/issues/1#issuecomment-647357796") from exc
 23: 
 24:     def predict_raw(self, frames: np.ndarray):
 25:         assert len(frames.shape) == 5 and frames.shape[2:] == self._input_size, \
 26:             "[TransNetV2] Input shape must be [batch, frames, height, width, 3]."
 27:         frames = tf.cast(frames, tf.float32)
 28: 
 29:         logits, dict_ = self._model(frames)
 30:         single_frame_pred = tf.sigmoid(logits)
 31:         all_frames_pred = tf.sigmoid(dict_["many_hot"])
 32: 
 33:         return single_frame_pred, all_frames_pred
 34: 
 35:     def predict_frames(self, frames: np.ndarray):
 36:         assert len(frames.shape) == 4 and frames.shape[1:] == self._input_size, \
 37:             "[TransNetV2] Input shape must be [frames, height, width, 3]."
 38: 
 39:         def input_iterator():
 40:             no_padded_frames_start = 25
 41:             no_padded_frames_end = 25 + 50 - (len(frames) % 50 if len(frames) % 50 != 0 else 50)  # 25 - 74
 42: 
 43:             start_frame = np.expand_dims(frames[0], 0)
 44:             end_frame = np.expand_dims(frames[-1], 0)
 45:             padded_inputs = np.concatenate(
 46:                 [start_frame] * no_padded_frames_start + [frames] + [end_frame] * no_padded_frames_end, 0
 47:             )
 48: 
 49:             ptr = 0
 50:             while ptr + 100 <= len(padded_inputs):
 51:                 out = padded_inputs[ptr:ptr + 100]
 52:                 ptr += 50
 53:                 yield out[np.newaxis]
 54: 
 55:         predictions = []
 56: 
 57:         for inp in input_iterator():
 58:             single_frame_pred, all_frames_pred = self.predict_raw(inp)
 59:             predictions.append((single_frame_pred.numpy()[0, 25:75, 0],
 60:                                 all_frames_pred.numpy()[0, 25:75, 0]))
 61: 
 62:             print("\r[TransNetV2] Processing video frames {}/{}".format(
 63:                 min(len(predictions) * 50, len(frames)), len(frames)
 64:             ), end="")
 65:         print("")
 66: 
 67:         single_frame_pred = np.concatenate([single_ for single_, all_ in predictions])
 68:         all_frames_pred = np.concatenate([all_ for single_, all_ in predictions])
 69: 
 70:         return single_frame_pred[:len(frames)], all_frames_pred[:len(frames)]  # remove extra padded frames
 71: 
 72:     def predict_video(self, video_fn: str):
 73:         try:
 74:             import ffmpeg
 75:         except ModuleNotFoundError:
 76:             raise ModuleNotFoundError("For `predict_video` function `ffmpeg` needs to be installed in order to extract "
 77:                                       "individual frames from video file. Install `ffmpeg` command line tool and then "
 78:                                       "install python wrapper by `pip install ffmpeg-python`.")
 79: 
 80:         print("[TransNetV2] Extracting frames from {}".format(video_fn))
 81:         video_stream, err = ffmpeg.input(video_fn).output(
 82:             "pipe:", format="rawvideo", pix_fmt="rgb24", s="48x27"
 83:         ).run(capture_stdout=True, capture_stderr=True)
 84: 
 85:         video = np.frombuffer(video_stream, np.uint8).reshape([-1, 27, 48, 3])
 86:         return (video, *self.predict_frames(video))
 87: 
 88:     @staticmethod
 89:     def predictions_to_scenes(predictions: np.ndarray, threshold: float = 0.5):
 90:         predictions = (predictions > threshold).astype(np.uint8)
 91: 
 92:         scenes = []
 93:         t, t_prev, start = -1, 0, 0
 94:         for i, t in enumerate(predictions):
 95:             if t_prev == 1 and t == 0:
 96:                 start = i
 97:             if t_prev == 0 and t == 1 and i != 0:
 98:                 scenes.append([start, i])
 99:             t_prev = t
100:         if t == 0:
101:             scenes.append([start, i])
102: 
103:         if len(scenes) == 0:
104:             return np.array([[0, len(predictions) - 1]], dtype=np.int32)
105: 
106:         return np.array(scenes, dtype=np.int32)
107: 
108:     @staticmethod
109:     def visualize_predictions(frames: np.ndarray, predictions):
110:         from PIL import Image, ImageDraw
111: 
112:         if isinstance(predictions, np.ndarray):
113:             predictions = [predictions]
114: 
115:         ih, iw, ic = frames.shape[1:]
116:         width = 25
117: 
118:         pad_with = width - len(frames) % width if len(frames) % width != 0 else 0
119:         frames = np.pad(frames, [(0, pad_with), (0, 1), (0, len(predictions)), (0, 0)])
120: 
121:         predictions = [np.pad(x, (0, pad_with)) for x in predictions]
122:         height = len(frames) // width
123: 
124:         img = frames.reshape([height, width, ih + 1, iw + len(predictions), ic])
125:         img = np.concatenate(np.split(
126:             np.concatenate(np.split(img, height), axis=2)[0], width
127:         ), axis=2)[0, :-1]
128: 
129:         img = Image.fromarray(img)
130:         draw = ImageDraw.Draw(img)
131: 
132:         for i, pred in enumerate(zip(*predictions)):
133:             x, y = i % width, i // width
134:             x, y = x * (iw + len(predictions)) + iw, y * (ih + 1) + ih - 1
135: 
136:             for j, p in enumerate(pred):
137:                 color = [0, 0, 0]
138:                 color[(j + 1) % 3] = 255
139: 
140:                 value = round(p * (ih - 1))
141:                 if value != 0:
142:                     draw.line((x + j, y, x + j, y - value), fill=tuple(color), width=1)
143:         return img
144: 
145: 
146: def main():
147:     import sys
148:     import argparse
149: 
150:     parser = argparse.ArgumentParser()
151:     parser.add_argument("files", type=str, nargs="+", help="path to video files to process")
152:     parser.add_argument("--weights", type=str, default=None,
153:                         help="path to TransNet V2 weights, tries to infer the location if not specified")
154:     parser.add_argument('--visualize', action="store_true",
155:                         help="save a png file with prediction visualization for each extracted video")
156:     args = parser.parse_args()
157: 
158:     model = TransNetV2(args.weights)
159:     for file in args.files:
160:         if os.path.exists(file + ".predictions.txt") or os.path.exists(file + ".scenes.txt"):
161:             print(f"[TransNetV2] {file}.predictions.txt or {file}.scenes.txt already exists. "
162:                   f"Skipping video {file}.", file=sys.stderr)
163:             continue
164: 
165:         video_frames, single_frame_predictions, all_frame_predictions = \
166:             model.predict_video(file)
167: 
168:         predictions = np.stack([single_frame_predictions, all_frame_predictions], 1)
169:         np.savetxt(file + ".predictions.txt", predictions, fmt="%.6f")
170: 
171:         scenes = model.predictions_to_scenes(single_frame_predictions)
172:         np.savetxt(file + ".scenes.txt", scenes, fmt="%d")
173: 
174:         if args.visualize:
175:             if os.path.exists(file + ".vis.png"):
176:                 print(f"[TransNetV2] {file}.vis.png already exists. "
177:                       f"Skipping visualization of video {file}.", file=sys.stderr)
178:                 continue
179: 
180:             pil_image = model.visualize_predictions(
181:                 video_frames, predictions=(single_frame_predictions, all_frame_predictions))
182:             pil_image.save(file + ".vis.png")
183: 
184: 
185: if __name__ == "__main__":
186:     main()
```

## File: utils/worker_wrapper.py
```python
 1: #!/usr/bin/env python
 2: # -*- coding: utf-8 -*-
 3: """
 4: Script wrapper qui adapte les arguments de run_with_monitoring.py aux arguments attendus
 5: par process_video_worker_blendshapes_good_backup3.py
 6: """
 7: 
 8: import os
 9: import sys
10: import argparse
11: import subprocess
12: 
13: def main():
14:     """
15:     Prend tous les arguments reçus et les passe directement à process_video_worker.
16:     """
17:     # Chemin vers le script worker final
18:     worker_script_path = os.path.join(
19:         os.path.dirname(__file__),
20:         "process_video_worker_blendshapes_good_backup3.py"
21:     )
22: 
23:     # La commande est simplement l'exécutable python, le script worker, et tous les arguments reçus
24:     # sys.argv[1:] contient tous les arguments passés au wrapper, y compris video_path.
25:     command = [
26:         sys.executable,
27:         worker_script_path,
28:     ] + sys.argv[1:]
29: 
30:     print(f"Wrapper: Lancement du worker avec la commande : {' '.join(command)}", file=sys.stderr)
31:     
32:     # Exécute le worker et retourne son code de sortie
33:     # Cela assure que la sortie (stdout/stderr) du worker est directement visible par le processus parent (run_with_monitoring)
34:     process = subprocess.Popen(command, stdout=sys.stdout, stderr=sys.stderr)
35:     process.wait()
36:     return process.returncode
37: 
38: if __name__ == "__main__":
39:     sys.exit(main())
```

## File: workflow_scripts/step1/extract_archives.py
```python
  1: #!/usr/bin/env python3
  2: # -*- coding: utf-8 -*-
  3: 
  4: """
  5: Script d'extraction d'archives pour le workflow de traitement vidéo
  6: Version Ubuntu - Étape 1 (Logique métier de la version originale préservée)
  7: """
  8: 
  9: import os
 10: import sys
 11: import shutil
 12: import zipfile
 13: import rarfile
 14: import tarfile
 15: import logging
 16: import argparse
 17: import re
 18: import unicodedata
 19: import time
 20: from pathlib import Path
 21: from datetime import datetime
 22: 
 23: sys.path.insert(0, str(Path(__file__).parent.parent.parent))
 24: from utils.filename_security import FilenameSanitizer, validate_extraction_path
 25: 
 26: # --- Configuration des chemins et du logger ---
 27: BASE_DIR = Path(os.path.dirname(os.path.abspath(__file__))).parent.parent
 28: LOG_DIR = BASE_DIR / "logs" / "step1"
 29: LOG_DIR.mkdir(parents=True, exist_ok=True)
 30: log_file = LOG_DIR / f"extract_archives_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
 31: 
 32: logging.basicConfig(
 33:     level=logging.INFO,
 34:     format='%(asctime)s - %(levelname)s - %(message)s',
 35:     handlers=[
 36:         logging.FileHandler(log_file, encoding='utf-8'),
 37:         logging.StreamHandler(sys.stdout)
 38:     ]
 39: )
 40: 
 41: # --- Configuration du traitement ---
 42: PROCESSED_ARCHIVES_FILE = LOG_DIR / "processed_archives.txt"
 43: PROCESSED_ARCHIVES_RESET_MARKER = LOG_DIR / "processed_archives.last_reset"
 44: WORK_DIR = BASE_DIR / "projets_extraits"
 45: DELETE_ARCHIVE_AFTER_SUCCESS = True
 46: 
 47: 
 48: def get_processed_archives():
 49:     """Récupère la liste des archives déjà traitées."""
 50:     if not PROCESSED_ARCHIVES_FILE.exists():
 51:         return set()
 52:     try:
 53:         with open(PROCESSED_ARCHIVES_FILE, 'r', encoding='utf-8') as f:
 54:             return {line.strip() for line in f if line.strip()}
 55:     except Exception as e:
 56:         logging.error(f"Erreur lecture du fichier des archives traitées: {e}")
 57:         return set()
 58: 
 59: 
 60: def reset_processed_archives_if_needed(now: datetime | None = None) -> bool:
 61:     """Réinitialise mensuellement le fichier des archives traitées.
 62: 
 63:     Cette fonction s'assure qu'au changement de mois, le fichier
 64:     `processed_archives.txt` est vidé pour éviter qu'une archive ayant
 65:     le même nom de fichier soit ignorée d'un mois sur l'autre. Un
 66:     marqueur de mois (`processed_archives.last_reset`) est utilisé pour
 67:     savoir si une réinitialisation a déjà eu lieu pour le mois courant.
 68: 
 69:     Args:
 70:         now: Date/heure à utiliser (principalement pour les tests). Si None, utilise l'heure courante.
 71: 
 72:     Returns:
 73:         bool: True si une réinitialisation a eu lieu, False sinon (erreur ou déjà à jour).
 74:     """
 75:     try:
 76:         current_dt = now or datetime.now()
 77:         current_month = current_dt.strftime("%Y-%m")
 78: 
 79:         last_month_value = None
 80:         if PROCESSED_ARCHIVES_RESET_MARKER.exists():
 81:             try:
 82:                 last_month_value = PROCESSED_ARCHIVES_RESET_MARKER.read_text(encoding='utf-8').strip()
 83:             except Exception as e:
 84:                 logging.warning(f"Impossible de lire le marqueur de réinitialisation: {e}")
 85: 
 86:         if last_month_value == current_month:
 87:             return False
 88: 
 89:         if PROCESSED_ARCHIVES_FILE.exists() and PROCESSED_ARCHIVES_FILE.stat().st_size > 0:
 90:             backup_name = LOG_DIR / (
 91:                 f"processed_archives_{last_month_value or 'previous'}_backup_"
 92:                 f"{current_dt.strftime('%Y%m%d_%H%M%S')}.txt"
 93:             )
 94:             try:
 95:                 shutil.copy2(PROCESSED_ARCHIVES_FILE, backup_name)
 96:                 logging.info(f"Réinitialisation mensuelle: sauvegarde créée '{backup_name.name}'.")
 97:             except Exception as e:
 98:                 logging.error(f"Échec de sauvegarde avant réinitialisation: {e}")
 99: 
100:             try:
101:                 with open(PROCESSED_ARCHIVES_FILE, 'w', encoding='utf-8'):
102:                     pass
103:                 logging.info("Réinitialisation mensuelle: fichier 'processed_archives.txt' vidé.")
104:             except Exception as e:
105:                 logging.error(f"Impossible de vider le fichier processed_archives.txt: {e}")
106:         else:
107:             logging.info("Réinitialisation mensuelle: aucun contenu existant à sauvegarder.")
108: 
109:         try:
110:             PROCESSED_ARCHIVES_RESET_MARKER.write_text(current_month, encoding='utf-8')
111:             logging.info(f"Marqueur de réinitialisation mis à jour pour le mois: {current_month}")
112:         except Exception as e:
113:             logging.error(f"Impossible d'écrire le marqueur de réinitialisation: {e}")
114: 
115:         return True
116: 
117:     except Exception as e:
118:         logging.error(f"Erreur inattendue lors de la réinitialisation mensuelle: {e}")
119:         return False
120: 
121: 
122: def mark_archive_as_processed(archive_path):
123:     """Marque une archive comme traitée."""
124:     try:
125:         with open(PROCESSED_ARCHIVES_FILE, 'a', encoding='utf-8') as f:
126:             f.write(f"{archive_path}\n")
127:     except Exception as e:
128:         logging.error(f"Erreur d'écriture dans le fichier des archives traitées: {e}")
129: 
130: 
131: def get_project_folder_name(archive_filename_str):
132:     """Dérive un nom de dossier de projet propre à partir du nom de l'archive."""
133:     name_without_ext = Path(archive_filename_str).stem
134:     parts = name_without_ext.split('_')
135:     folder_name_candidate = ""
136: 
137:     if parts[0].isdigit() and len(parts) > 1 and "camille" in parts[1].lower():
138:         folder_name_candidate = f"{parts[0]} {parts[1]}"
139:     elif "camille" in parts[0].lower():
140:         if len(parts) > 1 and parts[1].isalpha() and not parts[1].lower() == "camille":
141:             folder_name_candidate = f"{parts[0]} {parts[1]}"
142:         else:
143:             folder_name_candidate = parts[0]
144:     elif parts[0].isdigit() and len(parts) > 1 and parts[1].isalpha():
145:         folder_name_candidate = f"{parts[0]} {parts[1]}"
146:     else:
147:         folder_name_candidate = name_without_ext.replace("_", " ")
148: 
149:     clean_name = folder_name_candidate.strip()
150:     clean_name = re.sub(r'[<>:"/\\|?*&]', '_', clean_name)
151:     clean_name = re.sub(r'\s\s+', ' ', clean_name)
152:     clean_name = clean_name.strip('_ .')
153: 
154:     if not clean_name:
155:         logging.warning(f"Le nom de dossier pour '{archive_filename_str}' est vide. Utilisation d'un fallback.")
156:         fallback_name = re.sub(r'[<>:"/\\|?*&\s]', '_', Path(archive_filename_str).stem).strip('_')
157:         clean_name = f"projet_{fallback_name}" if fallback_name else "projet_sans_nom"
158:     return clean_name
159: 
160: 
161: def _format_timestamp(now: datetime | None = None) -> str:
162:     """Retourne un horodatage sans caractères interdits pour un nom de dossier.
163: 
164:     Format: YYYY-MM-DD_HH-MM-SS (ex: 2025-10-06_07-51-55)
165: 
166:     Args:
167:         now: Datetime à utiliser (principalement pour les tests). Si None, utilise datetime.now().
168: 
169:     Returns:
170:         str: Horodatage formaté.
171:     """
172:     dt = now or datetime.now()
173:     return dt.strftime("%Y-%m-%d_%H-%M-%S")
174: 
175: 
176: def compute_unique_project_dir(base_name: str, destination_base_dir: Path, now: datetime | None = None) -> Path:
177:     """Calcule un nom de dossier projet unique à créer sous destination_base_dir.
178: 
179:     - Ajoute un suffixe horodaté pour éviter les collisions entre projets portant le même nom logique (ex: "13 Camille").
180:     - En cas d'ultra-collision (même seconde), incrémente avec un compteur (-2, -3...).
181: 
182:     Exemple: base_name="13 Camille" => "13 Camille 2025-10-06_07-51-55"
183: 
184:     Args:
185:         base_name: Nom de base dérivé du fichier d'archive (déjà nettoyé).
186:         destination_base_dir: Dossier parent où créer le projet (ex: `projets_extraits/`).
187:         now: Datetime optionnelle pour tests.
188: 
189:     Returns:
190:         Path: Chemin complet du dossier unique (sans le sous-dossier "docs").
191:     """
192:     ts = _format_timestamp(now)
193:     candidate = destination_base_dir / f"{base_name} {ts}"
194: 
195:     if not candidate.exists():
196:         return candidate
197: 
198:         
199:     counter = 2
200:     while True:
201:         with_counter = destination_base_dir / f"{base_name} {ts}-{counter}"
202:         if not with_counter.exists():
203:             return with_counter
204:         counter += 1
205: 
206: 
207: def secure_extract_zip(zip_path, temp_extract_dir, sanitizer):
208:     """
209:     Securely extract ZIP archive with filename validation and path traversal protection.
210: 
211:     Args:
212:         zip_path: Path to ZIP file
213:         temp_extract_dir: Temporary extraction directory
214:         sanitizer: FilenameSanitizer instance
215: 
216:     Returns:
217:         tuple: (success: bool, security_issues_count: int)
218:     """
219:     security_issues_count = 0
220: 
221:     try:
222:         with zipfile.ZipFile(zip_path, 'r') as zip_ref:
223:             for member_info in zip_ref.infolist():
224:                 original_path = member_info.filename
225: 
226:                 if original_path.endswith('/'):
227:                     continue
228: 
229:                 sanitization_result = sanitizer.sanitize_archive_member_path(original_path)
230: 
231:                 if sanitization_result.security_issues:
232:                     security_issues_count += len(sanitization_result.security_issues)
233:                     logging.warning(f"Security issues in ZIP member '{original_path}': {sanitization_result.security_issues}")
234: 
235:                 if sanitization_result.was_modified:
236:                     logging.info(f"Sanitized ZIP member: '{original_path}' -> '{sanitization_result.sanitized_name}'")
237: 
238:                 final_path = temp_extract_dir / sanitization_result.sanitized_name
239:                 if not validate_extraction_path(sanitization_result.sanitized_name, temp_extract_dir):
240:                     logging.error(f"Unsafe extraction path detected, skipping: {original_path}")
241:                     security_issues_count += 1
242:                     continue
243: 
244:                 final_path.parent.mkdir(parents=True, exist_ok=True)
245: 
246:                 try:
247:                     with zip_ref.open(member_info) as source, open(final_path, 'wb') as target:
248:                         shutil.copyfileobj(source, target)
249:                 except Exception as e:
250:                     logging.error(f"Failed to extract ZIP member '{original_path}': {e}")
251:                     continue
252: 
253:         return True, security_issues_count
254: 
255:     except Exception as e:
256:         logging.error(f"Error during secure ZIP extraction: {e}")
257:         return False, security_issues_count
258: 
259: 
260: def secure_extract_rar(rar_path, temp_extract_dir, sanitizer):
261:     """
262:     Securely extract RAR archive with filename validation and path traversal protection.
263: 
264:     Args:
265:         rar_path: Path to RAR file
266:         temp_extract_dir: Temporary extraction directory
267:         sanitizer: FilenameSanitizer instance
268: 
269:     Returns:
270:         tuple: (success: bool, security_issues_count: int)
271:     """
272:     security_issues_count = 0
273: 
274:     try:
275:         with rarfile.RarFile(rar_path, 'r') as rar_ref:
276:             for member_info in rar_ref.infolist():
277:                 original_path = member_info.filename
278: 
279:                 if member_info.is_dir():
280:                     continue
281: 
282:                 sanitization_result = sanitizer.sanitize_archive_member_path(original_path)
283: 
284:                 if sanitization_result.security_issues:
285:                     security_issues_count += len(sanitization_result.security_issues)
286:                     logging.warning(f"Security issues in RAR member '{original_path}': {sanitization_result.security_issues}")
287: 
288:                 if sanitization_result.was_modified:
289:                     logging.info(f"Sanitized RAR member: '{original_path}' -> '{sanitization_result.sanitized_name}'")
290: 
291:                 final_path = temp_extract_dir / sanitization_result.sanitized_name
292:                 if not validate_extraction_path(sanitization_result.sanitized_name, temp_extract_dir):
293:                     logging.error(f"Unsafe extraction path detected, skipping: {original_path}")
294:                     security_issues_count += 1
295:                     continue
296: 
297:                 final_path.parent.mkdir(parents=True, exist_ok=True)
298: 
299:                 try:
300:                     with rar_ref.open(member_info) as source, open(final_path, 'wb') as target:
301:                         shutil.copyfileobj(source, target)
302:                 except Exception as e:
303:                     logging.error(f"Failed to extract RAR member '{original_path}': {e}")
304:                     continue
305: 
306:         return True, security_issues_count
307: 
308:     except Exception as e:
309:         logging.error(f"Error during secure RAR extraction: {e}")
310:         return False, security_issues_count
311: 
312: 
313: def secure_extract_tar(tar_path, temp_extract_dir, sanitizer):
314:     """
315:     Securely extract TAR archive with filename validation and path traversal protection.
316: 
317:     Args:
318:         tar_path: Path to TAR file
319:         temp_extract_dir: Temporary extraction directory
320:         sanitizer: FilenameSanitizer instance
321: 
322:     Returns:
323:         tuple: (success: bool, security_issues_count: int)
324:     """
325:     security_issues_count = 0
326: 
327:     try:
328:         with tarfile.open(tar_path, 'r:*') as tar_ref:
329:             for member_info in tar_ref.getmembers():
330:                 original_path = member_info.name
331: 
332:                 if member_info.isdir():
333:                     continue
334: 
335:                 if not member_info.isfile():
336:                     logging.warning(f"Skipping non-regular file in TAR: {original_path}")
337:                     security_issues_count += 1
338:                     continue
339: 
340:                 sanitization_result = sanitizer.sanitize_archive_member_path(original_path)
341: 
342:                 if sanitization_result.security_issues:
343:                     security_issues_count += len(sanitization_result.security_issues)
344:                     logging.warning(f"Security issues in TAR member '{original_path}': {sanitization_result.security_issues}")
345: 
346:                 if sanitization_result.was_modified:
347:                     logging.info(f"Sanitized TAR member: '{original_path}' -> '{sanitization_result.sanitized_name}'")
348: 
349:                 final_path = temp_extract_dir / sanitization_result.sanitized_name
350:                 if not validate_extraction_path(sanitization_result.sanitized_name, temp_extract_dir):
351:                     logging.error(f"Unsafe extraction path detected, skipping: {original_path}")
352:                     security_issues_count += 1
353:                     continue
354: 
355:                 final_path.parent.mkdir(parents=True, exist_ok=True)
356: 
357:                 try:
358:                     with tar_ref.extractfile(member_info) as source, open(final_path, 'wb') as target:
359:                         if source:  # extractfile can return None for some members
360:                             shutil.copyfileobj(source, target)
361:                 except Exception as e:
362:                     logging.error(f"Failed to extract TAR member '{original_path}': {e}")
363:                     continue
364: 
365:         return True, security_issues_count
366: 
367:     except Exception as e:
368:         logging.error(f"Error during secure TAR extraction: {e}")
369:         return False, security_issues_count
370: 
371: 
372: def extract_archive(archive_path, destination_base_dir):
373:     """Extrait une archive de manière sécurisée, gère les sous-dossiers et nettoie."""
374:     project_folder_name = get_project_folder_name(archive_path.name)
375:     # Le dossier final contiendra un sous-dossier "docs" pour la cohérence avec les étapes suivantes
376:     final_destination = destination_base_dir / project_folder_name / "docs"
377:     temp_extract_dir = destination_base_dir / f"_temp_{project_folder_name}_{int(time.time())}"
378: 
379:     # Initialize security sanitizer
380:     sanitizer = FilenameSanitizer()
381:     total_security_issues = 0
382: 
383:     try:
384:         logging.info(f"Extraction sécurisée de {archive_path.name} vers {final_destination}")
385: 
386:         # 1. Extraire dans un dossier temporaire avec validation de sécurité
387:         temp_extract_dir.mkdir(parents=True, exist_ok=True)
388: 
389:         suffix = archive_path.suffix.lower()
390:         extraction_success = False
391: 
392:         if suffix in ('.zip', '.zipx'):
393:             logging.info(f"Extraction ZIP sécurisée de {archive_path.name}")
394:             extraction_success, security_issues = secure_extract_zip(archive_path, temp_extract_dir, sanitizer)
395:             total_security_issues += security_issues
396:         elif suffix == '.rar':
397:             logging.info(f"Extraction RAR sécurisée de {archive_path.name}")
398:             extraction_success, security_issues = secure_extract_rar(archive_path, temp_extract_dir, sanitizer)
399:             total_security_issues += security_issues
400:         elif suffix in ('.tar', '.gz', '.bz2', '.xz', '.tgz'):
401:             logging.info(f"Extraction TAR sécurisée de {archive_path.name}")
402:             extraction_success, security_issues = secure_extract_tar(archive_path, temp_extract_dir, sanitizer)
403:             total_security_issues += security_issues
404:         else:
405:             logging.warning(f"Format non supporté: {archive_path}")
406:             return False
407: 
408:         if not extraction_success:
409:             logging.error(f"Échec de l'extraction sécurisée pour {archive_path.name}")
410:             return False
411: 
412:         # Log security statistics
413:         stats = sanitizer.get_stats()
414:         logging.info(f"Statistiques de sécurité pour {archive_path.name}: "
415:                     f"{stats['total_processed']} fichiers traités, "
416:                     f"{stats['modified_count']} modifiés, "
417:                     f"{total_security_issues} problèmes de sécurité détectés")
418: 
419:         if total_security_issues > 0:
420:             logging.warning(f"ATTENTION: {total_security_issues} problèmes de sécurité détectés dans {archive_path.name}")
421: 
422:         # Vérifier qu'il y a des fichiers extraits
423:         extracted_items = list(temp_extract_dir.rglob('*'))
424:         if not extracted_items:
425:             logging.warning(f"Aucun fichier extrait de {archive_path.name}")
426:             return False
427: 
428:         # 2. Nettoyer les fichiers inutiles de l'extraction (ex: __MACOSX)
429:         macosx_junk = temp_extract_dir / "__MACOSX"
430:         if macosx_junk.exists() and macosx_junk.is_dir():
431:             logging.info("Suppression du dossier __MACOSX.")
432:             shutil.rmtree(macosx_junk)
433: 
434:         # 3. Gérer les cas où le ZIP contient un seul dossier racine
435:         items_in_temp = list(temp_extract_dir.iterdir())
436:         source_content_root = temp_extract_dir
437:         if len(items_in_temp) == 1 and items_in_temp[0].is_dir():
438:             logging.info(f"Le ZIP contient un seul dossier racine '{items_in_temp[0].name}'. Utilisation comme source.")
439:             source_content_root = items_in_temp[0]
440: 
441:         # 4. Déplacer le contenu vers la destination finale
442:         final_destination.mkdir(parents=True, exist_ok=True)
443:         for item_to_move in source_content_root.iterdir():
444:             target_path = final_destination / item_to_move.name
445:             if target_path.exists():
446:                 logging.warning(f"'{target_path.name}' existe déjà dans la destination. Il sera écrasé.")
447:                 if target_path.is_dir():
448:                     shutil.rmtree(target_path)
449:                 else:
450:                     target_path.unlink()
451:             shutil.move(str(item_to_move), str(target_path))
452: 
453:         logging.info(f"Extraction terminée pour {archive_path.name}")
454:         return True
455: 
456:     except (zipfile.BadZipFile, rarfile.BadRarFile, tarfile.ReadError) as e:
457:         logging.error(f"Erreur: Fichier archive corrompu ou invalide - {archive_path.name}: {e}")
458:         return False
459:     except Exception as e:
460:         logging.error(f"Erreur inattendue lors de l'extraction de {archive_path.name}: {e}")
461:         return False
462:     finally:
463:         # 5. Nettoyer le dossier temporaire
464:         if temp_extract_dir.exists():
465:             shutil.rmtree(temp_extract_dir)
466: 
467: 
468: def find_archives_to_process(source_dir):
469:     """Trouve les archives non encore traitées et contenant le mot-clé 'Camille'."""
470:     processed = get_processed_archives()
471:     archives = []
472:     keyword = "Camille"
473: 
474:     if not source_dir.exists():
475:         logging.warning(f"Le dossier d'archives '{source_dir}' n'existe pas.")
476:         return []
477: 
478:     logging.info(f"Recherche des archives dans '{source_dir}' avec le mot-clé '{keyword}'...")
479: 
480:     archive_extensions = ('.zip', '.zipx', '.rar', '.tar.gz', '.tgz', '.tar.bz2', '.tar.xz', '.7z', '.tar')
481:     for archive in source_dir.iterdir():
482:         if archive.is_file() and archive.suffix.lower() in archive_extensions:
483:             if keyword.lower() in archive.name.lower() and str(archive.resolve()) not in processed:
484:                 archives.append(archive)
485:                 logging.info(f"Archive correspondante trouvée: {archive.name}")
486: 
487:     return archives
488: 
489: 
490: def main():
491:     parser = argparse.ArgumentParser(description="Script d'extraction d'archives intelligent.")
492:     parser.add_argument('--source-dir', type=str, required=True,
493:                         help="Spécifie le répertoire où chercher les archives.")
494:     args = parser.parse_args()
495: 
496:     source_archives_dir = Path(args.source_dir)
497:     logging.info(f"--- Démarrage du script d'extraction d'archives ---")
498:     logging.info(f"Dossier source: {source_archives_dir.resolve()}")
499:     logging.info(f"Dossier de destination des projets: {WORK_DIR.resolve()}")
500: 
501:     WORK_DIR.mkdir(parents=True, exist_ok=True)
502: 
503:     # Réinitialisation mensuelle du fichier des archives traitées (si nécessaire)
504:     try:
505:         did_reset = reset_processed_archives_if_needed()
506:         if did_reset:
507:             logging.info("Réinitialisation mensuelle exécutée (ou marqueur mis à jour).")
508:         else:
509:             logging.info("Aucune réinitialisation mensuelle nécessaire.")
510:     except Exception as e:
511:         logging.error(f"Échec de la vérification de réinitialisation mensuelle: {e}")
512: 
513:     archives = find_archives_to_process(source_archives_dir)
514:     total_to_process = len(archives)
515:     logging.info(f"Trouvé {total_to_process} nouvelle(s) archive(s) à traiter.")
516:     print(f"Trouvé {total_to_process} archive(s) à traiter")  # Pour l'UI
517: 
518:     if total_to_process == 0:
519:         logging.info("Aucune nouvelle archive à traiter. Fin du script.")
520:         return
521: 
522:     successful_count = 0
523:     for i, archive in enumerate(archives):
524:         logging.info(f"--- Traitement {i + 1}/{total_to_process}: {archive.name} ---")
525: 
526:         success = extract_archive(archive, WORK_DIR)
527: 
528:         if success:
529:             successful_count += 1
530:             mark_archive_as_processed(str(archive.resolve()))
531:             if DELETE_ARCHIVE_AFTER_SUCCESS:
532:                 try:
533:                     archive.unlink()
534:                     logging.info(f"Archive source '{archive.name}' supprimée avec succès.")
535:                 except Exception as e:
536:                     logging.error(f"Impossible de supprimer l'archive source '{archive.name}': {e}")
537:         else:
538:             logging.error(f"L'extraction de '{archive.name}' a échoué. Voir les logs précédents.")
539: 
540:     logging.info(f"--- Traitement terminé ---")
541:     logging.info(f"Résumé: {successful_count}/{total_to_process} archive(s) extraite(s) avec succès.")
542: 
543:     if successful_count < total_to_process:
544:         sys.exit(1)  # Quitter avec un code d'erreur s'il y a eu des échecs
545: 
546: 
547: if __name__ == "__main__":
548:     try:
549:         main()
550:     except Exception as e:
551:         logging.critical(f"Erreur critique non gérée dans le script: {e}")
552:         sys.exit(1)
```

## File: workflow_scripts/step2/convert_videos.py
```python
  1: #!/usr/bin/env python3
  2: # -*- coding: utf-8 -*-
  3: 
  4: """
  5: Script de conversion vidéo pour le workflow de traitement
  6: Version Ubuntu - Étape 2 (Logique optimisée et parallèle)
  7: """
  8: 
  9: import os
 10: import sys
 11: import shutil
 12: import subprocess
 13: import json
 14: import logging
 15: from pathlib import Path
 16: from datetime import datetime
 17: import concurrent.futures
 18: import threading
 19: import queue
 20: import time
 21: 
 22: # --- Configuration ---
 23: WORK_DIR = Path(os.getcwd())
 24: TARGET_FPS = 25.0
 25: VIDEO_EXTENSIONS = ('.mp4', '.mov', '.avi', '.mkv', '.webm', '.flv', '.wmv')
 26: MAX_CPU_WORKERS = max(1, os.cpu_count() - 2)
 27: FFMPEG_PATH = "ffmpeg"
 28: FFPROBE_PATH = "ffprobe"
 29: 
 30: # GPU worker queue system for continuous processing
 31: GPU_QUEUE = queue.Queue()
 32: GPU_RESULTS_QUEUE = queue.Queue()
 33: GPU_WORKER_SHUTDOWN = threading.Event()
 34: PROGRESS_LOCK = threading.Lock()
 35: COMPLETED_VIDEOS = 0
 36: TOTAL_VIDEOS_COUNT = 0
 37: COMPRESS_COMPLETED = 0
 38: COMPRESS_TOTAL = 0
 39: 
 40: BASE_DIR = Path(os.path.dirname(os.path.abspath(__file__))).parent.parent
 41: LOG_DIR = BASE_DIR / "logs" / "step2"
 42: LOG_DIR.mkdir(parents=True, exist_ok=True)
 43: log_file = LOG_DIR / f"convert_videos_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
 44: 
 45: logging.basicConfig(
 46:     level=logging.INFO,
 47:     format='%(asctime)s - %(threadName)s - %(levelname)s - %(message)s',
 48:     handlers=[
 49:         logging.FileHandler(log_file, encoding='utf-8'),
 50:         logging.StreamHandler(sys.stdout)
 51:     ]
 52: )
 53: 
 54: 
 55: def _parse_ffprobe_fraction(value: str):
 56:     if not value:
 57:         return None
 58:     value = str(value).strip()
 59:     if not value or value == "0/0":
 60:         return None
 61:     try:
 62:         if "/" in value:
 63:             num_str, den_str = value.split("/", 1)
 64:             num = float(num_str)
 65:             den = float(den_str)
 66:             if den == 0:
 67:                 return None
 68:             return num / den
 69:         return float(value)
 70:     except Exception:
 71:         return None
 72: 
 73: 
 74: def _parse_ffprobe_fps(payload: dict):
 75:     try:
 76:         streams = payload.get("streams") or []
 77:         if not streams:
 78:             return None
 79:         stream = streams[0] or {}
 80: 
 81:         avg_fps = _parse_ffprobe_fraction(stream.get("avg_frame_rate"))
 82:         r_fps = _parse_ffprobe_fraction(stream.get("r_frame_rate"))
 83: 
 84:         nb_frames_raw = stream.get("nb_frames")
 85:         duration_raw = stream.get("duration")
 86:         nb_frames = None
 87:         duration = None
 88:         try:
 89:             if nb_frames_raw not in (None, "N/A", ""):
 90:                 nb_frames = int(float(nb_frames_raw))
 91:         except Exception:
 92:             nb_frames = None
 93:         try:
 94:             if duration_raw not in (None, "N/A", ""):
 95:                 duration = float(duration_raw)
 96:         except Exception:
 97:             duration = None
 98: 
 99:         fps_from_counts = None
100:         if nb_frames and duration and duration > 0:
101:             fps_from_counts = float(nb_frames) / float(duration)
102: 
103:         for candidate in (fps_from_counts, avg_fps, r_fps):
104:             if candidate is None:
105:                 continue
106:             if candidate <= 0 or candidate > 240:
107:                 continue
108:             return float(candidate)
109:         return None
110:     except Exception:
111:         return None
112: 
113: 
114: def get_video_framerate(video_path):
115:     """Récupère le framerate effectif d'une vidéo."""
116:     try:
117:         command = [
118:             FFPROBE_PATH, "-v", "error",
119:             "-select_streams", "v:0",
120:             "-show_entries", "stream=avg_frame_rate,r_frame_rate,nb_frames,duration",
121:             "-of", "json",
122:             str(video_path),
123:         ]
124:         result = subprocess.run(command, capture_output=True, text=True, check=True, encoding='utf-8')
125:         payload = json.loads(result.stdout or "{}")
126:         fps = _parse_ffprobe_fps(payload)
127:         if fps is None:
128:             raise ValueError("ffprobe fps parse returned None")
129:         return fps
130:     except Exception as e:
131:         logging.error(f"Impossible de lire le framerate de {video_path.name}: {e}")
132:         return None
133: 
134: 
135: def find_videos_to_convert():
136:     """Trouve toutes les vidéos à convertir dans le répertoire de travail."""
137:     videos_to_check = []
138:     logging.info(f"Recherche de vidéos ({', '.join(VIDEO_EXTENSIONS)}) dans {WORK_DIR}...")
139: 
140:     for root, _, files in os.walk(WORK_DIR):
141:         for file in files:
142:             if "_temp_conversion" in file or "_converted" in file:
143:                 continue
144:             if file.lower().endswith(VIDEO_EXTENSIONS):
145:                 videos_to_check.append(Path(root) / file)
146: 
147:     logging.info(f"{len(videos_to_check)} vidéo(s) trouvée(s). Vérification du framerate...")
148: 
149:     videos_requiring_conversion = []
150:     for video_path in videos_to_check:
151:         current_fps = get_video_framerate(video_path)
152:         if current_fps is not None and abs(current_fps - TARGET_FPS) > 0.1:
153:             logging.info(f"Conversion requise pour {video_path.name} (FPS actuel: {current_fps:.2f})")
154:             videos_requiring_conversion.append(video_path)
155:         elif current_fps is not None:
156:             logging.info(f"Conversion non requise pour {video_path.name} (FPS actuel: {current_fps:.2f})")
157: 
158:     return videos_requiring_conversion
159: 
160: 
161: def find_mp4_videos():
162:     """Liste toutes les vidéos .mp4 à compresser (exclut les fichiers temporaires)."""
163:     mp4_videos = []
164:     logging.info(f"Recherche des vidéos .mp4 dans {WORK_DIR} pour compression…")
165: 
166:     for root, _, files in os.walk(WORK_DIR):
167:         for file in files:
168:             if "_temp_conversion" in file or "_converted" in file or ".temp_compress" in file:
169:                 continue
170:             if file.lower().endswith('.mp4'):
171:                 mp4_videos.append(Path(root) / file)
172: 
173:     logging.info(f"{len(mp4_videos)} fichier(s) .mp4 détecté(s) pour la compression.")
174:     return mp4_videos
175: 
176: 
177: def convert_single_video(video_path, use_gpu=False):
178:     """Convertit une seule vidéo avec l'encodeur spécifié."""
179:     worker_type = 'GPU' if use_gpu else 'CPU'
180: 
181:     try:
182:         logging.info(f"Conversion ({worker_type}) démarrée pour {video_path.name}")
183: 
184:         temp_output_path = video_path.with_suffix(f".temp_conversion{video_path.suffix}")
185: 
186:         command = [FFMPEG_PATH, '-y', '-hide_banner', '-i', str(video_path), '-vf', f'fps={TARGET_FPS}']
187: 
188:         if use_gpu:
189:             command.extend(['-c:v', 'h264_nvenc', '-preset', 'p5', '-tune', 'hq', '-cq', '23', '-pix_fmt', 'yuv420p'])
190:         else:
191:             command.extend(['-c:v', 'libx264', '-preset', 'medium', '-crf', '23', '-pix_fmt', 'yuv420p'])
192: 
193:         command_with_audio_copy = command + ['-c:a', 'copy', str(temp_output_path)]
194: 
195:         result = subprocess.run(command_with_audio_copy, capture_output=True, text=True, check=False, encoding='utf-8')
196: 
197:         if result.returncode != 0:
198:             logging.warning(f"La copie audio a échoué pour {video_path.name}, tentative de ré-encodage audio...")
199:             command_with_audio_reencode = command + ['-c:a', 'aac', '-b:a', '192k', str(temp_output_path)]
200:             result = subprocess.run(command_with_audio_reencode, capture_output=True, text=True, check=False,
201:                                     encoding='utf-8')
202: 
203:         if result.returncode != 0:
204:             logging.error(f"Erreur FFmpeg ({worker_type}) pour {video_path.name}.\nStderr: {result.stderr.strip()}")
205:             if temp_output_path.exists(): temp_output_path.unlink()
206:             return False
207: 
208:         shutil.move(str(temp_output_path), str(video_path))
209:         logging.info(f"Succès ({worker_type}): {video_path.name} a été converti et mis à jour.")
210:         return True
211: 
212:     except Exception as e:
213:         logging.error(f"Erreur inattendue dans le worker ({worker_type}) pour {video_path.name}: {e}")
214:         return False
215: 
216: 
217: def compress_single_video(video_path, use_gpu=False):
218:     """Compresse une vidéo .mp4 sans changer la résolution ni le framerate.
219: 
220:     Utilise des paramètres FFmpeg conservateurs pour réduire la taille tout en préservant la qualité visuelle.
221:     - GPU: h264_nvenc avec cq=28 (qualité élevée, réduction notable)
222:     - CPU: libx264 avec crf=28
223:     L'audio est copié si possible, sinon ré-encodé en AAC 192k.
224:     """
225:     worker_type = 'GPU' if use_gpu else 'CPU'
226: 
227:     try:
228:         logging.info(f"Compression ({worker_type}) démarrée pour {video_path.name}")
229: 
230:         temp_output_path = video_path.with_suffix(f".temp_compress{video_path.suffix}")
231: 
232:         command = [FFMPEG_PATH, '-y', '-hide_banner', '-i', str(video_path)]
233: 
234:         if use_gpu:
235:             command.extend(['-c:v', 'h264_nvenc', '-preset', 'p5', '-tune', 'hq', '-cq', '28', '-pix_fmt', 'yuv420p'])
236:         else:
237:             command.extend(['-c:v', 'libx264', '-preset', 'medium', '-crf', '28', '-pix_fmt', 'yuv420p'])
238: 
239:         command_with_audio_copy = command + ['-c:a', 'copy', str(temp_output_path)]
240:         result = subprocess.run(command_with_audio_copy, capture_output=True, text=True, check=False, encoding='utf-8')
241: 
242:         if result.returncode != 0:
243:             logging.warning(f"La copie audio a échoué pour {video_path.name}, tentative de ré-encodage audio…")
244:             command_with_audio_reencode = command + ['-c:a', 'aac', '-b:a', '192k', str(temp_output_path)]
245:             result = subprocess.run(command_with_audio_reencode, capture_output=True, text=True, check=False, encoding='utf-8')
246: 
247:         if result.returncode != 0:
248:             logging.error(f"Erreur FFmpeg ({worker_type}) lors de la compression de {video_path.name}.\nStderr: {result.stderr.strip()}")
249:             if temp_output_path.exists():
250:                 try:
251:                     temp_output_path.unlink()
252:                 except Exception:
253:                     pass
254:             return False
255: 
256:         shutil.move(str(temp_output_path), str(video_path))
257:         logging.info(f"Succès ({worker_type}): {video_path.name} a été compressé.")
258:         return True
259: 
260:     except Exception as e:
261:         logging.error(f"Erreur inattendue dans le worker ({worker_type}) pour {video_path.name} (compression): {e}")
262:         return False
263: 
264: 
265: 
266: def gpu_worker_thread():
267:     """Thread dédié pour le traitement GPU continu."""
268:     global COMPLETED_VIDEOS
269: 
270:     logging.info("GPU worker thread démarré")
271: 
272:     while not GPU_WORKER_SHUTDOWN.is_set():
273:         try:
274:             video_path = GPU_QUEUE.get(timeout=1.0)
275: 
276:             with PROGRESS_LOCK:
277:                 current_index = COMPLETED_VIDEOS + 1
278:                 logging.info(f"--- Traitement de la vidéo ({current_index}/{TOTAL_VIDEOS_COUNT}): {video_path.name} ---")
279:                 print(f"--- Traitement de la vidéo ({current_index}/{TOTAL_VIDEOS_COUNT}): {video_path.name} ---")
280: 
281:             success = convert_single_video(video_path, use_gpu=True)
282: 
283:             GPU_RESULTS_QUEUE.put((success, video_path))
284: 
285:             with PROGRESS_LOCK:
286:                 COMPLETED_VIDEOS += 1
287: 
288:             GPU_QUEUE.task_done()
289: 
290:         except queue.Empty:
291:             continue
292:         except Exception as e:
293:             logging.error(f"Erreur dans le GPU worker thread: {e}")
294:             try:
295:                 GPU_QUEUE.task_done()
296:             except ValueError:
297:                 pass
298: 
299:     logging.info("GPU worker thread terminé")
300: 
301: 
302: def gpu_compress_worker_thread():
303:     """Thread dédié pour la compression GPU séquentielle."""
304:     global COMPRESS_COMPLETED
305: 
306:     logging.info("GPU compression worker thread démarré")
307: 
308:     while not GPU_WORKER_SHUTDOWN.is_set():
309:         try:
310:             video_path = GPU_QUEUE.get(timeout=1.0)
311: 
312:             # Émission d'une ligne de progression compatible STEP2 avant traitement (compression)
313:             with PROGRESS_LOCK:
314:                 current_index = COMPRESS_COMPLETED + 1
315:                 logging.info(f"--- Traitement de la vidéo ({current_index}/{COMPRESS_TOTAL}): {video_path.name} ---")
316:                 print(f"--- Traitement de la vidéo ({current_index}/{COMPRESS_TOTAL}): {video_path.name} ---")
317: 
318:             success = compress_single_video(video_path, use_gpu=True)
319: 
320:             GPU_RESULTS_QUEUE.put((success, video_path))
321: 
322:             with PROGRESS_LOCK:
323:                 COMPRESS_COMPLETED += 1
324: 
325:             GPU_QUEUE.task_done()
326: 
327:         except queue.Empty:
328:             continue
329:         except Exception as e:
330:             logging.error(f"Erreur dans le GPU compression worker thread: {e}")
331:             try:
332:                 GPU_QUEUE.task_done()
333:             except ValueError:
334:                 pass
335: 
336:     logging.info("GPU compression worker thread terminé")
337: 
338: 
339: 
340: 
341: def main():
342:     global TOTAL_VIDEOS_COUNT, COMPLETED_VIDEOS, COMPRESS_TOTAL, COMPRESS_COMPLETED
343: 
344:     logging.info("--- Démarrage du script de conversion vidéo ---")
345: 
346:     try:
347:         subprocess.run([FFMPEG_PATH, "-version"], capture_output=True, check=True)
348:         subprocess.run([FFPROBE_PATH, "-version"], capture_output=True, check=True)
349:     except (FileNotFoundError, subprocess.CalledProcessError):
350:         logging.critical("ffmpeg ou ffprobe n'est pas installé ou non accessible dans le PATH. Arrêt.")
351:         sys.exit(1)
352: 
353:     videos = find_videos_to_convert()
354:     total_videos = len(videos)
355:     TOTAL_VIDEOS_COUNT = total_videos
356:     COMPLETED_VIDEOS = 0
357: 
358:     logging.info(f"TOTAL_VIDEOS_TO_PROCESS: {total_videos}")
359:     print(f"TOTAL_VIDEOS_TO_PROCESS: {total_videos}")
360: 
361:     if total_videos == 0:
362:         logging.info("Aucune vidéo à convertir. Passage direct à la compression.")
363: 
364:     total_successful = 0
365:     if total_videos > 0:
366:         gpu_videos = videos
367:         cpu_videos = []
368: 
369:         logging.info(f"Allocation GPU exclusive: {len(gpu_videos)} vidéo(s) pour traitement GPU séquentiel")
370: 
371:         logging.info(f"Lancement de la conversion avec 1 worker GPU dédié pour traitement séquentiel.")
372:         gpu_thread = threading.Thread(target=gpu_worker_thread, name='GPU-Worker', daemon=True)
373:         gpu_thread.start()
374: 
375:         for video in gpu_videos:
376:             GPU_QUEUE.put(video)
377: 
378:         GPU_QUEUE.join()
379: 
380:         gpu_successful_count = 0
381:         while not GPU_RESULTS_QUEUE.empty():
382:             try:
383:                 success, video_path = GPU_RESULTS_QUEUE.get_nowait()
384:                 if success:
385:                     gpu_successful_count += 1
386:             except queue.Empty:
387:                 break
388: 
389:         GPU_WORKER_SHUTDOWN.set()
390:         gpu_thread.join(timeout=5.0)
391: 
392:         total_successful = gpu_successful_count
393: 
394:         logging.info("--- Conversion de toutes les vidéos terminée ---")
395:         logging.info(f"Résumé: {total_successful}/{total_videos} conversion(s) réussie(s) (traitement GPU exclusif).")
396: 
397:     mp4_videos = find_mp4_videos()
398:     COMPRESS_TOTAL = len(mp4_videos)
399:     COMPRESS_COMPLETED = 0
400: 
401:     logging.info(f"TOTAL_VIDEOS_TO_PROCESS: {COMPRESS_TOTAL} (compression)")
402:     print(f"TOTAL_VIDEOS_TO_PROCESS: {COMPRESS_TOTAL}")
403: 
404:     if COMPRESS_TOTAL == 0:
405:         logging.info("Aucune vidéo .mp4 à compresser. Fin du script.")
406:         if total_successful < total_videos:
407:             sys.exit(1)
408:         return
409: 
410:     GPU_WORKER_SHUTDOWN.clear()
411:     gpu_compress_thread = threading.Thread(target=gpu_compress_worker_thread, name='GPU-Compress-Worker', daemon=True)
412:     gpu_compress_thread.start()
413: 
414:     for video in mp4_videos:
415:         GPU_QUEUE.put(video)
416: 
417:     GPU_QUEUE.join()
418: 
419:     compress_successful_count = 0
420:     while not GPU_RESULTS_QUEUE.empty():
421:         try:
422:             success, video_path = GPU_RESULTS_QUEUE.get_nowait()
423:             if success:
424:                 compress_successful_count += 1
425:         except queue.Empty:
426:             break
427: 
428:     GPU_WORKER_SHUTDOWN.set()
429:     gpu_compress_thread.join(timeout=5.0)
430: 
431:     logging.info("--- Compression de toutes les vidéos .mp4 terminée ---")
432:     logging.info(f"Résumé: {compress_successful_count}/{COMPRESS_TOTAL} compression(s) réussie(s) (GPU séquentiel).")
433: 
434:     if total_successful < total_videos:
435:         sys.exit(1)
436: 
437: 
438: if __name__ == "__main__":
439:     main()
```

## File: workflow_scripts/step3/run_transnet.py
```python
  1: #!/usr/bin/env python3
  2: # -*- coding: utf-8 -*-
  3: 
  4: """
  5: Script d'analyse des transitions vidéo avec TransNetV2 (PyTorch)
  6: Version Ubuntu - Étape 3
  7: """
  8: 
  9: import os
 10: import sys
 11: import csv
 12: import argparse
 13: import logging
 14: import json
 15: import numpy as np
 16: import torch
 17: import torch.nn as nn
 18: import ffmpeg
 19: from pathlib import Path
 20: from datetime import datetime
 21: from scenedetect import FrameTimecode
 22: import multiprocessing as mp
 23: import time
 24: 
 25: # --- Configuration ---
 26: WORK_DIR = Path(os.getcwd())
 27: VIDEO_EXTENSIONS = ('.mp4', '.avi', '.mov', '.mkv', '.webm')
 28: BASE_DIR = Path(os.path.dirname(os.path.abspath(__file__))).parent.parent
 29: 
 30: # --- Configuration du Logger ---
 31: LOG_DIR = BASE_DIR / "logs" / "step3"
 32: LOG_DIR.mkdir(parents=True, exist_ok=True)
 33: log_file = LOG_DIR / f"transnet_pytorch_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
 34: 
 35: logging.basicConfig(
 36:     level=logging.INFO,
 37:     format='%(asctime)s - %(levelname)s - %(message)s',
 38:     handlers=[
 39:         logging.FileHandler(log_file, encoding='utf-8'),
 40:         logging.StreamHandler(sys.stdout)
 41:     ]
 42: )
 43: 
 44: # --- Logique de TransNetV2 PyTorch ---
 45: # On s'attend à ce que transnetv2_pytorch.py soit dans le même dossier
 46: try:
 47:     from transnetv2_pytorch import TransNetV2 as TransNetV2_PyTorch_Model
 48: except ImportError:
 49:     logging.critical("ERREUR: Le module 'transnetv2_pytorch.py' n'a pas pu être importé. "
 50:                      "Assurez-vous qu'il est dans le dossier 'workflow_scripts/step3/'.")
 51:     sys.exit(1)
 52: 
 53: 
 54: def get_video_fps(video_path):
 55:     """Retourne toujours 25.0 FPS pour stabiliser les timecodes (Étape 2 force 25 FPS)."""
 56:     return 25.0
 57: 
 58: 
 59: def detect_scenes_with_pytorch(video_path, model, device, threshold=0.5):
 60:     """Détection de scènes avec lecture streaming (chunked) et batching glissant.
 61: 
 62:     - Décodage FFmpeg en streaming (48x27, fps=25) avec run_async.
 63:     - Fenêtre de taille WINDOW_SIZE, pas WINDOW_STRIDE, padding PADDING_FRAMES
 64:       en répétant les frames de bord pour les fenêtres au début/à la fin.
 65:     - Retourne des segments [start_frame, end_frame] basés sur un seuil.
 66:     """
 67:     try:
 68:         process = (
 69:             ffmpeg
 70:             .input(str(video_path))
 71:             .output(
 72:                 'pipe:',
 73:                 format='rawvideo',
 74:                 pix_fmt='rgb24',
 75:                 s='48x27',
 76:                 r=25  # forcer 25 FPS
 77:             )
 78:             .global_args('-threads', str(FFMPEG_THREADS) if FFMPEG_THREADS is not None else '0')
 79:             .run_async(pipe_stdout=True, pipe_stderr=True, quiet=True)
 80:         )
 81: 
 82:         FRAME_H, FRAME_W, FRAME_C = 27, 48, 3
 83:         FRAME_SIZE = FRAME_H * FRAME_W * FRAME_C
 84: 
 85:         def read_n_frames(n):
 86:             """Lit n frames depuis stdout et retourne une liste de np.ndarray shape (27,48,3)."""
 87:             buf = bytearray()
 88:             target = n * FRAME_SIZE
 89:             while len(buf) < target:
 90:                 chunk = process.stdout.read(target - len(buf))
 91:                 if not chunk:
 92:                     break
 93:                 buf.extend(chunk)
 94:             if not buf:
 95:                 return []
 96:             total_bytes = len(buf)
 97:             frames_count = total_bytes // FRAME_SIZE
 98:             if frames_count == 0:
 99:                 return []
100:             arr = np.frombuffer(bytes(buf[:frames_count * FRAME_SIZE]), np.uint8)
101:             return list(arr.reshape([frames_count, FRAME_H, FRAME_W, FRAME_C]))
102: 
103:         frames = []  # tampon de frames décodées
104:         predictions = []
105:         total_batches = 0
106:         batch_count = 0
107: 
108:         with torch.inference_mode():
109:             # Remplir suffisamment pour la première fenêtre (WINDOW_SIZE-PADDING_FRAMES) + PADDING_FRAMES à droite
110:             # On lit au moins WINDOW_SIZE frames réelles pour démarrer
111:             if len(frames) < WINDOW_SIZE:
112:                 frames.extend(read_n_frames(WINDOW_SIZE - len(frames)))
113: 
114:             # Si aucune frame
115:             if len(frames) == 0:
116:                 logging.warning(f"Aucune frame extraite pour {video_path.name}.")
117:                 return []
118: 
119:             # Calculer une estimation du nombre de batches (approx, peut ajuster à la fin)
120:             # Impossible de connaître la longueur totale à l'avance en streaming.
121:             # On loguera la progression basée sur le compteur de batches.
122: 
123:             start_idx = 0
124: 
125:             def build_window(idx, available_right):
126:                 """Construit une fenêtre de longueur WINDOW_SIZE autour de idx avec padding bords.
127: 
128:                 idx est l'index de départ des frames réelles (sans le padding gauche).
129:                 available_right indique combien de frames réelles existent à droite à partir de idx.
130:                 """
131:                 left_needed = PADDING_FRAMES
132:                 right_needed = PADDING_FRAMES
133: 
134:                 # gauche
135:                 if idx >= left_needed:
136:                     left_part = frames[idx - left_needed: idx]
137:                 else:
138:                     pad = [frames[0]] * (left_needed - idx)
139:                     left_part = pad + frames[0:idx]
140: 
141:                 # milieu
142:                 mid_len = WINDOW_SIZE - PADDING_FRAMES - PADDING_FRAMES
143:                 mid_part = frames[idx: idx + mid_len]
144: 
145:                 # droite
146:                 right_have = max(0, available_right - mid_len)
147:                 if right_have >= right_needed:
148:                     right_part = frames[idx + mid_len: idx + mid_len + right_needed]
149:                 else:
150:                     # compléter avec la dernière frame disponible
151:                     base = frames[idx + mid_len - 1] if (idx + mid_len - 1) < len(frames) else frames[-1]
152:                     right_part = frames[idx + mid_len: idx + mid_len + right_have] + [base] * (right_needed - right_have)
153: 
154:                 window = left_part + mid_part + right_part
155:                 return np.asarray(window, dtype=np.uint8)
156: 
157:             while True:
158:                 # S'assurer qu'on a assez de frames pour avancer d'un STRIDE
159:                 to_read = max(0, (start_idx + WINDOW_SIZE) - len(frames))
160:                 if to_read > 0:
161:                     new_frames = read_n_frames(to_read)
162:                     if new_frames:
163:                         frames.extend(new_frames)
164:                     else:
165:                         # fin du flux, on traitera ce qui reste avec padding à droite
166:                         pass
167: 
168:                 if start_idx >= len(frames):
169:                     break
170: 
171:                 # combien de frames réelles dispo à droite de idx
172:                 available_right = max(0, len(frames) - start_idx)
173:                 # Construire la fenêtre avec padding si nécessaire
174:                 window_np = build_window(start_idx, available_right)
175:                 if window_np.shape[0] != WINDOW_SIZE:
176:                     # cas limite si frames < 1
177:                     if len(frames) == 0:
178:                         break
179:                     # compléter strictement à WINDOW_SIZE
180:                     lastf = frames[-1]
181:                     add = WINDOW_SIZE - window_np.shape[0]
182:                     window_np = np.concatenate([window_np, np.repeat(np.expand_dims(lastf, 0), add, axis=0)], axis=0)
183: 
184:                 batch_torch = torch.from_numpy(window_np).unsqueeze(0).to(device, dtype=torch.uint8)
185: 
186:                 if USE_AMP and device.type == 'cuda':
187:                     try:
188:                         with torch.amp.autocast('cuda', dtype=AMP_DTYPE):
189:                             out = model(batch_torch)
190:                     except AttributeError:
191:                         with torch.cuda.amp.autocast(dtype=AMP_DTYPE):
192:                             out = model(batch_torch)
193:                 else:
194:                     out = model(batch_torch)
195: 
196:                 # Normaliser la sortie: prendre le tenseur principal si tuple/list
197:                 single_frame_pred_logits = out[0] if isinstance(out, (tuple, list)) else out
198: 
199:                 pred_slice = torch.sigmoid(single_frame_pred_logits).cpu().numpy()[0, PADDING_FRAMES:WINDOW_SIZE - PADDING_FRAMES, 0]
200:                 predictions.append(pred_slice)
201:                 batch_count += 1
202: 
203:                 # progression (approx)
204:                 if batch_count % 10 == 0:
205:                     logging.info(f"INTERNAL_PROGRESS: {batch_count} batches - {video_path.name}")
206:                     print(f"INTERNAL_PROGRESS: {batch_count} batches - {video_path.name}")
207: 
208:                 # avancer
209:                 start_idx += WINDOW_STRIDE
210: 
211:                 # condition d'arrêt: si on est à la fin et qu'aucune nouvelle frame lue
212:                 if start_idx >= len(frames):
213:                     # tenter de lire plus pour une dernière fenêtre sinon sortir
214:                     more = read_n_frames(WINDOW_STRIDE)
215:                     if more:
216:                         frames.extend(more)
217:                     else:
218:                         break
219: 
220:         # concat predictions and trim to actual frame count
221:         if not predictions:
222:             return [[0, len(frames) - 1]] if len(frames) > 0 else []
223: 
224:         final_predictions = np.concatenate(predictions)[:len(frames)]
225:         shot_boundaries = np.where(final_predictions > threshold)[0]
226: 
227:         if len(shot_boundaries) == 0:
228:             return [[0, len(frames) - 1]] if len(frames) > 0 else []
229: 
230:         # Création des scènes
231:         detected_scenes = []
232:         last_cut = -1
233:         for cut in shot_boundaries:
234:             if cut > last_cut:
235:                 detected_scenes.append([last_cut + 1, cut])
236:             last_cut = cut
237:         if last_cut < len(frames) - 1:
238:             detected_scenes.append([last_cut + 1, len(frames) - 1])
239: 
240:         return detected_scenes
241: 
242:     except Exception as e:
243:         logging.error(f"Erreur lors de la détection de scènes pour {video_path.name}: {e}", exc_info=True)
244:         return None
245: 
246: 
247: def main():
248:     # L'argument `weights_dir` de app_new.py n'est pas utilisé, on utilise un chemin fixe
249:     # pour le fichier .pth pour simplifier.
250:     parser = argparse.ArgumentParser(description="Analyse des transitions vidéo avec TransNetV2 (PyTorch).")
251:     parser.add_argument("--weights_dir", help="Argument ignoré, présent pour compatibilité.")
252:     parser.add_argument("--config", type=str, help="Chemin du fichier de configuration JSON (par défaut: config/step3_transnet.json si présent)")
253:     # Mettre None comme défaut pour permettre au JSON de définir la valeur quand le flag n'est pas fourni
254:     parser.add_argument("--threshold", type=float, default=None, help="Seuil de détection (0-1). Défaut: 0.5")
255:     parser.add_argument("--window", type=int, default=None, help="Taille de fenêtre (frames). Défaut: 100")
256:     parser.add_argument("--stride", type=int, default=None, help="Pas entre fenêtres (frames). Défaut: 50")
257:     parser.add_argument("--padding", type=int, default=None, help="Padding au début/fin (frames). Défaut: 25")
258:     parser.add_argument("--device", choices=["auto", "cpu", "cuda"], default=None, help="Sélection du device. Défaut: auto")
259:     parser.add_argument("--ffmpeg_threads", type=int, default=None, help="Nombre de threads FFmpeg (0 = auto)")
260:     parser.add_argument("--mixed_precision", action="store_true", help="Active l'AMP (CUDA seulement)")
261:     parser.add_argument("--num_workers", type=int, default=None, help="Nombre de workers parallèles (1 par défaut, 1 forcé en CUDA)")
262:     parser.add_argument("--torchscript", action="store_true", help="Active TorchScript (expérimental)")
263:     parser.add_argument("--warmup", action="store_true", help="Effectue un warm-up du modèle avant traitement")
264:     parser.add_argument("--warmup_batches", type=int, default=None, help="Nombre de passes de warm-up (défaut: 1)")
265:     parser.add_argument("--torchscript_auto_fallback", action="store_true", help="En cas d'échec par vidéo avec TorchScript, retenter en Eager")
266:     args = parser.parse_args()
267: 
268:     # Chargement du fichier de configuration JSON (optionnel)
269:     # Ordre de priorité: défauts internes < fichier JSON < CLI
270:     defaults = {
271:         "threshold": 0.5,
272:         "window": 100,
273:         "stride": 50,
274:         "padding": 25,
275:         "device": "auto",
276:         "ffmpeg_threads": 0,
277:         "mixed_precision": False,
278:         "amp_dtype": "float16",
279:         "num_workers": 1,
280:         "torchscript": False,
281:         "warmup": True,
282:         "warmup_batches": 1,
283:         "torchscript_auto_fallback": True,
284:     }
285: 
286:     # Déterminer le chemin de config effectif
287:     config_path = None
288:     if args.config:
289:         config_path = Path(args.config)
290:     else:
291:         candidate = BASE_DIR / "config" / "step3_transnet.json"
292:         if candidate.exists():
293:             config_path = candidate
294: 
295:     file_cfg = {}
296:     if config_path is not None:
297:         try:
298:             with open(config_path, "r", encoding="utf-8") as f:
299:                 file_cfg = json.load(f)
300:             logging.info(f"Configuration chargée depuis: {config_path}")
301:         except Exception as e:
302:             logging.error(f"Impossible de charger le fichier de configuration {config_path}: {e}")
303: 
304:     # Fusion des configurations
305:     effective_cfg = dict(defaults)
306:     effective_cfg.update({k: v for k, v in file_cfg.items() if v is not None})
307: 
308:     # Appliquer les overrides CLI seulement si fournis (None signifie non fourni)
309:     if args.threshold is not None:
310:         effective_cfg["threshold"] = float(args.threshold)
311:     if args.window is not None:
312:         effective_cfg["window"] = int(args.window)
313:     if args.stride is not None:
314:         effective_cfg["stride"] = int(args.stride)
315:     if args.padding is not None:
316:         effective_cfg["padding"] = int(args.padding)
317:     if args.device is not None:
318:         effective_cfg["device"] = args.device
319:     if args.ffmpeg_threads is not None:
320:         effective_cfg["ffmpeg_threads"] = int(args.ffmpeg_threads)
321:     # mixed_precision: le flag CLI l'active si présent, sinon on garde config/défaut
322:     if args.mixed_precision:
323:         effective_cfg["mixed_precision"] = True
324:     if args.num_workers is not None:
325:         effective_cfg["num_workers"] = int(args.num_workers)
326:     if args.torchscript:
327:         effective_cfg["torchscript"] = True
328:     if args.warmup:
329:         effective_cfg["warmup"] = True
330:     if args.warmup_batches is not None:
331:         effective_cfg["warmup_batches"] = int(args.warmup_batches)
332:     if args.torchscript_auto_fallback:
333:         effective_cfg["torchscript_auto_fallback"] = True
334: 
335:     # Propager les hyperparamètres globaux (utilisés dans detect_scenes_with_pytorch)
336:     global WINDOW_SIZE, WINDOW_STRIDE, PADDING_FRAMES, FFMPEG_THREADS, USE_AMP, AMP_DTYPE
337:     WINDOW_SIZE = int(effective_cfg["window"])
338:     WINDOW_STRIDE = int(effective_cfg["stride"])
339:     PADDING_FRAMES = int(effective_cfg["padding"])
340:     FFMPEG_THREADS = int(effective_cfg["ffmpeg_threads"]) if effective_cfg["ffmpeg_threads"] is not None else 0
341:     USE_AMP = bool(effective_cfg["mixed_precision"])
342:     # Convertir amp_dtype string en torch.dtype
343:     if effective_cfg["amp_dtype"] == "bfloat16":
344:         AMP_DTYPE = torch.bfloat16
345:     else:
346:         AMP_DTYPE = torch.float16
347: 
348:     # Log de la configuration effective
349:     logging.info(
350:         "CONFIG_EFFECTIVE: "
351:         f"threshold={effective_cfg['threshold']}, window={WINDOW_SIZE}, stride={WINDOW_STRIDE}, padding={PADDING_FRAMES}, "
352:         f"device={effective_cfg['device']}, ffmpeg_threads={FFMPEG_THREADS}, mixed_precision={USE_AMP}, amp_dtype={effective_cfg['amp_dtype']}, "
353:         f"num_workers={effective_cfg['num_workers']}, torchscript={effective_cfg['torchscript']}, warmup={effective_cfg['warmup']}, warmup_batches={effective_cfg['warmup_batches']}, "
354:         f"torchscript_auto_fallback={effective_cfg['torchscript_auto_fallback']}"
355:     )
356: 
357:     # Chemin vers le modèle PyTorch
358:     pytorch_weights_path = BASE_DIR / "assets" / "transnetv2-pytorch-weights.pth"
359: 
360:     logging.info("--- Démarrage de l'analyse des transitions (PyTorch) ---")
361: 
362:     # Préparer la liste des vidéos
363: 
364:     # Trouver les vidéos à traiter
365:     videos = [p for ext in VIDEO_EXTENSIONS for p in WORK_DIR.rglob(f'*{ext}') if not p.with_suffix('.csv').exists()]
366:     total_videos = len(videos)
367:     logging.info(f"TOTAL_VIDEOS_TO_PROCESS: {total_videos}")
368:     print(f"TOTAL_VIDEOS_TO_PROCESS: {total_videos}")
369: 
370:     if total_videos == 0:
371:         return
372: 
373:     # En CUDA, FORCER 1 worker pour éviter contention GPU (critique)
374:     device_mode = effective_cfg["device"]
375:     if device_mode == "cuda" or (device_mode == "auto" and torch.cuda.is_available()):
376:         if effective_cfg["num_workers"] and effective_cfg["num_workers"] > 1:
377:             logging.warning(f"CUDA mode détecté: limitation forcée des workers de {effective_cfg['num_workers']} à 1 pour éviter la contention GPU.")
378:             effective_cfg["num_workers"] = 1
379: 
380:     # Préparer les tâches
381:     tasks = [(str(p), effective_cfg, str(pytorch_weights_path)) for p in videos]
382: 
383:     successful_count = 0
384:     if effective_cfg["num_workers"] <= 1:
385:         for i, (vpath, cfg, wpath) in enumerate(tasks):
386:             ok = _process_single_video(i, total_videos, vpath, cfg, wpath)
387:             successful_count += 1 if ok else 0
388:     else:
389:         with mp.Pool(processes=effective_cfg["num_workers"]) as pool:
390:             for i, ok in enumerate(pool.imap_unordered(_pool_worker_wrapper, tasks), start=1):
391:                 successful_count += 1 if ok else 0
392: 
393:     logging.info(f"--- Analyse terminée. {successful_count}/{total_videos} réussie(s). ---")
394:     if successful_count < total_videos:
395:         sys.exit(1)
396: 
397: 
398: def _pool_worker_wrapper(task):
399:     """Wrapper compatible Pool pour traiter une vidéo."""
400:     return _process_single_video(None, None, *task)
401: 
402: 
403: def _load_model_for_cfg(device, weights_path, use_torchscript=False):
404:     """Charge le modèle, optionnellement TorchScript, sur le device donné.
405:     
406:     Args:
407:         device: torch.device où charger le modèle
408:         weights_path: chemin vers les poids .pth
409:         use_torchscript: si True, compile avec TorchScript
410:     
411:     Returns:
412:         Le modèle chargé ou None en cas d'échec
413:     """
414:     if not Path(weights_path).exists():
415:         logging.critical(f"Fichier de poids PyTorch non trouvé: {weights_path}")
416:         logging.critical("Téléchargez les poids depuis https://github.com/soCzech/TransNetV2 et placez-les dans assets/")
417:         return None
418:     
419:     try:
420:         model = TransNetV2_PyTorch_Model()
421:         # Charger d'abord en CPU pour éviter les erreurs CUDA au chargement
422:         state = torch.load(str(weights_path), map_location='cpu')
423:         model.load_state_dict(state)
424:         model.eval()
425:         # Puis déplacer sur le device cible
426:         model.to(device)
427:     except RuntimeError as e:
428:         if "CUDA" in str(e):
429:             logging.error(f"Erreur CUDA lors du chargement du modèle: {e}")
430:             logging.warning("Tentative de fallback sur CPU...")
431:             try:
432:                 # Fallback CPU
433:                 cpu_device = torch.device('cpu')
434:                 model = TransNetV2_PyTorch_Model()
435:                 state = torch.load(str(weights_path), map_location='cpu')
436:                 model.load_state_dict(state)
437:                 model.eval().to(cpu_device)
438:                 logging.info("Modèle chargé avec succès sur CPU (fallback)")
439:                 return model
440:             except Exception as cpu_err:
441:                 logging.error(f"Échec du fallback CPU: {cpu_err}")
442:                 return None
443:         else:
444:             logging.error(f"Erreur lors du chargement du modèle: {e}", exc_info=True)
445:             return None
446:     except Exception as e:
447:         logging.error(f"Erreur inattendue lors du chargement du modèle: {e}", exc_info=True)
448:         return None
449: 
450:     if device.type == 'cuda':
451:         torch.backends.cudnn.benchmark = True
452: 
453:     if use_torchscript:
454:         try:
455:             class InferenceWrapper(nn.Module):
456:                 def __init__(self, base: nn.Module):
457:                     super().__init__()
458:                     self.base = base
459: 
460:                 def forward(self, x: torch.Tensor) -> torch.Tensor:
461:                     out = self.base(x)
462:                     # Le modèle peut retourner (tensor, {"many_hot": tensor})
463:                     if isinstance(out, (tuple, list)):
464:                         return out[0]
465:                     return out
466: 
467:             wrapper = InferenceWrapper(model).to(device).eval()
468:             example = torch.zeros((1, WINDOW_SIZE, 27, 48, 3), dtype=torch.uint8, device=device)
469:             scripted = torch.jit.trace(wrapper, example)
470:             scripted = torch.jit.freeze(scripted)
471:             logging.info("TorchScript activé via wrapper pour TransNetV2 (sortie tensor-only).")
472:             return scripted
473:         except Exception as e:
474:             logging.warning(f"TorchScript a échoué, fallback modèle Eager: {e}")
475:             return model
476:     return model
477: 
478: 
479: def _process_single_video(idx, total, video_path_str, cfg, weights_path_str):
480:     """Traite une seule vidéo: charge le modèle, warm-up, détecte, écrit CSV.
481: 
482:     Args:
483:       idx: index (peut être None en pool)
484:       total: total de vidéos (peut être None en pool)
485:       video_path_str: chemin de la vidéo
486:       cfg: dict de configuration effectif
487:       weights_path_str: chemin vers les poids du modèle
488:     Returns: bool succès
489:     """
490:     try:
491:         video_path = Path(video_path_str)
492:         # Propager globals dans le worker
493:         global WINDOW_SIZE, WINDOW_STRIDE, PADDING_FRAMES, FFMPEG_THREADS, USE_AMP, AMP_DTYPE
494:         WINDOW_SIZE = int(cfg["window"])
495:         WINDOW_STRIDE = int(cfg["stride"])
496:         PADDING_FRAMES = int(cfg["padding"])
497:         FFMPEG_THREADS = int(cfg["ffmpeg_threads"]) if cfg["ffmpeg_threads"] is not None else 0
498:         USE_AMP = bool(cfg["mixed_precision"])
499:         AMP_DTYPE = torch.bfloat16 if cfg.get("amp_dtype") == "bfloat16" else torch.float16
500: 
501:         # Device selection avec fallback intelligent
502:         if cfg["device"] == "cpu":
503:             device = torch.device("cpu")
504:             logging.info(f"Device sélectionné: CPU (forcé par config)")
505:         elif cfg["device"] == "cuda":
506:             if torch.cuda.is_available():
507:                 device = torch.device("cuda")
508:                 logging.info(f"Device sélectionné: CUDA (GPU disponible)")
509:             else:
510:                 device = torch.device("cpu")
511:                 logging.warning("CUDA demandé mais non disponible, fallback sur CPU")
512:         else:  # auto
513:             if torch.cuda.is_available():
514:                 device = torch.device("cuda")
515:                 logging.info(f"Device sélectionné: CUDA (auto-détection)")
516:             else:
517:                 device = torch.device("cpu")
518:                 logging.info(f"Device sélectionné: CPU (auto-détection)")
519: 
520:         if idx is not None and total is not None:
521:             logging.info(f"PROCESSING_VIDEO: {idx + 1}/{total}: {video_path.name}")
522:             print(f"PROCESSING_VIDEO: {idx + 1}/{total}: {video_path.name}")
523:         else:
524:             logging.info(f"PROCESSING_VIDEO: {video_path.name}")
525: 
526:         # Charger modèle par worker
527:         use_ts = bool(cfg.get("torchscript"))
528:         model = _load_model_for_cfg(device, weights_path_str, use_torchscript=use_ts)
529:         if model is None:
530:             return False
531: 
532:         # Warm-up optionnel
533:         if bool(cfg.get("warmup", True)):
534:             warm_batches = int(cfg.get("warmup_batches", 1))
535:             for _ in range(max(1, warm_batches)):
536:                 dummy = torch.zeros((1, WINDOW_SIZE, 27, 48, 3), dtype=torch.uint8, device=device)
537:                 if USE_AMP and device.type == 'cuda':
538:                     try:
539:                         with torch.amp.autocast('cuda', dtype=AMP_DTYPE):
540:                             _ = model(dummy)
541:                     except AttributeError:
542:                         with torch.cuda.amp.autocast(dtype=AMP_DTYPE):
543:                             _ = model(dummy)
544:                 else:
545:                     _ = model(dummy)
546: 
547:         # Détection avec fallback multi-niveaux
548:         scenes = detect_scenes_with_pytorch(video_path, model, device, threshold=float(cfg["threshold"]))
549:         
550:         # Niveau 1: Si TorchScript activé et échec, retenter en Eager
551:         if scenes is None and use_ts and bool(cfg.get("torchscript_auto_fallback", True)):
552:             logging.warning(f"TorchScript a échoué pour {video_path.name}, tentative de fallback Eager...")
553:             model = _load_model_for_cfg(device, weights_path_str, use_torchscript=False)
554:             if model is None:
555:                 return False
556:             scenes = detect_scenes_with_pytorch(video_path, model, device, threshold=float(cfg["threshold"]))
557:         
558:         # Niveau 2: Si CUDA et échec, retenter en CPU
559:         if scenes is None and device.type == 'cuda':
560:             logging.warning(f"Erreur avec CUDA pour {video_path.name}, tentative de fallback CPU...")
561:             cpu_device = torch.device('cpu')
562:             model = _load_model_for_cfg(cpu_device, weights_path_str, use_torchscript=False)
563:             if model is None:
564:                 logging.error(f"Échec du fallback CPU pour {video_path.name}")
565:                 return False
566:             scenes = detect_scenes_with_pytorch(video_path, model, cpu_device, threshold=float(cfg["threshold"]))
567:             if scenes is None:
568:                 logging.error(f"Échec définitif du traitement de {video_path.name} (CUDA et CPU)")
569:                 return False
570:             logging.info(f"Succès avec fallback CPU pour {video_path.name}")
571:         
572:         if scenes is None:
573:             logging.error(f"Échec du traitement de {video_path.name}")
574:             return False
575: 
576:         # Écriture CSV
577:         output_csv_path = video_path.with_suffix('.csv')
578:         fps = get_video_fps(video_path)
579:         with open(output_csv_path, 'w', newline='', encoding='utf-8') as f:
580:             writer = csv.writer(f)
581:             writer.writerow(['No', 'Timecode In', 'Timecode Out', 'Frame In', 'Frame Out'])
582:             for j, (start, end) in enumerate(scenes):
583:                 start_frame = int(start)
584:                 end_frame = int(end)
585:                 timecode_in = FrameTimecode(start_frame, fps)
586:                 timecode_out = FrameTimecode(end_frame, fps)
587:                 writer.writerow([
588:                     j + 1,
589:                     timecode_in.get_timecode(),
590:                     timecode_out.get_timecode(),
591:                     start_frame + 1,
592:                     end_frame + 1
593:                 ])
594:         logging.info(f"Succès: {output_csv_path.name} créé avec {len(scenes)} scènes.")
595:         return True
596:     except Exception as e:
597:         logging.error(f"Erreur worker pour {video_path_str}: {e}", exc_info=True)
598:         return False
599: 
600: 
601: if __name__ == "__main__":
602:     main()
```

## File: workflow_scripts/step3/transnetv2_pytorch.py
```python
  1: """
  2: This work was downloaded from https://github.com/soCzech/TransNetV2
  3: All credit to https://github.com/soCzech/
  4: """
  5: 
  6: import torch
  7: import torch.nn as nn
  8: import torch.nn.functional as functional
  9: 
 10: import random
 11: 
 12: 
 13: class TransNetV2(nn.Module):
 14: 
 15:     def __init__(self,
 16:                  F=16, L=3, S=2, D=1024,
 17:                  use_many_hot_targets=True,
 18:                  use_frame_similarity=True,
 19:                  use_color_histograms=True,
 20:                  use_mean_pooling=False,
 21:                  dropout_rate=0.5,
 22:                  use_convex_comb_reg=False,
 23:                  use_resnet_features=False,
 24:                  use_resnet_like_top=False,
 25:                  frame_similarity_on_last_layer=False):
 26:         super(TransNetV2, self).__init__()
 27: 
 28:         if use_resnet_features or use_resnet_like_top or use_convex_comb_reg or frame_similarity_on_last_layer:
 29:             raise NotImplemented("Some options not implemented in Pytorch version of Transnet!")
 30: 
 31:         self.SDDCNN = nn.ModuleList(
 32:             [StackedDDCNNV2(in_filters=3, n_blocks=S, filters=F, stochastic_depth_drop_prob=0.)] +
 33:             [StackedDDCNNV2(in_filters=(F * 2 ** (i - 1)) * 4, n_blocks=S, filters=F * 2 ** i) for i in range(1, L)]
 34:         )
 35: 
 36:         self.frame_sim_layer = FrameSimilarity(
 37:             sum([(F * 2 ** i) * 4 for i in range(L)]), lookup_window=101, output_dim=128, similarity_dim=128, use_bias=True
 38:         ) if use_frame_similarity else None
 39:         self.color_hist_layer = ColorHistograms(
 40:             lookup_window=101, output_dim=128
 41:         ) if use_color_histograms else None
 42: 
 43:         self.dropout = nn.Dropout(dropout_rate) if dropout_rate is not None else None
 44: 
 45:         output_dim = ((F * 2 ** (L - 1)) * 4) * 3 * 6  # 3x6 for spatial dimensions
 46:         if use_frame_similarity: output_dim += 128
 47:         if use_color_histograms: output_dim += 128
 48: 
 49:         self.fc1 = nn.Linear(output_dim, D)
 50:         self.cls_layer1 = nn.Linear(D, 1)
 51:         self.cls_layer2 = nn.Linear(D, 1) if use_many_hot_targets else None
 52: 
 53:         self.use_mean_pooling = use_mean_pooling
 54:         self.eval()
 55: 
 56:     def forward(self, inputs):
 57:         if not isinstance(inputs, torch.Tensor):
 58:             raise TypeError(f"Expected torch.Tensor, got {type(inputs)}")
 59:         if inputs.dtype != torch.uint8:
 60:             raise TypeError(f"Expected dtype torch.uint8, got {inputs.dtype}")
 61:         if inputs.dim() != 5 or inputs.shape[2] != 27 or inputs.shape[3] != 48 or inputs.shape[4] != 3:
 62:             raise ValueError(f"Expected shape [B, T, 27, 48, 3], got {inputs.shape}")
 63:         
 64:         x = inputs.permute([0, 4, 1, 2, 3]).float()
 65:         x = x.div_(255.)
 66: 
 67:         block_features = []
 68:         for block in self.SDDCNN:
 69:             x = block(x)
 70:             block_features.append(x)
 71: 
 72:         if self.use_mean_pooling:
 73:             x = torch.mean(x, dim=[3, 4])
 74:             x = x.permute(0, 2, 1)
 75:         else:
 76:             x = x.permute(0, 2, 3, 4, 1)
 77:             x = x.reshape(x.shape[0], x.shape[1], -1)
 78: 
 79:         if self.frame_sim_layer is not None:
 80:             x = torch.cat([self.frame_sim_layer(block_features), x], 2)
 81: 
 82:         if self.color_hist_layer is not None:
 83:             x = torch.cat([self.color_hist_layer(inputs), x], 2)
 84: 
 85:         x = self.fc1(x)
 86:         x = functional.relu(x)
 87: 
 88:         if self.dropout is not None:
 89:             x = self.dropout(x)
 90: 
 91:         one_hot = self.cls_layer1(x)
 92: 
 93:         if self.cls_layer2 is not None:
 94:             return one_hot, {"many_hot": self.cls_layer2(x)}
 95: 
 96:         return one_hot
 97: 
 98: 
 99: class StackedDDCNNV2(nn.Module):
100: 
101:     def __init__(self,
102:                  in_filters,
103:                  n_blocks,
104:                  filters,
105:                  shortcut=True,
106:                  use_octave_conv=False,
107:                  pool_type="avg",
108:                  stochastic_depth_drop_prob=0.0):
109:         super(StackedDDCNNV2, self).__init__()
110: 
111:         if use_octave_conv:
112:             raise NotImplemented("Octave convolution not implemented in Pytorch version of Transnet!")
113: 
114:         assert pool_type == "max" or pool_type == "avg"
115:         if use_octave_conv and pool_type == "max":
116:             print("WARN: Octave convolution was designed with average pooling, not max pooling.")
117: 
118:         self.shortcut = shortcut
119:         self.DDCNN = nn.ModuleList([
120:             DilatedDCNNV2(in_filters if i == 1 else filters * 4, filters, octave_conv=use_octave_conv,
121:                           activation=functional.relu if i != n_blocks else None) for i in range(1, n_blocks + 1)
122:         ])
123:         self.pool = nn.MaxPool3d(kernel_size=(1, 2, 2)) if pool_type == "max" else nn.AvgPool3d(kernel_size=(1, 2, 2))
124:         self.stochastic_depth_drop_prob = stochastic_depth_drop_prob
125: 
126:     def forward(self, inputs):
127:         x = inputs
128:         shortcut = None
129: 
130:         for block in self.DDCNN:
131:             x = block(x)
132:             if shortcut is None:
133:                 shortcut = x
134: 
135:         x = functional.relu(x)
136: 
137:         if self.shortcut is not None:
138:             if self.stochastic_depth_drop_prob != 0.:
139:                 if self.training:
140:                     if random.random() < self.stochastic_depth_drop_prob:
141:                         x = shortcut
142:                     else:
143:                         x = x + shortcut
144:                 else:
145:                     x = (1 - self.stochastic_depth_drop_prob) * x + shortcut
146:             else:
147:                 x += shortcut
148: 
149:         x = self.pool(x)
150:         return x
151: 
152: 
153: class DilatedDCNNV2(nn.Module):
154: 
155:     def __init__(self,
156:                  in_filters,
157:                  filters,
158:                  batch_norm=True,
159:                  activation=None,
160:                  octave_conv=False):
161:         super(DilatedDCNNV2, self).__init__()
162: 
163:         if octave_conv:
164:             raise NotImplemented("Octave convolution not implemented in Pytorch version of Transnet!")
165: 
166:         assert not (octave_conv and batch_norm)
167: 
168:         self.Conv3D_1 = Conv3DConfigurable(in_filters, filters, 1, use_bias=not batch_norm)
169:         self.Conv3D_2 = Conv3DConfigurable(in_filters, filters, 2, use_bias=not batch_norm)
170:         self.Conv3D_4 = Conv3DConfigurable(in_filters, filters, 4, use_bias=not batch_norm)
171:         self.Conv3D_8 = Conv3DConfigurable(in_filters, filters, 8, use_bias=not batch_norm)
172: 
173:         self.bn = nn.BatchNorm3d(filters * 4, eps=1e-3) if batch_norm else None
174:         self.activation = activation
175: 
176:     def forward(self, inputs):
177:         conv1 = self.Conv3D_1(inputs)
178:         conv2 = self.Conv3D_2(inputs)
179:         conv3 = self.Conv3D_4(inputs)
180:         conv4 = self.Conv3D_8(inputs)
181: 
182:         x = torch.cat([conv1, conv2, conv3, conv4], dim=1)
183: 
184:         if self.bn is not None:
185:             x = self.bn(x)
186: 
187:         if self.activation is not None:
188:             x = self.activation(x)
189: 
190:         return x
191: 
192: 
193: class Conv3DConfigurable(nn.Module):
194: 
195:     def __init__(self,
196:                  in_filters,
197:                  filters,
198:                  dilation_rate,
199:                  separable=True,
200:                  octave=False,
201:                  use_bias=True,
202:                  kernel_initializer=None):  # not supported
203:         super(Conv3DConfigurable, self).__init__()
204: 
205:         if octave:
206:             raise NotImplemented("Octave convolution not implemented in Pytorch version of Transnet!")
207:         if kernel_initializer is not None:
208:             raise NotImplemented("Kernel initializers are not implemented in Pytorch version of Transnet!")
209: 
210:         assert not (separable and octave)
211: 
212:         if separable:
213:             # (2+1)D convolution https://arxiv.org/pdf/1711.11248.pdf
214:             conv1 = nn.Conv3d(in_filters, 2 * filters, kernel_size=(1, 3, 3),
215:                               dilation=(1, 1, 1), padding=(0, 1, 1), bias=False)
216:             conv2 = nn.Conv3d(2 * filters, filters, kernel_size=(3, 1, 1),
217:                               dilation=(dilation_rate, 1, 1), padding=(dilation_rate, 0, 0), bias=use_bias)
218:             self.layers = nn.ModuleList([conv1, conv2])
219:         else:
220:             conv = nn.Conv3d(in_filters, filters, kernel_size=3,
221:                              dilation=(dilation_rate, 1, 1), padding=(dilation_rate, 1, 1), bias=use_bias)
222:             self.layers = nn.ModuleList([conv])
223: 
224:     def forward(self, inputs):
225:         x = inputs
226:         for layer in self.layers:
227:             x = layer(x)
228:         return x
229: 
230: 
231: class FrameSimilarity(nn.Module):
232: 
233:     def __init__(self,
234:                  in_filters,
235:                  similarity_dim=128,
236:                  lookup_window=101,
237:                  output_dim=128,
238:                  stop_gradient=False,
239:                  use_bias=False):
240:         super(FrameSimilarity, self).__init__()
241: 
242:         if stop_gradient:
243:             raise NotImplemented("Stop gradient not implemented in Pytorch version of Transnet!")
244: 
245:         self.projection = nn.Linear(in_filters, similarity_dim, bias=use_bias)
246:         self.fc = nn.Linear(lookup_window, output_dim)
247: 
248:         self.lookup_window = lookup_window
249:         assert lookup_window % 2 == 1, "`lookup_window` must be odd integer"
250: 
251:     def forward(self, inputs):
252:         x = torch.cat([torch.mean(x, dim=[3, 4]) for x in inputs], dim=1)
253:         x = torch.transpose(x, 1, 2)
254: 
255:         x = self.projection(x)
256:         x = functional.normalize(x, p=2, dim=2)
257: 
258:         batch_size, time_window = x.shape[0], x.shape[1]
259:         similarities = torch.bmm(x, x.transpose(1, 2))  # [batch_size, time_window, time_window]
260:         similarities_padded = functional.pad(similarities, [(self.lookup_window - 1) // 2, (self.lookup_window - 1) // 2])
261: 
262:         batch_indices = torch.arange(0, batch_size, device=x.device).view([batch_size, 1, 1]).repeat(
263:             [1, time_window, self.lookup_window])
264:         time_indices = torch.arange(0, time_window, device=x.device).view([1, time_window, 1]).repeat(
265:             [batch_size, 1, self.lookup_window])
266:         lookup_indices = torch.arange(0, self.lookup_window, device=x.device).view([1, 1, self.lookup_window]).repeat(
267:             [batch_size, time_window, 1]) + time_indices
268: 
269:         similarities = similarities_padded[batch_indices, time_indices, lookup_indices]
270:         return functional.relu(self.fc(similarities))
271: 
272: 
273: class ColorHistograms(nn.Module):
274: 
275:     def __init__(self,
276:                  lookup_window=101,
277:                  output_dim=None):
278:         super(ColorHistograms, self).__init__()
279: 
280:         self.fc = nn.Linear(lookup_window, output_dim) if output_dim is not None else None
281:         self.lookup_window = lookup_window
282:         assert lookup_window % 2 == 1, "`lookup_window` must be odd integer"
283: 
284:     @staticmethod
285:     def compute_color_histograms(frames):
286:         frames = frames.int()
287: 
288:         def get_bin(frames):
289:             R, G, B = frames[:, :, 0], frames[:, :, 1], frames[:, :, 2]
290:             R, G, B = R >> 5, G >> 5, B >> 5
291:             return (R << 6) + (G << 3) + B
292: 
293:         batch_size, time_window, height, width, no_channels = frames.shape
294:         if no_channels != 3:
295:             raise ValueError(f"Expected 3 color channels, got {no_channels}")
296:         frames_flatten = frames.view(batch_size * time_window, height * width, 3)
297: 
298:         binned_values = get_bin(frames_flatten)
299:         frame_bin_prefix = (torch.arange(0, batch_size * time_window, device=frames.device) << 9).view(-1, 1)
300:         binned_values = (binned_values + frame_bin_prefix).view(-1)
301: 
302:         histograms = torch.zeros(batch_size * time_window * 512, dtype=torch.int32, device=frames.device)
303:         histograms.scatter_add_(0, binned_values, torch.ones(binned_values.shape[0], dtype=torch.int32, device=frames.device))
304: 
305:         histograms = histograms.view(batch_size, time_window, 512).float()
306:         histograms_normalized = functional.normalize(histograms, p=2, dim=2)
307:         return histograms_normalized
308: 
309:     def forward(self, inputs):
310:         x = self.compute_color_histograms(inputs)
311: 
312:         batch_size, time_window = x.shape[0], x.shape[1]
313:         similarities = torch.bmm(x, x.transpose(1, 2))  # [batch_size, time_window, time_window]
314:         similarities_padded = functional.pad(similarities, [(self.lookup_window - 1) // 2, (self.lookup_window - 1) // 2])
315: 
316:         batch_indices = torch.arange(0, batch_size, device=x.device).view([batch_size, 1, 1]).repeat(
317:             [1, time_window, self.lookup_window])
318:         time_indices = torch.arange(0, time_window, device=x.device).view([1, time_window, 1]).repeat(
319:             [batch_size, 1, self.lookup_window])
320:         lookup_indices = torch.arange(0, self.lookup_window, device=x.device).view([1, 1, self.lookup_window]).repeat(
321:             [batch_size, time_window, 1]) + time_indices
322: 
323:         similarities = similarities_padded[batch_indices, time_indices, lookup_indices]
324: 
325:         if self.fc is not None:
326:             return functional.relu(self.fc(similarities))
327:         return similarities
```

## File: workflow_scripts/step4/run_audio_analysis_lemonfox.py
```python
  1: #!/usr/bin/env python3
  2: # -*- coding: utf-8 -*-
  3: 
  4: import argparse
  5: import importlib.util
  6: import logging
  7: import os
  8: import subprocess
  9: import sys
 10: from datetime import datetime
 11: from pathlib import Path
 12: 
 13: 
 14: def _configure_file_logger(log_dir: Path) -> None:
 15:     log_dir.mkdir(parents=True, exist_ok=True)
 16:     log_file = log_dir / f"audio_analysis_lemonfox_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
 17:     file_handler = logging.FileHandler(log_file, encoding='utf-8')
 18:     file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
 19:     logging.getLogger().addHandler(file_handler)
 20: 
 21: 
 22: def _find_videos_for_audio_analysis(work_dir: Path) -> list[Path]:
 23:     video_extensions = ('.mp4', '.mov', '.avi', '.mkv', '.webm')
 24:     videos_to_process: list[Path] = []
 25: 
 26:     all_videos = [p for ext in video_extensions for p in work_dir.rglob(f'*{ext}')]
 27: 
 28:     for video_path in all_videos:
 29:         if video_path.suffix.lower() == '.mov':
 30:             continue
 31: 
 32:         output_json_path = video_path.with_name(f"{video_path.stem}_audio.json")
 33:         if not output_json_path.exists():
 34:             videos_to_process.append(video_path)
 35: 
 36:     return videos_to_process
 37: 
 38: 
 39: def _resolve_project_and_video_name(work_dir: Path, video_path: Path) -> tuple[str, str]:
 40:     rel = video_path.relative_to(work_dir)
 41:     parts = rel.parts
 42:     if len(parts) < 2:
 43:         raise ValueError(f"Video path is not inside a project folder: {rel}")
 44:     project_name = parts[0]
 45:     video_name = str(Path(*parts[1:]))
 46:     return project_name, video_name
 47: 
 48: 
 49: def _ensure_repo_root_on_sys_path() -> None:
 50:     repo_root = Path(__file__).resolve().parents[2]
 51:     repo_root_str = str(repo_root)
 52:     if repo_root_str not in sys.path:
 53:         sys.path.insert(0, repo_root_str)
 54: 
 55: 
 56: def _import_lemonfox_audio_service():
 57:     _ensure_repo_root_on_sys_path()
 58: 
 59:     repo_root = Path(__file__).resolve().parents[2]
 60:     service_path = repo_root / "services" / "lemonfox_audio_service.py"
 61:     spec = importlib.util.spec_from_file_location("lemonfox_audio_service", service_path)
 62:     if spec is None or spec.loader is None:
 63:         raise ImportError(f"Unable to load Lemonfox service module from {service_path}")
 64:     module = importlib.util.module_from_spec(spec)
 65:     spec.loader.exec_module(module)
 66:     return module.LemonfoxAudioService
 67: 
 68: 
 69: def _run_pyannote_fallback(log_dir: Path) -> int:
 70:     repo_root = Path(__file__).resolve().parents[2]
 71:     original_script = repo_root / "workflow_scripts" / "step4" / "run_audio_analysis.py"
 72: 
 73:     cmd = [
 74:         sys.executable,
 75:         str(original_script),
 76:         "--log_dir",
 77:         str(log_dir),
 78:     ]
 79: 
 80:     logging.error("Fallback STEP4: exécution de la méthode originale (Pyannote) suite à une erreur Lemonfox")
 81:     completed = subprocess.run(cmd)
 82:     return int(completed.returncode)
 83: 
 84: 
 85: def main() -> int:
 86:     parser = argparse.ArgumentParser()
 87:     parser.add_argument("--log_dir", type=str, required=True)
 88:     args = parser.parse_args()
 89: 
 90:     logging.basicConfig(
 91:         level=logging.INFO,
 92:         format='%(asctime)s - %(levelname)s - %(message)s',
 93:         handlers=[logging.StreamHandler(sys.stdout)]
 94:     )
 95: 
 96:     log_dir = Path(args.log_dir)
 97:     _configure_file_logger(log_dir)
 98: 
 99:     work_dir = Path(os.getcwd())
100: 
101:     try:
102:         lemonfox_service = _import_lemonfox_audio_service()
103:     except Exception as e:
104:         logging.error(f"Impossible d'importer LemonfoxAudioService: {e}")
105:         return _run_pyannote_fallback(log_dir)
106: 
107:     videos_to_process = _find_videos_for_audio_analysis(work_dir)
108:     logging.info(f"TOTAL_AUDIO_TO_ANALYZE: {len(videos_to_process)}")
109: 
110:     if not videos_to_process:
111:         logging.info("Aucune vidéo à analyser (tous les _audio.json existent déjà).")
112:         return 0
113: 
114:     for idx, video_path in enumerate(videos_to_process, start=1):
115:         logging.info(f"ANALYZING_AUDIO: {idx}/{len(videos_to_process)}: {video_path.name}")
116: 
117:         try:
118:             project_name, video_name = _resolve_project_and_video_name(work_dir, video_path)
119:         except Exception as e:
120:             logging.error(f"Erreur résolution projet/vidéo pour {video_path}: {e}")
121:             return _run_pyannote_fallback(log_dir)
122: 
123:         try:
124:             duration_sec = lemonfox_service._get_video_duration_ffprobe(video_path)
125:             fps = 25.0
126:             total_frames = int(round((duration_sec or 0.0) * fps))
127:             if total_frames > 0:
128:                 logging.info(f"INTERNAL_PROGRESS: 0/{total_frames} frames (0%) - Lemonfox API call")
129: 
130:             result = lemonfox_service.process_video_with_lemonfox(
131:                 project_name=project_name,
132:                 video_name=video_name,
133:             )
134: 
135:             if not result.success:
136:                 logging.error(f"Erreur Lemonfox pour {video_path.name}: {result.error}")
137:                 return _run_pyannote_fallback(log_dir)
138: 
139:             if result.total_frames > 0:
140:                 logging.info(
141:                     f"INTERNAL_PROGRESS: {result.total_frames}/{result.total_frames} frames (100%) - Lemonfox done"
142:                 )
143: 
144:             logging.info(f"Succès: analyse audio terminée pour {video_path.name}")
145: 
146:         except Exception as e:
147:             logging.error(f"Erreur inattendue Lemonfox pour {video_path.name}: {e}", exc_info=True)
148:             return _run_pyannote_fallback(log_dir)
149: 
150:     return 0
151: 
152: 
153: if __name__ == "__main__":
154:     raise SystemExit(main())
```

## File: workflow_scripts/step4/run_audio_analysis.py
```python
  1: #!/usr/bin/env python3
  2: # -*- coding: utf-8 -*-
  3: 
  4: """
  5: Script d'analyse audio (diarisation) avec Pyannote.audio
  6: Version Ubuntu - Étape 4
  7: 
  8: Optimisations clés:
  9: - Extraction audio via ffmpeg (remplace MoviePy) vers tmpfs si disponible
 10: - Suppression d'OpenCV/MoviePy pour les métadonnées (utilisation ffprobe + fallback FPS=25)
 11: - Écriture JSON en streaming (évite le stockage complet en mémoire)
 12: - Mapping segments->frames sans matérialiser toute la diarisation
 13: - Journalisation unique (suppression des prints dupliqués)
 14: - Inference PyTorch optimisée (CUDA prioritaire, CPU fallback; no_grad/inference_mode)
 15: - Politique device/workers configurable via variables d'environnement
 16: - Nettoyage robuste des répertoires temporaires
 17: - Compression gzip optionnelle (désactivée par défaut pour compatibilité STEP5)
 18: """
 19: 
 20: import os
 21: import sys
 22: import json
 23: import argparse
 24: import logging
 25: import subprocess
 26: import tempfile
 27: import gzip
 28: import time
 29: import gc
 30: from contextlib import nullcontext
 31: import torch
 32: from pathlib import Path
 33: from datetime import datetime
 34: 
 35: # --- Configuration ---
 36: WORK_DIR = Path(os.getcwd())
 37: VIDEO_EXTENSIONS = ('.mp4', '.mov', '.avi', '.mkv', '.webm')
 38: OUTPUT_SUFFIX = "_audio.json"
 39: DEFAULT_FPS = 25
 40: 
 41: LOG_DIR_PATH = None
 42: 
 43: 
 44: def _load_optimal_tv_config() -> dict:
 45:     try:
 46:         repo_root = Path(__file__).resolve().parents[2]
 47:         config_path = repo_root / "config" / "optimal_tv_config.json"
 48:         if not config_path.exists():
 49:             return {}
 50:         with open(config_path, "r", encoding="utf-8") as f:
 51:             data = json.load(f)
 52:         if not isinstance(data, dict):
 53:             logging.warning("optimal_tv_config.json ignoré (JSON racine non-objet).")
 54:             return {}
 55:         logging.info(f"optimal_tv_config.json chargé: {config_path}")
 56:         return data
 57:     except Exception as e:
 58:         logging.warning(f"Impossible de charger optimal_tv_config.json: {e}")
 59:         return {}
 60: 
 61: # Le dossier de log est AUDIO_ANALYSIS_LOG_DIR, passé par argument
 62: # On configure un logger de base qui sera complété dans main()
 63: logging.basicConfig(
 64:     level=logging.INFO,
 65:     format='%(asctime)s - %(levelname)s - %(message)s',
 66:     handlers=[logging.StreamHandler(sys.stdout)]
 67: )
 68: 
 69: 
 70: def find_videos_for_audio_analysis():
 71:     """Trouve toutes les vidéos à analyser qui n'ont pas encore de fichier _audio.json."""
 72:     videos_to_process = []
 73:     logging.info(f"Recherche de vidéos dans {WORK_DIR}...")
 74: 
 75:     all_videos = [p for ext in VIDEO_EXTENSIONS for p in WORK_DIR.rglob(f'*{ext}')]
 76: 
 77:     skipped_mov = 0
 78:     filtered_videos = []
 79:     for video_path in all_videos:
 80:         if video_path.suffix.lower() == '.mov':
 81:             skipped_mov += 1
 82:             continue
 83:         filtered_videos.append(video_path)
 84: 
 85:     for video_path in filtered_videos:
 86:         output_json_path = video_path.with_name(f"{video_path.stem}{OUTPUT_SUFFIX}")
 87:         if not output_json_path.exists():
 88:             videos_to_process.append(video_path)
 89:     
 90:     return videos_to_process
 91: 
 92: 
 93: def _run_ffprobe_duration(video_path: Path) -> float:
 94:     """Retourne la durée (en secondes) via ffprobe, ou -1 en cas d'échec."""
 95:     try:
 96:         result = subprocess.run(
 97:             [
 98:                 "ffprobe", "-v", "error", "-show_entries", "format=duration",
 99:                 "-of", "default=nw=1:nk=1", str(video_path)
100:             ],
101:             stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True, text=True
102:         )
103:         return float(result.stdout.strip())
104:     except Exception as e:
105:         logging.warning(f"ffprobe a échoué pour {video_path.name}: {e}")
106:         return -1.0
107: 
108: 
109: def _extract_audio_ffmpeg(input_video: Path, output_wav: Path) -> bool:
110:     """Extrait l'audio en WAV mono 16kHz via ffmpeg. Retourne True si OK."""
111:     try:
112:         cmd = [
113:             "ffmpeg", "-y", "-hide_banner", "-loglevel", "error",
114:             "-i", str(input_video),
115:             "-vn", "-ac", "1", "-ar", "16000", "-f", "wav", "-acodec", "pcm_s16le",
116:             str(output_wav)
117:         ]
118:         subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
119:         return True
120:     except subprocess.CalledProcessError as e:
121:         logging.error(f"ffmpeg extraction audio a échoué pour {input_video.name}: {e}")
122:         return False
123: 
124: 
125: def _write_empty_audio_json_streaming(output_json_path: Path, video_name: str, total_frames: int, fps: float) -> None:
126:     """Écrit un JSON vide compatible STEP5 en streaming."""
127:     with open(output_json_path, 'w', encoding='utf-8') as f:
128:         f.write('{\n')
129:         f.write(f'  "video_filename": "{video_name}",\n')
130:         f.write(f'  "total_frames": {total_frames},\n')
131:         f.write(f'  "fps": {round(fps, 2)},\n')
132:         f.write('  "frames_analysis": [')
133:         if total_frames > 0:
134:             f.write('\n')
135:             for frame_num in range(1, total_frames + 1):
136:                 timecode = round((frame_num - 1) / fps, 3)
137:                 obj = {
138:                     "frame": frame_num,
139:                     "audio_info": {
140:                         "is_speech_present": False,
141:                         "num_distinct_speakers_audio": 0,
142:                         "active_speaker_labels": [],
143:                         "timecode_sec": timecode,
144:                     },
145:                 }
146:                 if frame_num > 1:
147:                     f.write(',\n')
148:                 f.write(json.dumps(obj))
149:         f.write('\n  ]\n')
150:         f.write('}\n')
151: 
152: 
153: def _cleanup_cuda_memory() -> None:
154:     try:
155:         if torch.cuda.is_available():
156:             torch.cuda.empty_cache()
157:     except Exception:
158:         pass
159:     try:
160:         gc.collect()
161:     except Exception:
162:         pass
163: 
164: 
165: def _get_total_vram_gb() -> float:
166:     try:
167:         if not torch.cuda.is_available():
168:             return 0.0
169:         props = torch.cuda.get_device_properties(0)
170:         return float(props.total_memory) / (1024**3)
171:     except Exception:
172:         return 0.0
173: 
174: 
175: def _is_low_vram_gpu(threshold_gb: float = 6.0) -> bool:
176:     total_gb = _get_total_vram_gb()
177:     return total_gb > 0.0 and total_gb <= threshold_gb
178: 
179: 
180: def _should_enable_amp(device: str) -> bool:
181:     if device != "cuda":
182:         return False
183:     env_value = os.getenv("AUDIO_ENABLE_AMP")
184:     if env_value is not None:
185:         return env_value == "1"
186:     return _is_low_vram_gpu()
187: 
188: 
189: def _get_pyannote_batch_size(device: str) -> int | None:
190:     raw = os.getenv("AUDIO_PYANNOTE_BATCH_SIZE")
191:     if raw is not None:
192:         try:
193:             value = int(raw)
194:             return value if value > 0 else None
195:         except Exception:
196:             return None
197:     if device == "cuda" and _is_low_vram_gpu():
198:         return 1
199:     return None
200: 
201: 
202: def _import_pyannote_pipeline():
203:     from pyannote.audio import Pipeline
204:     return Pipeline
205: 
206: 
207: def _load_pyannote_pipeline(model_id: str, hf_token: str, pipeline_cls=None):
208:     """
209:     Load pyannote Pipeline while handling token/use_auth_token compatibility.
210:     """
211:     pipeline_cls = pipeline_cls or _import_pyannote_pipeline()
212:     try:
213:         return pipeline_cls.from_pretrained(model_id, token=hf_token)
214:     except TypeError as type_err:
215:         if "token" not in str(type_err):
216:             raise
217:         logging.info(
218:             "Pipeline.from_pretrained(%s) ne supporte pas 'token'. "
219:             "Tentative avec use_auth_token.",
220:             model_id,
221:         )
222:         return pipeline_cls.from_pretrained(model_id, use_auth_token=hf_token)
223: 
224: 
225: def _run_diarization_and_extract_segments(diarization_pipeline, wav_path: Path, device: str) -> list:
226:     use_amp = _should_enable_amp(device)
227:     if use_amp and hasattr(torch, "cuda") and hasattr(torch.cuda, "amp"):
228:         amp_ctx = torch.cuda.amp.autocast(dtype=torch.float16)
229:     else:
230:         amp_ctx = nullcontext()
231: 
232:     pyannote_batch_size = _get_pyannote_batch_size(device)
233:     diarization_kwargs = {"num_speakers": None}
234:     if pyannote_batch_size is not None:
235:         diarization_kwargs["batch_size"] = pyannote_batch_size
236:         logging.info(f"Diarisation paramètres: batch_size={pyannote_batch_size}")
237: 
238:     with torch.inference_mode():
239:         with amp_ctx:
240:             try:
241:                 diarization = diarization_pipeline(str(wav_path), **diarization_kwargs)
242:             except TypeError as e:
243:                 if "batch_size" in str(e):
244:                     diarization_kwargs.pop("batch_size", None)
245:                     diarization = diarization_pipeline(str(wav_path), **diarization_kwargs)
246:                 else:
247:                     raise
248:     segments = [(t.start, t.end, spk) for t, _, spk in diarization.itertracks(yield_label=True)]
249:     del diarization
250:     return segments
251: 
252: 
253: def _apply_audio_profile_from_env() -> None:
254:     profile = (os.getenv("AUDIO_PROFILE") or "").strip().lower()
255:     if not profile:
256:         return
257: 
258:     if profile == "gpu_optimized":
259:         os.environ["AUDIO_DISABLE_GPU"] = "0"
260:         os.environ["AUDIO_ENABLE_AMP"] = "1"
261:         os.environ["AUDIO_PYANNOTE_BATCH_SIZE"] = "1"
262:         logging.info("AUDIO_PROFILE=gpu_optimized appliqué (AMP=1, batch_size=1)")
263:         logging.warning("ATTENTION: AMP peut réduire significativement la qualité de diarisation (faux négatifs).")
264:         return
265: 
266:     if profile == "gpu_fp32":
267:         os.environ["AUDIO_DISABLE_GPU"] = "0"
268:         os.environ["AUDIO_ENABLE_AMP"] = "0"
269:         os.environ["AUDIO_PYANNOTE_BATCH_SIZE"] = "1"
270:         logging.info("AUDIO_PROFILE=gpu_fp32 appliqué (AMP=0, batch_size=1, FP32 pur - cohérence CPU)")
271:         return
272: 
273:     if profile == "gpu_no_amp":
274:         os.environ["AUDIO_DISABLE_GPU"] = "0"
275:         os.environ["AUDIO_ENABLE_AMP"] = "0"
276:         os.environ["AUDIO_PYANNOTE_BATCH_SIZE"] = "1"
277:         logging.info("AUDIO_PROFILE=gpu_no_amp appliqué (AMP=0, batch_size=1)")
278:         return
279: 
280:     if profile == "cpu_only":
281:         os.environ["AUDIO_DISABLE_GPU"] = "1"
282:         os.environ["AUDIO_ENABLE_AMP"] = "0"
283:         os.environ.pop("AUDIO_PYANNOTE_BATCH_SIZE", None)
284:         logging.info("AUDIO_PROFILE=cpu_only appliqué (GPU désactivé)")
285:         return
286: 
287:     logging.warning(
288:         f"AUDIO_PROFILE inconnu: '{profile}'. Valeurs supportées: gpu_optimized, gpu_fp32, gpu_no_amp, cpu_only"
289:     )
290: 
291: 
292: def _run_cpu_diarization_subprocess(wav_path: Path, output_segments_json: Path, hf_token: str) -> None:
293:     """Fallback CPU avec mêmes paramètres que GPU (sauf AMP) pour cohérence."""
294:     env = os.environ.copy()
295:     env["AUDIO_DISABLE_GPU"] = "1"
296:     env["CUDA_VISIBLE_DEVICES"] = ""
297:     env["HUGGINGFACE_HUB_TOKEN"] = hf_token
298:     # Forcer FP32 (pas d'AMP) pour cohérence avec mode GPU sans AMP
299:     env["AUDIO_ENABLE_AMP"] = "0"
300:     pyannote_batch_size = os.getenv("AUDIO_PYANNOTE_BATCH_SIZE")
301:     if pyannote_batch_size:
302:         env["AUDIO_PYANNOTE_BATCH_SIZE"] = pyannote_batch_size
303: 
304:     cmd = [
305:         sys.executable,
306:         str(Path(__file__).resolve()),
307:         "--log_dir",
308:         str(LOG_DIR_PATH or Path(tempfile.gettempdir())),
309:         "--disable_gpu",
310:         "--cpu_diarize_wav",
311:         str(wav_path),
312:         "--cpu_diarize_out",
313:         str(output_segments_json),
314:     ]
315:     subprocess.run(
316:         cmd,
317:         check=True,
318:         env=env,
319:         stdout=subprocess.DEVNULL,
320:         stderr=subprocess.PIPE,
321:         text=True,
322:     )
323: 
324: 
325: def _load_segments_from_json(segments_json: Path) -> list:
326:     with open(segments_json, "r", encoding="utf-8") as f:
327:         data = json.load(f)
328:     segments = []
329:     for item in data:
330:         start = float(item["start"])
331:         end = float(item["end"])
332:         speaker = str(item["speaker"])
333:         segments.append((start, end, speaker))
334:     return segments
335: 
336: 
337: def analyze_audio_file(video_path, diarization_pipeline, hf_token, device: str):
338:     """Analyse une vidéo, extrait l'audio via ffmpeg, effectue la diarisation et sauvegarde le JSON (streaming)."""
339:     output_json_path = video_path.with_name(f"{video_path.stem}{OUTPUT_SUFFIX}")
340: 
341:     duration_sec = _run_ffprobe_duration(video_path)
342:     video_fps = DEFAULT_FPS
343:     if duration_sec > 0:
344:         total_frames = int(round(duration_sec * video_fps))
345:     else:
346:         logging.warning(f"Durée inconnue, fallback frames basé sur DEFAULT_FPS={DEFAULT_FPS} pour {video_path.name}")
347:         # On fixera total_frames après avoir déterminé le max de frame touché par la timeline.
348:         total_frames = -1
349: 
350:     tmp_dir_root = "/dev/shm" if os.path.isdir("/dev/shm") else None
351:     try:
352:         with tempfile.TemporaryDirectory(dir=tmp_dir_root) as tmp_dir:
353:             tmp_wav = Path(tmp_dir) / f"{video_path.stem}_temp.wav"
354: 
355:             logging.info(f"Extraction audio (ffmpeg) de {video_path.name} -> {tmp_wav.name}...")
356:             if not _extract_audio_ffmpeg(video_path, tmp_wav):
357:                 if total_frames < 0:
358:                     total_frames = 0
359:                 _write_empty_audio_json_streaming(output_json_path, video_path.name, total_frames, video_fps)
360:                 return True
361: 
362:             logging.info(f"Diarisation en cours sur {tmp_wav.name}...")
363:             start_infer_t = time.time()
364:             segments = None
365:             try:
366:                 if device == "cuda":
367:                     _cleanup_cuda_memory()
368:                 segments = _run_diarization_and_extract_segments(diarization_pipeline, tmp_wav, device)
369:                 logging.info(f"Diarisation: {len(segments)} segment(s) détecté(s)")
370:             except RuntimeError as e_oom:
371:                 if "CUDA out of memory" in str(e_oom):
372:                     logging.warning("CUDA OOM durant la diarisation, tentative de fallback CPU pour ce fichier...")
373:                     _cleanup_cuda_memory()
374:                     try:
375:                         cpu_segments_json = Path(tmp_dir) / f"{video_path.stem}_cpu_segments.json"
376:                         logging.info("Fallback CPU: mêmes paramètres que GPU (batch_size) sauf AMP, pour cohérence")
377:                         _run_cpu_diarization_subprocess(tmp_wav, cpu_segments_json, hf_token)
378:                         segments = _load_segments_from_json(cpu_segments_json)
379:                         logging.info(f"Fallback CPU: {len(segments)} segment(s) détecté(s)")
380:                     except subprocess.CalledProcessError as cpu_e:
381:                         safe_stderr = (cpu_e.stderr or "")
382:                         logging.error(
383:                             "Echec du fallback CPU (subprocess). "
384:                             f"returncode={cpu_e.returncode}. stderr:\n{safe_stderr}"
385:                         )
386:                         raise
387:                     except Exception:
388:                         logging.error("Echec du fallback CPU (erreur inattendue).", exc_info=True)
389:                         raise
390:                 else:
391:                     segments = _run_diarization_and_extract_segments(diarization_pipeline, tmp_wav, device)
392:             infer_ms = int((time.time() - start_infer_t) * 1000)
393:             logging.info(f"Diarisation terminée en ~{infer_ms} ms")
394: 
395:             audio_timeline = {}
396:             max_frame_seen = 0
397:             for start_sec, end_sec, speaker_label in (segments or []):
398:                 start_frame = max(1, int(start_sec * video_fps))
399:                 end_frame = int(end_sec * video_fps)
400:                 if end_frame < start_frame:
401:                     continue
402:                 max_frame_seen = max(max_frame_seen, end_frame)
403:                 for frame_num in range(start_frame, end_frame + 1):
404:                     frame_entry = audio_timeline.get(frame_num)
405:                     if frame_entry is None:
406:                         frame_entry = set()
407:                         audio_timeline[frame_num] = frame_entry
408:                     frame_entry.add(speaker_label)
409:             
410:             num_speech_frames = len(audio_timeline)
411:             speech_pct = (num_speech_frames / total_frames * 100) if total_frames > 0 else 0
412:             logging.info(f"Timeline audio: {num_speech_frames}/{total_frames} frames avec parole ({speech_pct:.1f}%)")
413: 
414:             if total_frames < 0:
415:                 total_frames = max_frame_seen
416: 
417:             # Écriture JSON streaming (préserve le schéma pour STEP5)
418:             use_gzip = os.getenv("AUDIO_JSON_GZIP", "0") == "1"
419:             if use_gzip:
420:                 json_path = str(output_json_path) + ".gz"
421:                 open_fn = lambda p: gzip.open(p, mode="wt", encoding="utf-8")
422:             else:
423:                 json_path = str(output_json_path)
424:                 open_fn = lambda p: open(p, mode="w", encoding="utf-8")
425: 
426:             with open_fn(json_path) as f:
427:                 f.write("{\n")
428:                 f.write(f"  \"video_filename\": \"{video_path.name}\",\n")
429:                 f.write(f"  \"total_frames\": {total_frames},\n")
430:                 f.write(f"  \"fps\": {round(video_fps, 2)},\n")
431:                 f.write("  \"frames_analysis\": [\n")
432:     
433:                 frames_processed = 0
434:                 last_log_t = time.time()
435:                 for frame_num in range(1, total_frames + 1):
436:                     speakers = sorted(audio_timeline.get(frame_num, []))
437:                     is_speech = len(speakers) > 0
438:                     timecode = round((frame_num - 1) / video_fps, 3)
439:                     obj = {
440:                         "frame": frame_num,
441:                         "audio_info": {
442:                             "is_speech_present": is_speech,
443:                             "num_distinct_speakers_audio": len(speakers),
444:                             "active_speaker_labels": speakers,
445:                             "timecode_sec": timecode,
446:                         },
447:                     }
448:                     # Écriture streaming avec virgules correctes
449:                     if frame_num > 1:
450:                         f.write(",\n")
451:                     f.write(json.dumps(obj))
452:     
453:                     frames_processed += 1
454:                     if time.time() - last_log_t >= 2 or frames_processed == total_frames:
455:                         progress_percent = int((frames_processed / total_frames) * 100) if total_frames else 100
456:                         logging.info(
457:                             f"INTERNAL_PROGRESS: {frames_processed}/{total_frames} frames ({progress_percent}%) - {video_path.name}")
458:                         last_log_t = time.time()
459:     
460:                 f.write("\n  ]\n")
461:                 f.write("}\n")
462:     
463:             logging.info(f"Succès: analyse audio terminée pour {video_path.name}")
464:             return True
465: 
466:     except Exception as e:
467:         logging.error(f"Erreur inattendue lors de l'analyse de {video_path.name}: {e}", exc_info=True)
468:         return False
469: 
470: 
471: def main():
472:     parser = argparse.ArgumentParser(description="Analyse audio (diarisation) des vidéos.")
473:     parser.add_argument("--log_dir", type=str, required=True, help="Répertoire de logs")
474:     parser.add_argument("--hf_auth_token", type=str, help="Token d'authentification HuggingFace")
475:     parser.add_argument("--disable_gpu", action="store_true", help="Forcer l'utilisation CPU")
476:     parser.add_argument("--cpu_diarize_wav", type=str, help="Mode interne: diarisation CPU-only d'un WAV")
477:     parser.add_argument("--cpu_diarize_out", type=str, help="Mode interne: sortie JSON segments")
478:     args = parser.parse_args()
479: 
480:     log_dir_path = Path(args.log_dir)
481:     log_dir_path.mkdir(parents=True, exist_ok=True)
482:     global LOG_DIR_PATH
483:     LOG_DIR_PATH = log_dir_path
484:     log_file = log_dir_path / f"audio_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
485:     file_handler = logging.FileHandler(log_file, encoding='utf-8')
486:     file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
487:     logging.getLogger().addHandler(file_handler)
488: 
489:     logging.info("--- Démarrage du script d'analyse audio (Diarisation) ---")
490: 
491:     _apply_audio_profile_from_env()
492: 
493:     try:
494:         os.environ.setdefault("PYTORCH_CUDA_ALLOC_CONF", "max_split_size_mb:32")
495: 
496:         env_disable_gpu = os.getenv("AUDIO_DISABLE_GPU", "0") == "1"
497:         use_cuda = torch.cuda.is_available() and not env_disable_gpu and not args.disable_gpu
498:         device = "cuda" if use_cuda else "cpu"
499:         if device == "cpu":
500:             cpu_workers_env = os.getenv("AUDIO_CPU_WORKERS")
501:             try:
502:                 if cpu_workers_env:
503:                     n_threads = max(1, int(cpu_workers_env))
504:                     torch.set_num_threads(n_threads)
505:                     os.environ.setdefault("OMP_NUM_THREADS", str(n_threads))
506:                     os.environ.setdefault("MKL_NUM_THREADS", str(n_threads))
507:             except Exception:
508:                 pass
509:         logging.info(f"Utilisation du device: {device}")
510: 
511:         hf_token = None
512:         if args.hf_auth_token:
513:             hf_token = args.hf_auth_token
514:             try:
515:                 os.environ["HUGGINGFACE_HUB_TOKEN"] = args.hf_auth_token
516:             except Exception:
517:                 pass
518:         else:
519:             hf_token = os.getenv("HUGGINGFACE_HUB_TOKEN") or os.getenv("HF_AUTH_TOKEN")
520: 
521:         if not hf_token:
522:             logging.critical(
523:                 "Aucun token Hugging Face fourni. Passez --hf_auth_token ou définissez HUGGINGFACE_HUB_TOKEN/HF_AUTH_TOKEN."
524:             )
525:             sys.exit(1)
526:         try:
527:             from huggingface_hub import HfApi
528:             try:
529:                 from huggingface_hub.hf_api import HfFolder
530:                 HfFolder.save_token(hf_token)
531:             except Exception:
532:                 pass
533:             _id = HfApi().whoami(token=hf_token)
534:             safe_tail = hf_token[-6:] if len(hf_token) >= 6 else "***"
535:             logging.info(
536:                 "Authentifié sur Hugging Face (token tail=***%s) en tant que: %s",
537:                 safe_tail,
538:                 _id.get("name") or _id.get("email"),
539:             )
540:         except Exception as auth_e:
541:             logging.warning(
542:                 "Impossible de valider le token Hugging Face: %s. On tente quand même le téléchargement.",
543:                 auth_e,
544:             )
545: 
546:         pipeline = None
547:         try:
548:             pipeline = _load_pyannote_pipeline("pyannote/speaker-diarization-3.1", hf_token)
549:         except Exception as e_v3:
550:             logging.warning(f"Impossible de charger la pipeline v3.1: {e_v3}. Tentative avec v2...")
551:             try:
552:                 pipeline = _load_pyannote_pipeline("pyannote/speaker-diarization", hf_token)
553:             except Exception as e_v2:
554:                 logging.critical(
555:                     "Echec du chargement des pipelines pyannote (v3.1 et v2). "
556:                     "Le modèle peut être privé/gated. Assurez-vous que votre token HF a accès "
557:                     "(accepter les conditions sur la page du modèle) et réessayez."
558:                 )
559:                 logging.critical(f"Détails v3.1: {e_v3}")
560:                 logging.critical(f"Détails v2: {e_v2}")
561:                 sys.exit(1)
562:         optimal_tv_config = _load_optimal_tv_config()
563:         pyannote_batch_size = _get_pyannote_batch_size(device)
564:         if pyannote_batch_size is not None:
565:             for key in ("segmentation", "embedding"):
566:                 section = optimal_tv_config.get(key)
567:                 if not isinstance(section, dict):
568:                     optimal_tv_config[key] = {}
569:                 optimal_tv_config[key]["batch_size"] = pyannote_batch_size
570: 
571:         if optimal_tv_config and hasattr(pipeline, "instantiate"):
572:             try:
573:                 pipeline.instantiate(optimal_tv_config)
574:                 if pyannote_batch_size is not None:
575:                     logging.info(
576:                         f"Pyannote configuration appliquée (optimal_tv_config + batch_size={pyannote_batch_size})"
577:                     )
578:                 else:
579:                     logging.info("Pyannote configuration appliquée (optimal_tv_config)")
580:             except Exception as e:
581:                 msg = str(e)
582:                 if pyannote_batch_size is not None and "batch_size" in msg and "does not exist" in msg:
583:                     logging.info(
584:                         f"Pipeline.instantiate ne supporte pas batch_size (AUDIO_PYANNOTE_BATCH_SIZE={pyannote_batch_size}). "
585:                         "Le batch_size sera appliqué lors de l'appel diarization si possible."
586:                     )
587:                 else:
588:                     logging.warning(f"Impossible d'appliquer optimal_tv_config.json via pipeline.instantiate: {e}")
589: 
590:                     if pyannote_batch_size is not None:
591:                         try:
592:                             pipeline.instantiate(
593:                                 {
594:                                     "segmentation": {"batch_size": pyannote_batch_size},
595:                                     "embedding": {"batch_size": pyannote_batch_size},
596:                                 }
597:                             )
598:                             logging.info(
599:                                 "Fallback: configuration minimale appliquée (batch_size seulement) suite à un échec optimal_tv_config"
600:                             )
601:                         except Exception as fallback_e:
602:                             logging.warning(
603:                                 "Fallback: impossible d'appliquer la configuration minimale (batch_size seulement): "
604:                                 f"{fallback_e}"
605:                             )
606: 
607:         if _should_enable_amp(device):
608:             total_gb = _get_total_vram_gb()
609:             logging.info(f"AMP activé pour l'inférence (VRAM total ~{total_gb:.2f} GiB)")
610: 
611:         if args.cpu_diarize_wav:
612:             if not args.cpu_diarize_out:
613:                 logging.critical("Mode cpu_diarize_wav: --cpu_diarize_out est requis")
614:                 sys.exit(1)
615:             wav_path = Path(args.cpu_diarize_wav)
616:             out_path = Path(args.cpu_diarize_out)
617:             out_path.parent.mkdir(parents=True, exist_ok=True)
618:             
619:             diarization_kwargs = {"num_speakers": None}
620:             pyannote_batch_size = _get_pyannote_batch_size("cpu")
621:             if pyannote_batch_size is not None:
622:                 diarization_kwargs["batch_size"] = pyannote_batch_size
623:                 logging.info(f"CPU subprocess: batch_size={pyannote_batch_size}")
624:             
625:             with torch.inference_mode():
626:                 try:
627:                     diarization = pipeline(str(wav_path), **diarization_kwargs)
628:                 except TypeError as e:
629:                     if "batch_size" in str(e):
630:                         diarization_kwargs.pop("batch_size", None)
631:                         diarization = pipeline(str(wav_path), **diarization_kwargs)
632:                     else:
633:                         raise
634:             
635:             segments = [
636:                 {"start": float(t.start), "end": float(t.end), "speaker": str(spk)}
637:                 for t, _, spk in diarization.itertracks(yield_label=True)
638:             ]
639:             logging.info(f"CPU subprocess: {len(segments)} segment(s) extrait(s)")
640:             with open(out_path, "w", encoding="utf-8") as f:
641:                 json.dump(segments, f)
642:             return
643: 
644:         if pipeline is None:
645:             logging.critical(
646:                 "Impossible de charger la pipeline pyannote (pipeline=None). Le modèle peut être privé/gated. "
647:                 "Vérifiez votre token Hugging Face (HUGGINGFACE_HUB_TOKEN) et acceptez les conditions du modèle."
648:             )
649:             sys.exit(1)
650: 
651:         try:
652:             if hasattr(pipeline, "to"):
653:                 pipeline.to(torch.device(device))
654:                 logging.info(f"Pipeline de diarisation chargée avec succès sur {device}.")
655:             else:
656:                 logging.info("Pipeline pyannote ne supporte pas .to(); continuation sans déplacement explicite de device.")
657:         except RuntimeError as e:
658:             if "NVIDIA driver" in str(e) or "CUDA" in str(e):
659:                 logging.warning(f"GPU incompatible ({e}), fallback sur CPU.")
660:                 device = "cpu"
661:                 if hasattr(pipeline, "to"):
662:                     pipeline.to(torch.device("cpu"))
663:                 logging.info("Pipeline de diarisation chargée avec succès sur CPU (fallback).")
664:             else:
665:                 raise
666:     except ImportError as e:
667:         logging.critical(f"Erreur lors du chargement de la pipeline Pyannote: {e}")
668:         sys.exit(1)
669: 
670:     videos = find_videos_for_audio_analysis()
671:     total_videos = len(videos)
672:     logging.info(f"TOTAL_AUDIO_TO_ANALYZE: {total_videos}")
673: 
674:     if total_videos == 0:
675:         logging.info("Aucune nouvelle vidéo à analyser.")
676:         return
677: 
678:     successful_count = 0
679:     for i, video_path in enumerate(videos):
680:         logging.info(f"ANALYZING_AUDIO: {i + 1}/{total_videos}: {video_path.name}")
681: 
682:         success = analyze_audio_file(video_path, pipeline, hf_token, device)
683:         if success:
684:             successful_count += 1
685: 
686:         try:
687:             if device == "cuda" and torch.cuda.is_available():
688:                 torch.cuda.empty_cache()
689:         except Exception:
690:             pass
691: 
692:     logging.info("--- Analyse audio terminée ---")
693:     logging.info(f"Résumé: {successful_count}/{total_videos} analyse(s) réussie(s).")
694: 
695:     if successful_count < total_videos:
696:         # Permettre un succès partiel si demandé (utile pour pipelines tolérantes)
697:         allow_partial = os.getenv("AUDIO_PARTIAL_SUCCESS_OK", "0") == "1"
698:         if allow_partial and successful_count > 0:
699:             logging.warning(
700:                 f"Partial success autorisé: {successful_count}/{total_videos} analyses ont réussi. Code de sortie 0."
701:             )
702:             return
703:         sys.exit(1)
704: 
705: 
706: if __name__ == "__main__":
707:     try:
708:         main()
709:     except Exception as e:
710:         logging.critical(f"Erreur critique non gérée: {e}", exc_info=True)
711:         sys.exit(1)
```

## File: workflow_scripts/step5/face_engines.py
```python
   1: import logging
   2: import os
   3: import threading
   4: import time
   5: from pathlib import Path
   6: from typing import Optional
   7: 
   8: import cv2
   9: import numpy as np
  10: 
  11: # Load .env file BEFORE reading any environment variables (critical for multiprocessing workers)
  12: try:
  13:     from dotenv import load_dotenv
  14:     _env_path = Path(__file__).resolve().parent.parent.parent / '.env'
  15:     if _env_path.exists():
  16:         load_dotenv(_env_path)
  17: except ImportError:
  18:     pass  # dotenv not available, rely on system env vars
  19: 
  20: logger = logging.getLogger(__name__)
  21: 
  22: ARKIT_52_BLENDSHAPE_NAMES = [
  23:     "browDownLeft",
  24:     "browDownRight",
  25:     "browInnerUp",
  26:     "browOuterUpLeft",
  27:     "browOuterUpRight",
  28:     "cheekPuff",
  29:     "cheekSquintLeft",
  30:     "cheekSquintRight",
  31:     "eyeBlinkLeft",
  32:     "eyeBlinkRight",
  33:     "eyeLookDownLeft",
  34:     "eyeLookDownRight",
  35:     "eyeLookInLeft",
  36:     "eyeLookInRight",
  37:     "eyeLookOutLeft",
  38:     "eyeLookOutRight",
  39:     "eyeLookUpLeft",
  40:     "eyeLookUpRight",
  41:     "eyeSquintLeft",
  42:     "eyeSquintRight",
  43:     "eyeWideLeft",
  44:     "eyeWideRight",
  45:     "jawForward",
  46:     "jawLeft",
  47:     "jawOpen",
  48:     "jawRight",
  49:     "mouthClose",
  50:     "mouthDimpleLeft",
  51:     "mouthDimpleRight",
  52:     "mouthFrownLeft",
  53:     "mouthFrownRight",
  54:     "mouthFunnel",
  55:     "mouthLeft",
  56:     "mouthLowerDownLeft",
  57:     "mouthLowerDownRight",
  58:     "mouthPressLeft",
  59:     "mouthPressRight",
  60:     "mouthPucker",
  61:     "mouthRight",
  62:     "mouthRollLower",
  63:     "mouthRollUpper",
  64:     "mouthShrugLower",
  65:     "mouthShrugUpper",
  66:     "mouthSmileLeft",
  67:     "mouthSmileRight",
  68:     "mouthStretchLeft",
  69:     "mouthStretchRight",
  70:     "mouthUpperUpLeft",
  71:     "mouthUpperUpRight",
  72:     "noseSneerLeft",
  73:     "noseSneerRight",
  74:     "tongueOut",
  75: ]
  76: 
  77: 
  78: def _parse_optional_positive_int(raw: Optional[str]) -> Optional[int]:
  79:     if raw is None:
  80:         return None
  81:     raw = raw.strip()
  82:     if not raw:
  83:         return None
  84:     try:
  85:         value = int(raw)
  86:     except Exception:
  87:         return None
  88:     if value <= 0:
  89:         return None
  90:     return value
  91: 
  92: 
  93: def _apply_jawopen_scale(blendshapes: Optional[dict], scale: float) -> Optional[dict]:
  94:     if not blendshapes or not isinstance(blendshapes, dict):
  95:         return blendshapes
  96:     try:
  97:         jaw_open_raw = blendshapes.get("jawOpen")
  98:         if jaw_open_raw is None:
  99:             return blendshapes
 100:         jaw_open_scaled = float(jaw_open_raw) * float(scale)
 101:         jaw_open_scaled = float(np.clip(jaw_open_scaled, 0.0, 1.0))
 102:         out = dict(blendshapes)
 103:         out["jawOpen"] = jaw_open_scaled
 104:         return out
 105:     except Exception:
 106:         return blendshapes
 107: 
 108: 
 109: def _openseeface_logit_arr(p: np.ndarray, factor: float = 16.0) -> np.ndarray:
 110:     p = np.clip(p, 0.0000001, 0.9999999)
 111:     return np.log(p / (1 - p)) / float(factor)
 112: 
 113: 
 114: class OpenSeeFaceEngine:
 115:     def __init__(
 116:         self,
 117:         models_dir: Optional[str] = None,
 118:         model_id: Optional[int] = None,
 119:         detection_threshold: Optional[float] = None,
 120:         max_faces: Optional[int] = None,
 121:         use_gpu: bool = False,
 122:     ):
 123:         self._lock = threading.Lock()
 124: 
 125:         try:
 126:             import onnxruntime as ort
 127:         except ImportError as e:
 128:             raise RuntimeError(
 129:                 "OpenSeeFace engine requires onnxruntime. Install it in tracking_env."
 130:             ) from e
 131: 
 132:         self._ort = ort
 133:         self._use_gpu = use_gpu
 134:         self._frame_counter = 0
 135:         self._last_detections = []
 136: 
 137:         self._enable_profiling = os.environ.get("STEP5_ENABLE_PROFILING", "0").strip().lower() in {
 138:             "1",
 139:             "true",
 140:             "yes",
 141:         }
 142:         self._profiling_stats = {
 143:             "resize_total": 0.0,
 144:             "detect_total": 0.0,
 145:             "landmarks_total": 0.0,
 146:             "post_total": 0.0,
 147:             "frame_count": 0,
 148:         }
 149: 
 150:         env_models_dir = os.environ.get("STEP5_OPENSEEFACE_MODELS_DIR")
 151:         models_dir_raw = models_dir or (env_models_dir.strip() if env_models_dir else "")
 152:         if models_dir_raw:
 153:             self._models_dir_explicit = True
 154:             models_dir_candidate = Path(models_dir_raw)
 155:             if models_dir_candidate.is_absolute() and models_dir_candidate.exists():
 156:                 self._models_dir = models_dir_candidate
 157:             elif models_dir_candidate.exists():
 158:                 self._models_dir = models_dir_candidate.resolve()
 159:             else:
 160:                 step5_dir = Path(__file__).resolve().parent
 161:                 project_root = step5_dir.parent.parent
 162:                 resolved_models_dir = None
 163:                 for c in [project_root / models_dir_candidate, step5_dir / models_dir_candidate]:
 164:                     if c.exists():
 165:                         resolved_models_dir = c
 166:                         break
 167:                 self._models_dir = (resolved_models_dir or models_dir_candidate).resolve()
 168:         else:
 169:             self._models_dir_explicit = False
 170:             step5_dir = Path(__file__).resolve().parent
 171:             self._models_dir = step5_dir / "models" / "engines" / "openseeface"
 172: 
 173:         env_model_id = os.environ.get("STEP5_OPENSEEFACE_MODEL_ID")
 174:         self._model_id = int(model_id if model_id is not None else (env_model_id or "1"))
 175: 
 176:         env_detect_every = os.environ.get("STEP5_OPENSEEFACE_DETECT_EVERY_N")
 177:         env_blendshapes_throttle = os.environ.get("STEP5_BLENDSHAPES_THROTTLE_N")
 178:         detect_every_raw = env_detect_every if (env_detect_every is not None and env_detect_every.strip()) else env_blendshapes_throttle
 179:         self._detect_every_n = max(1, int(detect_every_raw or "1"))
 180: 
 181:         env_max_faces = os.environ.get("STEP5_OPENSEEFACE_MAX_FACES")
 182:         self._max_faces = max(1, int(max_faces if max_faces is not None else (env_max_faces or "1")))
 183: 
 184:         env_detection_threshold = os.environ.get("STEP5_OPENSEEFACE_DETECTION_THRESHOLD")
 185:         self._detection_threshold = float(
 186:             detection_threshold if detection_threshold is not None else (env_detection_threshold or "0.6")
 187:         )
 188: 
 189:         env_jaw_scale = os.environ.get("STEP5_OPENSEEFACE_JAWOPEN_SCALE")
 190:         self._jaw_open_scale = float(env_jaw_scale or "1.0")
 191: 
 192:         try:
 193:             cv2.setNumThreads(1)
 194:         except Exception:
 195:             pass
 196: 
 197:         env_openseeface_max_width = os.environ.get("STEP5_OPENSEEFACE_MAX_WIDTH")
 198:         self._max_detection_width = max(
 199:             1,
 200:             int(
 201:                 (env_openseeface_max_width.strip() if env_openseeface_max_width else "")
 202:                 or os.environ.get("STEP5_YUNET_MAX_WIDTH", "640")
 203:             ),
 204:         )
 205: 
 206:         detection_override_path = os.environ.get("STEP5_OPENSEEFACE_DETECTION_MODEL_PATH")
 207:         landmark_override_path = os.environ.get("STEP5_OPENSEEFACE_LANDMARK_MODEL_PATH")
 208: 
 209:         self._detection_model_path = self._resolve_model_path(
 210:             detection_override_path,
 211:             default_filename="mnv3_detection_opt.onnx",
 212:         )
 213:         self._landmark_model_path = self._resolve_model_path(
 214:             landmark_override_path,
 215:             default_filename=f"lm_model{self._model_id}_opt.onnx",
 216:         )
 217: 
 218:         if not self._detection_model_path:
 219:             raise RuntimeError(
 220:                 "OpenSeeFace detection model not found. Set STEP5_OPENSEEFACE_DETECTION_MODEL_PATH or "
 221:                 "STEP5_OPENSEEFACE_MODELS_DIR with mnv3_detection_opt.onnx."
 222:             )
 223:         if not self._landmark_model_path:
 224:             raise RuntimeError(
 225:                 "OpenSeeFace landmark model not found. Set STEP5_OPENSEEFACE_LANDMARK_MODEL_PATH or "
 226:                 "STEP5_OPENSEEFACE_MODELS_DIR with lm_model{N}_opt.onnx."
 227:             )
 228: 
 229:         intra_threads = int(os.environ.get("STEP5_ONNX_INTRA_OP_THREADS", "2"))
 230:         inter_threads = int(os.environ.get("STEP5_ONNX_INTER_OP_THREADS", "1"))
 231: 
 232:         # Configure ONNX providers (GPU or CPU)
 233:         available_providers = ort.get_available_providers()
 234:         logger.info(f"[OpenSeeFace] Available ONNXRuntime providers: {available_providers}")
 235: 
 236:         if use_gpu:
 237:             providers = ["CUDAExecutionProvider"]
 238:             logger.info("[OpenSeeFace] Attempting to use CUDA provider for GPU inference (no CPU fallback)")
 239:         else:
 240:             providers = ["CPUExecutionProvider"]
 241: 
 242:         detection_sess_options = ort.SessionOptions()
 243:         detection_sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
 244:         if use_gpu:
 245:             try:
 246:                 detection_sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_BASIC
 247:                 detection_sess_options.add_session_config_entry("session.disable_contrib_ops", "1")
 248:                 detection_sess_options.add_session_config_entry("session.disable_prepacking", "1")
 249:                 detection_sess_options.add_session_config_entry("session.disable_aot_function_inlining", "1")
 250:                 logger.info("[OpenSeeFace] Disabled contrib/fused ops to improve CUDA compatibility")
 251:             except Exception as cfg_err:
 252:                 logger.warning("[OpenSeeFace] Failed to set session config entries: %s", cfg_err)
 253:         detection_sess_options.intra_op_num_threads = max(1, int(intra_threads))
 254:         detection_sess_options.inter_op_num_threads = max(1, int(inter_threads))
 255:         detection_sess_options.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL
 256: 
 257:         def _create_session(model_path: Path, sess_options: "ort.SessionOptions", provider_list: list[str]):
 258:             return ort.InferenceSession(
 259:                 str(model_path),
 260:                 sess_options=sess_options,
 261:                 providers=provider_list,
 262:             )
 263: 
 264:         try:
 265:             self._detection_session = _create_session(self._detection_model_path, detection_sess_options, providers)
 266:         except Exception as session_error:
 267:             logger.exception(
 268:                 "[OpenSeeFace] Failed to create detection session with providers %s: %s",
 269:                 providers,
 270:                 session_error,
 271:             )
 272:             if use_gpu:
 273:                 logger.warning("[OpenSeeFace] Falling back to CPUExecutionProvider due to CUDA initialization failure")
 274:                 providers = ["CPUExecutionProvider"]
 275:                 self._use_gpu = False
 276:                 self._detection_session = _create_session(self._detection_model_path, detection_sess_options, providers)
 277:             else:
 278:                 raise
 279:         self._detection_input_name = self._detection_session.get_inputs()[0].name
 280:         
 281:         # Log active provider
 282:         active_provider = self._detection_session.get_providers()[0]
 283:         logger.info(f"[OpenSeeFace] Detection session using provider: {active_provider}")
 284:         if use_gpu and active_provider != "CUDAExecutionProvider":
 285:             logger.error("[OpenSeeFace] CUDA provider requested but not active despite GPU mode (should have failed)")
 286: 
 287:         landmark_sess_options = ort.SessionOptions()
 288:         landmark_sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
 289:         landmark_sess_options.intra_op_num_threads = max(1, int(intra_threads))
 290:         landmark_sess_options.inter_op_num_threads = max(1, int(inter_threads))
 291:         landmark_sess_options.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL
 292: 
 293:         try:
 294:             self._landmark_session = ort.InferenceSession(
 295:                 str(self._landmark_model_path),
 296:                 sess_options=landmark_sess_options,
 297:                 providers=providers,
 298:             )
 299:         except Exception as session_error:
 300:             logger.exception(
 301:                 "[OpenSeeFace] Failed to create landmark session with providers %s: %s",
 302:                 providers,
 303:                 session_error,
 304:             )
 305:             raise
 306:         self._landmark_input_name = self._landmark_session.get_inputs()[0].name
 307:         
 308:         active_provider_lm = self._landmark_session.get_providers()[0]
 309:         logger.info(f"[OpenSeeFace] Landmark session using provider: {active_provider_lm}")
 310: 
 311:         self._mean = np.float32(np.array([0.485, 0.456, 0.406]))
 312:         self._std = np.float32(np.array([0.229, 0.224, 0.225]))
 313:         mean = (self._mean / self._std)
 314:         std = (self._std * 255.0)
 315:         self._mean = -mean
 316:         self._std = 1.0 / std
 317: 
 318:         self._mean_224 = np.tile(self._mean, [224, 224, 1])
 319:         self._std_224 = np.tile(self._std, [224, 224, 1])
 320: 
 321:         self._res = 224.0
 322:         self._out_res = 27.0
 323:         self._out_res_i = int(self._out_res) + 1
 324:         self._logit_factor = 16.0
 325: 
 326:     def _resolve_model_path(self, override_path: Optional[str], default_filename: str) -> Optional[Path]:
 327:         if override_path:
 328:             candidate = Path(override_path)
 329:             if candidate.is_absolute() and candidate.exists():
 330:                 return candidate
 331:             if candidate.exists():
 332:                 return candidate.resolve()
 333: 
 334:             step5_dir = Path(__file__).resolve().parent
 335:             project_root = step5_dir.parent.parent
 336:             for c in [project_root / candidate, step5_dir / candidate]:
 337:                 if c.exists():
 338:                     return c
 339: 
 340:         candidate = self._models_dir / default_filename
 341:         if candidate.exists():
 342:             return candidate
 343: 
 344:         if getattr(self, "_models_dir_explicit", False):
 345:             return None
 346: 
 347:         step5_dir = Path(__file__).resolve().parent
 348:         fallback_candidates = [
 349:             step5_dir / "models" / "engines" / "openseeface" / default_filename,
 350:             step5_dir / "models" / default_filename,
 351:         ]
 352:         for fallback in fallback_candidates:
 353:             if fallback.exists():
 354:                 return fallback
 355: 
 356:         return None
 357: 
 358:     def _preprocess_detection(self, frame_bgr: np.ndarray) -> np.ndarray:
 359:         resized = cv2.resize(frame_bgr, (224, 224), interpolation=cv2.INTER_LINEAR)
 360:         rgb = resized[:, :, ::-1].astype(np.float32)
 361:         im = rgb * self._std_224 + self._mean_224
 362:         im = np.expand_dims(im, 0)
 363:         im = np.transpose(im, (0, 3, 1, 2))
 364:         return np.ascontiguousarray(im)
 365: 
 366:     def _detect_faces(self, frame_bgr: np.ndarray) -> list[tuple[float, float, float, float, float]]:
 367:         im = self._preprocess_detection(frame_bgr)
 368:         outputs = self._detection_session.run([], {self._detection_input_name: im})
 369:         if not outputs or len(outputs) < 2:
 370:             return []
 371:         out_map = np.array(outputs[0])
 372:         maxpool = np.array(outputs[1])
 373:         if out_map.ndim != 4 or maxpool.ndim != 4:
 374:             return []
 375:         out_map[0, 0, out_map[0, 0] != maxpool[0, 0]] = 0
 376: 
 377:         detections = np.flip(np.argsort(out_map[0, 0].flatten()))
 378:         results = []
 379:         for det in detections[0 : self._max_faces]:
 380:             y = int(det // 56)
 381:             x = int(det % 56)
 382:             conf = float(out_map[0, 0, y, x])
 383:             radius = float(out_map[0, 1, y, x]) * 112.0
 384:             x_px = float(x * 4)
 385:             y_px = float(y * 4)
 386:             if conf < self._detection_threshold:
 387:                 break
 388:             results.append((x_px - radius, y_px - radius, 2 * radius, 2 * radius, conf))
 389: 
 390:         if not results:
 391:             return []
 392: 
 393:         results_np = np.array([r[:4] for r in results], dtype=np.float32)
 394:         results_np[:, [0, 2]] *= (frame_bgr.shape[1] / 224.0)
 395:         results_np[:, [1, 3]] *= (frame_bgr.shape[0] / 224.0)
 396:         scaled = []
 397:         for i, (x, y, w, h) in enumerate(results_np.tolist()):
 398:             scaled.append((x, y, w, h, float(results[i][4])))
 399:         return scaled
 400: 
 401:     def _preprocess_landmarks(
 402:         self,
 403:         frame_bgr: np.ndarray,
 404:         crop: tuple[int, int, int, int],
 405:     ) -> tuple[np.ndarray, tuple[int, int, float, float]]:
 406:         x1, y1, x2, y2 = crop
 407:         im = frame_bgr[y1:y2, x1:x2, ::-1].astype(np.float32)
 408:         im = cv2.resize(im, (int(self._res), int(self._res)), interpolation=cv2.INTER_LINEAR)
 409:         im = im * np.tile(self._std, [int(self._res), int(self._res), 1]) + np.tile(
 410:             self._mean, [int(self._res), int(self._res), 1]
 411:         )
 412:         im = np.expand_dims(im, 0)
 413:         im = np.transpose(im, (0, 3, 1, 2))
 414:         im = np.ascontiguousarray(im)
 415: 
 416:         scale_x = float(x2 - x1) / float(self._res)
 417:         scale_y = float(y2 - y1) / float(self._res)
 418:         return im, (x1, y1, scale_x, scale_y)
 419: 
 420:     def _decode_landmarks(self, tensor: np.ndarray, crop_info: tuple[int, int, float, float]) -> tuple[float, np.ndarray]:
 421:         crop_x1, crop_y1, scale_x, scale_y = crop_info
 422:         res = self._res - 1
 423: 
 424:         c0, c1, c2 = 66, 132, 198
 425:         t_main = tensor[0:c0].reshape((c0, self._out_res_i * self._out_res_i))
 426:         t_m = t_main.argmax(1)
 427:         indices = np.expand_dims(t_m, 1)
 428:         t_conf = np.take_along_axis(t_main, indices, 1).reshape((c0,))
 429: 
 430:         t_off_x = np.take_along_axis(
 431:             tensor[c0:c1].reshape((c0, self._out_res_i * self._out_res_i)),
 432:             indices,
 433:             1,
 434:         ).reshape((c0,))
 435:         t_off_y = np.take_along_axis(
 436:             tensor[c1:c2].reshape((c0, self._out_res_i * self._out_res_i)),
 437:             indices,
 438:             1,
 439:         ).reshape((c0,))
 440:         t_off_x = res * _openseeface_logit_arr(t_off_x, self._logit_factor)
 441:         t_off_y = res * _openseeface_logit_arr(t_off_y, self._logit_factor)
 442: 
 443:         t_x = crop_y1 + scale_y * (res * np.floor(t_m / self._out_res_i) / self._out_res + t_off_x)
 444:         t_y = crop_x1 + scale_x * (res * np.floor(np.mod(t_m, self._out_res_i)) / self._out_res + t_off_y)
 445:         avg_conf = float(np.average(t_conf))
 446:         lms_yx_conf = np.stack([t_x, t_y, t_conf], 1)
 447:         if np.isnan(lms_yx_conf).any():
 448:             lms_yx_conf[np.isnan(lms_yx_conf).any(axis=1)] = np.array([0.0, 0.0, 0.0], dtype=np.float32)
 449:         return avg_conf, lms_yx_conf
 450: 
 451:     def _compute_jaw_open(self, landmarks_xy: np.ndarray) -> float:
 452:         if landmarks_xy.shape[0] < 66:
 453:             return 0.0
 454: 
 455:         norm_distance_y = float(
 456:             np.mean(
 457:                 [
 458:                     landmarks_xy[27, 1] - landmarks_xy[28, 1],
 459:                     landmarks_xy[28, 1] - landmarks_xy[29, 1],
 460:                     landmarks_xy[29, 1] - landmarks_xy[30, 1],
 461:                 ]
 462:             )
 463:         )
 464:         denom = max(abs(norm_distance_y), 1e-6)
 465: 
 466:         upper = float(np.mean(landmarks_xy[[59, 60, 61], 1], axis=0))
 467:         lower = float(np.mean(landmarks_xy[[63, 64, 65], 1], axis=0))
 468:         mouth_open_ratio = abs(upper - lower) / denom
 469:         return float(np.clip(mouth_open_ratio * self._jaw_open_scale, 0.0, 1.0))
 470: 
 471:     def _build_arkit_blendshapes(self, jaw_open: float) -> dict:
 472:         out = {}
 473:         for name in ARKIT_52_BLENDSHAPE_NAMES:
 474:             out[name] = 0.0
 475:         out["jawOpen"] = float(jaw_open)
 476:         return out
 477: 
 478:     def detect(self, frame_bgr):
 479:         self._frame_counter += 1
 480:         if (self._frame_counter % self._detect_every_n) != 0:
 481:             return list(self._last_detections)
 482: 
 483:         if frame_bgr is None or getattr(frame_bgr, "shape", None) is None:
 484:             self._last_detections = []
 485:             return []
 486: 
 487:         orig_h, orig_w = frame_bgr.shape[:2]
 488: 
 489:         t_resize_start = time.perf_counter() if self._enable_profiling else 0.0
 490:         work_frame = frame_bgr
 491:         scale_x = 1.0
 492:         scale_y = 1.0
 493: 
 494:         if orig_w > self._max_detection_width:
 495:             detect_w = self._max_detection_width
 496:             scale_factor = detect_w / float(orig_w)
 497:             detect_h = max(1, int(orig_h * scale_factor))
 498:             work_frame = cv2.resize(frame_bgr, (detect_w, detect_h), interpolation=cv2.INTER_LINEAR)
 499:             scale_x = orig_w / float(detect_w)
 500:             scale_y = orig_h / float(detect_h)
 501: 
 502:         if self._enable_profiling:
 503:             self._profiling_stats["resize_total"] += time.perf_counter() - t_resize_start
 504: 
 505:         work_h, work_w = work_frame.shape[:2]
 506: 
 507:         t_detect_start = time.perf_counter() if self._enable_profiling else 0.0
 508:         faces = self._detect_faces(work_frame)
 509:         if self._enable_profiling:
 510:             self._profiling_stats["detect_total"] += time.perf_counter() - t_detect_start
 511:         if not faces:
 512:             self._last_detections = []
 513:             return []
 514: 
 515:         detections = []
 516:         for (x, y, bw, bh, conf) in faces:
 517:             t_landmarks_start = time.perf_counter() if self._enable_profiling else 0.0
 518:             x1 = int(max(0, x - 0.1 * bw))
 519:             y1 = int(max(0, y - 0.125 * bh))
 520:             x2 = int(min(work_w, x + bw + 0.1 * bw))
 521:             y2 = int(min(work_h, y + bh + 0.125 * bh))
 522:             if x2 - x1 < 4 or y2 - y1 < 4:
 523:                 continue
 524: 
 525:             with self._lock:
 526:                 input_tensor, crop_info = self._preprocess_landmarks(work_frame, (x1, y1, x2, y2))
 527:                 out = self._landmark_session.run([], {self._landmark_input_name: input_tensor})[0]
 528: 
 529:             if out is None:
 530:                 continue
 531: 
 532:             if out.ndim == 4:
 533:                 tensor = out[0]
 534:             else:
 535:                 tensor = out
 536: 
 537:             avg_conf, lms_yx_conf = self._decode_landmarks(np.asarray(tensor), crop_info)
 538: 
 539:             if self._enable_profiling:
 540:                 self._profiling_stats["landmarks_total"] += time.perf_counter() - t_landmarks_start
 541: 
 542:             landmarks_xy = np.stack([lms_yx_conf[:, 1], lms_yx_conf[:, 0]], axis=1)
 543: 
 544:             t_post_start = time.perf_counter() if self._enable_profiling else 0.0
 545:             if scale_x != 1.0 or scale_y != 1.0:
 546:                 landmarks_xy = landmarks_xy.astype(np.float32, copy=True)
 547:                 landmarks_xy[:, 0] *= float(scale_x)
 548:                 landmarks_xy[:, 1] *= float(scale_y)
 549:                 logger.debug(f"OpenSeeFace upscale: landmarks rescaled by x={scale_x:.2f}, y={scale_y:.2f}")
 550: 
 551:             x_min = int(max(0, np.min(landmarks_xy[:, 0])))
 552:             y_min = int(max(0, np.min(landmarks_xy[:, 1])))
 553:             x_max = int(min(orig_w - 1, np.max(landmarks_xy[:, 0])))
 554:             y_max = int(min(orig_h - 1, np.max(landmarks_xy[:, 1])))
 555:             bbox_w = max(0, x_max - x_min)
 556:             bbox_h = max(0, y_max - y_min)
 557: 
 558:             centroid = (x_min + (bbox_w // 2), y_min + (bbox_h // 2))
 559:             jaw_open = self._compute_jaw_open(landmarks_xy)
 560: 
 561:             landmarks_xyz = np.zeros((int(lms_yx_conf.shape[0]), 3), dtype=np.float32)
 562:             landmarks_xyz[:, 0] = landmarks_xy[:, 0]
 563:             landmarks_xyz[:, 1] = landmarks_xy[:, 1]
 564: 
 565:             detections.append(
 566:                 {
 567:                     "bbox": (x_min, y_min, bbox_w, bbox_h),
 568:                     "centroid": centroid,
 569:                     "source_detector": "face_landmarker",
 570:                     "label": "face",
 571:                     "confidence": float(avg_conf if avg_conf > 0 else conf),
 572:                     "landmarks": landmarks_xyz.tolist(),
 573:                     "blendshapes": self._build_arkit_blendshapes(jaw_open),
 574:                 }
 575:             )
 576: 
 577:             if self._enable_profiling:
 578:                 self._profiling_stats["post_total"] += time.perf_counter() - t_post_start
 579: 
 580:         self._last_detections = detections
 581: 
 582:         if self._enable_profiling:
 583:             self._profiling_stats["frame_count"] += 1
 584:             if (self._profiling_stats["frame_count"] % 20) == 0:
 585:                 self._log_profiling_stats()
 586: 
 587:         return detections
 588: 
 589:     def _log_profiling_stats(self) -> None:
 590:         frame_count = int(self._profiling_stats.get("frame_count", 0) or 0)
 591:         if frame_count <= 0:
 592:             return
 593: 
 594:         resize_ms = (self._profiling_stats["resize_total"] / frame_count) * 1000.0
 595:         detect_ms = (self._profiling_stats["detect_total"] / frame_count) * 1000.0
 596:         landmarks_ms = (self._profiling_stats["landmarks_total"] / frame_count) * 1000.0
 597:         post_ms = (self._profiling_stats["post_total"] / frame_count) * 1000.0
 598: 
 599:         logger.info(
 600:             "[PROFILING] OpenSeeFace after %s frames: resize=%.2fms/frame, detect=%.2fms/frame, "
 601:             "landmarks=%.2fms/frame, post=%.2fms/frame",
 602:             frame_count,
 603:             resize_ms,
 604:             detect_ms,
 605:             landmarks_ms,
 606:             post_ms,
 607:         )
 608: 
 609: 
 610: class InsightFaceEngine:
 611:     def __init__(
 612:         self,
 613:         model_name: Optional[str] = None,
 614:         det_size: Optional[int] = None,
 615:         use_gpu: bool = False,
 616:     ):
 617:         if not use_gpu:
 618:             raise RuntimeError(
 619:                 "InsightFace engine is GPU-only. Set STEP5_ENABLE_GPU=1, include 'insightface' in STEP5_GPU_ENGINES, "
 620:                 "and run with use_gpu=True."
 621:             )
 622: 
 623:         self._lock = threading.Lock()
 624:         self._use_gpu = True
 625: 
 626:         try:
 627:             import onnxruntime as ort
 628:         except ImportError as e:
 629:             raise RuntimeError(
 630:                 "InsightFace engine requires onnxruntime in insightface_env. Install it (and onnxruntime-gpu) in that venv."
 631:             ) from e
 632: 
 633:         available_providers = ort.get_available_providers()
 634:         logger.info(f"[InsightFace] Available ONNXRuntime providers: {available_providers}")
 635:         if "CUDAExecutionProvider" not in available_providers:
 636:             raise RuntimeError(
 637:                 "InsightFace engine requires CUDAExecutionProvider (onnxruntime-gpu). "
 638:                 "Install onnxruntime-gpu in insightface_env and ensure CUDA libs are available."
 639:             )
 640: 
 641:         try:
 642:             from insightface.app import FaceAnalysis
 643:         except ImportError as e:
 644:             raise RuntimeError(
 645:                 "InsightFace engine requires the 'insightface' Python package inside insightface_env."
 646:             ) from e
 647: 
 648:         self._enable_profiling = os.environ.get("STEP5_ENABLE_PROFILING", "0").strip().lower() in {
 649:             "1",
 650:             "true",
 651:             "yes",
 652:         }
 653:         self._profiling_stats = {
 654:             "resize_total": 0.0,
 655:             "detect_total": 0.0,
 656:             "post_total": 0.0,
 657:             "frame_count": 0,
 658:         }
 659: 
 660:         env_max_faces = os.environ.get("STEP5_INSIGHTFACE_MAX_FACES")
 661:         self._max_faces = _parse_optional_positive_int(env_max_faces)
 662: 
 663:         env_max_width = os.environ.get("STEP5_INSIGHTFACE_MAX_WIDTH")
 664:         if env_max_width is None or env_max_width.strip() == "":
 665:             env_max_width = os.environ.get("STEP5_YUNET_MAX_WIDTH", "1280")
 666:         self._max_detection_width = max(1, int(str(env_max_width).strip() or "1280"))
 667: 
 668:         env_detect_every = os.environ.get("STEP5_INSIGHTFACE_DETECT_EVERY_N")
 669:         if env_detect_every is None or env_detect_every.strip() == "":
 670:             env_detect_every = os.environ.get("STEP5_BLENDSHAPES_THROTTLE_N", "1")
 671:         self._detect_every_n = max(1, int(str(env_detect_every).strip() or "1"))
 672: 
 673:         self._jaw_open_scale = float(os.environ.get("STEP5_INSIGHTFACE_JAWOPEN_SCALE", "1.0"))
 674: 
 675:         env_model_name = os.environ.get("STEP5_INSIGHTFACE_MODEL_NAME")
 676:         self._model_name = (model_name or (env_model_name.strip() if env_model_name else "") or "antelopev2")
 677: 
 678:         env_det_size = os.environ.get("STEP5_INSIGHTFACE_DET_SIZE")
 679:         det_size_value = det_size
 680:         if det_size_value is None:
 681:             try:
 682:                 det_size_value = int(str(env_det_size).strip()) if env_det_size else 640
 683:             except Exception:
 684:                 det_size_value = 640
 685:         self._det_size = max(64, int(det_size_value))
 686: 
 687:         ctx_id_raw = os.environ.get("STEP5_INSIGHTFACE_CTX_ID")
 688:         try:
 689:             ctx_id = int(str(ctx_id_raw).strip()) if ctx_id_raw else 0
 690:         except Exception:
 691:             ctx_id = 0
 692:         if ctx_id < 0:
 693:             ctx_id = 0
 694: 
 695:         providers = ["CUDAExecutionProvider"]
 696:         insightface_root_raw = os.environ.get("INSIGHTFACE_HOME", "").strip()
 697:         insightface_root = str(Path(insightface_root_raw or "~/.insightface").expanduser())
 698: 
 699:         allowed_modules_env = os.environ.get("STEP5_INSIGHTFACE_ALLOWED_MODULES", "").strip()
 700:         allowed_modules = None
 701:         if allowed_modules_env:
 702:             allowed_modules = [m.strip() for m in allowed_modules_env.split(",") if m.strip()]
 703:             if "detection" not in allowed_modules:
 704:                 raise RuntimeError(
 705:                     "STEP5_INSIGHTFACE_ALLOWED_MODULES must include 'detection' (required by FaceAnalysis)."
 706:                 )
 707: 
 708:         try:
 709:             self._app = FaceAnalysis(
 710:                 name=self._model_name,
 711:                 root=insightface_root,
 712:                 providers=providers,
 713:                 allowed_modules=allowed_modules,
 714:             )
 715:         except FileExistsError as e:
 716:             model_dir = Path(insightface_root) / "models" / self._model_name
 717:             quarantine_suffix = f"corrupt_{int(time.time())}"
 718:             quarantine_dir = model_dir.with_name(f"{model_dir.name}.{quarantine_suffix}")
 719:             try:
 720:                 if model_dir.exists():
 721:                     model_dir.rename(quarantine_dir)
 722:                     logger.warning(
 723:                         "[InsightFace] Model cache directory already exists but download attempted; "
 724:                         "quarantined %s -> %s and retrying initialization.",
 725:                         model_dir,
 726:                         quarantine_dir,
 727:                     )
 728:             except Exception as rename_err:
 729:                 raise RuntimeError(
 730:                     f"InsightFace model cache appears corrupted at {model_dir}. "
 731:                     f"Automatic quarantine failed ({rename_err}). "
 732:                     "Please remove or rename the directory and retry."
 733:                 ) from e
 734: 
 735:             self._app = FaceAnalysis(
 736:                 name=self._model_name,
 737:                 root=insightface_root,
 738:                 providers=providers,
 739:                 allowed_modules=allowed_modules,
 740:             )
 741:         self._app.prepare(ctx_id=ctx_id, det_size=(self._det_size, self._det_size))
 742: 
 743:         self._frame_counter = 0
 744:         self._last_detections = []
 745: 
 746:         logger.info(
 747:             f"[InsightFace] Initialized model_name={self._model_name}, det_size={self._det_size}, "
 748:             f"max_width={self._max_detection_width}, max_faces={self._max_faces}, detect_every_n={self._detect_every_n}"
 749:         )
 750: 
 751:     def _build_arkit_blendshapes(self, jaw_open: float) -> dict:
 752:         out = {}
 753:         for name in ARKIT_52_BLENDSHAPE_NAMES:
 754:             out[name] = 0.0
 755:         out["jawOpen"] = float(jaw_open)
 756:         return out
 757: 
 758:     def _compute_jaw_open_from_dlib68(self, landmarks_68: np.ndarray) -> float:
 759:         lm = np.asarray(landmarks_68, dtype=np.float32)
 760:         if lm.ndim != 2 or lm.shape[0] < 68:
 761:             return 0.0
 762:         try:
 763:             upper_inner = float(np.mean(lm[60:65, 1], axis=0))
 764:             lower_inner = float(np.mean(lm[65:68, 1], axis=0))
 765:             mouth_open = abs(lower_inner - upper_inner)
 766: 
 767:             nose_y = float(lm[33, 1])
 768:             chin_y = float(lm[8, 1])
 769:             denom = max(abs(chin_y - nose_y), 1e-6)
 770:             ratio = (mouth_open / denom) * float(self._jaw_open_scale)
 771:             return float(np.clip(ratio, 0.0, 1.0))
 772:         except Exception:
 773:             return 0.0
 774: 
 775:     def _get_face_landmarks_68(self, face_obj) -> Optional[np.ndarray]:
 776:         candidates = [
 777:             "landmark_3d_68",
 778:             "landmark_2d_68",
 779:         ]
 780:         for attr in candidates:
 781:             try:
 782:                 val = getattr(face_obj, attr, None)
 783:             except Exception:
 784:                 val = None
 785:             if val is None:
 786:                 try:
 787:                     val = face_obj.get(attr) if hasattr(face_obj, "get") else None
 788:                 except Exception:
 789:                     val = None
 790:             if val is None:
 791:                 continue
 792: 
 793:             lm = np.asarray(val)
 794:             if lm.ndim != 2 or lm.shape[0] < 68:
 795:                 continue
 796:             if lm.shape[1] >= 2:
 797:                 return lm[:, :2].astype(np.float32)
 798:         return None
 799: 
 800:     def detect(self, frame_bgr):
 801:         self._frame_counter += 1
 802:         if (self._frame_counter % self._detect_every_n) != 0:
 803:             return list(self._last_detections)
 804: 
 805:         if frame_bgr is None or getattr(frame_bgr, "shape", None) is None:
 806:             self._last_detections = []
 807:             return []
 808: 
 809:         orig_h, orig_w = frame_bgr.shape[:2]
 810:         work_frame = frame_bgr
 811:         scale_x = 1.0
 812:         scale_y = 1.0
 813: 
 814:         t_resize_start = time.perf_counter() if self._enable_profiling else 0.0
 815:         if orig_w > self._max_detection_width:
 816:             detect_w = self._max_detection_width
 817:             scale_factor = detect_w / float(orig_w)
 818:             detect_h = max(1, int(orig_h * scale_factor))
 819:             work_frame = cv2.resize(frame_bgr, (detect_w, detect_h), interpolation=cv2.INTER_LINEAR)
 820:             scale_x = orig_w / float(detect_w)
 821:             scale_y = orig_h / float(detect_h)
 822:         if self._enable_profiling:
 823:             self._profiling_stats["resize_total"] += time.perf_counter() - t_resize_start
 824: 
 825:         t_detect_start = time.perf_counter() if self._enable_profiling else 0.0
 826:         try:
 827:             with self._lock:
 828:                 faces = self._app.get(work_frame)
 829:         except Exception as e:
 830:             logger.warning("[InsightFace] Detection failed: %s", e)
 831:             self._last_detections = []
 832:             return []
 833:         if self._enable_profiling:
 834:             self._profiling_stats["detect_total"] += time.perf_counter() - t_detect_start
 835: 
 836:         if not faces:
 837:             self._last_detections = []
 838:             return []
 839: 
 840:         if self._max_faces is not None:
 841:             faces = faces[: self._max_faces]
 842: 
 843:         detections = []
 844:         t_post_start = time.perf_counter() if self._enable_profiling else 0.0
 845:         for face in faces:
 846:             bbox = None
 847:             try:
 848:                 bbox = getattr(face, "bbox", None)
 849:             except Exception:
 850:                 bbox = None
 851:             if bbox is None:
 852:                 try:
 853:                     bbox = face.get("bbox") if hasattr(face, "get") else None
 854:                 except Exception:
 855:                     bbox = None
 856:             if bbox is None:
 857:                 continue
 858: 
 859:             bbox_arr = np.asarray(bbox, dtype=np.float32).reshape(-1)
 860:             if bbox_arr.size < 4:
 861:                 continue
 862: 
 863:             x1, y1, x2, y2 = [float(v) for v in bbox_arr[:4]]
 864:             if scale_x != 1.0 or scale_y != 1.0:
 865:                 x1 *= float(scale_x)
 866:                 x2 *= float(scale_x)
 867:                 y1 *= float(scale_y)
 868:                 y2 *= float(scale_y)
 869: 
 870:             x1_i = max(0, int(x1))
 871:             y1_i = max(0, int(y1))
 872:             x2_i = min(int(orig_w), int(x2))
 873:             y2_i = min(int(orig_h), int(y2))
 874:             if x2_i <= x1_i or y2_i <= y1_i:
 875:                 continue
 876: 
 877:             bbox_w = int(x2_i - x1_i)
 878:             bbox_h = int(y2_i - y1_i)
 879:             centroid = (x1_i + (bbox_w // 2), y1_i + (bbox_h // 2))
 880: 
 881:             try:
 882:                 score = float(getattr(face, "det_score", 1.0))
 883:             except Exception:
 884:                 score = 1.0
 885: 
 886:             landmarks_68 = self._get_face_landmarks_68(face)
 887:             if landmarks_68 is not None and (scale_x != 1.0 or scale_y != 1.0):
 888:                 landmarks_68 = landmarks_68.astype(np.float32, copy=True)
 889:                 landmarks_68[:, 0] *= float(scale_x)
 890:                 landmarks_68[:, 1] *= float(scale_y)
 891: 
 892:             jaw_open = self._compute_jaw_open_from_dlib68(landmarks_68) if landmarks_68 is not None else 0.0
 893:             blendshapes = self._build_arkit_blendshapes(jaw_open)
 894: 
 895:             detections.append(
 896:                 {
 897:                     "bbox": (x1_i, y1_i, bbox_w, bbox_h),
 898:                     "centroid": centroid,
 899:                     "source_detector": "face_landmarker",
 900:                     "label": "face",
 901:                     "confidence": score,
 902:                     "landmarks": landmarks_68.tolist() if landmarks_68 is not None else [],
 903:                     "blendshapes": blendshapes,
 904:                 }
 905:             )
 906: 
 907:         if self._enable_profiling:
 908:             self._profiling_stats["post_total"] += time.perf_counter() - t_post_start
 909:             self._profiling_stats["frame_count"] += 1
 910:             if (self._profiling_stats["frame_count"] % 20) == 0:
 911:                 frame_count = int(self._profiling_stats.get("frame_count", 0) or 0)
 912:                 if frame_count > 0:
 913:                     resize_ms = (self._profiling_stats["resize_total"] / frame_count) * 1000.0
 914:                     detect_ms = (self._profiling_stats["detect_total"] / frame_count) * 1000.0
 915:                     post_ms = (self._profiling_stats["post_total"] / frame_count) * 1000.0
 916:                     logger.info(
 917:                         "[PROFILING] InsightFace after %s frames: resize=%.2fms/frame, detect=%.2fms/frame, post=%.2fms/frame",
 918:                         frame_count,
 919:                         resize_ms,
 920:                         detect_ms,
 921:                         post_ms,
 922:                     )
 923: 
 924:         self._last_detections = detections
 925:         return detections
 926: 
 927: 
 928: class OpenCVHaarFaceEngine:
 929:     def __init__(self, cascade_path: Optional[str] = None):
 930:         self._lock = threading.Lock()
 931:         self._cascade_path = cascade_path or os.environ.get("STEP5_OPENCV_HAAR_CASCADE_PATH")
 932:         if not self._cascade_path:
 933:             self._cascade_path = str(Path(cv2.data.haarcascades) / "haarcascade_frontalface_default.xml")
 934: 
 935:         self._max_faces = _parse_optional_positive_int(os.environ.get("STEP5_OPENCV_MAX_FACES"))
 936: 
 937:         self._classifier = cv2.CascadeClassifier(self._cascade_path)
 938:         if self._classifier.empty():
 939:             raise RuntimeError(f"Unable to load Haar cascade: {self._cascade_path}")
 940: 
 941:     def detect(self, frame_bgr):
 942:         gray = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2GRAY)
 943:         with self._lock:
 944:             faces = self._classifier.detectMultiScale(
 945:                 gray,
 946:                 scaleFactor=1.1,
 947:                 minNeighbors=5,
 948:                 minSize=(30, 30),
 949:             )
 950: 
 951:         if self._max_faces is not None:
 952:             faces = faces[: self._max_faces]
 953: 
 954:         detections = []
 955:         for (x, y, w, h) in faces:
 956:             x_i = int(x)
 957:             y_i = int(y)
 958:             w_i = int(w)
 959:             h_i = int(h)
 960:             detections.append(
 961:                 {
 962:                     "bbox": (x_i, y_i, w_i, h_i),
 963:                     "centroid": (x_i + (w_i // 2), y_i + (h_i // 2)),
 964:                     "source_detector": "face_landmarker",
 965:                     "label": "face",
 966:                     "confidence": 1.0,
 967:                     "blendshapes": None,
 968:                 }
 969:             )
 970: 
 971:         return detections
 972: 
 973: 
 974: class OpenCVYuNetFaceEngine:
 975:     def __init__(
 976:         self,
 977:         model_path: Optional[str] = None,
 978:         score_threshold: float = 0.7,
 979:         nms_threshold: float = 0.3,
 980:         top_k: int = 5000,
 981:         use_gpu: bool = False,
 982:     ):
 983:         self._lock = threading.Lock()
 984:         self._use_gpu = use_gpu
 985:         if use_gpu:
 986:             logger.info(
 987:                 "YuNet detection currently repose sur cv2.FaceDetectorYN (CPU). "
 988:                 "Le flag GPU optimise uniquement les composants aval (FaceMesh/PyFeat)."
 989:             )
 990:         
 991:         # Force OpenCV to use single thread to avoid contention with multiprocessing
 992:         cv2.setNumThreads(1)
 993:         
 994:         # Configure downscaling for performance (coordinates will be rescaled to original)
 995:         self._max_detection_width = int(os.environ.get("STEP5_YUNET_MAX_WIDTH", "640"))
 996:         logger.info(f"YuNet downscale enabled: max_width={self._max_detection_width}px (coordinates auto-rescaled to original resolution)")
 997: 
 998:         self._max_faces = _parse_optional_positive_int(os.environ.get("STEP5_OPENCV_MAX_FACES"))
 999:         
1000:         self._model_path = model_path or os.environ.get("STEP5_YUNET_MODEL_PATH")
1001:         if not self._model_path:
1002:             raise RuntimeError("Missing STEP5_YUNET_MODEL_PATH (YuNet model .onnx)")
1003: 
1004:         self._model_path = self._resolve_model_path(self._model_path)
1005:         if not Path(self._model_path).exists():
1006:             raise RuntimeError(f"YuNet ONNX model not found: {self._model_path}")
1007: 
1008:         if not hasattr(cv2, "FaceDetectorYN"):
1009:             raise RuntimeError("cv2.FaceDetectorYN is not available (opencv-contrib-python required)")
1010: 
1011:         self._score_threshold = float(score_threshold)
1012:         self._nms_threshold = float(nms_threshold)
1013:         self._top_k = int(top_k)
1014: 
1015:         self._detector = cv2.FaceDetectorYN.create(
1016:             self._model_path,
1017:             "",
1018:             (320, 320),
1019:             self._score_threshold,
1020:             self._nms_threshold,
1021:             self._top_k,
1022:         )
1023: 
1024:     def _resolve_model_path(self, model_path: str) -> str:
1025:         candidate = Path(model_path)
1026:         if candidate.is_absolute():
1027:             return str(candidate)
1028: 
1029:         if candidate.exists():
1030:             return str(candidate)
1031: 
1032:         step5_dir = Path(__file__).resolve().parent
1033:         project_root = step5_dir.parent.parent
1034: 
1035:         candidates = [
1036:             project_root / candidate,
1037:             step5_dir / candidate,
1038:         ]
1039:         for c in candidates:
1040:             if c.exists():
1041:                 return str(c)
1042: 
1043:         return str(candidate)
1044: 
1045:     def detect(self, frame_bgr):
1046:         orig_height, orig_width = frame_bgr.shape[:2]
1047:         
1048:         # Downscale for detection if needed
1049:         if orig_width > self._max_detection_width:
1050:             scale_factor = self._max_detection_width / orig_width
1051:             detect_width = self._max_detection_width
1052:             detect_height = int(orig_height * scale_factor)
1053:             frame_resized = cv2.resize(frame_bgr, (detect_width, detect_height), interpolation=cv2.INTER_LINEAR)
1054:         else:
1055:             frame_resized = frame_bgr
1056:             detect_width = orig_width
1057:             detect_height = orig_height
1058:             scale_factor = 1.0
1059:         
1060:         with self._lock:
1061:             self._detector.setInputSize((detect_width, detect_height))
1062:             _, faces = self._detector.detect(frame_resized)
1063: 
1064:         detections = []
1065:         if faces is None:
1066:             return detections
1067: 
1068:         # Rescale coordinates to original resolution
1069:         rescale = orig_width / detect_width
1070:         if scale_factor != 1.0:
1071:             logger.debug(f"YuNet upscale: {detect_width}x{detect_height} -> {orig_width}x{orig_height} (rescale={rescale:.2f})")
1072:         
1073:         face_limit = self._max_faces
1074:         for idx, row in enumerate(faces):
1075:             if face_limit is not None and idx >= face_limit:
1076:                 break
1077:             x, y, w, h = row[:4]
1078:             score = float(row[-1]) if len(row) >= 15 else 0.0
1079:             
1080:             # Rescale to original coordinates
1081:             x_orig = int(max(0, x * rescale))
1082:             y_orig = int(max(0, y * rescale))
1083:             w_orig = int(max(0, w * rescale))
1084:             h_orig = int(max(0, h * rescale))
1085:             
1086:             detections.append(
1087:                 {
1088:                     "bbox": (x_orig, y_orig, w_orig, h_orig),
1089:                     "centroid": (x_orig + (w_orig // 2), y_orig + (h_orig // 2)),
1090:                     "source_detector": "face_landmarker",
1091:                     "label": "face",
1092:                     "confidence": score,
1093:                     "blendshapes": None,
1094:                 }
1095:             )
1096: 
1097:         return detections
1098: 
1099: 
1100: class OpenCVYuNetPyFeatEngine:
1101:     def __init__(
1102:         self,
1103:         yunet_model_path: Optional[str] = None,
1104:         facemesh_model_path: Optional[str] = None,
1105:         pyfeat_model_path: Optional[str] = None,
1106:         score_threshold: float = 0.7,
1107:         nms_threshold: float = 0.3,
1108:         top_k: int = 5000,
1109:         use_gpu: bool = False,
1110:     ):
1111:         self._lock = threading.Lock()
1112:         self._use_gpu = use_gpu
1113:         
1114:         # Debug: log environment variable values
1115:         profiling_env = os.environ.get("STEP5_ENABLE_PROFILING", "NOT_SET")
1116:         throttle_env = os.environ.get("STEP5_BLENDSHAPES_THROTTLE_N", "NOT_SET")
1117:         logger.info(f"[DEBUG] STEP5_ENABLE_PROFILING={profiling_env}, STEP5_BLENDSHAPES_THROTTLE_N={throttle_env}")
1118:         
1119:         self._enable_profiling = os.environ.get("STEP5_ENABLE_PROFILING", "0").strip() in {"1", "true", "yes"}
1120:         logger.info(f"[DEBUG] Profiling enabled: {self._enable_profiling}")
1121:         
1122:         self._profiling_stats = {
1123:             "yunet_total": 0.0,
1124:             "roi_extraction": 0.0,
1125:             "facemesh_total": 0.0,
1126:             "pyfeat_total": 0.0,
1127:             "frame_count": 0
1128:         }
1129:         
1130:         # Blendshapes throttling: compute every N frames to reduce CPU cost
1131:         self._blendshapes_throttle_n = max(1, int(os.environ.get("STEP5_BLENDSHAPES_THROTTLE_N", "1")))
1132:         logger.info(f"[DEBUG] Blendshapes throttle N: {self._blendshapes_throttle_n}")
1133:         self._frame_counter = 0
1134:         self._last_blendshapes_cache = {}  # object_id -> blendshapes dict
1135: 
1136:         self._jaw_open_scale = float(os.environ.get("STEP5_OPENCV_JAWOPEN_SCALE", "1.0"))
1137:         
1138:         self._yunet = OpenCVYuNetFaceEngine(
1139:             model_path=yunet_model_path,
1140:             score_threshold=score_threshold,
1141:             nms_threshold=nms_threshold,
1142:             top_k=top_k,
1143:             use_gpu=use_gpu,
1144:         )
1145:         
1146:         try:
1147:             from onnx_facemesh_detector import ONNXFaceMeshDetector
1148:             self._facemesh_detector = ONNXFaceMeshDetector(
1149:                 model_path=facemesh_model_path,
1150:                 use_gpu=use_gpu,
1151:             )
1152:         except Exception as e:
1153:             raise RuntimeError(f"Failed to initialize FaceMesh ONNX detector: {e}")
1154:         
1155:         self._blendshape_extractor = None
1156:         strict_pyfeat = os.environ.get("STEP5_PYFEAT_STRICT", "0").strip().lower() in {"1", "true", "yes"}
1157:         try:
1158:             from pyfeat_blendshape_extractor import PyFeatBlendshapeExtractor
1159: 
1160:             self._blendshape_extractor = PyFeatBlendshapeExtractor(
1161:                 model_path=pyfeat_model_path,
1162:                 use_gpu=use_gpu,
1163:             )
1164:         except Exception as e:
1165:             if strict_pyfeat:
1166:                 raise RuntimeError(f"Failed to initialize py-feat blendshape extractor: {e}")
1167: 
1168:             logger.warning(
1169:                 "py-feat blendshape extractor disabled (continuing without blendshapes): %s",
1170:                 e,
1171:             )
1172: 
1173:     def detect(self, frame_bgr):
1174:         height, width = frame_bgr.shape[:2]
1175:         self._frame_counter += 1
1176:         should_compute_blendshapes = (self._frame_counter % self._blendshapes_throttle_n) == 0
1177:         
1178:         t_start_yunet = time.perf_counter() if self._enable_profiling else 0
1179:         yunet_detections = self._yunet.detect(frame_bgr)
1180:         if self._enable_profiling:
1181:             self._profiling_stats["yunet_total"] += time.perf_counter() - t_start_yunet
1182:         
1183:         detections_with_blendshapes = []
1184:         
1185:         for det in yunet_detections:
1186:             bbox = det["bbox"]
1187:             x, y, w, h = bbox
1188:             
1189:             x_safe = max(0, x)
1190:             y_safe = max(0, y)
1191:             x2_safe = min(width, x + w)
1192:             y2_safe = min(height, y + h)
1193:             
1194:             if x2_safe <= x_safe or y2_safe <= y_safe:
1195:                 detections_with_blendshapes.append(det)
1196:                 continue
1197:             
1198:             t_start_roi = time.perf_counter() if self._enable_profiling else 0
1199:             face_roi = frame_bgr[y_safe:y2_safe, x_safe:x2_safe]
1200:             if self._enable_profiling:
1201:                 self._profiling_stats["roi_extraction"] += time.perf_counter() - t_start_roi
1202:             
1203:             t_start_facemesh = time.perf_counter() if self._enable_profiling else 0
1204:             landmarks_478 = self._facemesh_detector.detect_landmarks(
1205:                 face_roi,
1206:                 (x_safe, y_safe, x2_safe - x_safe, y2_safe - y_safe)
1207:             )
1208:             if self._enable_profiling:
1209:                 self._profiling_stats["facemesh_total"] += time.perf_counter() - t_start_facemesh
1210:             
1211:             blendshapes = None
1212:             if landmarks_478 is not None and self._blendshape_extractor is not None:
1213:                 # Use simple bbox center as object identifier for caching
1214:                 object_id = f"{x_safe}_{y_safe}_{x2_safe}_{y2_safe}"
1215:                 
1216:                 if should_compute_blendshapes:
1217:                     t_start_pyfeat = time.perf_counter() if self._enable_profiling else 0
1218:                     blendshapes = self._blendshape_extractor.extract_blendshapes(
1219:                         landmarks_478,
1220:                         width,
1221:                         height,
1222:                     )
1223:                     if self._enable_profiling:
1224:                         self._profiling_stats["pyfeat_total"] += time.perf_counter() - t_start_pyfeat
1225:                     
1226:                     # Cache for next frames
1227:                     if blendshapes is not None:
1228:                         blendshapes_scaled = _apply_jawopen_scale(blendshapes, self._jaw_open_scale)
1229:                         self._last_blendshapes_cache[object_id] = blendshapes_scaled
1230:                         blendshapes = blendshapes_scaled
1231:                 else:
1232:                     # Reuse cached blendshapes from previous computation
1233:                     blendshapes = self._last_blendshapes_cache.get(object_id)
1234:                     # If no cache available (first frames), compute anyway
1235:                     if blendshapes is None:
1236:                         t_start_pyfeat = time.perf_counter() if self._enable_profiling else 0
1237:                         blendshapes = self._blendshape_extractor.extract_blendshapes(
1238:                             landmarks_478,
1239:                             width,
1240:                             height,
1241:                         )
1242:                         if self._enable_profiling:
1243:                             self._profiling_stats["pyfeat_total"] += time.perf_counter() - t_start_pyfeat
1244:                         if blendshapes is not None:
1245:                             blendshapes_scaled = _apply_jawopen_scale(blendshapes, self._jaw_open_scale)
1246:                             self._last_blendshapes_cache[object_id] = blendshapes_scaled
1247:                             blendshapes = blendshapes_scaled
1248:             
1249:             detection_with_bs = det.copy()
1250:             detection_with_bs["bbox"] = (x_safe, y_safe, x2_safe - x_safe, y2_safe - y_safe)
1251:             detection_with_bs["landmarks"] = landmarks_478.tolist() if landmarks_478 is not None else []
1252:             detection_with_bs["blendshapes"] = blendshapes
1253:             detections_with_blendshapes.append(detection_with_bs)
1254:         
1255:         if self._enable_profiling:
1256:             self._profiling_stats["frame_count"] += 1
1257:             # Log every 20 frames (compatible with multiprocessing chunk size)
1258:             if self._profiling_stats["frame_count"] % 20 == 0:
1259:                 self._log_profiling_stats()
1260:         
1261:         return detections_with_blendshapes
1262:     
1263:     def _log_profiling_stats(self):
1264:         """Log accumulated profiling statistics."""
1265:         fc = self._profiling_stats["frame_count"]
1266:         if fc == 0:
1267:             return
1268:         logger.info(
1269:             f"[PROFILING] After {fc} frames: "
1270:             f"YuNet={self._profiling_stats['yunet_total']/fc*1000:.2f}ms/frame, "
1271:             f"ROI={self._profiling_stats['roi_extraction']/fc*1000:.2f}ms/frame, "
1272:             f"FaceMesh={self._profiling_stats['facemesh_total']/fc*1000:.2f}ms/frame, "
1273:             f"py-feat={self._profiling_stats['pyfeat_total']/fc*1000:.2f}ms/frame"
1274:         )
1275: 
1276: 
1277: class EosFaceEngine:
1278:     def __init__(
1279:         self,
1280:         yunet_model_path: Optional[str] = None,
1281:         facemesh_model_path: Optional[str] = None,
1282:     ):
1283:         try:
1284:             import eos as eos_module
1285:         except Exception as e:
1286:             raise RuntimeError(
1287:                 "eos engine requires eos-py (install it inside eos_env)."
1288:             ) from e
1289: 
1290:         self._eos = eos_module
1291:         self._lock = threading.Lock()
1292:         self._frame_counter = 0
1293:         self._last_detections = []
1294: 
1295:         fit_every_raw = os.environ.get("STEP5_EOS_FIT_EVERY_N")
1296:         if fit_every_raw is None or str(fit_every_raw).strip() == "":
1297:             fit_every_raw = os.environ.get("STEP5_BLENDSHAPES_THROTTLE_N", "1")
1298:         try:
1299:             self._fit_every_n = max(1, int(str(fit_every_raw)))
1300:         except Exception:
1301:             self._fit_every_n = 1
1302: 
1303:         self._jaw_open_scale = float(os.environ.get("STEP5_EOS_JAWOPEN_SCALE", "1.0"))
1304:         self._max_faces = (
1305:             _parse_optional_positive_int(os.environ.get("STEP5_EOS_MAX_FACES"))
1306:             or _parse_optional_positive_int(os.environ.get("STEP5_OPENCV_MAX_FACES"))
1307:         )
1308:         
1309:         self._max_detection_width = int(os.environ.get("STEP5_EOS_MAX_WIDTH", "1280"))
1310:         self._enable_profiling = os.environ.get("STEP5_ENABLE_PROFILING", "").strip().lower() in {"1", "true", "yes"}
1311:         self._profiling_interval = 20
1312: 
1313:         self._yunet = OpenCVYuNetFaceEngine(model_path=yunet_model_path)
1314: 
1315:         try:
1316:             from onnx_facemesh_detector import ONNXFaceMeshDetector
1317:         except Exception as e:
1318:             raise RuntimeError(
1319:                 "eos engine requires ONNXFaceMeshDetector dependencies (onnxruntime + FaceMesh model) inside eos_env."
1320:             ) from e
1321: 
1322:         self._facemesh_detector = ONNXFaceMeshDetector(model_path=facemesh_model_path)
1323: 
1324:         self._models_dir = os.environ.get("STEP5_EOS_MODELS_DIR")
1325:         self._sfm_model_path_raw = os.environ.get("STEP5_EOS_SFM_MODEL_PATH")
1326:         self._expression_blendshapes_path_raw = os.environ.get("STEP5_EOS_EXPRESSION_BLENDSHAPES_PATH")
1327:         self._landmark_mapper_path_raw = os.environ.get("STEP5_EOS_LANDMARK_MAPPER_PATH")
1328:         self._edge_topology_path_raw = os.environ.get("STEP5_EOS_EDGE_TOPOLOGY_PATH")
1329:         self._model_contour_path_raw = os.environ.get("STEP5_EOS_MODEL_CONTOUR_PATH")
1330:         self._contour_landmarks_path_raw = os.environ.get("STEP5_EOS_CONTOUR_LANDMARKS_PATH")
1331: 
1332:         self._morphable_model = None
1333:         self._landmark_mapper = None
1334:         self._edge_topology = None
1335:         self._contour_landmarks = None
1336:         self._model_contour = None
1337: 
1338:         correspondence = [
1339:             (127, 127),
1340:             (234, 234),
1341:             (93, 93),
1342:             (132, 58),
1343:             (58, 172),
1344:             (136, 136),
1345:             (150, 150),
1346:             (176, 176),
1347:             (152, 152),
1348:             (400, 400),
1349:             (379, 379),
1350:             (365, 365),
1351:             (397, 288),
1352:             (361, 361),
1353:             (323, 323),
1354:             (454, 454),
1355:             (356, 356),
1356:             (70, 70),
1357:             (63, 63),
1358:             (105, 105),
1359:             (66, 66),
1360:             (107, 107),
1361:             (336, 336),
1362:             (296, 296),
1363:             (334, 334),
1364:             (293, 293),
1365:             (300, 300),
1366:             (168, 6),
1367:             (197, 195),
1368:             (5, 5),
1369:             (4, 4),
1370:             (75, 75),
1371:             (97, 97),
1372:             (2, 2),
1373:             (326, 326),
1374:             (305, 305),
1375:             (33, 33),
1376:             (160, 160),
1377:             (158, 158),
1378:             (133, 133),
1379:             (153, 153),
1380:             (144, 144),
1381:             (362, 362),
1382:             (385, 385),
1383:             (387, 387),
1384:             (263, 263),
1385:             (373, 373),
1386:             (380, 380),
1387:             (61, 61),
1388:             (39, 39),
1389:             (37, 37),
1390:             (0, 0),
1391:             (267, 267),
1392:             (269, 269),
1393:             (291, 291),
1394:             (321, 321),
1395:             (314, 314),
1396:             (17, 17),
1397:             (84, 84),
1398:             (91, 91),
1399:             (78, 78),
1400:             (82, 82),
1401:             (13, 13),
1402:             (312, 312),
1403:             (308, 308),
1404:             (317, 317),
1405:             (14, 14),
1406:             (87, 87),
1407:         ]
1408: 
1409:         self._mp2dlib_pairs = np.asarray(correspondence, dtype=np.int32)
1410: 
1411:     def _resolve_model_path(self, model_path: str) -> str:
1412:         candidate = Path(model_path)
1413:         if candidate.is_absolute() and candidate.exists():
1414:             return str(candidate)
1415:         if candidate.exists():
1416:             return str(candidate)
1417: 
1418:         step5_dir = Path(__file__).resolve().parent
1419:         project_root = step5_dir.parent.parent
1420: 
1421:         candidates = [
1422:             project_root / candidate,
1423:             step5_dir / candidate,
1424:         ]
1425:         for c in candidates:
1426:             if c.exists():
1427:                 return str(c)
1428: 
1429:         return str(candidate)
1430: 
1431:     def _get_models_base_dir(self) -> Path:
1432:         if self._models_dir and str(self._models_dir).strip():
1433:             return Path(self._resolve_model_path(self._models_dir)).resolve()
1434: 
1435:         step5_dir = Path(__file__).resolve().parent
1436:         models_dir = step5_dir / "models" / "engines" / "eos"
1437:         return models_dir.resolve()
1438: 
1439:     def _ensure_eos_assets_loaded(self) -> None:
1440:         if self._morphable_model is not None:
1441:             return
1442: 
1443:         models_base_dir = self._get_models_base_dir()
1444:         sfm_model_path = self._sfm_model_path_raw or str(models_base_dir / "sfm_shape_3448.bin")
1445:         expression_blendshapes_path = self._expression_blendshapes_path_raw or str(
1446:             models_base_dir / "expression_blendshapes_3448.bin"
1447:         )
1448:         landmark_mapper_path = self._landmark_mapper_path_raw or str(models_base_dir / "ibug_to_sfm.txt")
1449:         edge_topology_path = self._edge_topology_path_raw or str(models_base_dir / "sfm_3448_edge_topology.json")
1450:         model_contour_path = self._model_contour_path_raw or str(models_base_dir / "sfm_model_contours.json")
1451:         contour_landmarks_path = self._contour_landmarks_path_raw or landmark_mapper_path
1452: 
1453:         sfm_model_path = self._resolve_model_path(sfm_model_path)
1454:         expression_blendshapes_path = self._resolve_model_path(expression_blendshapes_path)
1455:         landmark_mapper_path = self._resolve_model_path(landmark_mapper_path)
1456:         edge_topology_path = self._resolve_model_path(edge_topology_path)
1457:         model_contour_path = self._resolve_model_path(model_contour_path)
1458:         contour_landmarks_path = self._resolve_model_path(contour_landmarks_path)
1459: 
1460:         required_files = {
1461:             "sfm_model": sfm_model_path,
1462:             "expression_blendshapes": expression_blendshapes_path,
1463:             "landmark_mapper": landmark_mapper_path,
1464:             "edge_topology": edge_topology_path,
1465:             "model_contour": model_contour_path,
1466:             "contour_landmarks": contour_landmarks_path,
1467:         }
1468:         missing = [k for k, v in required_files.items() if not Path(v).exists()]
1469:         if missing:
1470:             raise RuntimeError(
1471:                 "Missing eos asset files: "
1472:                 + ", ".join(missing)
1473:                 + ". Configure STEP5_EOS_MODELS_DIR and/or STEP5_EOS_*_PATH variables."
1474:             )
1475: 
1476:         model = self._eos.morphablemodel.load_model(sfm_model_path)
1477:         blendshapes = self._eos.morphablemodel.load_blendshapes(expression_blendshapes_path)
1478: 
1479:         self._morphable_model = self._eos.morphablemodel.MorphableModel(
1480:             model.get_shape_model(),
1481:             blendshapes,
1482:             color_model=self._eos.morphablemodel.PcaModel(),
1483:             vertex_definitions=None,
1484:             texture_coordinates=model.get_texture_coordinates(),
1485:         )
1486:         self._landmark_mapper = self._eos.core.LandmarkMapper(landmark_mapper_path)
1487:         self._edge_topology = self._eos.morphablemodel.load_edge_topology(edge_topology_path)
1488:         self._contour_landmarks = self._eos.fitting.ContourLandmarks.load(contour_landmarks_path)
1489:         self._model_contour = self._eos.fitting.ModelContour.load(model_contour_path)
1490: 
1491:     def _convert_478_to_68(self, landmarks_478: np.ndarray) -> np.ndarray:
1492:         lm = np.asarray(landmarks_478, dtype=np.float32)
1493:         if lm.ndim != 2 or lm.shape[0] < 468:
1494:             raise ValueError(f"Invalid FaceMesh landmarks shape: {lm.shape}")
1495:         if lm.shape[0] < 478:
1496:             last = lm[-1:, :]
1497:             pad = np.repeat(last, 478 - lm.shape[0], axis=0)
1498:             lm = np.concatenate([lm, pad], axis=0)
1499:         return lm[self._mp2dlib_pairs].mean(axis=1)
1500: 
1501:     def _compute_jaw_open_from_dlib68(self, landmarks_68: np.ndarray) -> float:
1502:         lm = np.asarray(landmarks_68, dtype=np.float32)
1503:         if lm.ndim != 2 or lm.shape[0] < 68:
1504:             return 0.0
1505: 
1506:         try:
1507:             upper_inner = float(np.mean(lm[60:65, 1], axis=0))
1508:             lower_inner = float(np.mean(lm[65:68, 1], axis=0))
1509:             mouth_open = abs(lower_inner - upper_inner)
1510: 
1511:             nose_y = float(lm[33, 1])
1512:             chin_y = float(lm[8, 1])
1513:             denom = max(abs(chin_y - nose_y), 1e-6)
1514:             ratio = (mouth_open / denom) * float(self._jaw_open_scale)
1515:             return float(np.clip(ratio, 0.0, 1.0))
1516:         except Exception:
1517:             return 0.0
1518: 
1519:     def _build_arkit_blendshapes(self, jaw_open: float) -> dict:
1520:         out = {}
1521:         for name in ARKIT_52_BLENDSHAPE_NAMES:
1522:             out[name] = 0.0
1523:         out["jawOpen"] = float(jaw_open)
1524:         return out
1525: 
1526:     def _safe_to_list(self, value) -> list:
1527:         if value is None:
1528:             return []
1529:         try:
1530:             return [float(x) for x in value]
1531:         except Exception:
1532:             try:
1533:                 return [float(x) for x in list(value)]
1534:             except Exception:
1535:                 return []
1536: 
1537:     def detect(self, frame_bgr):
1538:         import time
1539:         
1540:         self._frame_counter += 1
1541:         
1542:         if self._enable_profiling and (self._frame_counter % self._profiling_interval) == 0:
1543:             frame_start_time = time.time()
1544:         else:
1545:             frame_start_time = None
1546:         
1547:         if (self._frame_counter % self._fit_every_n) != 0:
1548:             return list(self._last_detections)
1549: 
1550:         if frame_bgr is None or getattr(frame_bgr, "shape", None) is None:
1551:             self._last_detections = []
1552:             return []
1553: 
1554:         orig_height, orig_width = frame_bgr.shape[:2]
1555:         
1556:         if orig_width > self._max_detection_width:
1557:             scale_factor = self._max_detection_width / orig_width
1558:             detect_width = self._max_detection_width
1559:             detect_height = int(orig_height * scale_factor)
1560:             frame_resized = cv2.resize(frame_bgr, (detect_width, detect_height), interpolation=cv2.INTER_LINEAR)
1561:             height, width = detect_height, detect_width
1562:         else:
1563:             frame_resized = frame_bgr
1564:             height, width = orig_height, orig_width
1565:             scale_factor = 1.0
1566: 
1567:         try:
1568:             self._ensure_eos_assets_loaded()
1569:         except Exception as e:
1570:             logger.error("EOS assets load failed: %s", e)
1571:             self._last_detections = []
1572:             return []
1573:         
1574:         if frame_start_time is not None:
1575:             assets_time = time.time()
1576: 
1577:         yunet_detections = self._yunet.detect(frame_resized)
1578:         if self._max_faces is not None:
1579:             yunet_detections = yunet_detections[: self._max_faces]
1580:         
1581:         if frame_start_time is not None:
1582:             yunet_time = time.time()
1583: 
1584:         detections = []
1585:         for det in yunet_detections:
1586:             bbox = det.get("bbox")
1587:             if not bbox or len(bbox) < 4:
1588:                 continue
1589: 
1590:             x, y, w, h = bbox
1591:             
1592:             if scale_factor != 1.0:
1593:                 x = int(x / scale_factor)
1594:                 y = int(y / scale_factor)
1595:                 w = int(w / scale_factor)
1596:                 h = int(h / scale_factor)
1597:                 logger.debug(f"EOS upscale: bbox rescaled by 1/{scale_factor:.2f} to original resolution")
1598:             
1599:             x_safe = max(0, int(x))
1600:             y_safe = max(0, int(y))
1601:             x2_safe = min(int(orig_width), int(x + w))
1602:             y2_safe = min(int(orig_height), int(y + h))
1603:             if x2_safe <= x_safe or y2_safe <= y_safe:
1604:                 continue
1605: 
1606:             face_roi = frame_bgr[y_safe:y2_safe, x_safe:x2_safe]
1607:             landmarks_478 = self._facemesh_detector.detect_landmarks(
1608:                 face_roi,
1609:                 (x_safe, y_safe, x2_safe - x_safe, y2_safe - y_safe),
1610:             )
1611:             if landmarks_478 is None:
1612:                 continue
1613:             
1614:             if frame_start_time is not None:
1615:                 facemesh_time = time.time()
1616: 
1617:             try:
1618:                 landmarks_68 = self._convert_478_to_68(landmarks_478)
1619:             except Exception:
1620:                 continue
1621: 
1622:             jaw_open = self._compute_jaw_open_from_dlib68(landmarks_68)
1623: 
1624:             try:
1625:                 eos_landmarks = []
1626:                 for i in range(68):
1627:                     eos_landmarks.append(
1628:                         self._eos.core.Landmark(
1629:                             str(i + 1),
1630:                             [float(landmarks_68[i, 0]), float(landmarks_68[i, 1])],
1631:                         )
1632:                     )
1633: 
1634:                 with self._lock:
1635:                     (
1636:                         _mesh,
1637:                         _pose,
1638:                         shape_coeffs,
1639:                         blendshape_coeffs,
1640:                     ) = self._eos.fitting.fit_shape_and_pose(
1641:                         self._morphable_model,
1642:                         eos_landmarks,
1643:                         self._landmark_mapper,
1644:                         int(orig_width),
1645:                         int(orig_height),
1646:                         self._edge_topology,
1647:                         self._contour_landmarks,
1648:                         self._model_contour,
1649:                     )
1650:             except Exception as e:
1651:                 logger.warning("EOS fitting failed: %s", e)
1652:                 continue
1653:             
1654:             if frame_start_time is not None:
1655:                 eos_fit_time = time.time()
1656: 
1657:             landmarks_xyz = np.zeros((68, 3), dtype=np.float32)
1658:             landmarks_xyz[:, 0] = landmarks_68[:, 0]
1659:             landmarks_xyz[:, 1] = landmarks_68[:, 1]
1660: 
1661:             eos_payload = {
1662:                 "shape_coeffs": self._safe_to_list(shape_coeffs),
1663:                 "expression_coeffs": self._safe_to_list(blendshape_coeffs),
1664:             }
1665: 
1666:             out_det = dict(det)
1667:             out_det["bbox"] = (x_safe, y_safe, x2_safe - x_safe, y2_safe - y_safe)
1668:             out_det["centroid"] = (x_safe + ((x2_safe - x_safe) // 2), y_safe + ((y2_safe - y_safe) // 2))
1669:             out_det["source_detector"] = "face_landmarker"
1670:             out_det["label"] = "face"
1671:             out_det["landmarks"] = landmarks_xyz.tolist()
1672:             out_det["blendshapes"] = self._build_arkit_blendshapes(jaw_open)
1673:             out_det["eos"] = eos_payload
1674:             detections.append(out_det)
1675: 
1676:         self._last_detections = detections
1677:         
1678:         if frame_start_time is not None:
1679:             total_time = time.time() - frame_start_time
1680:             assets_duration = (assets_time - frame_start_time) * 1000 if 'assets_time' in locals() else 0
1681:             yunet_duration = (yunet_time - assets_time) * 1000 if 'yunet_time' in locals() and 'assets_time' in locals() else 0
1682:             facemesh_duration = (facemesh_time - yunet_time) * 1000 if 'facemesh_time' in locals() and 'yunet_time' in locals() else 0
1683:             eos_duration = (eos_fit_time - facemesh_time) * 1000 if 'eos_fit_time' in locals() and 'facemesh_time' in locals() else 0
1684:             logger.info(
1685:                 f"[PROFILING] Frame {self._frame_counter}: total={total_time*1000:.1f}ms "
1686:                 f"(assets={assets_duration:.1f}ms, yunet={yunet_duration:.1f}ms, "
1687:                 f"facemesh={facemesh_duration:.1f}ms, eos_fit={eos_duration:.1f}ms, "
1688:                 f"faces={len(detections)}, downscale={scale_factor:.2f})"
1689:             )
1690:         
1691:         return detections
1692: 
1693: 
1694: def create_face_engine(engine_name: str, use_gpu: bool = False):
1695:     """
1696:     Factory function to create face tracking engines.
1697:     
1698:     Args:
1699:         engine_name: Name of the engine (mediapipe_landmarker, openseeface, etc.)
1700:         use_gpu: If True, enable GPU acceleration for supported engines (v4.2+)
1701:     
1702:     Returns:
1703:         Engine instance or None for MediaPipe (handled separately in workers)
1704:     """
1705:     normalized = (engine_name or "").strip().lower()
1706:     if not normalized or normalized in {"mediapipe", "mediapipe_landmarker"}:
1707:         return None
1708:     if normalized == "openseeface":
1709:         return OpenSeeFaceEngine(use_gpu=use_gpu)
1710:     if normalized == "insightface":
1711:         return InsightFaceEngine(use_gpu=use_gpu)
1712:     if normalized == "eos":
1713:         return EosFaceEngine()
1714:     if normalized == "opencv_haar":
1715:         return OpenCVHaarFaceEngine()
1716:     if normalized == "opencv_yunet":
1717:         return OpenCVYuNetFaceEngine(use_gpu=use_gpu)
1718:     if normalized == "opencv_yunet_pyfeat":
1719:         return OpenCVYuNetPyFeatEngine(use_gpu=use_gpu)
1720:     raise ValueError(f"Unsupported tracking engine: {engine_name}")
```

## File: workflow_scripts/step5/object_detector_registry.py
```python
  1: #!/usr/bin/env python3
  2: # -*- coding: utf-8 -*-
  3: """
  4: Object Detector Model Registry for STEP5
  5: 
  6: Centralized configuration and resolution for object detection models
  7: used as fallback when face detection fails in MediaPipe tracking engine.
  8: 
  9: Supports multiple model backends (TFLite, ONNX) with hardware-aware recommendations.
 10: """
 11: 
 12: import os
 13: import logging
 14: from pathlib import Path
 15: from typing import Optional, Dict, Any
 16: from dataclasses import dataclass
 17: 
 18: logger = logging.getLogger(__name__)
 19: 
 20: 
 21: @dataclass
 22: class ObjectDetectorModelSpec:
 23:     """Specification for an object detection model."""
 24:     
 25:     name: str
 26:     backend: str  # 'tflite' or 'onnx'
 27:     filename: str
 28:     description: str
 29:     recommended_hardware: str  # 'edge_tpu', 'cpu_arm', 'cpu_desktop', 'gpu'
 30:     map_coco: float  # mAP on COCO dataset
 31:     latency_note: str
 32: 
 33: 
 34: class ObjectDetectorRegistry:
 35:     """
 36:     Registry for object detection models used in STEP5 fallback detection.
 37:     
 38:     Provides centralized configuration, model resolution, and hardware-aware
 39:     recommendations for object detection models.
 40:     """
 41:     
 42:     # Model catalog with specifications from technical analysis
 43:     MODELS = {
 44:         'efficientdet_lite0': ObjectDetectorModelSpec(
 45:             name='efficientdet_lite0',
 46:             backend='tflite',
 47:             filename='EfficientDet-Lite0.tflite',
 48:             description='EfficientDet-Lite0 (320x320) - Fastest, Edge TPU compatible',
 49:             recommended_hardware='edge_tpu',
 50:             map_coco=25.69,
 51:             latency_note='~37ms on Pixel 4 CPU, ~50% faster than Lite2'
 52:         ),
 53:         'efficientdet_lite1': ObjectDetectorModelSpec(
 54:             name='efficientdet_lite1',
 55:             backend='tflite',
 56:             filename='EfficientDet-Lite1.tflite',
 57:             description='EfficientDet-Lite1 (384x384) - Balanced, Edge TPU compatible',
 58:             recommended_hardware='edge_tpu',
 59:             map_coco=30.55,
 60:             latency_note='~49ms on Pixel 4 CPU'
 61:         ),
 62:         'efficientdet_lite2': ObjectDetectorModelSpec(
 63:             name='efficientdet_lite2',
 64:             backend='tflite',
 65:             filename='EfficientDet-Lite2-32.tflite',
 66:             description='EfficientDet-Lite2 (448x448) - Current baseline',
 67:             recommended_hardware='cpu_desktop',
 68:             map_coco=33.97,
 69:             latency_note='~69ms on Pixel 4 CPU'
 70:         ),
 71:         'ssd_mobilenet_v3': ObjectDetectorModelSpec(
 72:             name='ssd_mobilenet_v3',
 73:             backend='tflite',
 74:             filename='ssd_mobilenet_v3.tflite',
 75:             description='SSD-MobileNetV3 - Stable ecosystem, CPU optimized',
 76:             recommended_hardware='cpu_arm',
 77:             map_coco=28.0,
 78:             latency_note='~40% faster than Lite2, high confidence stability'
 79:         ),
 80:         'yolo11n_onnx': ObjectDetectorModelSpec(
 81:             name='yolo11n_onnx',
 82:             backend='onnx',
 83:             filename='yolo11n.onnx',
 84:             description='YOLO11 Nano (ONNX) - Best CPU performance (experimental)',
 85:             recommended_hardware='cpu_desktop',
 86:             map_coco=39.5,
 87:             latency_note='~56ms CPU ONNX, 20-30% faster with better precision'
 88:         ),
 89:         'nanodet_plus': ObjectDetectorModelSpec(
 90:             name='nanodet_plus',
 91:             backend='onnx',
 92:             filename='nanodet-plus-m_416.onnx',
 93:             description='NanoDet-Plus (416x416) - Ultra-lightweight (experimental)',
 94:             recommended_hardware='cpu_arm',
 95:             map_coco=34.1,
 96:             latency_note='~25ms on ARM, 60-70% faster than Lite2'
 97:         ),
 98:     }
 99:     
100:     # Default model (backward compatible with existing setup)
101:     DEFAULT_MODEL = 'efficientdet_lite2'
102: 
103:     @classmethod
104:     def _try_resolve_path(cls, raw_path: str, models_dir: Optional[Path]) -> Optional[Path]:
105:         if not raw_path:
106:             return None
107: 
108:         candidate = Path(raw_path)
109:         if candidate.exists():
110:             return candidate if candidate.is_absolute() else candidate.resolve()
111: 
112:         step5_dir = Path(__file__).resolve().parent
113:         project_root = step5_dir.parent.parent
114: 
115:         for base in [models_dir, project_root, step5_dir]:
116:             if base is None:
117:                 continue
118:             if candidate.is_absolute():
119:                 continue
120:             resolved = base / candidate
121:             if resolved.exists():
122:                 return resolved.resolve()
123: 
124:         return None
125:     
126:     @classmethod
127:     def get_model_spec(cls, model_name: Optional[str] = None) -> ObjectDetectorModelSpec:
128:         """
129:         Get model specification by name.
130:         
131:         Args:
132:             model_name: Model identifier (e.g., 'efficientdet_lite0').
133:                        If None, returns default model.
134:         
135:         Returns:
136:             ObjectDetectorModelSpec for the requested model.
137:         
138:         Raises:
139:             ValueError: If model_name is not found in registry.
140:         """
141:         if not model_name:
142:             model_name = cls.DEFAULT_MODEL
143:         
144:         model_name_normalized = model_name.strip().lower()
145:         
146:         if model_name_normalized not in cls.MODELS:
147:             available = ', '.join(cls.MODELS.keys())
148:             raise ValueError(
149:                 f"Unknown object detector model: '{model_name}'. "
150:                 f"Available models: {available}"
151:             )
152:         
153:         return cls.MODELS[model_name_normalized]
154:     
155:     @classmethod
156:     def resolve_model_path(
157:         cls,
158:         model_name: Optional[str] = None,
159:         models_dir: Optional[Path] = None,
160:         override_path: Optional[str] = None
161:     ) -> Path:
162:         """
163:         Resolve the full path to an object detection model file.
164:         
165:         Priority order:
166:         1. override_path (if provided and exists)
167:         2. Environment variable STEP5_OBJECT_DETECTOR_MODEL_PATH
168:         3. models_dir / model_spec.filename
169:         4. Default models directory / model_spec.filename
170:         
171:         Args:
172:             model_name: Model identifier from registry.
173:             models_dir: Directory containing model files.
174:             override_path: Explicit path override.
175:         
176:         Returns:
177:             Resolved Path to model file.
178:         
179:         Raises:
180:             FileNotFoundError: If model file cannot be found.
181:             ValueError: If model_name is invalid.
182:         """
183:         # Priority 1: Explicit override path
184:         if override_path:
185:             override_path_obj = cls._try_resolve_path(override_path, models_dir=models_dir)
186:             if override_path_obj is not None:
187:                 logger.info(f"Using override model path: {override_path_obj}")
188:                 return override_path_obj
189:             logger.warning(f"Override path does not exist: {override_path}")
190: 
191:         # Priority 2: Environment variable
192:         env_path = os.environ.get('STEP5_OBJECT_DETECTOR_MODEL_PATH')
193:         if env_path:
194:             env_path_obj = cls._try_resolve_path(env_path, models_dir=models_dir)
195:             if env_path_obj is not None:
196:                 logger.info(f"Using model path from STEP5_OBJECT_DETECTOR_MODEL_PATH: {env_path_obj}")
197:                 return env_path_obj
198:             logger.warning(f"STEP5_OBJECT_DETECTOR_MODEL_PATH does not exist: {env_path}")
199:         
200:         # Get model spec
201:         spec = cls.get_model_spec(model_name)
202:         
203:         # Priority 3/4: Provided models_dir and default models directory
204:         search_base_dirs = []
205:         if models_dir is not None:
206:             search_base_dirs.append(models_dir)
207: 
208:         default_models_dir = Path(__file__).resolve().parent / "models"
209:         search_base_dirs.append(default_models_dir)
210: 
211:         backend_dir = "tflite" if spec.backend == "tflite" else "onnx"
212:         relative_candidates = [
213:             Path(spec.filename),
214:             Path("object_detectors") / backend_dir / spec.filename,
215:             Path("object_detectors") / spec.backend / spec.filename,
216:         ]
217: 
218:         for base_dir in search_base_dirs:
219:             for rel in relative_candidates:
220:                 model_path = base_dir / rel
221:                 if model_path.exists():
222:                     logger.info(f"Using object detector model: {model_path}")
223:                     return model_path
224: 
225:         searched_in = ", ".join(str(d) for d in search_base_dirs)
226:         raise FileNotFoundError(
227:             f"Object detector model not found: {spec.filename}\n"
228:             f"Model: {spec.name} ({spec.description})\n"
229:             f"Expected filename: {spec.filename}\n"
230:             f"Searched in: {searched_in}\n"
231:             f"To use this model, download it under workflow_scripts/step5/models/object_detectors/ "
232:             f"or set STEP5_OBJECT_DETECTOR_MODEL_PATH environment variable."
233:         )
234:     
235:     @classmethod
236:     def get_recommended_model(cls, hardware_target: str = 'cpu_desktop') -> str:
237:         """
238:         Get recommended model name for a specific hardware target.
239:         
240:         Args:
241:             hardware_target: One of 'edge_tpu', 'cpu_arm', 'cpu_desktop', 'gpu'
242:         
243:         Returns:
244:             Recommended model name for the hardware target.
245:         """
246:         recommendations = {
247:             'edge_tpu': 'efficientdet_lite0',
248:             'cpu_arm': 'efficientdet_lite0',  # or nanodet_plus if ONNX available
249:             'cpu_desktop': 'efficientdet_lite0',  # balance of speed and accuracy
250:             'gpu': 'efficientdet_lite2',  # can handle higher resolution
251:         }
252:         
253:         return recommendations.get(hardware_target, cls.DEFAULT_MODEL)
254:     
255:     @classmethod
256:     def list_available_models(cls) -> Dict[str, ObjectDetectorModelSpec]:
257:         """
258:         Get all registered models with their specifications.
259:         
260:         Returns:
261:             Dictionary of model_name -> ObjectDetectorModelSpec
262:         """
263:         return cls.MODELS.copy()
264:     
265:     @classmethod
266:     def validate_backend_support(cls, backend: str) -> bool:
267:         """
268:         Check if a model backend is supported in the current environment.
269:         
270:         Args:
271:             backend: 'tflite' or 'onnx'
272:         
273:         Returns:
274:             True if backend is supported, False otherwise.
275:         """
276:         if backend == 'tflite':
277:             # MediaPipe always supports TFLite
278:             return True
279:         elif backend == 'onnx':
280:             # ONNX support is experimental/future work
281:             try:
282:                 import onnxruntime
283:                 return True
284:             except ImportError:
285:                 logger.warning("ONNX Runtime not available. ONNX models require: pip install onnxruntime")
286:                 return False
287:         else:
288:             return False
```

## File: workflow_scripts/step5/onnx_facemesh_detector.py
```python
  1: import logging
  2: import os
  3: from pathlib import Path
  4: from typing import Optional, Tuple
  5: 
  6: import cv2
  7: import numpy as np
  8: 
  9: logger = logging.getLogger(__name__)
 10: 
 11: 
 12: class ONNXFaceMeshDetector:
 13:     def __init__(self, model_path: Optional[str] = None, use_gpu: bool = False):
 14:         self._model_path = model_path or os.environ.get("STEP5_FACEMESH_ONNX_PATH")
 15:         self._model_path = self._resolve_model_path(self._model_path)
 16:         
 17:         if not self._model_path:
 18:             self._model_path = self._find_mediapipe_facemesh_model()
 19:         
 20:         self._model_path = self._resolve_model_path(self._model_path)
 21: 
 22:         if not self._model_path or not Path(self._model_path).exists():
 23:             raise RuntimeError(
 24:                 f"FaceMesh ONNX model not found. "
 25:                 f"Set STEP5_FACEMESH_ONNX_PATH or place face_landmark.onnx under models/face_landmarks/"
 26:             )
 27: 
 28:         try:
 29:             import onnxruntime as ort
 30:             self._ort = ort
 31:             
 32:             sess_options = ort.SessionOptions()
 33:             sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
 34:             
 35:             # Configure threading for optimal CPU performance
 36:             intra_threads = int(os.environ.get("STEP5_ONNX_INTRA_OP_THREADS", "2"))
 37:             inter_threads = int(os.environ.get("STEP5_ONNX_INTER_OP_THREADS", "1"))
 38:             sess_options.intra_op_num_threads = intra_threads
 39:             sess_options.inter_op_num_threads = inter_threads
 40:             
 41:             # Enable memory optimizations
 42:             sess_options.enable_cpu_mem_arena = True
 43:             sess_options.enable_mem_pattern = True
 44:             
 45:             providers = []
 46:             if use_gpu:
 47:                 providers.append('CUDAExecutionProvider')
 48:             providers.append('CPUExecutionProvider')
 49:             
 50:             self._session = ort.InferenceSession(
 51:                 self._model_path,
 52:                 sess_options=sess_options,
 53:                 providers=providers
 54:             )
 55:             active_providers = self._session.get_providers()
 56:             
 57:             logger.info(
 58:                 f"ONNX Runtime configured: intra_threads={intra_threads}, inter_threads={inter_threads}, "
 59:                 f"use_gpu={use_gpu}"
 60:             )
 61:             logger.info("FaceMesh ONNX providers active: %s", active_providers)
 62:             
 63:             self._input_name = self._session.get_inputs()[0].name
 64:             self._input_shape = self._session.get_inputs()[0].shape
 65:             self._output_names = [output.name for output in self._session.get_outputs()]
 66:             
 67:             logger.info(f"FaceMesh ONNX model loaded: {self._model_path}")
 68:             logger.info(f"Input shape: {self._input_shape}, Outputs: {self._output_names}")
 69:         except ImportError:
 70:             raise RuntimeError(
 71:                 "onnxruntime is required for ONNX FaceMesh. "
 72:                 "Install with: pip install onnxruntime"
 73:             )
 74:         except Exception as e:
 75:             logger.error(f"Failed to load ONNX FaceMesh model: {e}")
 76:             raise
 77: 
 78:     def _find_mediapipe_facemesh_model(self) -> Optional[str]:
 79:         search_paths = [
 80:             Path(__file__).parent / "models" / "face_landmarks" / "opencv" / "face_landmark.onnx",
 81:             Path(__file__).parent / "models" / "face_landmarks" / "opencv" / "face_landmarker.onnx",
 82:             Path(__file__).parent / "models" / "face_landmarks" / "opencv" / "facemesh.onnx",
 83:             Path(__file__).parent / "models" / "face_landmark.onnx",
 84:             Path(__file__).parent / "models" / "face_landmarker.onnx",
 85:             Path(__file__).parent / "models" / "facemesh.onnx",
 86:         ]
 87:         
 88:         for path in search_paths:
 89:             if path.exists():
 90:                 return str(path)
 91:         
 92:         return None
 93: 
 94:     def _resolve_model_path(self, model_path: Optional[str]) -> Optional[str]:
 95:         if not model_path:
 96:             return None
 97: 
 98:         candidate = Path(model_path)
 99:         if candidate.is_absolute():
100:             return str(candidate)
101: 
102:         if candidate.exists():
103:             return str(candidate)
104: 
105:         step5_dir = Path(__file__).resolve().parent
106:         project_root = step5_dir.parent.parent
107: 
108:         candidates = [
109:             project_root / candidate,
110:             step5_dir / candidate,
111:         ]
112:         for c in candidates:
113:             if c.exists():
114:                 return str(c)
115: 
116:         return str(candidate)
117: 
118:     def detect_landmarks(
119:         self,
120:         face_roi: np.ndarray,
121:         original_bbox: Tuple[int, int, int, int]
122:     ) -> Optional[np.ndarray]:
123:         try:
124:             input_size = 192
125:             if len(self._input_shape) >= 3 and self._input_shape[2] is not None:
126:                 input_size = self._input_shape[2]
127:             
128:             # Optimize resize and preprocessing
129:             if face_roi.shape[:2] != (input_size, input_size):
130:                 resized = cv2.resize(face_roi, (input_size, input_size), interpolation=cv2.INTER_LINEAR)
131:             else:
132:                 resized = face_roi
133:             
134:             # Single operation: convert to float32, normalize, transpose
135:             input_tensor = np.ascontiguousarray(
136:                 np.transpose(resized, (2, 0, 1))[np.newaxis, :, :, :].astype(np.float32) / 255.0
137:             )
138: 
139:             x, y, w, h = original_bbox
140:             feed_dict = {}
141:             for input_meta in self._session.get_inputs():
142:                 input_name = input_meta.name
143:                 if input_name == self._input_name:
144:                     feed_dict[input_name] = input_tensor
145:                 elif input_name == "crop_x1":
146:                     feed_dict[input_name] = np.array([[int(x)]], dtype=np.int32)
147:                 elif input_name == "crop_y1":
148:                     feed_dict[input_name] = np.array([[int(y)]], dtype=np.int32)
149:                 elif input_name == "crop_width":
150:                     feed_dict[input_name] = np.array([[int(w)]], dtype=np.int32)
151:                 elif input_name == "crop_height":
152:                     feed_dict[input_name] = np.array([[int(h)]], dtype=np.int32)
153: 
154:             missing_inputs = [i.name for i in self._session.get_inputs() if i.name not in feed_dict]
155:             if missing_inputs:
156:                 raise RuntimeError(
157:                     f"FaceMesh ONNX model requires additional inputs not supported: {missing_inputs}"
158:                 )
159: 
160:             outputs = self._session.run(self._output_names, feed_dict)
161:             
162:             landmarks_normalized = None
163:             for output in outputs:
164:                 if output.shape[-1] == 3 and output.shape[-2] >= 468:
165:                     landmarks_normalized = output[0]
166:                     break
167:             
168:             if landmarks_normalized is None:
169:                 logger.warning("Could not find landmarks in ONNX output")
170:                 return None
171: 
172:             landmarks_normalized = landmarks_normalized[:478, :]
173: 
174:             if landmarks_normalized.shape[0] < 478:
175:                 pad_count = 478 - int(landmarks_normalized.shape[0])
176:                 if pad_count > 0 and landmarks_normalized.shape[0] > 0:
177:                     pad = np.repeat(landmarks_normalized[-1:, :], pad_count, axis=0)
178:                     landmarks_normalized = np.concatenate([landmarks_normalized, pad], axis=0)
179: 
180:             if np.issubdtype(landmarks_normalized.dtype, np.integer):
181:                 return landmarks_normalized.astype(np.float32)
182: 
183:             max_abs = float(np.nanmax(np.abs(landmarks_normalized[:, :2]))) if landmarks_normalized.size else 0.0
184:             if max_abs > 2.0:
185:                 return landmarks_normalized.astype(np.float32)
186: 
187:             landmarks_absolute = landmarks_normalized.copy()
188:             landmarks_absolute[:, 0] = landmarks_normalized[:, 0] * w + x
189:             landmarks_absolute[:, 1] = landmarks_normalized[:, 1] * h + y
190: 
191:             return landmarks_absolute.astype(np.float32)
192:         
193:         except Exception as e:
194:             logger.warning(f"Failed to detect landmarks with ONNX: {e}")
195:             return None
```

## File: workflow_scripts/step5/process_video_worker_multiprocessing.py
```python
  1: #!/usr/bin/env python3
  2: # -*- coding: utf-8 -*-
  3: """
  4: Enhanced CPU Worker with Multiprocessing Optimization
  5: Combines proven techniques from backup implementations for maximum CPU performance.
  6: """
  7: 
  8: import os
  9: import sys
 10: import json
 11: import argparse
 12: import logging
 13: import cv2
 14: import numpy as np
 15: try:
 16:     import mediapipe as mp
 17: except Exception:
 18:     mp = None
 19: import multiprocessing as mp_proc
 20: import time
 21: import math
 22: from pathlib import Path
 23: from functools import partial
 24: from concurrent.futures import ProcessPoolExecutor, as_completed
 25: 
 26: # Load .env file before anything else (critical for multiprocessing workers)
 27: try:
 28:     from dotenv import load_dotenv
 29:     env_path = Path(__file__).resolve().parent.parent.parent / '.env'
 30:     if env_path.exists():
 31:         load_dotenv(env_path)
 32: except ImportError:
 33:     pass  # dotenv not available, rely on system env vars
 34: 
 35: # Add project root to path
 36: sys.path.append(str(Path(__file__).resolve().parent.parent.parent))
 37: from utils.tracking_optimizations import apply_tracking_and_management
 38: from utils.resource_manager import safe_video_processing, get_video_metadata, resource_tracker
 39: from utils.enhanced_speaking_detection import EnhancedSpeakingDetector
 40: from object_detector_registry import ObjectDetectorRegistry
 41: 
 42: # Configuration du logger
 43: log_dir = Path(__file__).resolve().parent.parent.parent / "logs" / "step5"
 44: log_dir.mkdir(parents=True, exist_ok=True)
 45: worker_type_str = "CPU_MP" if "--mp_num_workers_internal" in sys.argv else "CPU"
 46: video_name_for_log = Path(sys.argv[1]).stem if len(sys.argv) > 1 else "unknown"
 47: log_file = log_dir / f"worker_{worker_type_str}_{video_name_for_log}_{os.getpid()}.log"
 48: logging.basicConfig(level=logging.INFO, format='%(asctime)s - [%(levelname)s] - %(message)s',
 49:                     handlers=[logging.FileHandler(log_file, encoding='utf-8'), logging.StreamHandler(sys.stdout)])
 50: 
 51: 
 52: def _parse_optional_positive_int(raw):
 53:     if raw is None:
 54:         return None
 55:     raw_str = str(raw).strip()
 56:     if not raw_str:
 57:         return None
 58:     try:
 59:         value = int(raw_str)
 60:     except Exception:
 61:         return None
 62:     if value <= 0:
 63:         return None
 64:     return value
 65: 
 66: 
 67: def _is_truthy_env(raw):
 68:     if raw is None:
 69:         return False
 70:     return str(raw).strip().lower() in {"1", "true", "yes"}
 71: 
 72: 
 73: def _apply_jawopen_scale(blendshapes, scale):
 74:     if not blendshapes or not isinstance(blendshapes, dict):
 75:         return blendshapes
 76:     try:
 77:         jaw_open_raw = blendshapes.get("jawOpen")
 78:         if jaw_open_raw is None:
 79:             return blendshapes
 80:         jaw_open_scaled = float(jaw_open_raw) * float(scale)
 81:         jaw_open_scaled = float(np.clip(jaw_open_scaled, 0.0, 1.0))
 82:         out = dict(blendshapes)
 83:         out["jawOpen"] = jaw_open_scaled
 84:         return out
 85:     except Exception:
 86:         return blendshapes
 87: 
 88: 
 89: # Global variables for worker processes
 90: landmarker_global = None
 91: object_detector_global = None
 92: face_engine_global = None
 93: tracking_engine_type = None
 94: 
 95: 
 96: def init_worker_process(models_dir, args_dict):
 97:     """
 98:     Initialize tracking models once per worker process.
 99:     Supports MediaPipe and OpenCV-based engines.
100:     """
101:     global landmarker_global, object_detector_global, face_engine_global, tracking_engine_type
102:     
103:     worker_pid = os.getpid()
104:     engine_name = args_dict.get('tracking_engine', '')
105:     engine_norm = (str(engine_name).strip().lower() if engine_name else "")
106:     
107:     logging.info(f"[WORKER-{worker_pid}] Initializing process with engine: {engine_norm or 'mediapipe'}")
108:     
109:     try:
110:         if engine_norm and engine_norm not in {"mediapipe", "mediapipe_landmarker"}:
111:             # Inject env vars for OpenCV engines (not inherited from parent process)
112:             if args_dict.get('yunet_model_path'):
113:                 os.environ['STEP5_YUNET_MODEL_PATH'] = args_dict['yunet_model_path']
114:             if args_dict.get('eos_models_dir'):
115:                 os.environ['STEP5_EOS_MODELS_DIR'] = str(args_dict['eos_models_dir'])
116:             if args_dict.get('eos_sfm_model_path'):
117:                 os.environ['STEP5_EOS_SFM_MODEL_PATH'] = str(args_dict['eos_sfm_model_path'])
118:             if args_dict.get('eos_expression_blendshapes_path'):
119:                 os.environ['STEP5_EOS_EXPRESSION_BLENDSHAPES_PATH'] = str(args_dict['eos_expression_blendshapes_path'])
120:             if args_dict.get('eos_landmark_mapper_path'):
121:                 os.environ['STEP5_EOS_LANDMARK_MAPPER_PATH'] = str(args_dict['eos_landmark_mapper_path'])
122:             if args_dict.get('eos_edge_topology_path'):
123:                 os.environ['STEP5_EOS_EDGE_TOPOLOGY_PATH'] = str(args_dict['eos_edge_topology_path'])
124:             if args_dict.get('eos_model_contour_path'):
125:                 os.environ['STEP5_EOS_MODEL_CONTOUR_PATH'] = str(args_dict['eos_model_contour_path'])
126:             if args_dict.get('eos_contour_landmarks_path'):
127:                 os.environ['STEP5_EOS_CONTOUR_LANDMARKS_PATH'] = str(args_dict['eos_contour_landmarks_path'])
128:             if args_dict.get('eos_fit_every_n') is not None:
129:                 os.environ['STEP5_EOS_FIT_EVERY_N'] = str(args_dict['eos_fit_every_n'])
130:             if args_dict.get('eos_max_faces') is not None:
131:                 os.environ['STEP5_EOS_MAX_FACES'] = str(args_dict['eos_max_faces'])
132:             if args_dict.get('eos_max_width') is not None:
133:                 os.environ['STEP5_EOS_MAX_WIDTH'] = str(args_dict['eos_max_width'])
134:             if args_dict.get('eos_jawopen_scale') is not None:
135:                 os.environ['STEP5_EOS_JAWOPEN_SCALE'] = str(args_dict['eos_jawopen_scale'])
136:             if args_dict.get('opencv_max_faces') is not None:
137:                 os.environ['STEP5_OPENCV_MAX_FACES'] = str(args_dict['opencv_max_faces'])
138:             if args_dict.get('opencv_jawopen_scale') is not None:
139:                 os.environ['STEP5_OPENCV_JAWOPEN_SCALE'] = str(args_dict['opencv_jawopen_scale'])
140:             if args_dict.get('facemesh_onnx_path'):
141:                 os.environ['STEP5_FACEMESH_ONNX_PATH'] = args_dict['facemesh_onnx_path']
142:             if args_dict.get('pyfeat_model_path'):
143:                 os.environ['STEP5_PYFEAT_MODEL_PATH'] = args_dict['pyfeat_model_path']
144:             if args_dict.get('step5_onnx_intra_op_threads'):
145:                 os.environ['STEP5_ONNX_INTRA_OP_THREADS'] = str(args_dict['step5_onnx_intra_op_threads'])
146:             if args_dict.get('step5_onnx_inter_op_threads'):
147:                 os.environ['STEP5_ONNX_INTER_OP_THREADS'] = str(args_dict['step5_onnx_inter_op_threads'])
148:             if args_dict.get('openseeface_models_dir'):
149:                 os.environ['STEP5_OPENSEEFACE_MODELS_DIR'] = str(args_dict['openseeface_models_dir'])
150:             if args_dict.get('openseeface_model_id') is not None:
151:                 os.environ['STEP5_OPENSEEFACE_MODEL_ID'] = str(args_dict['openseeface_model_id'])
152:             if args_dict.get('openseeface_detection_model_path'):
153:                 os.environ['STEP5_OPENSEEFACE_DETECTION_MODEL_PATH'] = str(args_dict['openseeface_detection_model_path'])
154:             if args_dict.get('openseeface_landmark_model_path'):
155:                 os.environ['STEP5_OPENSEEFACE_LANDMARK_MODEL_PATH'] = str(args_dict['openseeface_landmark_model_path'])
156:             if args_dict.get('openseeface_detect_every_n') is not None:
157:                 os.environ['STEP5_OPENSEEFACE_DETECT_EVERY_N'] = str(args_dict['openseeface_detect_every_n'])
158:             if args_dict.get('openseeface_detection_threshold') is not None:
159:                 os.environ['STEP5_OPENSEEFACE_DETECTION_THRESHOLD'] = str(args_dict['openseeface_detection_threshold'])
160:             if args_dict.get('openseeface_max_faces') is not None:
161:                 os.environ['STEP5_OPENSEEFACE_MAX_FACES'] = str(args_dict['openseeface_max_faces'])
162:             if args_dict.get('openseeface_jawopen_scale') is not None:
163:                 os.environ['STEP5_OPENSEEFACE_JAWOPEN_SCALE'] = str(args_dict['openseeface_jawopen_scale'])
164:             if args_dict.get('object_detector_model'):
165:                 os.environ['STEP5_OBJECT_DETECTOR_MODEL'] = args_dict['object_detector_model']
166:             if args_dict.get('object_detector_model_path'):
167:                 os.environ['STEP5_OBJECT_DETECTOR_MODEL_PATH'] = args_dict['object_detector_model_path']
168:             
169:             
170:             # Ensure profiling / throttling hints reach OpenCV-based engines
171:             profiling_enabled = bool(args_dict.get('enable_profiling'))
172:             os.environ['STEP5_ENABLE_PROFILING'] = "1" if profiling_enabled else "0"
173: 
174:             throttle_raw = args_dict.get('blendshapes_throttle_n', 1)
175:             try:
176:                 throttle_value = str(max(1, int(throttle_raw or 1)))
177:             except Exception:
178:                 throttle_value = "1"
179:             os.environ['STEP5_BLENDSHAPES_THROTTLE_N'] = throttle_value
180:             
181:             if engine_norm == "openseeface":
182:                 logging.info(
183:                     f"[WORKER-{worker_pid}] OpenSeeFace config: "
184:                     f"model_id={os.environ.get('STEP5_OPENSEEFACE_MODEL_ID')} "
185:                     f"models_dir={os.environ.get('STEP5_OPENSEEFACE_MODELS_DIR')} "
186:                     f"detection_model_path={os.environ.get('STEP5_OPENSEEFACE_DETECTION_MODEL_PATH')} "
187:                     f"landmark_model_path={os.environ.get('STEP5_OPENSEEFACE_LANDMARK_MODEL_PATH')} "
188:                     f"detect_every_n={os.environ.get('STEP5_OPENSEEFACE_DETECT_EVERY_N')} "
189:                     f"detection_threshold={os.environ.get('STEP5_OPENSEEFACE_DETECTION_THRESHOLD')} "
190:                     f"max_faces={os.environ.get('STEP5_OPENSEEFACE_MAX_FACES')} "
191:                     f"jawopen_scale={os.environ.get('STEP5_OPENSEEFACE_JAWOPEN_SCALE')} "
192:                     f"max_width={os.environ.get('STEP5_OPENSEEFACE_MAX_WIDTH') or os.environ.get('STEP5_YUNET_MAX_WIDTH')}"
193:                 )
194: 
195:             from face_engines import create_face_engine
196:             use_gpu = args_dict.get('use_gpu', False)
197:             face_engine_global = create_face_engine(engine_norm, use_gpu=use_gpu)
198: 
199:             if args_dict.get('enable_object_detection', False):
200:                 try:
201:                     if mp is None:
202:                         raise RuntimeError("mediapipe is not available")
203: 
204:                     BaseOptions = mp.tasks.BaseOptions
205:                     VisionRunningMode = mp.tasks.vision.RunningMode
206:                     ObjectDetector = mp.tasks.vision.ObjectDetector
207:                     ObjectDetectorOptions = mp.tasks.vision.ObjectDetectorOptions
208: 
209:                     delegate = BaseOptions.Delegate.CPU
210: 
211:                     object_detector_model_name = args_dict.get('object_detector_model', 'efficientdet_lite2')
212:                     object_model_path = ObjectDetectorRegistry.resolve_model_path(
213:                         model_name=object_detector_model_name,
214:                         models_dir=models_dir,
215:                         override_path=args_dict.get('object_detector_model_path'),
216:                     )
217:                     logging.info(
218:                         f"[WORKER-{worker_pid}] Using object detector (face_engine mode): "
219:                         f"{object_detector_model_name} at {object_model_path}"
220:                     )
221: 
222:                     object_options = ObjectDetectorOptions(
223:                         base_options=BaseOptions(model_asset_path=str(object_model_path), delegate=delegate),
224:                         running_mode=VisionRunningMode.VIDEO,
225:                         max_results=args_dict.get('object_max_results', 5),
226:                         score_threshold=args_dict.get('object_score_threshold', 0.4),
227:                     )
228:                     object_detector_global = ObjectDetector.create_from_options(object_options)
229:                 except Exception as e:
230:                     logging.error(
231:                         f"[WORKER-{worker_pid}] Failed to initialize object detector (face_engine mode): {e}"
232:                     )
233:                     object_detector_global = None
234: 
235:             tracking_engine_type = "face_engine"
236:             logging.info(f"[WORKER-{worker_pid}] Initialized {engine_norm} engine")
237:         else:
238:             if mp is None:
239:                 raise RuntimeError(
240:                     "mediapipe is required for the default tracking engine but is not available in this environment"
241:                 )
242: 
243:             BaseOptions = mp.tasks.BaseOptions
244:             VisionRunningMode = mp.tasks.vision.RunningMode
245:             FaceLandmarker = mp.tasks.vision.FaceLandmarker
246:             FaceLandmarkerOptions = mp.tasks.vision.FaceLandmarkerOptions
247:             ObjectDetector = mp.tasks.vision.ObjectDetector
248:             ObjectDetectorOptions = mp.tasks.vision.ObjectDetectorOptions
249:             
250:             # Configure delegate (CPU or GPU)
251:             use_gpu = args_dict.get('use_gpu', False)
252:             if use_gpu:
253:                 try:
254:                     # Attempt to use GPU delegate for MediaPipe
255:                     delegate = BaseOptions.Delegate.GPU
256:                     logging.info(f"[WORKER-{worker_pid}] Attempting to use GPU delegate for MediaPipe")
257:                 except AttributeError:
258:                     logging.warning(f"[WORKER-{worker_pid}] GPU delegate not available in MediaPipe, falling back to CPU")
259:                     delegate = BaseOptions.Delegate.CPU
260:             else:
261:                 delegate = BaseOptions.Delegate.CPU
262:             face_model_candidates = [
263:                 models_dir / "face_detectors" / "mediapipe" / "face_landmarker_v2_with_blendshapes.task",
264:                 models_dir / "face_landmarker_v2_with_blendshapes.task",
265:             ]
266:             face_model_path = next((p for p in face_model_candidates if p.exists()), face_model_candidates[0])
267:             
268:             # Resolve object detector model using registry
269:             object_detector_model_name = args_dict.get('object_detector_model', 'efficientdet_lite2')
270:             try:
271:                 object_model_path = ObjectDetectorRegistry.resolve_model_path(
272:                     model_name=object_detector_model_name,
273:                     models_dir=models_dir,
274:                     override_path=args_dict.get('object_detector_model_path')
275:                 )
276:                 logging.info(f"[WORKER-{worker_pid}] Using object detector: {object_detector_model_name} at {object_model_path}")
277:             except (ValueError, FileNotFoundError) as e:
278:                 logging.error(f"[WORKER-{worker_pid}] Failed to resolve object detector model: {e}")
279:                 raise
280:             
281:             env_max_faces = _parse_optional_positive_int(args_dict.get('mediapipe_max_faces'))
282:             num_faces = int(env_max_faces if env_max_faces is not None else (args_dict.get('mp_landmarker_num_faces', 5) or 5))
283: 
284:             face_options = FaceLandmarkerOptions(
285:                 base_options=BaseOptions(model_asset_path=str(face_model_path), delegate=delegate),
286:                 running_mode=VisionRunningMode.VIDEO,
287:                 num_faces=num_faces,
288:                 min_face_detection_confidence=args_dict.get('mp_landmarker_min_face_detection_confidence', 0.3),
289:                 min_face_presence_confidence=args_dict.get('mp_landmarker_min_face_presence_confidence', 0.2),
290:                 min_tracking_confidence=args_dict.get('mp_landmarker_min_tracking_confidence', 0.3),
291:                 output_face_blendshapes=True
292:             )
293:             
294:             object_options = ObjectDetectorOptions(
295:                 base_options=BaseOptions(model_asset_path=str(object_model_path), delegate=delegate),
296:                 running_mode=VisionRunningMode.VIDEO,
297:                 max_results=args_dict.get('object_max_results', 5),
298:                 score_threshold=args_dict.get('object_score_threshold', 0.4)
299:             )
300:             
301:             landmarker_global = FaceLandmarker.create_from_options(face_options)
302:             object_detector_global = ObjectDetector.create_from_options(object_options)
303:             tracking_engine_type = "mediapipe"
304:             logging.info(f"[WORKER-{worker_pid}] Initialized MediaPipe engine")
305:         
306:         logging.info(f"[WORKER-{worker_pid}] Initialization complete")
307:         
308:     except Exception as e:
309:         logging.error(f"[WORKER-{worker_pid}] Initialization failed: {e}")
310:         landmarker_global = None
311:         object_detector_global = None
312:         face_engine_global = None
313: 
314: 
315: def process_frame_chunk(chunk_data):
316:     """
317:     Process a chunk of frames using the initialized models.
318:     Returns detection results for all frames in the chunk.
319:     """
320:     global landmarker_global, object_detector_global, face_engine_global, tracking_engine_type
321:     
322:     worker_pid = os.getpid()
323:     
324:     if tracking_engine_type == "face_engine":
325:         if not face_engine_global:
326:             logging.error(f"[WORKER-{worker_pid}] Face engine not properly initialized")
327:             return []
328:     elif tracking_engine_type == "mediapipe":
329:         if not landmarker_global:
330:             logging.error(f"[WORKER-{worker_pid}] MediaPipe not properly initialized")
331:             return []
332:     else:
333:         logging.error(f"[WORKER-{worker_pid}] No tracking engine initialized")
334:         return []
335:     
336:     chunk_start, chunk_end, video_path, args_dict = chunk_data
337:     results = []
338:     
339:     try:
340:         logging.info(f"[WORKER-{worker_pid}] Processing chunk [{chunk_start}, {chunk_end}] ({chunk_end - chunk_start + 1} frames)")
341: 
342:         enable_profiling = bool(args_dict.get('enable_profiling'))
343:         profiling_stats = {
344:             "to_rgb_total": 0.0,
345:             "detect_total": 0.0,
346:             "post_total": 0.0,
347:             "frame_count": 0,
348:         }
349:         blendshapes_throttle_n = max(1, int(args_dict.get('blendshapes_throttle_n', 1) or 1))
350:         jaw_open_scale = float(args_dict.get('mediapipe_jawopen_scale', 1.0) or 1.0)
351:         max_width = _parse_optional_positive_int(args_dict.get('mediapipe_max_width'))
352:         blendshapes_cache = {}
353:         
354:         # Open video capture for this chunk
355:         cap = cv2.VideoCapture(video_path)
356:         if not cap.isOpened():
357:             logging.error(f"[WORKER-{worker_pid}] Failed to open video: {video_path}")
358:             return []
359:         
360:         cap.read()
361:         cap.set(cv2.CAP_PROP_POS_FRAMES, chunk_start)
362:         
363:         frames_processed = 0
364:         for frame_idx in range(chunk_start, chunk_end + 1):
365:             frames_processed += 1
366:             
367:             # Log progression every 10 frames
368:             if frames_processed % 10 == 0:
369:                 logging.debug(f"[WORKER-{worker_pid}] Chunk [{chunk_start}, {chunk_end}]: processed {frames_processed}/{chunk_end - chunk_start + 1} frames")
370:             if enable_profiling:
371:                 profiling_stats["frame_count"] += 1
372:             ret, frame = cap.read()
373:             if not ret:
374:                 try:
375:                     cap.release()
376:                 except Exception:
377:                     pass
378: 
379:                 cap = cv2.VideoCapture(video_path)
380:                 if cap.isOpened():
381:                     cap.read()
382:                     cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
383:                     ret, frame = cap.read()
384: 
385:                     if (not ret) and frame_idx > 0:
386:                         cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx - 1)
387:                         cap.read()
388:                         ret, frame = cap.read()
389: 
390:                     if (not ret) and args_dict.get('fps'):
391:                         try:
392:                             cap.set(
393:                                 cv2.CAP_PROP_POS_MSEC,
394:                                 int(frame_idx * 1000 / float(args_dict.get('fps', 25))),
395:                             )
396:                             ret, frame = cap.read()
397:                         except Exception:
398:                             pass
399: 
400:                 if not ret:
401:                     # Tolerate last frame read failures (common with certain codecs)
402:                     if frame_idx >= args_dict.get('total_frames', 0) - 1:
403:                         logging.debug(
404:                             f"Skipping unreadable frame {frame_idx} (near end of video) "
405:                             f"for chunk [{chunk_start}, {chunk_end}] in {Path(video_path).name}."
406:                         )
407:                     else:
408:                         logging.warning(
409:                             f"Failed to read frame {frame_idx} for chunk [{chunk_start}, {chunk_end}] "
410:                             f"in {Path(video_path).name}."
411:                         )
412:                     results.append({
413:                         'frame_idx': frame_idx,
414:                         'detections': [],
415:                         'face_detected': False
416:                     })
417: 
418:                     next_frame_idx = frame_idx + 1
419:                     if next_frame_idx <= chunk_end:
420:                         try:
421:                             cap.release()
422:                         except Exception:
423:                             pass
424:                         cap = cv2.VideoCapture(video_path)
425:                         if cap.isOpened():
426:                             cap.read()
427:                             cap.set(cv2.CAP_PROP_POS_FRAMES, next_frame_idx)
428:                     continue
429:             
430:             current_detections = []
431:             face_detected = False
432:             
433:             if tracking_engine_type == "face_engine":
434:                 try:
435:                     detections = face_engine_global.detect(frame)
436:                     if detections:
437:                         face_detected = True
438:                         current_detections.extend(detections)
439:                 except Exception as e:
440:                     logging.warning(f"Face engine detection failed for frame {frame_idx}: {e}")
441:             else:
442:                 orig_h, orig_w = frame.shape[:2]
443:                 work_frame = frame
444:                 scale_to_original = 1.0
445:                 if max_width is not None and orig_w > max_width:
446:                     scale_factor = float(max_width) / float(orig_w)
447:                     work_h = max(1, int(orig_h * scale_factor))
448:                     work_frame = cv2.resize(frame, (int(max_width), int(work_h)), interpolation=cv2.INTER_LINEAR)
449:                     scale_to_original = float(orig_w) / float(work_frame.shape[1])
450: 
451:                 t_to_rgb = time.perf_counter() if enable_profiling else 0.0
452:                 mp_image = mp.Image(
453:                     image_format=mp.ImageFormat.SRGB,
454:                     data=cv2.cvtColor(work_frame, cv2.COLOR_BGR2RGB)
455:                 )
456:                 if enable_profiling:
457:                     profiling_stats["to_rgb_total"] += time.perf_counter() - t_to_rgb
458: 
459:                 timestamp_ms = int(frame_idx * 1000 / args_dict.get('fps', 25))
460:                 
461:                 try:
462:                     t_detect = time.perf_counter() if enable_profiling else 0.0
463:                     face_result = landmarker_global.detect_for_video(mp_image, timestamp_ms)
464:                     if enable_profiling:
465:                         profiling_stats["detect_total"] += time.perf_counter() - t_detect
466:                     
467:                     if face_result.face_landmarks:
468:                         face_detected = True
469:                         
470:                         for i, landmarks in enumerate(face_result.face_landmarks):
471:                             t_post = time.perf_counter() if enable_profiling else 0.0
472:                             x_coords = [lm.x * orig_w for lm in landmarks]
473:                             y_coords = [lm.y * orig_h for lm in landmarks]
474:                             bbox = (
475:                                 int(min(x_coords)), int(min(y_coords)),
476:                                 int(max(x_coords) - min(x_coords)),
477:                                 int(max(y_coords) - min(y_coords))
478:                             )
479:                             centroid = (int(np.mean(x_coords)), int(np.mean(y_coords)))
480:                             
481:                             det = {
482:                                 "bbox": bbox,
483:                                 "centroid": centroid,
484:                                 "source_detector": "face_landmarker",
485:                                 "label": "face"
486:                             }
487:                             
488:                             if face_result.face_blendshapes:
489:                                 raw_blendshapes = {
490:                                     cat.category_name: cat.score
491:                                     for cat in face_result.face_blendshapes[i]
492:                                 }
493:                                 scaled_blendshapes = _apply_jawopen_scale(raw_blendshapes, jaw_open_scale)
494: 
495:                                 current_frame_num = frame_idx + 1
496:                                 should_update_blendshapes = (blendshapes_throttle_n <= 1) or ((current_frame_num % blendshapes_throttle_n) == 0)
497:                                 object_id = f"{bbox[0]}_{bbox[1]}_{bbox[2]}_{bbox[3]}"
498:                                 if should_update_blendshapes or object_id not in blendshapes_cache:
499:                                     blendshapes_cache[object_id] = scaled_blendshapes
500:                                     det["blendshapes"] = scaled_blendshapes
501:                                 else:
502:                                     det["blendshapes"] = blendshapes_cache.get(object_id)
503:                             
504:                             current_detections.append(det)
505: 
506:                             if enable_profiling:
507:                                 profiling_stats["post_total"] += time.perf_counter() - t_post
508:                 
509:                 except Exception as e:
510:                     logging.warning(f"Face detection failed for frame {frame_idx}: {e}")
511: 
512:                 if enable_profiling:
513:                     fc = int(profiling_stats.get("frame_count", 0) or 0)
514:                     if fc > 0 and (fc % 20) == 0:
515:                         to_rgb_ms = (profiling_stats["to_rgb_total"] / fc) * 1000.0
516:                         detect_ms = (profiling_stats["detect_total"] / fc) * 1000.0
517:                         post_ms = (profiling_stats["post_total"] / fc) * 1000.0
518:                         logging.info(
519:                             "[PROFILING] MediaPipe after %s frames: to_rgb=%.2fms/frame, detect=%.2fms/frame, post=%.2fms/frame",
520:                             fc,
521:                             to_rgb_ms,
522:                             detect_ms,
523:                             post_ms,
524:                         )
525:             
526:             # Object detection fallback if no faces detected
527:             if not face_detected and args_dict.get('enable_object_detection', False) and object_detector_global:
528:                 try:
529:                     if tracking_engine_type == "face_engine":
530:                         mp_image = mp.Image(
531:                             image_format=mp.ImageFormat.SRGB,
532:                             data=cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
533:                         )
534:                         timestamp_ms = int(frame_idx * 1000 / args_dict.get('fps', 25))
535:                     object_result = object_detector_global.detect_for_video(mp_image, timestamp_ms)
536:                     
537:                     if object_result.detections:
538:                         for detection in object_result.detections:
539:                             bbox = detection.bounding_box
540:                             x_min = int(bbox.origin_x)
541:                             y_min = int(bbox.origin_y)
542:                             width = int(bbox.width)
543:                             height = int(bbox.height)
544: 
545:                             if tracking_engine_type != "face_engine":
546:                                 x_min = int(max(0, x_min * scale_to_original))
547:                                 y_min = int(max(0, y_min * scale_to_original))
548:                                 width = int(max(0, width * scale_to_original))
549:                                 height = int(max(0, height * scale_to_original))
550:                             
551:                             centroid = (x_min + width // 2, y_min + height // 2)
552:                             
553:                             best_category = detection.categories[0] if detection.categories else None
554:                             label = best_category.category_name if best_category else "object"
555:                             confidence = best_category.score if best_category else 0.0
556:                             
557:                             det = {
558:                                 "bbox": (x_min, y_min, width, height),
559:                                 "centroid": centroid,
560:                                 "source_detector": "object_detector",
561:                                 "label": label,
562:                                 "confidence": confidence,
563:                                 "blendshapes": None,
564:                                 "is_speaking": None
565:                             }
566:                             current_detections.append(det)
567:                             
568:                 except Exception as e:
569:                     logging.warning(f"Object detection failed for frame {frame_idx}: {e}")
570:             
571:             results.append({
572:                 'frame_idx': frame_idx,
573:                 'detections': current_detections,
574:                 'face_detected': face_detected
575:             })
576:         
577:         cap.release()
578:         logging.info(f"[WORKER-{worker_pid}] Chunk [{chunk_start}, {chunk_end}] completed: {len(results)} frames processed")
579:         return results
580:         
581:     except Exception as e:
582:         logging.error(f"[WORKER-{worker_pid}] CRITICAL ERROR in chunk [{chunk_start}, {chunk_end}]: {type(e).__name__}: {e}")
583:         import traceback
584:         logging.error(f"[WORKER-{worker_pid}] Traceback: {traceback.format_exc()}")
585:         try:
586:             cap.release()
587:         except:
588:             pass
589:         return []
590: 
591: 
592: def process_video_multiprocessing(args, video_capture, total_frames):
593:     """
594:     Process video using multiprocessing with frame chunking.
595:     """
596:     logging.info(f"Starting multiprocessing with {args.mp_num_workers_internal} workers")
597:     
598:     # Get video metadata
599:     fps = video_capture.get(cv2.CAP_PROP_FPS)
600:     
601:     # Prepare arguments for workers
602:     models_dir = Path(args.models_dir)
603:     args_dict = {
604:         'tracking_engine': getattr(args, 'tracking_engine', None),
605:         'mp_landmarker_num_faces': args.mp_landmarker_num_faces,
606:         'mp_landmarker_min_face_detection_confidence': args.mp_landmarker_min_face_detection_confidence,
607:         'mp_landmarker_min_face_presence_confidence': args.mp_landmarker_min_face_presence_confidence,
608:         'mp_landmarker_min_tracking_confidence': args.mp_landmarker_min_tracking_confidence,
609:         'object_max_results': getattr(args, 'object_max_results', 5),
610:         'object_score_threshold': getattr(args, 'object_score_threshold', 0.4),
611:         'enable_object_detection': getattr(args, 'enable_object_detection', False),
612:         'fps': fps,
613:         'enable_profiling': _is_truthy_env(os.environ.get('STEP5_ENABLE_PROFILING', '0')),
614:         'blendshapes_throttle_n': max(1, int(os.environ.get('STEP5_BLENDSHAPES_THROTTLE_N', '1'))),
615:         'mediapipe_max_width': os.environ.get('STEP5_MEDIAPIPE_MAX_WIDTH'),
616:         'mediapipe_max_faces': os.environ.get('STEP5_MEDIAPIPE_MAX_FACES'),
617:         'mediapipe_jawopen_scale': os.environ.get('STEP5_MEDIAPIPE_JAWOPEN_SCALE', '1.0'),
618:         # OpenCV engine paths (env vars not inherited by ProcessPoolExecutor)
619:         'yunet_model_path': os.environ.get('STEP5_YUNET_MODEL_PATH'),
620:         'opencv_max_faces': os.environ.get('STEP5_OPENCV_MAX_FACES'),
621:         'opencv_jawopen_scale': os.environ.get('STEP5_OPENCV_JAWOPEN_SCALE'),
622:         'facemesh_onnx_path': os.environ.get('STEP5_FACEMESH_ONNX_PATH'),
623:         'pyfeat_model_path': os.environ.get('STEP5_PYFEAT_MODEL_PATH'),
624:         # ONNX Runtime performance tuning (used by several engines)
625:         'step5_onnx_intra_op_threads': os.environ.get('STEP5_ONNX_INTRA_OP_THREADS'),
626:         'step5_onnx_inter_op_threads': os.environ.get('STEP5_ONNX_INTER_OP_THREADS'),
627:         # OpenSeeFace engine configuration
628:         'openseeface_models_dir': os.environ.get('STEP5_OPENSEEFACE_MODELS_DIR'),
629:         'openseeface_model_id': os.environ.get('STEP5_OPENSEEFACE_MODEL_ID'),
630:         'openseeface_detection_model_path': os.environ.get('STEP5_OPENSEEFACE_DETECTION_MODEL_PATH'),
631:         'openseeface_landmark_model_path': os.environ.get('STEP5_OPENSEEFACE_LANDMARK_MODEL_PATH'),
632:         'openseeface_detect_every_n': os.environ.get('STEP5_OPENSEEFACE_DETECT_EVERY_N'),
633:         'openseeface_detection_threshold': os.environ.get('STEP5_OPENSEEFACE_DETECTION_THRESHOLD'),
634:         'openseeface_max_faces': os.environ.get('STEP5_OPENSEEFACE_MAX_FACES'),
635:         'openseeface_jawopen_scale': os.environ.get('STEP5_OPENSEEFACE_JAWOPEN_SCALE'),
636:         'eos_models_dir': os.environ.get('STEP5_EOS_MODELS_DIR'),
637:         'eos_sfm_model_path': os.environ.get('STEP5_EOS_SFM_MODEL_PATH'),
638:         'eos_expression_blendshapes_path': os.environ.get('STEP5_EOS_EXPRESSION_BLENDSHAPES_PATH'),
639:         'eos_landmark_mapper_path': os.environ.get('STEP5_EOS_LANDMARK_MAPPER_PATH'),
640:         'eos_edge_topology_path': os.environ.get('STEP5_EOS_EDGE_TOPOLOGY_PATH'),
641:         'eos_model_contour_path': os.environ.get('STEP5_EOS_MODEL_CONTOUR_PATH'),
642:         'eos_contour_landmarks_path': os.environ.get('STEP5_EOS_CONTOUR_LANDMARKS_PATH'),
643:         'eos_fit_every_n': os.environ.get('STEP5_EOS_FIT_EVERY_N'),
644:         'eos_max_faces': os.environ.get('STEP5_EOS_MAX_FACES'),
645:         'eos_max_width': os.environ.get('STEP5_EOS_MAX_WIDTH'),
646:         'eos_jawopen_scale': os.environ.get('STEP5_EOS_JAWOPEN_SCALE'),
647:         'object_detector_model': getattr(args, 'object_detector_model', None) or os.environ.get('STEP5_OBJECT_DETECTOR_MODEL'),
648:         'object_detector_model_path': getattr(args, 'object_detector_model_path', None) or os.environ.get('STEP5_OBJECT_DETECTOR_MODEL_PATH'),
649:         'total_frames': total_frames,
650:     }
651:     
652:     # Create frame chunks (adaptive chunk size to keep CPU workers saturated)
653:     # If chunk_size is provided (>0), honor it; otherwise, derive adaptively.
654:     provided_chunk_size = int(getattr(args, 'chunk_size', 400))
655:     if provided_chunk_size <= 0:
656:         # Target ~5 chunks per worker, minimum 20 chunks overall.
657:         mp_workers = max(1, int(getattr(args, 'mp_num_workers_internal', 1)))
658:         target_chunks = max(mp_workers * 5, 20)
659:         # Compute chunk size from total frames and clamp to avoid too fine granularity.
660:         adaptive_chunk = int(math.ceil(total_frames / max(1, target_chunks)))
661:         # Allow small chunks to reach ~5x workers (e.g., ~75 chunks for ~1600 frames & 15 workers)
662:         # Bounds can be overridden via args.chunk_min/chunk_max if provided.
663:         min_bound = int(getattr(args, 'chunk_min', 20) or 20)
664:         max_bound = int(getattr(args, 'chunk_max', 400) or 400)
665:         if min_bound > max_bound:
666:             min_bound, max_bound = max_bound, min_bound
667:         chunk_size = max(min_bound, min(max_bound, adaptive_chunk))
668:         logging.info(
669:             f"Adaptive chunking enabled: total_frames={total_frames}, workers={mp_workers}, "
670:             f"target_chunks={target_chunks}, bounds=[{min_bound},{max_bound}], selected_chunk_size={chunk_size}"
671:         )
672:     else:
673:         chunk_size = provided_chunk_size
674:         logging.info(f"Using provided chunk_size={chunk_size}")
675: 
676:     chunks = []
677:     
678:     for start_frame in range(0, total_frames, chunk_size):
679:         end_frame = min(start_frame + chunk_size - 1, total_frames - 1)
680:         chunks.append((start_frame, end_frame, args.video_file_path, args_dict))
681:     
682:     logging.info(f"Created {len(chunks)} chunks for processing (chunk_size={chunk_size})")
683:     
684:     # Process chunks using multiprocessing
685:     all_results = {}
686:     face_detection_count = 0
687:     processing_start_time = time.time()
688:     
689:     # Use ProcessPoolExecutor for better resource management
690:     with ProcessPoolExecutor(
691:         max_workers=args.mp_num_workers_internal,
692:         initializer=init_worker_process,
693:         initargs=(models_dir, args_dict)
694:     ) as executor:
695:         
696:         # Submit all chunks
697:         future_to_chunk = {executor.submit(process_frame_chunk, chunk): chunk for chunk in chunks}
698:         
699:         completed_chunks = 0
700:         # Throttle progress logs to ~10% intervals
701:         progress_every = max(1, len(chunks) // 10)
702:         for future in as_completed(future_to_chunk):
703:             chunk = future_to_chunk[future]
704:             try:
705:                 chunk_results = future.result()
706:                 
707:                 # Store results by frame index
708:                 for result in chunk_results:
709:                     all_results[result['frame_idx']] = result
710:                     if result['face_detected']:
711:                         face_detection_count += 1
712:                 
713:                 completed_chunks += 1
714:                 
715:                 # Progress logging (throttled)
716:                 if completed_chunks % progress_every == 0:
717:                     progress_percent = (completed_chunks / len(chunks)) * 100
718:                     elapsed_time = time.time() - processing_start_time
719:                     fps_rate = (completed_chunks * chunk_size) / elapsed_time if elapsed_time > 0 else 0
720:                     print(f"[Progression]|{int(progress_percent)}|{min(completed_chunks * chunk_size, total_frames)}|{total_frames}")
721:                     logging.info(f"Multiprocessing: {completed_chunks}/{len(chunks)} chunks ({progress_percent:.1f}%) - {fps_rate:.1f} fps (chunk_size={chunk_size})")
722:                     
723:             except Exception as e:
724:                 logging.error(f"Chunk processing failed: {e}")
725:     
726:     logging.info("Multiprocessing complete, applying sequential tracking...")
727:     
728:     # Apply tracking sequentially (must be sequential for consistency)
729:     tracked_objects = {}
730:     next_id_counter = {'value': 0}
731:     final_output = {
732:         "metadata": {
733:             "video_path": args.video_file_path,
734:             "total_frames": total_frames,
735:             "fps": fps
736:         },
737:         "frames": []
738:     }
739:     
740:     # Initialize enhanced speaking detector
741:     enhanced_speaking_detector = None
742:     try:
743:         enhanced_speaking_detector = EnhancedSpeakingDetector(
744:             video_path=args.video_file_path,
745:             jaw_threshold=args.speaking_detection_jaw_open_threshold
746:         )
747:         logging.info("Enhanced speaking detection initialized")
748:     except Exception as e:
749:         logging.warning(f"Failed to initialize enhanced speaking detector: {e}")
750:     
751:     missing_frame_count = 0
752:     first_missing_frame = None
753:     for frame_idx in range(total_frames):
754:         result = all_results.get(frame_idx)
755:         if result is None:
756:             missing_frame_count += 1
757:             if first_missing_frame is None:
758:                 first_missing_frame = frame_idx
759:             detections = []
760:         else:
761:             detections = result.get('detections', [])
762:         
763:         tracked_for_frame = apply_tracking_and_management(
764:             tracked_objects, detections, next_id_counter,
765:             args.mp_max_distance_tracking, args.mp_frames_unseen_deregister,
766:             args.speaking_detection_jaw_open_threshold,
767:             enhanced_speaking_detector=enhanced_speaking_detector,
768:             current_frame_num=frame_idx + 1
769:         )
770:         
771:         final_output["frames"].append({
772:             "frame": frame_idx + 1,
773:             "tracked_objects": tracked_for_frame if tracked_for_frame else []
774:         })
775: 
776:     if missing_frame_count > 0:
777:         logging.warning(
778:             f"Missing detection results for {missing_frame_count}/{total_frames} frames "
779:             f"(first missing frame index: {first_missing_frame}). Output remains dense with empty tracked_objects." 
780:         )
781:     
782:     # Calculate final statistics
783:     face_detection_rate = (face_detection_count / total_frames * 100) if total_frames > 0 else 0
784:     processing_time = time.time() - processing_start_time
785:     
786:     logging.info("Multiprocessing summary:")
787:     logging.info(f"  Total frames expected: {total_frames}")
788:     logging.info(f"  Total frames with detection results: {len(all_results)}")
789:     logging.info(f"  Frames with faces: {face_detection_count}")
790:     logging.info(f"  Face detection success rate: {face_detection_rate:.2f}%")
791:     logging.info(f"  Processing time: {processing_time:.2f} seconds")
792:     logging.info(f"  Average FPS: {total_frames / processing_time:.2f}")
793:     logging.info(f"  Total frames exported: {len(final_output['frames'])}")
794:     
795:     return final_output
796: 
797: 
798: def main(args):
799:     """Main processing function with multiprocessing support."""
800:     engine_norm = (str(getattr(args, 'tracking_engine', '') or '').strip().lower())
801:     
802:     worker_type = "CPU_MULTIPROCESSING" if getattr(args, 'mp_num_workers_internal', 1) > 1 else "CPU"
803:     logging.info(f"Starting {worker_type} processing for: {Path(args.video_file_path).name}")
804:     
805:     # Use safe video processing with automatic resource cleanup
806:     try:
807:         with safe_video_processing(args.video_file_path) as (video_capture, temp_manager):
808:             # Get video metadata safely
809:             fps = video_capture.get(cv2.CAP_PROP_FPS)
810:             total_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))
811:             
812:             logging.info(f"Video metadata - FPS: {fps}, Total frames: {total_frames}")
813:             
814:             # Check if multiprocessing should be used
815:             use_multiprocessing = (not getattr(args, 'use_gpu', False) and 
816:                                  getattr(args, 'mp_num_workers_internal', 1) > 1)
817:             
818:             if use_multiprocessing:
819:                 final_output = process_video_multiprocessing(args, video_capture, total_frames)
820:             else:
821:                 # Fallback to sequential processing (existing implementation)
822:                 logging.info("Using sequential processing (fallback)")
823:                 # ... existing sequential logic would go here
824:                 return False
825:                 
826:     except Exception as e:
827:         logging.error(f"Error processing video {args.video_file_path}: {e}")
828:         raise
829:     
830:     # Save output
831:     output_path = Path(args.video_file_path).with_suffix('.json')
832:     try:
833:         with open(output_path, 'w', encoding='utf-8') as f:
834:             json.dump(final_output, f, indent=2, ensure_ascii=False)
835:         logging.info(f"Processing complete. JSON saved: {output_path.name}")
836:         print(f"[Progression]|100|{total_frames}|{total_frames}", flush=True)
837:     except Exception as e:
838:         logging.error(f"Failed to save output file {output_path}: {e}")
839:         raise
840: 
841: 
842: if __name__ == "__main__":
843:     parser = argparse.ArgumentParser(description="Enhanced CPU Worker with Multiprocessing")
844:     parser.add_argument("video_file_path")
845:     parser.add_argument("--models_dir", required=True)
846:     parser.add_argument("--use_gpu", action="store_true")
847:     parser.add_argument("--tracking_engine", default=None, help="Tracking engine: mediapipe_landmarker (default), opencv_haar, opencv_yunet, opencv_yunet_pyfeat, openseeface")
848:     parser.add_argument("--mp_landmarker_num_faces", type=int, default=5)
849:     parser.add_argument("--mp_landmarker_min_face_detection_confidence", type=float, default=0.3)
850:     parser.add_argument("--mp_landmarker_min_face_presence_confidence", type=float, default=0.2)
851:     parser.add_argument("--mp_landmarker_min_tracking_confidence", type=float, default=0.3)
852:     parser.add_argument("--mp_max_distance_tracking", type=int, default=80)
853:     parser.add_argument("--mp_frames_unseen_deregister", type=int, default=7)
854:     parser.add_argument("--speaking_detection_jaw_open_threshold", type=float, default=0.08)
855:     parser.add_argument("--enable_object_detection", action="store_true")
856:     parser.add_argument("--object_score_threshold", type=float, default=0.4)
857:     parser.add_argument("--object_max_results", type=int, default=5)
858:     parser.add_argument("--mp_num_workers_internal", type=int, default=1)
859:     parser.add_argument("--chunk_size", type=int, default=0, help="Chunk size (frames) for multiprocessing splitting; 0=adaptive")
860:     parser.add_argument("--chunk_min", type=int, default=None, help="Minimum chunk size when adaptive is used")
861:     parser.add_argument("--chunk_max", type=int, default=None, help="Maximum chunk size when adaptive is used")
862:     
863:     args, _ = parser.parse_known_args()
864:     
865:     try:
866:         mp_proc.freeze_support()  # Required for Windows compatibility
867:         main(args)
868:         sys.exit(0)
869:     except Exception as e:
870:         logging.error(f"Critical error in worker: {e}", exc_info=True)
871:         sys.exit(1)
```

## File: workflow_scripts/step5/process_video_worker.py
```python
   1: #!/usr/bin/env python3
   2: # -*- coding: utf-8 -*-
   3: import os, sys, json, argparse, logging, importlib, cv2, numpy as np
   4: import threading, queue, time
   5: from concurrent.futures import ThreadPoolExecutor, as_completed
   6: from pathlib import Path
   7: 
   8: sys.path.append(str(Path(__file__).resolve().parent.parent.parent))
   9: from utils.tracking_optimizations import apply_tracking_and_management
  10: from utils.resource_manager import safe_video_processing, get_video_metadata, resource_tracker
  11: from utils.enhanced_speaking_detection import EnhancedSpeakingDetector
  12: 
  13: from face_engines import create_face_engine
  14: from object_detector_registry import ObjectDetectorRegistry
  15: 
  16: mp = None
  17: 
  18: 
  19: def _ensure_mediapipe_loaded(required: bool = False):
  20:     """Lazy import Mediapipe to avoid TensorFlow dependency unless needed."""
  21:     global mp
  22:     if mp is not None:
  23:         return mp
  24:     try:
  25:         if not hasattr(np, "complex_"):
  26:             np.complex_ = np.complex128
  27:         mp = importlib.import_module("mediapipe")
  28:         return mp
  29:     except Exception as exc:
  30:         msg = f"MediaPipe import failed: {exc}"
  31:         if required:
  32:             logging.error(msg)
  33:             raise
  34:         logging.warning(msg)
  35:         return None
  36: 
  37: # Configuration du logger
  38: log_dir = Path(__file__).resolve().parent.parent.parent / "logs" / "step5"
  39: log_dir.mkdir(parents=True, exist_ok=True)
  40: worker_type_str = "GPU" if "--use_gpu" in sys.argv else "CPU"
  41: video_name_for_log = Path(sys.argv[1]).stem if len(sys.argv) > 1 else "unknown"
  42: log_file = log_dir / f"worker_{worker_type_str}_{video_name_for_log}_{os.getpid()}.log"
  43: logging.basicConfig(level=logging.INFO, format='%(asctime)s - [%(levelname)s] - %(message)s',
  44:                     handlers=[logging.FileHandler(log_file, encoding='utf-8'), logging.StreamHandler(sys.stdout)])
  45: 
  46: # Limiter le threading interne d'OpenCV pour éviter la contention avec nos propres pools
  47: try:
  48:     cv2.setNumThreads(1)
  49:     cv2.ocl.setUseOpenCL(False)
  50: except Exception:
  51:     # Sécurité: ignorer si non supporté sur la plateforme
  52:     pass
  53: 
  54: 
  55: def _parse_optional_positive_int(raw):
  56:     if raw is None:
  57:         return None
  58:     raw_str = str(raw).strip()
  59:     if not raw_str:
  60:         return None
  61:     try:
  62:         value = int(raw_str)
  63:     except Exception:
  64:         return None
  65:     if value <= 0:
  66:         return None
  67:     return value
  68: 
  69: 
  70: def _is_truthy_env(raw):
  71:     if raw is None:
  72:         return False
  73:     return str(raw).strip().lower() in {"1", "true", "yes"}
  74: 
  75: 
  76: def _apply_jawopen_scale(blendshapes, scale):
  77:     if not blendshapes or not isinstance(blendshapes, dict):
  78:         return blendshapes
  79:     try:
  80:         jaw_open_raw = blendshapes.get("jawOpen")
  81:         if jaw_open_raw is None:
  82:             return blendshapes
  83:         jaw_open_scaled = float(jaw_open_raw) * float(scale)
  84:         jaw_open_scaled = float(np.clip(jaw_open_scaled, 0.0, 1.0))
  85:         out = dict(blendshapes)
  86:         out["jawOpen"] = jaw_open_scaled
  87:         return out
  88:     except Exception:
  89:         return blendshapes
  90: 
  91: 
  92: class FrameProcessor:
  93:     """Thread-safe frame processor for multi-threaded CPU processing."""
  94: 
  95:     def __init__(self, landmarker, object_detector, args, enhanced_speaking_detector=None):
  96:         self.landmarker = landmarker
  97:         self.object_detector = object_detector
  98:         self.args = args
  99:         self.enhanced_speaking_detector = enhanced_speaking_detector
 100:         self.lock = threading.Lock()
 101: 
 102:         self._enable_profiling = _is_truthy_env(os.environ.get("STEP5_ENABLE_PROFILING", "0"))
 103:         self._profiling_stats = {
 104:             "to_rgb_total": 0.0,
 105:             "detect_total": 0.0,
 106:             "post_total": 0.0,
 107:             "frame_count": 0,
 108:         }
 109:         self._blendshapes_throttle_n = max(1, int(os.environ.get("STEP5_BLENDSHAPES_THROTTLE_N", "1")))
 110:         self._jaw_open_scale = float(os.environ.get("STEP5_MEDIAPIPE_JAWOPEN_SCALE", "1.0"))
 111:         self._max_width = _parse_optional_positive_int(os.environ.get("STEP5_MEDIAPIPE_MAX_WIDTH"))
 112:         self._blendshapes_cache = {}
 113: 
 114:     def process_frame(self, frame_data):
 115:         """Process a single frame and return detection results."""
 116:         frame, frame_idx, timestamp_ms = frame_data
 117: 
 118:         try:
 119:             orig_h, orig_w = frame.shape[:2]
 120:             work_frame = frame
 121:             scale_to_original = 1.0
 122:             if self._max_width is not None and orig_w > self._max_width:
 123:                 scale_factor = float(self._max_width) / float(orig_w)
 124:                 work_h = max(1, int(orig_h * scale_factor))
 125:                 work_frame = cv2.resize(frame, (int(self._max_width), int(work_h)), interpolation=cv2.INTER_LINEAR)
 126:                 scale_to_original = float(orig_w) / float(work_frame.shape[1])
 127: 
 128:             if self._enable_profiling:
 129:                 with self.lock:
 130:                     self._profiling_stats["frame_count"] += 1
 131: 
 132:             t_to_rgb = time.perf_counter() if self._enable_profiling else 0.0
 133:             mp_image = mp.Image(
 134:                 image_format=mp.ImageFormat.SRGB,
 135:                 data=cv2.cvtColor(work_frame, cv2.COLOR_BGR2RGB)
 136:             )
 137:             if self._enable_profiling:
 138:                 with self.lock:
 139:                     self._profiling_stats["to_rgb_total"] += time.perf_counter() - t_to_rgb
 140: 
 141:             current_detections = []
 142:             face_detected = False
 143: 
 144:             # Try face detection first
 145:             t_detect = time.perf_counter() if self._enable_profiling else 0.0
 146:             face_result = self.landmarker.detect_for_video(mp_image, timestamp_ms)
 147:             if self._enable_profiling:
 148:                 with self.lock:
 149:                     self._profiling_stats["detect_total"] += time.perf_counter() - t_detect
 150: 
 151:             if face_result.face_landmarks:
 152:                 face_detected = True
 153: 
 154:                 for i, landmarks in enumerate(face_result.face_landmarks):
 155:                     t_post = time.perf_counter() if self._enable_profiling else 0.0
 156:                     x_coords = [lm.x * orig_w for lm in landmarks]
 157:                     y_coords = [lm.y * orig_h for lm in landmarks]
 158:                     bbox = (
 159:                         int(min(x_coords)), int(min(y_coords)),
 160:                         int(max(x_coords) - min(x_coords)),
 161:                         int(max(y_coords) - min(y_coords))
 162:                     )
 163:                     centroid = (int(np.mean(x_coords)), int(np.mean(y_coords)))
 164:                     det = {
 165:                         "bbox": bbox,
 166:                         "centroid": centroid,
 167:                         "source_detector": "face_landmarker",
 168:                         "label": "face"
 169:                     }
 170:                     if face_result.face_blendshapes:
 171:                         raw_blendshapes = {
 172:                             cat.category_name: cat.score
 173:                             for cat in face_result.face_blendshapes[i]
 174:                         }
 175:                         scaled_blendshapes = _apply_jawopen_scale(raw_blendshapes, self._jaw_open_scale)
 176: 
 177:                         current_frame_num = frame_idx + 1
 178:                         should_update_blendshapes = (self._blendshapes_throttle_n <= 1) or ((current_frame_num % self._blendshapes_throttle_n) == 0)
 179:                         object_id = f"{bbox[0]}_{bbox[1]}_{bbox[2]}_{bbox[3]}"
 180:                         with self.lock:
 181:                             if should_update_blendshapes or object_id not in self._blendshapes_cache:
 182:                                 self._blendshapes_cache[object_id] = scaled_blendshapes
 183:                                 det["blendshapes"] = scaled_blendshapes
 184:                             else:
 185:                                 det["blendshapes"] = self._blendshapes_cache.get(object_id)
 186:                     current_detections.append(det)
 187: 
 188:                     if self._enable_profiling:
 189:                         with self.lock:
 190:                             self._profiling_stats["post_total"] += time.perf_counter() - t_post
 191: 
 192:             if self._enable_profiling:
 193:                 with self.lock:
 194:                     fc = int(self._profiling_stats.get("frame_count", 0) or 0)
 195:                     if fc > 0 and (fc % 20) == 0:
 196:                         to_rgb_ms = (self._profiling_stats["to_rgb_total"] / fc) * 1000.0
 197:                         detect_ms = (self._profiling_stats["detect_total"] / fc) * 1000.0
 198:                         post_ms = (self._profiling_stats["post_total"] / fc) * 1000.0
 199:                         logging.info(
 200:                             "[PROFILING] MediaPipe after %s frames: to_rgb=%.2fms/frame, detect=%.2fms/frame, post=%.2fms/frame",
 201:                             fc,
 202:                             to_rgb_ms,
 203:                             detect_ms,
 204:                             post_ms,
 205:                         )
 206: 
 207:             # Use object detection fallback if no faces detected
 208:             if not face_detected and getattr(self.args, 'enable_object_detection', False):
 209:                 try:
 210:                     object_result = self.object_detector.detect_for_video(mp_image, timestamp_ms)
 211: 
 212:                     if object_result.detections:
 213:                         for detection in object_result.detections:
 214:                             bbox = detection.bounding_box
 215:                             x_min = int(bbox.origin_x)
 216:                             y_min = int(bbox.origin_y)
 217:                             width = int(bbox.width)
 218:                             height = int(bbox.height)
 219: 
 220:                             if scale_to_original != 1.0:
 221:                                 x_min = int(max(0, x_min * scale_to_original))
 222:                                 y_min = int(max(0, y_min * scale_to_original))
 223:                                 width = int(max(0, width * scale_to_original))
 224:                                 height = int(max(0, height * scale_to_original))
 225: 
 226:                             centroid = (x_min + width // 2, y_min + height // 2)
 227: 
 228:                             best_category = detection.categories[0] if detection.categories else None
 229:                             label = best_category.category_name if best_category else "object"
 230:                             confidence = best_category.score if best_category else 0.0
 231: 
 232:                             det = {
 233:                                 "bbox": (x_min, y_min, width, height),
 234:                                 "centroid": centroid,
 235:                                 "source_detector": "object_detector",
 236:                                 "label": label,
 237:                                 "confidence": confidence,
 238:                                 "blendshapes": None,
 239:                                 "is_speaking": None
 240:                             }
 241:                             current_detections.append(det)
 242:                 except Exception as e:
 243:                     logging.warning(f"Object detection failed for frame {frame_idx + 1}: {e}")
 244: 
 245:             return {
 246:                 'frame_idx': frame_idx,
 247:                 'detections': current_detections,
 248:                 'face_detected': face_detected,
 249:                 'timestamp_ms': timestamp_ms
 250:             }
 251: 
 252:         except Exception as e:
 253:             logging.error(f"Error processing frame {frame_idx + 1}: {e}")
 254:             return {
 255:                 'frame_idx': frame_idx,
 256:                 'detections': [],
 257:                 'face_detected': False,
 258:                 'timestamp_ms': timestamp_ms,
 259:                 'error': str(e)
 260:             }
 261: 
 262: 
 263: def process_video_multithreaded(args, video_capture, landmarker, object_detector, enhanced_speaking_detector, total_frames):
 264:     """Process video using multiple threads for CPU workers."""
 265:     logging.info(f"Starting multi-threaded processing with {args.mp_num_workers_internal} workers (bounded queue)")
 266: 
 267:     # Initialize tracking variables
 268:     tracked_objects = {}
 269:     next_id_counter = {'value': 0}
 270: 
 271:     # Get video metadata
 272:     fps = video_capture.get(cv2.CAP_PROP_FPS)
 273: 
 274:     final_output = {
 275:         "metadata": {
 276:             "video_path": args.video_file_path,
 277:             "total_frames": total_frames,
 278:             "fps": fps
 279:         },
 280:         "frames": []
 281:     }
 282: 
 283:     # Performance tracking
 284:     face_detection_success_count = 0
 285:     frames_processed = 0
 286:     processing_start_time = time.time()
 287: 
 288:     # Create frame processor
 289:     frame_processor = FrameProcessor(landmarker, object_detector, args, enhanced_speaking_detector)
 290: 
 291:     # Bounded queue pipeline: un thread lecteur + N threads workers
 292:     q = queue.Queue(maxsize=64)
 293:     results = {}
 294:     results_lock = threading.Lock()
 295: 
 296:     def worker_loop(worker_id: int):
 297:         nonlocal frames_processed
 298:         completed_local = 0
 299:         while True:
 300:             item = q.get()
 301:             if item is None:
 302:                 q.task_done()
 303:                 break
 304:             frame, idx, ts_ms = item
 305:             try:
 306:                 result = frame_processor.process_frame((frame, idx, ts_ms))
 307:                 with results_lock:
 308:                     results[idx] = result
 309:                 completed_local += 1
 310:                 frames_processed += 1
 311:                 if frames_processed % 50 == 0:
 312:                     progress_percent = (frames_processed / total_frames) * 100 if total_frames else 0
 313:                     elapsed_time = time.time() - processing_start_time
 314:                     fps_now = frames_processed / elapsed_time if elapsed_time > 0 else 0
 315:                     print(f"[Progression]|{int(progress_percent)}|{frames_processed}|{total_frames}", flush=True)
 316:                     logging.info(f"MT workers: {frames_processed}/{total_frames} ({progress_percent:.1f}%) - {fps_now:.1f} fps")
 317:             except Exception as e:
 318:                 logging.error(f"Frame {idx} processing failed: {e}")
 319:                 with results_lock:
 320:                     results[idx] = {
 321:                         'frame_idx': idx,
 322:                         'detections': [],
 323:                         'face_detected': False,
 324:                         'error': str(e)
 325:                     }
 326:             finally:
 327:                 # Libérer la référence à la frame pour GC
 328:                 del frame
 329:                 q.task_done()
 330: 
 331:     # Lancer les workers
 332:     workers = [threading.Thread(target=worker_loop, args=(i,), daemon=True)
 333:                for i in range(int(args.mp_num_workers_internal))]
 334:     for t in workers:
 335:         t.start()
 336: 
 337:     # Thread lecteur: lit et pousse les frames dans la queue (bornée)
 338:     frame_read = 0
 339:     logging.info("Reading frames and feeding bounded queue...")
 340:     while video_capture.isOpened():
 341:         ret, frame = video_capture.read()
 342:         if not ret:
 343:             break
 344:         ts_ms = int(video_capture.get(cv2.CAP_PROP_POS_MSEC))
 345:         q.put((frame, frame_read, ts_ms))
 346:         frame_read += 1
 347:         if frame_read % 100 == 0:
 348:             logging.info(f"Read {frame_read} frames...")
 349: 
 350:     # Envoyer les sentinelles d'arrêt aux workers
 351:     for _ in workers:
 352:         q.put(None)
 353: 
 354:     # Attendre la fin du traitement
 355:     q.join()
 356:     for t in workers:
 357:         t.join(timeout=1)
 358: 
 359:     logging.info("Parallel processing complete, applying tracking...")
 360: 
 361:     # Apply tracking in sequential order (must be sequential for consistency)
 362:     for frame_idx in sorted(results.keys()):
 363:         result = results[frame_idx]
 364: 
 365:         if result.get('face_detected', False):
 366:             face_detection_success_count += 1
 367: 
 368:         # Apply tracking and management
 369:         tracked_for_frame = apply_tracking_and_management(
 370:             tracked_objects, result['detections'], next_id_counter,
 371:             args.mp_max_distance_tracking, args.mp_frames_unseen_deregister,
 372:             args.speaking_detection_jaw_open_threshold,
 373:             enhanced_speaking_detector=enhanced_speaking_detector,
 374:             current_frame_num=frame_idx + 1
 375:         )
 376: 
 377:         # Add to final output
 378:         final_output["frames"].append({
 379:             "frame": frame_idx + 1,
 380:             "tracked_objects": tracked_for_frame if tracked_for_frame else []
 381:         })
 382: 
 383:     # Calculate final statistics
 384:     face_detection_rate = (face_detection_success_count / frames_processed * 100) if frames_processed > 0 else 0
 385:     processing_time = time.time() - processing_start_time
 386: 
 387:     logging.info("Multi-threaded processing summary:")
 388:     logging.info(f"  Total frames processed: {frames_processed}")
 389:     logging.info(f"  Frames with faces: {face_detection_success_count}")
 390:     logging.info(f"  Face detection success rate: {face_detection_rate:.2f}%")
 391:     logging.info(f"  Processing time: {processing_time:.2f} seconds")
 392:     logging.info(f"  Average FPS: {frames_processed / processing_time:.2f}")
 393:     logging.info(f"  Object detection fallback used: {getattr(args, 'enable_object_detection', False)}")
 394:     logging.info(f"  Total frames exported: {len(final_output['frames'])}")
 395: 
 396:     return final_output
 397: 
 398: 
 399: def main(args):
 400:     """Traite une vidéo de manière séquentielle (un seul thread)."""
 401:     worker_type = "GPU" if args.use_gpu else "CPU"
 402:     logging.info(f"Démarrage du traitement séquentiel {worker_type} pour: {Path(args.video_file_path).name}")
 403:     logging.info(f"LD_LIBRARY_PATH (worker) = {os.environ.get('LD_LIBRARY_PATH', '')}")
 404:     logging.info(f"INSIGHTFACE_HOME (worker) = {os.environ.get('INSIGHTFACE_HOME', '')}")
 405: 
 406:     engine_name = getattr(args, 'tracking_engine', None)
 407:     use_gpu_flag = bool(getattr(args, 'use_gpu', False))
 408:     face_engine = None
 409:     if engine_name:
 410:         try:
 411:             face_engine = create_face_engine(engine_name, use_gpu=use_gpu_flag)
 412:         except Exception as e:
 413:             logging.exception(f"Failed to initialize tracking engine '{engine_name}': {e}")
 414:             sys.exit(1)
 415: 
 416:     final_output = None
 417:     total_frames = 0
 418: 
 419:     if face_engine is not None:
 420:         logging.info(f"Using OpenCV tracking engine: {engine_name}")
 421:         try:
 422:             with safe_video_processing(args.video_file_path) as (video_capture, temp_manager):
 423:                 fps = video_capture.get(cv2.CAP_PROP_FPS)
 424:                 total_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))
 425: 
 426:                 object_detector = None
 427:                 object_options = None
 428:                 mp_module = None
 429:                 ObjectDetector = None
 430:                 object_detection_workers = max(1, int(getattr(args, 'mp_num_workers_internal', 1) or 1))
 431:                 if args.use_gpu and getattr(args, 'enable_object_detection', False):
 432:                     logging.info(
 433:                         "Object detection fallback workers (GPU face engine mode): %s",
 434:                         object_detection_workers,
 435:                     )
 436:                 if getattr(args, 'enable_object_detection', False):
 437:                     try:
 438:                         mp_module = _ensure_mediapipe_loaded(required=False)
 439:                         if mp_module is None:
 440:                             raise RuntimeError("mediapipe is not available (lazy import failed)")
 441: 
 442:                         BaseOptions = mp_module.tasks.BaseOptions
 443:                         VisionRunningMode = mp_module.tasks.vision.RunningMode
 444:                         ObjectDetector = mp_module.tasks.vision.ObjectDetector
 445:                         ObjectDetectorOptions = mp_module.tasks.vision.ObjectDetectorOptions
 446: 
 447:                         delegate = BaseOptions.Delegate.CPU
 448:                         models_dir = Path(args.models_dir)
 449:                         object_detector_model_name = (
 450:                             getattr(args, 'object_detector_model', None)
 451:                             or os.environ.get('STEP5_OBJECT_DETECTOR_MODEL', 'efficientdet_lite2')
 452:                         )
 453:                         object_detector_model_path = (
 454:                             getattr(args, 'object_detector_model_path', None)
 455:                             or os.environ.get('STEP5_OBJECT_DETECTOR_MODEL_PATH')
 456:                         )
 457:                         object_model_path = ObjectDetectorRegistry.resolve_model_path(
 458:                             model_name=object_detector_model_name,
 459:                             models_dir=models_dir,
 460:                             override_path=object_detector_model_path,
 461:                         )
 462:                         logging.info(
 463:                             f"Using object detector (face_engine mode): {object_detector_model_name} at {object_model_path}"
 464:                         )
 465:                         object_options = ObjectDetectorOptions(
 466:                             base_options=BaseOptions(model_asset_path=str(object_model_path), delegate=delegate),
 467:                             running_mode=VisionRunningMode.IMAGE,
 468:                             max_results=getattr(args, 'object_max_results', 5),
 469:                             score_threshold=getattr(args, 'object_score_threshold', 0.4),
 470:                         )
 471:                         object_detector = ObjectDetector.create_from_options(object_options)
 472:                         if object_detection_workers > 1:
 473:                             logging.info(
 474:                                 "Object detection fallback: using one MediaPipe ObjectDetector instance per thread (IMAGE mode)."
 475:                             )
 476:                     except Exception as e:
 477:                         logging.warning(
 478:                             f"Failed to initialize object detector (face_engine mode); continuing without it: {e}"
 479:                         )
 480:                         object_detector = None
 481:                         object_options = None
 482:                         mp_module = None
 483:                         ObjectDetector = None
 484: 
 485:                 final_output = {
 486:                     "metadata": {
 487:                         "video_path": args.video_file_path,
 488:                         "total_frames": total_frames,
 489:                         "fps": fps,
 490:                         "tracking_engine": engine_name,
 491:                     },
 492:                     "frames": [],
 493:                 }
 494: 
 495:                 tracked_objects, next_id_counter = {}, {'value': 0}
 496: 
 497:                 resource_id = resource_tracker.register_resource(
 498:                     video_capture, 'video_capture', f"Processing {Path(args.video_file_path).name}"
 499:                 )
 500: 
 501:                 try:
 502:                     enhanced_speaking_detector = None
 503:                     if getattr(args, 'speaking_detection_enabled', True):
 504:                         try:
 505:                             enhanced_speaking_detector = EnhancedSpeakingDetector(
 506:                                 video_path=args.video_file_path,
 507:                                 jaw_threshold=args.speaking_detection_jaw_open_threshold,
 508:                             )
 509:                             logging.info("Enhanced speaking detection initialized successfully")
 510:                         except Exception as e:
 511:                             logging.warning(f"Failed to initialize enhanced speaking detector: {e}")
 512:                             enhanced_speaking_detector = None
 513: 
 514:                     object_tasks = None
 515:                     object_results = {}
 516:                     object_results_lock = threading.Lock()
 517:                     object_worker_threads = []
 518: 
 519:                     def _object_worker_loop(worker_id: int):
 520:                         thread_object_detector = None
 521:                         if object_options is not None and ObjectDetector is not None:
 522:                             try:
 523:                                 thread_object_detector = ObjectDetector.create_from_options(object_options)
 524:                             except Exception as e:
 525:                                 logging.warning(
 526:                                     "Object detection failed to initialize detector for worker %s (face_engine mode): %s",
 527:                                     worker_id,
 528:                                     e,
 529:                                 )
 530:                                 thread_object_detector = None
 531:                         while True:
 532:                             task = object_tasks.get() if object_tasks is not None else None
 533:                             if task is None:
 534:                                 try:
 535:                                     if thread_object_detector is not None and hasattr(thread_object_detector, 'close'):
 536:                                         thread_object_detector.close()
 537:                                 except Exception:
 538:                                     pass
 539:                                 if object_tasks is not None:
 540:                                     object_tasks.task_done()
 541:                                 break
 542:                             frame_local, frame_idx_local, timestamp_ms_local = task
 543:                             try:
 544:                                 if mp_module is None or thread_object_detector is None:
 545:                                     raise RuntimeError("object detector is not available for object detection fallback")
 546:                                 mp_image = mp_module.Image(
 547:                                     image_format=mp_module.ImageFormat.SRGB,
 548:                                     data=cv2.cvtColor(frame_local, cv2.COLOR_BGR2RGB)
 549:                                 )
 550:                                 object_result = thread_object_detector.detect(mp_image)
 551:                                 detections_out = []
 552:                                 if object_result.detections:
 553:                                     for detection in object_result.detections:
 554:                                         bbox = detection.bounding_box
 555:                                         x_min = int(bbox.origin_x)
 556:                                         y_min = int(bbox.origin_y)
 557:                                         width = int(bbox.width)
 558:                                         height = int(bbox.height)
 559:                                         centroid = (x_min + width // 2, y_min + height // 2)
 560:                                         best_category = detection.categories[0] if detection.categories else None
 561:                                         label = best_category.category_name if best_category else "object"
 562:                                         confidence = best_category.score if best_category else 0.0
 563:                                         detections_out.append(
 564:                                             {
 565:                                                 "bbox": (x_min, y_min, width, height),
 566:                                                 "centroid": centroid,
 567:                                                 "source_detector": "object_detector",
 568:                                                 "label": label,
 569:                                                 "confidence": confidence,
 570:                                                 "blendshapes": None,
 571:                                                 "is_speaking": None,
 572:                                             }
 573:                                         )
 574:                                 with object_results_lock:
 575:                                     object_results[frame_idx_local] = detections_out
 576:                             except Exception as e:
 577:                                 logging.warning(
 578:                                     f"Object detection failed for frame {frame_idx_local + 1} (face_engine mode): {e}"
 579:                                 )
 580:                                 with object_results_lock:
 581:                                     object_results[frame_idx_local] = []
 582:                             finally:
 583:                                 del frame_local
 584:                                 object_tasks.task_done()
 585: 
 586:                     enable_object_fallback = bool(getattr(args, 'enable_object_detection', False))
 587:                     enable_object_threads = bool(object_detector is not None and object_detection_workers > 1 and enable_object_fallback)
 588:                     if enable_object_threads:
 589:                         object_tasks = queue.Queue(maxsize=max(1, object_detection_workers) * 2)
 590:                         for i in range(int(object_detection_workers)):
 591:                             t = threading.Thread(target=_object_worker_loop, args=(i,), daemon=True)
 592:                             object_worker_threads.append(t)
 593:                             t.start()
 594: 
 595:                     frame_idx = 0
 596:                     frame_detections = {}
 597:                     while video_capture.isOpened():
 598:                         ret, frame = video_capture.read()
 599:                         if not ret:
 600:                             break
 601: 
 602:                         current_detections = face_engine.detect(frame)
 603:                         frame_idx += 1
 604:                         if frame_idx % 50 == 0:
 605:                             progress_percent = int((frame_idx / total_frames) * 90) if total_frames else 0
 606:                             print(f"[Progression]|{progress_percent}|{frame_idx}|{total_frames}", flush=True)
 607: 
 608:                         if (not current_detections) and object_detector is not None and enable_object_fallback:
 609:                             if enable_object_threads and object_tasks is not None:
 610:                                 object_tasks.put((frame, frame_idx - 1, None))
 611:                             else:
 612:                                 try:
 613:                                     if mp_module is None:
 614:                                         raise RuntimeError("mediapipe is not available for object detection fallback")
 615:                                     mp_image = mp_module.Image(
 616:                                         image_format=mp_module.ImageFormat.SRGB,
 617:                                         data=cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
 618:                                     )
 619:                                     object_result = object_detector.detect(mp_image)
 620:                                     if object_result.detections:
 621:                                         for detection in object_result.detections:
 622:                                             bbox = detection.bounding_box
 623:                                             x_min = int(bbox.origin_x)
 624:                                             y_min = int(bbox.origin_y)
 625:                                             width = int(bbox.width)
 626:                                             height = int(bbox.height)
 627:                                             centroid = (x_min + width // 2, y_min + height // 2)
 628:                                             best_category = detection.categories[0] if detection.categories else None
 629:                                             label = best_category.category_name if best_category else "object"
 630:                                             confidence = best_category.score if best_category else 0.0
 631:                                             current_detections.append(
 632:                                                 {
 633:                                                     "bbox": (x_min, y_min, width, height),
 634:                                                     "centroid": centroid,
 635:                                                     "source_detector": "object_detector",
 636:                                                     "label": label,
 637:                                                     "confidence": confidence,
 638:                                                     "blendshapes": None,
 639:                                                     "is_speaking": None,
 640:                                                 }
 641:                                             )
 642:                                 except Exception as e:
 643:                                     logging.warning(
 644:                                         f"Object detection failed for frame {frame_idx} (face_engine mode): {e}"
 645:                                     )
 646: 
 647:                         frame_detections[frame_idx - 1] = list(current_detections)
 648:                         del frame
 649: 
 650:                     if enable_object_threads and object_tasks is not None:
 651:                         object_tasks.join()
 652:                         for _ in object_worker_threads:
 653:                             object_tasks.put(None)
 654:                         object_tasks.join()
 655:                         for t in object_worker_threads:
 656:                             t.join(timeout=1)
 657: 
 658:                     for frame_idx_out in range(total_frames):
 659:                         current_detections = frame_detections.get(frame_idx_out, [])
 660:                         if (not current_detections) and object_detector is not None and enable_object_fallback:
 661:                             with object_results_lock:
 662:                                 current_detections = list(object_results.get(frame_idx_out, []))
 663: 
 664:                         tracked_for_frame = apply_tracking_and_management(
 665:                             tracked_objects,
 666:                             current_detections,
 667:                             next_id_counter,
 668:                             args.mp_max_distance_tracking,
 669:                             args.mp_frames_unseen_deregister,
 670:                             args.speaking_detection_jaw_open_threshold,
 671:                             enhanced_speaking_detector=enhanced_speaking_detector,
 672:                             current_frame_num=frame_idx_out + 1,
 673:                         )
 674:                         final_output["frames"].append(
 675:                             {
 676:                                 "frame": frame_idx_out + 1,
 677:                                 "tracked_objects": tracked_for_frame if tracked_for_frame else [],
 678:                             }
 679:                         )
 680:                         if (frame_idx_out + 1) % 50 == 0:
 681:                             progress_percent = 90 + int(((frame_idx_out + 1) / total_frames) * 10) if total_frames else 100
 682:                             print(f"[Progression]|{progress_percent}|{frame_idx_out + 1}|{total_frames}", flush=True)
 683: 
 684:                     frame_idx = total_frames
 685: 
 686:                     if total_frames and frame_idx < total_frames:
 687:                         for fill_idx in range(frame_idx, total_frames):
 688:                             tracked_for_frame = apply_tracking_and_management(
 689:                                 tracked_objects,
 690:                                 [],
 691:                                 next_id_counter,
 692:                                 args.mp_max_distance_tracking,
 693:                                 args.mp_frames_unseen_deregister,
 694:                                 args.speaking_detection_jaw_open_threshold,
 695:                                 enhanced_speaking_detector=enhanced_speaking_detector,
 696:                                 current_frame_num=fill_idx + 1,
 697:                             )
 698:                             final_output["frames"].append(
 699:                                 {
 700:                                     "frame": fill_idx + 1,
 701:                                     "tracked_objects": tracked_for_frame if tracked_for_frame else [],
 702:                                 }
 703:                             )
 704:                 finally:
 705:                     try:
 706:                         if object_detector is not None and hasattr(object_detector, 'close'):
 707:                             object_detector.close()
 708:                     except Exception:
 709:                         pass
 710:                     resource_tracker.unregister_resource(resource_id)
 711: 
 712:         except Exception as e:
 713:             logging.error(f"Error processing video {args.video_file_path} with OpenCV engine: {e}")
 714:             raise
 715: 
 716:         output_path = Path(args.video_file_path).with_suffix('.json')
 717:         try:
 718:             with open(output_path, 'w', encoding='utf-8') as f:
 719:                 json.dump(final_output, f, indent=2, ensure_ascii=False)
 720:             logging.info(f"Traitement terminé. JSON sauvegardé: {output_path.name}")
 721:             print(f"[Progression]|100|{total_frames}|{total_frames}", flush=True)
 722:         except Exception as e:
 723:             logging.error(f"Failed to save output file {output_path}: {e}")
 724:             raise
 725:         return
 726: 
 727:     mp_module = _ensure_mediapipe_loaded(required=True)
 728:     BaseOptions = mp_module.tasks.BaseOptions
 729:     VisionRunningMode = mp_module.tasks.vision.RunningMode
 730:     FaceLandmarker = mp_module.tasks.vision.FaceLandmarker
 731:     FaceLandmarkerOptions = mp_module.tasks.vision.FaceLandmarkerOptions
 732:     ObjectDetector = mp_module.tasks.vision.ObjectDetector
 733:     ObjectDetectorOptions = mp_module.tasks.vision.ObjectDetectorOptions
 734: 
 735:     delegate = BaseOptions.Delegate.GPU if args.use_gpu else BaseOptions.Delegate.CPU
 736:     models_dir = Path(args.models_dir)
 737:     face_model_candidates = [
 738:         models_dir / "face_detectors" / "mediapipe" / "face_landmarker_v2_with_blendshapes.task",
 739:         models_dir / "face_landmarker_v2_with_blendshapes.task",
 740:     ]
 741:     face_model_path = next((p for p in face_model_candidates if p.exists()), face_model_candidates[0])
 742:     
 743:     if not face_model_path.exists():
 744:         logging.error(f"Modèle FaceLandmarker non trouvé: {face_model_path}");
 745:         sys.exit(1)
 746:     
 747:     # Resolve object detector model using registry
 748:     object_detector_model_name = getattr(args, 'object_detector_model', 'efficientdet_lite2')
 749:     try:
 750:         object_model_path = ObjectDetectorRegistry.resolve_model_path(
 751:             model_name=object_detector_model_name,
 752:             models_dir=models_dir,
 753:             override_path=getattr(args, 'object_detector_model_path', None)
 754:         )
 755:         logging.info(f"Using object detector: {object_detector_model_name} at {object_model_path}")
 756:     except (ValueError, FileNotFoundError) as e:
 757:         logging.error(f"Failed to resolve object detector model: {e}")
 758:         sys.exit(1)
 759: 
 760:     face_options = FaceLandmarkerOptions(
 761:         base_options=BaseOptions(model_asset_path=str(face_model_path), delegate=delegate),
 762:         running_mode=VisionRunningMode.VIDEO,
 763:         num_faces=int(_parse_optional_positive_int(os.environ.get("STEP5_MEDIAPIPE_MAX_FACES")) or args.mp_landmarker_num_faces),
 764:         min_face_detection_confidence=args.mp_landmarker_min_face_detection_confidence,
 765:         min_face_presence_confidence=args.mp_landmarker_min_face_presence_confidence,
 766:         min_tracking_confidence=args.mp_landmarker_min_tracking_confidence,
 767:         output_face_blendshapes=True
 768:     )
 769: 
 770:     object_options = ObjectDetectorOptions(
 771:         base_options=BaseOptions(model_asset_path=str(object_model_path), delegate=delegate),
 772:         running_mode=VisionRunningMode.VIDEO,
 773:         max_results=getattr(args, 'object_max_results', 5),
 774:         score_threshold=getattr(args, 'object_score_threshold', 0.5)
 775:     )
 776: 
 777:     # Use safe video processing with automatic resource cleanup
 778:     try:
 779:         with safe_video_processing(args.video_file_path) as (video_capture, temp_manager):
 780:             # Get video metadata safely
 781:             fps = video_capture.get(cv2.CAP_PROP_FPS)
 782:             total_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))
 783: 
 784:             logging.info(f"Video metadata - FPS: {fps}, Total frames: {total_frames}")
 785: 
 786:             # Register video capture for tracking
 787:             resource_id = resource_tracker.register_resource(
 788:                 video_capture, 'video_capture', f"Processing {Path(args.video_file_path).name}"
 789:             )
 790: 
 791:             try:
 792:                 with FaceLandmarker.create_from_options(face_options) as landmarker, \
 793:                      ObjectDetector.create_from_options(object_options) as object_detector:
 794: 
 795:                     final_output = {
 796:                         "metadata": {
 797:                             "video_path": args.video_file_path,
 798:                             "total_frames": total_frames,
 799:                             "fps": fps
 800:                         },
 801:                         "frames": []
 802:                     }
 803:                     tracked_objects, next_id_counter = {}, {'value': 0}
 804: 
 805:                     # Fallback tracking variables
 806:                     face_detection_success_count = 0
 807:                     frames_processed = 0
 808:                     use_object_detection_fallback = getattr(args, 'enable_object_detection', False)
 809:                     fallback_threshold = 50  # Check every 50 frames for fallback decision
 810: 
 811:                     # Initialize enhanced speaking detector
 812:                     enhanced_speaking_detector = None
 813:                     # Speaking detector activé par défaut
 814:                     if getattr(args, 'speaking_detection_enabled', True):
 815:                         try:
 816:                             enhanced_speaking_detector = EnhancedSpeakingDetector(
 817:                                 video_path=args.video_file_path,
 818:                                 jaw_threshold=args.speaking_detection_jaw_open_threshold
 819:                             )
 820:                             logging.info("Enhanced speaking detection initialized successfully")
 821:                             logging.info(f"Detection stats: {enhanced_speaking_detector.get_detection_stats()}")
 822:                         except Exception as e:
 823:                             logging.warning(f"Failed to initialize enhanced speaking detector: {e}")
 824:                             enhanced_speaking_detector = None
 825: 
 826:                     # Check if multi-threading should be used (CPU workers with > 1 internal workers)
 827:                     use_multithreading = (not args.use_gpu and
 828:                                         getattr(args, 'mp_num_workers_internal', 1) > 1)
 829: 
 830:                     if use_multithreading:
 831:                         logging.info(f"Using multi-threaded processing with {args.mp_num_workers_internal} workers")
 832:                         final_output = process_video_multithreaded(
 833:                             args, video_capture, landmarker, object_detector,
 834:                             enhanced_speaking_detector, total_frames
 835:                         )
 836:                     else:
 837:                         logging.info("Using sequential processing")
 838:                         # Sequential processing (original logic)
 839:                         frame_idx = 0
 840:                         while video_capture.isOpened():
 841:                             ret, frame = video_capture.read()
 842:                             if not ret:
 843:                                 break
 844: 
 845:                             mp_image = mp.Image(
 846:                                 image_format=mp.ImageFormat.SRGB,
 847:                                 data=cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
 848:                             )
 849:                             timestamp_ms = int(video_capture.get(cv2.CAP_PROP_POS_MSEC))
 850: 
 851:                             # Try face detection first
 852:                             face_result = landmarker.detect_for_video(mp_image, timestamp_ms)
 853:                             frames_processed += 1
 854: 
 855:                             current_detections = []
 856:                             face_detected = False
 857: 
 858:                             if face_result.face_landmarks:
 859:                                 face_detected = True
 860:                                 face_detection_success_count += 1
 861: 
 862:                                 for i, landmarks in enumerate(face_result.face_landmarks):
 863:                                     h, w, _ = frame.shape
 864:                                     x_coords = [lm.x * w for lm in landmarks]
 865:                                     y_coords = [lm.y * h for lm in landmarks]
 866:                                     bbox = (
 867:                                         int(min(x_coords)), int(min(y_coords)),
 868:                                         int(max(x_coords) - min(x_coords)),
 869:                                         int(max(y_coords) - min(y_coords))
 870:                                     )
 871:                                     centroid = (int(np.mean(x_coords)), int(np.mean(y_coords)))
 872:                                     det = {
 873:                                         "bbox": bbox,
 874:                                         "centroid": centroid,
 875:                                         "source_detector": "face_landmarker",
 876:                                         "label": "face"
 877:                                     }
 878:                                     if face_result.face_blendshapes:
 879:                                         det["blendshapes"] = {
 880:                                             cat.category_name: cat.score
 881:                                             for cat in face_result.face_blendshapes[i]
 882:                                         }
 883:                                     current_detections.append(det)
 884: 
 885:                             # Check if we should enable object detection fallback
 886:                             if frames_processed % fallback_threshold == 0:
 887:                                 face_success_rate = face_detection_success_count / frames_processed
 888:                                 if face_success_rate < 0.1:  # Less than 10% face detection success
 889:                                     if not use_object_detection_fallback:
 890:                                         logging.info(f"Low face detection rate ({face_success_rate:.2%}). Enabling object detection fallback.")
 891:                                         use_object_detection_fallback = True
 892: 
 893:                             # Use object detection fallback if no faces detected and fallback is enabled
 894:                             if not face_detected and use_object_detection_fallback:
 895:                                 try:
 896:                                     object_result = object_detector.detect_for_video(mp_image, timestamp_ms)
 897: 
 898:                                     if object_result.detections:
 899:                                         for detection in object_result.detections:
 900:                                             bbox = detection.bounding_box
 901:                                             # MediaPipe object detection returns coordinates already in pixel format
 902:                                             # No conversion needed - use values directly
 903:                                             x_min = int(bbox.origin_x)
 904:                                             y_min = int(bbox.origin_y)
 905:                                             width = int(bbox.width)
 906:                                             height = int(bbox.height)
 907: 
 908:                                             centroid = (x_min + width // 2, y_min + height // 2)
 909: 
 910:                                             # Get the best category
 911:                                             best_category = detection.categories[0] if detection.categories else None
 912:                                             label = best_category.category_name if best_category else "object"
 913:                                             confidence = best_category.score if best_category else 0.0
 914: 
 915:                                             det = {
 916:                                                 "bbox": (x_min, y_min, width, height),
 917:                                                 "centroid": centroid,
 918:                                                 "source_detector": "object_detector",
 919:                                                 "label": label,
 920:                                                 "confidence": confidence,
 921:                                                 "blendshapes": None,  # Not applicable for objects
 922:                                                 "is_speaking": None   # Not applicable for objects
 923:                                             }
 924:                                             current_detections.append(det)
 925:                                 except Exception as e:
 926:                                     logging.warning(f"Object detection failed for frame {frame_idx + 1}: {e}")
 927: 
 928:                             tracked_for_frame = apply_tracking_and_management(
 929:                                 tracked_objects, current_detections, next_id_counter,
 930:                                 args.mp_max_distance_tracking, args.mp_frames_unseen_deregister,
 931:                                 args.speaking_detection_jaw_open_threshold,
 932:                                 enhanced_speaking_detector=enhanced_speaking_detector,
 933:                                 current_frame_num=frame_idx + 1
 934:                             )
 935: 
 936:                             # Always export frame data to maintain consistent structure
 937:                             # Even if no objects are tracked, we include the frame with empty tracked_objects
 938:                             final_output["frames"].append({
 939:                                 "frame": frame_idx + 1,
 940:                                 "tracked_objects": tracked_for_frame if tracked_for_frame else []
 941:                             })
 942: 
 943:                             frame_idx += 1
 944:                             if frame_idx % 50 == 0:
 945:                                 progress_percent = int((frame_idx / total_frames) * 100)
 946:                                 print(f"[Progression]|{progress_percent}|{frame_idx}|{total_frames}", flush=True)
 947: 
 948:                         # Log processing summary for sequential processing
 949:                         face_success_rate = face_detection_success_count / frames_processed if frames_processed > 0 else 0
 950:                         logging.info(f"Processing summary:")
 951:                         logging.info(f"  Total frames processed: {frames_processed}")
 952:                         logging.info(f"  Frames with faces: {face_detection_success_count}")
 953:                         logging.info(f"  Face detection success rate: {face_success_rate:.2%}")
 954:                         logging.info(f"  Object detection fallback used: {use_object_detection_fallback}")
 955:                         logging.info(f"  Total frames exported: {len(final_output['frames'])}")
 956: 
 957:             finally:
 958:                 # Unregister the video capture resource
 959:                 resource_tracker.unregister_resource(resource_id)
 960: 
 961:     except Exception as e:
 962:         logging.error(f"Error processing video {args.video_file_path}: {e}")
 963:         raise
 964: 
 965:     # Save output with proper error handling
 966:     output_path = Path(args.video_file_path).with_suffix('.json')
 967:     try:
 968:         with open(output_path, 'w', encoding='utf-8') as f:
 969:             json.dump(final_output, f, indent=2, ensure_ascii=False)
 970:         logging.info(f"Traitement terminé. JSON sauvegardé: {output_path.name}")
 971:         print(f"[Progression]|100|{total_frames}|{total_frames}", flush=True)
 972:     except Exception as e:
 973:         logging.error(f"Failed to save output file {output_path}: {e}")
 974:         raise
 975: 
 976: 
 977: if __name__ == "__main__":
 978:     parser = argparse.ArgumentParser(description="Worker séquentiel pour MediaPipe.")
 979:     parser.add_argument("video_file_path")
 980:     parser.add_argument("--models_dir", required=True)
 981:     parser.add_argument("--use_gpu", action="store_true")
 982:     parser.add_argument("--tracking_engine", default=None, help="Tracking engine: mediapipe_landmarker (default), opencv_haar, opencv_yunet")
 983:     # Conserver les arguments pour la compatibilité avec le manager, même s'ils ne sont pas tous utilisés
 984:     parser.add_argument("--mp_landmarker_num_faces", type=int, default=1)
 985:     parser.add_argument("--mp_landmarker_min_face_detection_confidence", type=float, default=0.5)
 986:     parser.add_argument("--mp_landmarker_min_face_presence_confidence", type=float, default=0.3)
 987:     parser.add_argument("--mp_landmarker_min_tracking_confidence", type=float, default=0.5)
 988:     parser.add_argument("--mp_landmarker_output_blendshapes", action="store_true")
 989:     parser.add_argument("--mp_max_distance_tracking", type=int, default=70)
 990:     parser.add_argument("--mp_frames_unseen_deregister", type=int, default=7)
 991:     parser.add_argument("--speaking_detection_jaw_open_threshold", type=float, default=0.08)
 992:     parser.add_argument("--speaking_detection_enabled", action="store_true", default=True, help="Enable enhanced speaking detection module (enabled by default)")
 993:     # Object detection parameters
 994:     parser.add_argument("--enable_object_detection", action="store_true", help="Enable object detection fallback")
 995:     parser.add_argument("--object_score_threshold", type=float, default=0.5, help="Object detection confidence threshold")
 996:     parser.add_argument("--object_max_results", type=int, default=5, help="Maximum number of objects to detect")
 997:     # Multi-threading parameter for CPU workers
 998:     parser.add_argument("--mp_num_workers_internal", type=int, default=1, help="Number of internal worker threads for CPU processing")
 999: 
1000:     args, _ = parser.parse_known_args()
1001: 
1002:     try:
1003:         main(args)
1004:         sys.exit(0)
1005:     except Exception as e:
1006:         logging.error(f"Erreur critique dans le worker: {e}", exc_info=True)
1007:         sys.exit(1)
```

## File: workflow_scripts/step5/pyfeat_blendshape_extractor.py
```python
  1: import logging
  2: import os
  3: from pathlib import Path
  4: from typing import Optional, Dict, Any, List
  5: 
  6: import numpy as np
  7: 
  8: logger = logging.getLogger(__name__)
  9: 
 10: 
 11: class PyFeatBlendshapeExtractor:
 12:     def __init__(
 13:         self,
 14:         model_path: Optional[str] = None,
 15:         device: Optional[str] = None,
 16:         use_gpu: bool = False,
 17:     ):
 18:         self._model_path = model_path or os.environ.get("STEP5_PYFEAT_MODEL_PATH")
 19:         self._model = None
 20:         self._blendshape_names = None
 21:         self._use_gpu = use_gpu
 22:         
 23:         try:
 24:             import torch
 25:             self._torch = torch
 26:         except ImportError:
 27:             raise RuntimeError(
 28:                 "PyTorch is required for py-feat blendshape extraction. "
 29:                 "Install with: pip install torch"
 30:             )
 31: 
 32:         if device:
 33:             self._device = device
 34:         else:
 35:             if use_gpu and torch.cuda.is_available():
 36:                 self._device = "cuda"
 37:             else:
 38:                 if use_gpu and not torch.cuda.is_available():
 39:                     logger.warning("CUDA requested for py-feat but no GPU detected; falling back to CPU mode")
 40:                 self._device = "cpu"
 41: 
 42:         self._initialize_model()
 43: 
 44:     def _initialize_model(self):
 45:         if not self._model_path:
 46:             self._download_model()
 47:         
 48:         if not Path(self._model_path).exists():
 49:             raise RuntimeError(f"py-feat model not found at: {self._model_path}")
 50: 
 51:         try:
 52:             checkpoint = self._torch.load(
 53:                 self._model_path,
 54:                 map_location=self._device,
 55:                 weights_only=True,
 56:             )
 57: 
 58:             if isinstance(checkpoint, dict) and "net" in checkpoint:
 59:                 state_dict = checkpoint["net"]
 60:             else:
 61:                 state_dict = checkpoint
 62: 
 63:             self._model = self._create_model_architecture(state_dict)
 64:             self._model.load_state_dict(state_dict)
 65:             self._model.eval()
 66:             self._model.to(self._device)
 67:             
 68:             self._blendshape_names = self._get_blendshape_names()
 69:             
 70:             logger.info(f"py-feat blendshape model loaded: {self._model_path}")
 71:         except Exception as e:
 72:             logger.error(f"Failed to load py-feat model: {e}")
 73:             raise
 74: 
 75:     def _download_model(self):
 76:         try:
 77:             from huggingface_hub import hf_hub_download
 78:             
 79:             preferred_cache_dir = (
 80:                 Path(__file__).parent
 81:                 / "models"
 82:                 / "blendshapes"
 83:                 / "opencv"
 84:                 / "pyfeat_models"
 85:             )
 86:             legacy_cache_dir = Path(__file__).parent / "models" / "pyfeat"
 87: 
 88:             cache_dir = legacy_cache_dir if legacy_cache_dir.exists() else preferred_cache_dir
 89:             cache_dir.mkdir(parents=True, exist_ok=True)
 90:             
 91:             self._model_path = hf_hub_download(
 92:                 repo_id="py-feat/mp_blendshapes",
 93:                 filename="face_blendshapes.pth",
 94:                 cache_dir=str(cache_dir),
 95:             )
 96:             logger.info(f"Downloaded py-feat model to: {self._model_path}")
 97:         except Exception as e:
 98:             raise RuntimeError(
 99:                 f"Failed to download py-feat model from HuggingFace: {e}. "
100:                 "Set STEP5_PYFEAT_MODEL_PATH manually or install: pip install huggingface_hub"
101:             )
102: 
103:     def _create_model_architecture(self, state_dict: Dict[str, Any]):
104:         torch = self._torch
105: 
106:         if isinstance(state_dict, dict) and any(k.startswith("mlpmixer_blocks.") for k in state_dict.keys()):
107: 
108:             class _ChannelLayerNormNoBias(torch.nn.Module):
109:                 def __init__(self, channels: int, eps: float = 1e-5):
110:                     super().__init__()
111:                     self.weight = torch.nn.Parameter(torch.ones(channels))
112:                     self.eps = eps
113: 
114:                 def forward(self, x):
115:                     mean = x.mean(dim=1, keepdim=True)
116:                     var = (x - mean).pow(2).mean(dim=1, keepdim=True)
117:                     x = (x - mean) / torch.sqrt(var + self.eps)
118:                     return x * self.weight.view(1, -1, 1, 1)
119: 
120:             class _MLPMixerBlock(torch.nn.Module):
121:                 def __init__(
122:                     self,
123:                     tokens: int,
124:                     channels: int,
125:                     token_mlp_dim: int,
126:                     channel_mlp_dim: int,
127:                 ):
128:                     super().__init__()
129:                     self.norm1 = _ChannelLayerNormNoBias(channels)
130:                     self.mlp_token_mixing = torch.nn.Sequential(
131:                         torch.nn.Conv2d(tokens, token_mlp_dim, kernel_size=1, bias=True),
132:                         torch.nn.GELU(),
133:                         torch.nn.Identity(),
134:                         torch.nn.Conv2d(token_mlp_dim, tokens, kernel_size=1, bias=True),
135:                     )
136:                     self.norm2 = _ChannelLayerNormNoBias(channels)
137:                     self.mlp_channel_mixing = torch.nn.Sequential(
138:                         torch.nn.Conv2d(channels, channel_mlp_dim, kernel_size=1, bias=True),
139:                         torch.nn.GELU(),
140:                         torch.nn.Identity(),
141:                         torch.nn.Conv2d(channel_mlp_dim, channels, kernel_size=1, bias=True),
142:                     )
143: 
144:                 def forward(self, x):
145:                     y = self.norm1(x)
146:                     y = y.permute(0, 2, 1, 3)
147:                     y = self.mlp_token_mixing(y)
148:                     y = y.permute(0, 2, 1, 3)
149:                     x = x + y
150: 
151:                     y = self.norm2(x)
152:                     y = self.mlp_channel_mixing(y)
153:                     x = x + y
154:                     return x
155: 
156:             class MediaPipeBlendshapesMLPMixer(torch.nn.Module):
157:                 def __init__(self, sd: Dict[str, Any]):
158:                     super().__init__()
159: 
160:                     conv1_w = sd["conv1.weight"]
161:                     conv2_w = sd["conv2.weight"]
162:                     out_w = sd["output_mlp.weight"]
163: 
164:                     tokens_without_extra = int(conv1_w.shape[0])
165:                     num_landmarks = int(conv1_w.shape[1])
166:                     num_blendshapes = int(out_w.shape[0])
167: 
168:                     tokens = tokens_without_extra + 1
169:                     channels = int(conv2_w.shape[0])
170:                     token_mlp_dim = int(sd["mlpmixer_blocks.0.mlp_token_mixing.0.weight"].shape[0])
171:                     channel_mlp_dim = int(sd["mlpmixer_blocks.0.mlp_channel_mixing.0.weight"].shape[0])
172:                     num_blocks = len({k.split(".")[1] for k in sd.keys() if k.startswith("mlpmixer_blocks.")})
173: 
174:                     self.num_landmarks = num_landmarks
175:                     self.num_blendshapes = num_blendshapes
176: 
177:                     self.conv1 = torch.nn.Conv2d(num_landmarks, tokens_without_extra, kernel_size=1, bias=True)
178:                     self.conv2 = torch.nn.Conv2d(2, channels, kernel_size=1, bias=True)
179:                     self.extra_token = torch.nn.Parameter(torch.zeros(1, channels, 1, 1))
180: 
181:                     self.mlpmixer_blocks = torch.nn.ModuleList(
182:                         [
183:                             _MLPMixerBlock(
184:                                 tokens=tokens,
185:                                 channels=channels,
186:                                 token_mlp_dim=token_mlp_dim,
187:                                 channel_mlp_dim=channel_mlp_dim,
188:                             )
189:                             for _ in range(int(num_blocks))
190:                         ]
191:                     )
192: 
193:                     self.output_mlp = torch.nn.Conv2d(channels, num_blendshapes, kernel_size=1, bias=True)
194:                     self.sigmoid = torch.nn.Sigmoid()
195: 
196:                 def forward(self, landmarks_xy):
197:                     if landmarks_xy.ndim != 3:
198:                         raise ValueError(f"Expected landmarks_xy to have shape [B, N, 2], got {tuple(landmarks_xy.shape)}")
199: 
200:                     x = landmarks_xy.unsqueeze(-1)
201:                     x = self.conv1(x)
202:                     x = x.permute(0, 2, 1, 3)
203:                     x = self.conv2(x)
204: 
205:                     extra = self.extra_token.expand(x.shape[0], -1, -1, -1)
206:                     x = torch.cat([x, extra], dim=2)
207: 
208:                     for block in self.mlpmixer_blocks:
209:                         x = block(x)
210: 
211:                     x = self.output_mlp(x)
212:                     x = x[:, :, -1, 0]
213:                     x = self.sigmoid(x)
214:                     return x
215: 
216:             return MediaPipeBlendshapesMLPMixer(state_dict)
217: 
218:         class MediaPipeBlendshapesSimpleMLP(torch.nn.Module):
219:             def __init__(self):
220:                 super().__init__()
221:                 self.num_landmarks = 146
222:                 self.num_blendshapes = 52
223: 
224:                 self.fc1 = torch.nn.Linear(self.num_landmarks * 2, 256)
225:                 self.bn1 = torch.nn.BatchNorm1d(256)
226:                 self.fc2 = torch.nn.Linear(256, 256)
227:                 self.bn2 = torch.nn.BatchNorm1d(256)
228:                 self.fc3 = torch.nn.Linear(256, 128)
229:                 self.bn3 = torch.nn.BatchNorm1d(128)
230:                 self.fc4 = torch.nn.Linear(128, self.num_blendshapes)
231:                 self.relu = torch.nn.ReLU()
232:                 self.sigmoid = torch.nn.Sigmoid()
233: 
234:             def forward(self, x):
235:                 x = x.view(x.size(0), -1)
236:                 x = self.relu(self.bn1(self.fc1(x)))
237:                 x = self.relu(self.bn2(self.fc2(x)))
238:                 x = self.relu(self.bn3(self.fc3(x)))
239:                 x = self.sigmoid(self.fc4(x))
240:                 return x
241: 
242:         return MediaPipeBlendshapesSimpleMLP()
243: 
244:     def _get_blendshape_names(self) -> List[str]:
245:         return [
246:             "browDownLeft", "browDownRight", "browInnerUp", "browOuterUpLeft",
247:             "browOuterUpRight", "cheekPuff", "cheekSquintLeft", "cheekSquintRight",
248:             "eyeBlinkLeft", "eyeBlinkRight", "eyeLookDownLeft", "eyeLookDownRight",
249:             "eyeLookInLeft", "eyeLookInRight", "eyeLookOutLeft", "eyeLookOutRight",
250:             "eyeLookUpLeft", "eyeLookUpRight", "eyeSquintLeft", "eyeSquintRight",
251:             "eyeWideLeft", "eyeWideRight", "jawForward", "jawLeft", "jawOpen",
252:             "jawRight", "mouthClose", "mouthDimpleLeft", "mouthDimpleRight",
253:             "mouthFrownLeft", "mouthFrownRight", "mouthFunnel", "mouthLeft",
254:             "mouthLowerDownLeft", "mouthLowerDownRight", "mouthPressLeft",
255:             "mouthPressRight", "mouthPucker", "mouthRight", "mouthRollLower",
256:             "mouthRollUpper", "mouthShrugLower", "mouthShrugUpper", "mouthSmileLeft",
257:             "mouthSmileRight", "mouthStretchLeft", "mouthStretchRight",
258:             "mouthUpperUpLeft", "mouthUpperUpRight", "noseSneerLeft", "noseSneerRight",
259:             "tongueOut"
260:         ]
261: 
262:     def _select_landmark_subset(self, landmarks_478: np.ndarray) -> np.ndarray:
263:         landmark_indices = [
264:             0, 1, 4, 5, 6, 7, 8, 10, 13, 14, 17, 21, 33, 37, 39, 40, 46, 52, 53, 54,
265:             55, 58, 61, 63, 65, 66, 67, 68, 69, 70, 78, 80, 81, 82, 84, 87, 88, 91,
266:             93, 95, 103, 105, 107, 109, 127, 132, 133, 136, 144, 145, 146, 148, 149,
267:             150, 152, 153, 154, 155, 157, 158, 159, 160, 161, 162, 163, 168, 172,
268:             173, 176, 178, 181, 185, 191, 195, 197, 234, 246, 249, 251, 263, 267,
269:             269, 270, 276, 282, 283, 284, 285, 288, 291, 293, 295, 296, 297, 298,
270:             299, 300, 308, 310, 311, 312, 314, 317, 318, 321, 323, 324, 332, 334,
271:             336, 338, 356, 361, 362, 365, 373, 374, 375, 377, 378, 379, 380, 381,
272:             382, 384, 385, 386, 387, 388, 389, 390, 397, 398, 402, 405, 415, 466,
273:             468, 469, 470, 471, 472, 473, 474, 475, 476
274:         ]
275:         expected = getattr(self._model, "num_landmarks", None)
276:         if isinstance(expected, int) and expected > 0 and len(landmark_indices) != expected:
277:             logger.warning(
278:                 "py-feat landmark subset size mismatch (got %s, expected %s). Truncating indices.",
279:                 len(landmark_indices),
280:                 expected,
281:             )
282:             landmark_indices = landmark_indices[:expected]
283:         return landmarks_478[landmark_indices, :2]
284: 
285:     def extract_blendshapes(
286:         self,
287:         landmarks_478: np.ndarray,
288:         image_width: int,
289:         image_height: int
290:     ) -> Optional[Dict[str, float]]:
291:         if landmarks_478 is None or len(landmarks_478) < 478:
292:             return None
293: 
294:         try:
295:             landmarks_subset = self._select_landmark_subset(landmarks_478)
296:             
297:             # Optimize: check normalization before tensor creation
298:             max_coord = float(np.max(np.abs(landmarks_subset))) if landmarks_subset.size > 0 else 0.0
299:             if max_coord <= 2.0:
300:                 # Apply scaling in numpy (faster than tensor ops)
301:                 landmarks_subset = landmarks_subset * np.array([image_width, image_height], dtype=np.float32)
302:             
303:             # Single tensor creation with optimal dtype
304:             landmarks_tensor = self._torch.from_numpy(landmarks_subset.copy()).float().unsqueeze(0)
305:             landmarks_tensor = landmarks_tensor.to(self._device)
306: 
307:             with self._torch.no_grad():
308:                 blendshapes_tensor = self._model(landmarks_tensor)
309: 
310:             blendshapes_array = blendshapes_tensor.squeeze(0).detach().cpu().numpy()
311:             
312:             return {
313:                 name: float(value)
314:                 for name, value in zip(self._blendshape_names, blendshapes_array)
315:             }
316:         except Exception as e:
317:             logger.warning(f"Failed to extract blendshapes: {e}")
318:             return None
```

## File: workflow_scripts/step6/json_reducer.py
```python
  1: import os
  2: import sys
  3: import json
  4: import argparse
  5: import logging
  6: from datetime import datetime
  7: 
  8: logger = logging.getLogger(__name__)
  9: logger.setLevel(logging.INFO)
 10: 
 11: 
 12: def setup_logging(log_dir: str):
 13:     os.makedirs(log_dir, exist_ok=True)
 14:     timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
 15:     log_path = os.path.join(log_dir, f"json_reducer_{timestamp}.log")
 16: 
 17:     formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
 18: 
 19:     fh = logging.FileHandler(log_path, encoding='utf-8')
 20:     fh.setLevel(logging.INFO)
 21:     fh.setFormatter(formatter)
 22: 
 23:     ch = logging.StreamHandler(sys.stdout)
 24:     ch.setLevel(logging.INFO)
 25:     ch.setFormatter(formatter)
 26: 
 27:     logger.handlers.clear()
 28:     logger.addHandler(fh)
 29:     logger.addHandler(ch)
 30:     logger.propagate = False
 31: 
 32:     logger.info(f"Log file initialized: {log_path}")
 33:     return log_path
 34: 
 35: 
 36: def reduce_video_json(data):
 37:     """
 38:     Réduit un objet JSON de données vidéo pour ne conserver que les clés
 39:     utiles au script After Effects.
 40:     """
 41:     if "frames" not in data:
 42:         return {"frames_analysis": []}  # Retourne une structure vide si le format est inattendu
 43: 
 44:     new_frames_data = []
 45:     for frame in data["frames"]:
 46:         new_tracked_objects = []
 47:         if "tracked_objects" in frame and frame["tracked_objects"] is not None:
 48:             for obj in frame["tracked_objects"]:
 49:                 # Initialisation de l'objet simplifié
 50:                 new_obj = {
 51:                     "id": obj.get("id"),
 52:                     "centroid_x": obj.get("centroid_x"),
 53:                     "source": obj.get("source"),
 54:                     "label": obj.get("label"),
 55:                     "active_speakers": []  # Valeur par défaut
 56:                 }
 57: 
 58:                 # Inclure la taille du bbox si disponible (ajout depuis l'étape 5)
 59:                 bbox_w = obj.get("bbox_width")
 60:                 bbox_h = obj.get("bbox_height")
 61:                 if bbox_w is not None and bbox_h is not None:
 62:                     new_obj["bbox_width"] = bbox_w
 63:                     new_obj["bbox_height"] = bbox_h
 64: 
 65:                 # Extraction sécurisée de active_speakers
 66:                 if (obj.get("speaking_sources") and
 67:                         isinstance(obj["speaking_sources"], dict) and
 68:                         obj["speaking_sources"].get("audio") and
 69:                         isinstance(obj["speaking_sources"]["audio"], dict)):
 70:                     new_obj["active_speakers"] = obj["speaking_sources"]["audio"].get("active_speakers", [])
 71: 
 72:                 new_tracked_objects.append(new_obj)
 73: 
 74:         new_frames_data.append({
 75:             "frame": frame.get("frame"),
 76:             "tracked_objects": new_tracked_objects
 77:         })
 78: 
 79:     # La structure finale utilise "frames_analysis" pour correspondre au script AE
 80:     return {"frames_analysis": new_frames_data}
 81: 
 82: 
 83: def reduce_audio_json(data):
 84:     """
 85:     Réduit un objet JSON de données audio pour ne conserver que les clés
 86:     utiles au script After Effects.
 87:     """
 88:     if "frames_analysis" not in data:
 89:         return {"frames_analysis": []}  # Retourne une structure vide si le format est inattendu
 90: 
 91:     new_frames_analysis = []
 92:     for frame_data in data["frames_analysis"]:
 93:         if "audio_info" in frame_data and frame_data["audio_info"] is not None:
 94:             new_audio_info = {
 95:                 "is_speech_present": frame_data["audio_info"].get("is_speech_present", False),
 96:                 "active_speaker_labels": frame_data["audio_info"].get("active_speaker_labels", [])
 97:             }
 98:             new_frames_analysis.append({
 99:                 "frame": frame_data.get("frame"),
100:                 "audio_info": new_audio_info
101:             })
102: 
103:     return {"frames_analysis": new_frames_analysis}
104: 
105: 
106: def process_directory(base_path, keyword="Camille"):
107:     """
108:     Analyse les dossiers dans le chemin de base, recherche le mot-clé,
109:     et traite les paires de fichiers JSON trouvées dans les sous-dossiers "docs".
110:     """
111:     logger.info(f"Démarrage du scan dans : {base_path}")
112:     if not os.path.isdir(base_path):
113:         logger.error(f"Erreur : Le répertoire de base '{base_path}' n'existe pas.")
114:         return
115: 
116:     # 1. Lister les dossiers de projet
117:     project_folders = [d for d in os.listdir(base_path)
118:                        if os.path.isdir(os.path.join(base_path, d)) and keyword in d]
119: 
120:     if not project_folders:
121:         print(f"Aucun dossier contenant le mot-clé '{keyword}' n'a été trouvé.")
122:         return
123: 
124:     logger.info(f"Dossiers de projet trouvés : {len(project_folders)}")
125: 
126:     for folder in project_folders:
127:         docs_path = os.path.join(base_path, folder, "docs")
128: 
129:         if not os.path.isdir(docs_path):
130:             logger.warning(f"-> Avertissement : Le dossier 'docs' est manquant dans '{folder}'.")
131:             continue
132: 
133:         logger.info(f"\n--- Traitement du dossier : {docs_path} ---")
134: 
135:         # 2. Identifier les paires de fichiers JSON
136:         all_files = os.listdir(docs_path)
137:         video_json_files = [f for f in all_files if f.endswith(".json") and not f.endswith("_audio.json")]
138: 
139:         if not video_json_files:
140:             logger.info("Aucun fichier JSON vidéo principal trouvé.")
141:             continue
142: 
143:         for video_file in video_json_files:
144:             base_name = video_file.rsplit('.', 1)[0]
145:             audio_file = f"{base_name}_audio.json"
146: 
147:             video_path = os.path.join(docs_path, video_file)
148:             audio_path = os.path.join(docs_path, audio_file)
149: 
150:             if audio_file not in all_files:
151:                 logger.warning(f"  - Fichier audio '{audio_file}' manquant pour '{video_file}'. Ignoré.")
152:                 continue
153: 
154:             logger.info(f"  - Paire trouvée : \n    - {video_file}\n    - {audio_file}")
155: 
156:             try:
157:                 # 3. Traiter le JSON vidéo
158:                 with open(video_path, 'r', encoding='utf-8') as f:
159:                     video_data = json.load(f)
160: 
161:                 reduced_video_data = reduce_video_json(video_data)
162: 
163:                 with open(video_path, 'w', encoding='utf-8') as f:
164:                     json.dump(reduced_video_data, f, indent=2)
165: 
166:                 logger.info("    - Fichier vidéo réduit avec succès.")
167: 
168:                 # 4. Traiter le JSON audio
169:                 with open(audio_path, 'r', encoding='utf-8') as f:
170:                     audio_data = json.load(f)
171: 
172:                 reduced_audio_data = reduce_audio_json(audio_data)
173: 
174:                 with open(audio_path, 'w', encoding='utf-8') as f:
175:                     json.dump(reduced_audio_data, f, indent=2)
176: 
177:                 logger.info("    - Fichier audio réduit avec succès.")
178: 
179:             except json.JSONDecodeError as e:
180:                 logger.error(f"    - ERREUR : Impossible de lire un fichier JSON. Erreur : {e}")
181:             except Exception as e:
182:                 logger.error(f"    - ERREUR : Une erreur inattendue est survenue. Erreur : {e}")
183: 
184:     logger.info("\n--- Traitement terminé ! ---")
185: 
186: 
187: def main():
188:     parser = argparse.ArgumentParser(description="Étape 6 - Réduction JSON (vidéo + audio)")
189:     parser.add_argument('--base_dir', type=str, default=os.environ.get('BASE_PATH_SCRIPTS', ''), help='Chemin base du projet (contenant projets_extraits)')
190:     parser.add_argument('--work_dir', type=str, default=None, help='Chemin explicite vers projets_extraits')
191:     parser.add_argument('--keyword', type=str, default=os.environ.get('FOLDER_KEYWORD', 'Camille'), help='Mot-clé pour filtrer les dossiers projet')
192:     parser.add_argument('--log_dir', type=str, default=str(os.path.join(os.path.dirname(__file__), '..', '..', '..', 'logs', 'step6')),
193:                         help='Répertoire pour les logs (par défaut logs/step6)')
194: 
195:     args = parser.parse_args()
196: 
197:     # Resolve working directory
198:     if args.work_dir:
199:         work_dir = args.work_dir
200:     else:
201:         base_dir = args.base_dir if args.base_dir else os.getcwd()
202:         work_dir = os.path.join(base_dir, 'projets_extraits')
203: 
204:     # Setup logging
205:     setup_logging(args.log_dir)
206: 
207:     # Progress total: count candidate projects
208:     try:
209:         if not os.path.isdir(work_dir):
210:             logger.warning(f"Répertoire de travail introuvable: {work_dir}")
211:             print(f"TOTAL_JSON_TO_REDUCE: 0")
212:             sys.exit(0)
213:         projects = [d for d in os.listdir(work_dir) if os.path.isdir(os.path.join(work_dir, d)) and args.keyword in d]
214:         print(f"TOTAL_JSON_TO_REDUCE: {len(projects)}")
215:     except Exception:
216:         print("TOTAL_JSON_TO_REDUCE: 0")
217: 
218:     # Run processing
219:     process_directory(work_dir, keyword=args.keyword)
220: 
221: 
222: if __name__ == "__main__":
223:     main()
```

## File: workflow_scripts/step7/finalize_and_copy.py
```python
  1: #!/usr/bin/env python3
  2: # -*- coding: utf-8 -*-
  3: 
  4: """
  5: Script de finalisation et copie des projets vers la destination finale.
  6: Étape 7 (Ubuntu)
  7: \- Archive d'abord les artefacts d'analyse (scènes/tracking/audio) avant suppression
  8: \- Copie ensuite le dossier du projet vers la destination finale
  9: """
 10: 
 11: import os
 12: import errno
 13: import sys
 14: import json
 15: import shutil
 16: import logging
 17: import subprocess
 18: import tempfile
 19: from pathlib import Path
 20: from datetime import datetime
 21: 
 22: ROOT_DIR = Path(__file__).resolve().parents[2]
 23: if str(ROOT_DIR) not in sys.path:
 24:     sys.path.insert(0, str(ROOT_DIR))
 25: 
 26: from config.settings import config
 27: from services.results_archiver import ResultsArchiver, SCENES_SUFFIX, AUDIO_SUFFIX, TRACKING_SUFFIX, VIDEO_METADATA_NAME
 28: 
 29: # --- Configuration ---
 30: WORK_DIR = Path(os.getcwd())
 31: # --- MODIFICATION: Changer la destination ---
 32: OUTPUT_DIR = Path(os.environ.get("OUTPUT_DIR", "/mnt/cache"))
 33: # --- FIN DE LA MODIFICATION ---
 34: FINALIZE_MODE = os.environ.get("FINALIZE_MODE", "lenient").lower()
 35: BASE_DIR = ROOT_DIR
 36: LOG_DIR = BASE_DIR / "logs" / "step7"
 37: 
 38: # --- Configuration du Logger ---
 39: LOG_DIR.mkdir(parents=True, exist_ok=True)
 40: log_file = LOG_DIR / f"finalize_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
 41: 
 42: logging.basicConfig(
 43:     level=logging.INFO,
 44:     format='%(asctime)s - %(levelname)s - %(message)s',
 45:     handlers=[
 46:         logging.FileHandler(log_file, encoding='utf-8'),
 47:         logging.StreamHandler(sys.stdout)
 48:     ]
 49: )
 50: 
 51: 
 52: def find_projects_to_finalize():
 53:     """Trouve tous les projets dans WORK_DIR qui sont prêts à être finalisés."""
 54:     projects = []
 55:     logging.info(f"Recherche de projets à finaliser dans: {WORK_DIR}")
 56: 
 57:     for project_dir in WORK_DIR.iterdir():
 58:         if not project_dir.is_dir() or project_dir.name.startswith("_temp_"):
 59:             continue
 60: 
 61:         is_ready = False
 62:         found_reason = ""
 63:         for video_file in project_dir.rglob("*.mp4"):
 64:             stem = video_file.stem
 65:             scenes_csv = next(project_dir.rglob(f"{stem}{SCENES_SUFFIX}"), None)
 66:             tracking_json = next(project_dir.rglob(f"{stem}{TRACKING_SUFFIX}"), None)
 67:             audio_json = next(project_dir.rglob(f"{stem}{AUDIO_SUFFIX}"), None)
 68:             if FINALIZE_MODE == "strict":
 69:                 if scenes_csv and scenes_csv.exists() and tracking_json and tracking_json.exists():
 70:                     found_reason = f"artefacts scènes+tracking pour '{video_file.name}'"
 71:                     is_ready = True
 72:                     break
 73:             elif FINALIZE_MODE == "videos":
 74:                 found_reason = f"vidéo présente '{video_file.name}'"
 75:                 is_ready = True
 76:                 break
 77:             else:
 78:                 if (scenes_csv and scenes_csv.exists()) or (tracking_json and tracking_json.exists()) or (audio_json and audio_json.exists()):
 79:                     found_reason = f"au moins un artefact pour '{video_file.name}'"
 80:                     is_ready = True
 81:                     break
 82: 
 83:         if is_ready:
 84:             logging.info(f"Projet '{project_dir.name}' est prêt ({found_reason}). Mode: {FINALIZE_MODE}")
 85:             projects.append(project_dir)
 86:         else:
 87:             logging.info(f"Projet '{project_dir.name}' ignoré (mode={FINALIZE_MODE}). Aucune vidéo/artefact requis trouvés.")
 88: 
 89:     return projects
 90: 
 91: 
 92: def _is_dir_writable(path: Path) -> bool:
 93:     """Teste la capacité d'écriture/suppression dans un répertoire cible.
 94: 
 95:     Plus fiable que os.access() sur des montages FUSE/NTFS.
 96:     """
 97:     try:
 98:         path.mkdir(parents=True, exist_ok=True)
 99:         with tempfile.NamedTemporaryFile(dir=str(path), delete=True) as tmp:
100:             tmp.write(b"writable-check")
101:             tmp.flush()
102:         return True
103:     except Exception:
104:         return False
105: 
106: 
107: def _select_output_dir(preferred: Path, base_dir: Path) -> Path:
108:     """Sélectionne une destination inscriptible, avec repli si nécessaire.
109: 
110:     - Utilise `preferred` si inscriptible.
111:     - Sinon, utilise `FALLBACK_OUTPUT_DIR` si défini et inscriptible.
112:     - Sinon, utilise `base_dir / "_finalized_output"`.
113:     """
114:     if _is_dir_writable(preferred):
115:         logging.info(f"Destination vérifiée: '{preferred}' est inscriptible")
116:         return preferred
117: 
118:     logging.warning(
119:         f"La destination préférée '{preferred}' n'est pas inscriptible (montage RO ? permissions ?). "
120:         "Activation d'une destination de repli."
121:     )
122: 
123:     env_fallback = os.environ.get("FALLBACK_OUTPUT_DIR")
124:     if env_fallback:
125:         fb = Path(env_fallback)
126:         if _is_dir_writable(fb):
127:             logging.warning(f"Bascule vers FALLBACK_OUTPUT_DIR: '{fb}'")
128:             return fb
129:         else:
130:             logging.warning(f"FALLBACK_OUTPUT_DIR défini mais non inscriptible: '{fb}'")
131: 
132:     local_fb = base_dir / "_finalized_output"
133:     local_fb.mkdir(parents=True, exist_ok=True)
134:     logging.warning(f"Bascule vers le répertoire de repli local: '{local_fb}'")
135:     return local_fb
136: 
137: 
138: def _safe_rmtree(path: Path) -> None:
139:     """Supprime un dossier en consignant proprement les erreurs de permissions."""
140:     def _onerror(func, p, exc_info):
141:         logging.error(f"Suppression échouée sur '{p}': {exc_info[1]}")
142:     shutil.rmtree(path, onerror=_onerror)
143: 
144: 
145: def _destination_supports_chmod(dest_dir: Path) -> bool:
146:     """Détecte si le FS destination supporte chmod (NTFS typiquement renvoie EPERM).
147:     
148:     Si on ne peut pas créer de fichier, la question du chmod est secondaire; laisser True.
149:     """
150:     try:
151:         dest_dir.mkdir(parents=True, exist_ok=True)
152:         with tempfile.NamedTemporaryFile(dir=str(dest_dir), delete=True) as tmp:
153:             try:
154:                 os.chmod(tmp.name, 0o664)
155:             except PermissionError:
156:                 return False
157:             except OSError as e:
158:                 err = getattr(e, 'errno', None)
159:                 if err in (errno.EPERM, errno.EACCES, getattr(errno, 'EOPNOTSUPP', None)):
160:                     return False
161:                 raise
162:     except Exception:
163:         return True
164: 
165: 
166: def _copy_project_tree(src: Path, dst: Path) -> None:
167:     """Copie le projet sans préserver les permissions sur FS type NTFS.
168: 
169:     Stratégie:
170:     - Si chmod supporté: utiliser shutil.copytree (comportement standard).
171:     - Sinon: tenter rsync --no-perms/owner/group; à défaut cp --no-preserve; à défaut copie Python manuelle.
172:     """
173:     dst.parent.mkdir(parents=True, exist_ok=True)
174:     supports_chmod = _destination_supports_chmod(dst)
175:     if supports_chmod:
176:         shutil.copytree(src, dst, dirs_exist_ok=True)
177:         return
178: 
179:     logging.info("Destination ne supporte pas chmod — copie sans préservation des permissions.")
180:     # 1) rsync si disponible
181:     try:
182:         subprocess.run([
183:             "rsync", "-a", "--no-perms", "--no-owner", "--no-group", "--no-times",
184:             f"{str(src)}/", f"{str(dst)}/"
185:         ], check=True)
186:         return
187:     except FileNotFoundError:
188:         logging.info("rsync non disponible, tentative via cp --no-preserve")
189:     except subprocess.CalledProcessError as e:
190:         logging.warning(f"rsync a échoué: {e}")
191: 
192:     # 2) cp --no-preserve si disponible (fusionner le contenu dans dst)
193:     try:
194:         dst.mkdir(parents=True, exist_ok=True)
195:         subprocess.run([
196:             "bash", "-lc",
197:             f"cp -r --no-preserve=mode,ownership '{str(src)}/.' '{str(dst)}/'"
198:         ], check=True)
199:         return
200:     except subprocess.CalledProcessError as e:
201:         logging.warning(f"cp --no-preserve a échoué: {e}")
202: 
203:     # 3) Fallback Python: os.walk et shutil.copyfile sans copystat
204:     for root, dirs, files in os.walk(src):
205:         rel = os.path.relpath(root, src)
206:         target_dir = dst / rel if rel != "." else dst
207:         target_dir.mkdir(parents=True, exist_ok=True)
208:         for d in dirs:
209:             (target_dir / d).mkdir(parents=True, exist_ok=True)
210:         for f in files:
211:             s = Path(root) / f
212:             t = target_dir / f
213:             shutil.copyfile(s, t)
214: 
215: 
216: def _compute_alternative_output_dir(existing_dst: Path) -> Path:
217:     """Calcule un répertoire de sortie alternatif si `existing_dst` ne peut pas être supprimé.
218: 
219:     Ex.: /mnt/cache/33 Camille -> /mnt/cache/33 Camille__finalized_YYYYmmdd_HHMMSS[_n]
220:     """
221:     base = existing_dst.parent
222:     name = existing_dst.name
223:     ts = datetime.now().strftime('%Y%m%d_%H%M%S')
224:     candidate = base / f"{name}__finalized_{ts}"
225:     i = 1
226:     while candidate.exists():
227:         candidate = base / f"{name}__finalized_{ts}_{i}"
228:         i += 1
229:     return candidate
230: 
231: 
232: def _normalize_project_docs_structure(dst_project_dir: Path) -> None:
233:     """Ensure destination project has a docs/ subfolder containing media and related files.
234: 
235:     If files are currently at the project root (common when previous steps didn't use a docs/
236:     directory), move recognized media and analysis files into docs/.
237:     """
238:     docs_dir = dst_project_dir / "docs"
239:     docs_dir.mkdir(parents=True, exist_ok=True)
240: 
241:     media_exts = {".mp4", ".mov", ".avi", ".mkv", ".webm", ".png", ".jpg", ".jpeg"}
242:     analysis_suffixes = {SCENES_SUFFIX, AUDIO_SUFFIX, TRACKING_SUFFIX}
243: 
244:     video_exts = {".mp4", ".mov", ".avi", ".mkv", ".webm"}
245:     video_stems: set[str] = set()
246:     try:
247:         for p in dst_project_dir.iterdir():
248:             if p.is_file() and p.suffix.lower() in video_exts:
249:                 video_stems.add(p.stem)
250:         if docs_dir.exists():
251:             for p in docs_dir.rglob("*"):
252:                 if p.is_file() and p.suffix.lower() in video_exts:
253:                     video_stems.add(p.stem)
254:     except Exception:
255:         pass
256: 
257:     for entry in dst_project_dir.iterdir():
258:         if entry.name == "docs":
259:             continue
260:         if entry.is_file():
261:             ext = entry.suffix.lower()
262:             stem = entry.stem
263:             move = False
264:             if ext in media_exts:
265:                 move = True
266:             else:
267:                 for suf in analysis_suffixes:
268:                     if entry.name.endswith(suf) or entry.name == f"{stem}.csv":
269:                         move = True
270:                         break
271:                 if not move and ext == ".json" and stem in video_stems:
272:                     move = True
273:             if move:
274:                 target = docs_dir / entry.name
275:                 try:
276:                     if target.exists():
277:                         target.unlink()
278:                     shutil.move(str(entry), str(target))
279:                 except Exception as e:
280:                     logging.warning(f"Impossible de déplacer '{entry}' vers '{target}': {e}")
281: 
282: 
283: def restore_archived_analysis(project_name: str, output_project_dir: Path) -> None:
284:     """Restore archived analysis artifacts into the destination project docs/ folder.
285: 
286:     For each detected video file in docs/, attempt to retrieve archived artifacts
287:     (scenes CSV, audio JSON, tracking JSON) using ResultsArchiver and copy them next to the video.
288:     """
289:     try:
290:         docs_dir = output_project_dir / "docs"
291:         docs_dir.mkdir(parents=True, exist_ok=True)
292: 
293:         video_exts = (".mp4", ".mov", ".avi", ".mkv", ".webm")
294:         videos = []
295:         if docs_dir.exists():
296:             videos.extend([p for p in docs_dir.rglob("*") if p.is_file() and p.suffix.lower() in video_exts])
297:         videos.extend([p for p in output_project_dir.iterdir() if p.is_file() and p.suffix.lower() in video_exts])
298: 
299:         for v in videos:
300:             target_dir = docs_dir if v.parent != docs_dir else v.parent
301:             stem = v.stem
302: 
303:             scenes_path = ResultsArchiver.find_analysis_file(project_name, v, SCENES_SUFFIX)
304:             if scenes_path:
305:                 dst = target_dir / f"{stem}{SCENES_SUFFIX}"
306:                 try:
307:                     shutil.copy2(scenes_path, dst)
308:                 except Exception as e:
309:                     logging.warning(f"Restauration scenes échouée {scenes_path} -> {dst}: {e}")
310: 
311:             audio_path = ResultsArchiver.find_analysis_file(project_name, v, AUDIO_SUFFIX)
312:             if audio_path:
313:                 dst = target_dir / f"{stem}{AUDIO_SUFFIX}"
314:                 try:
315:                     shutil.copy2(audio_path, dst)
316:                 except Exception as e:
317:                     logging.warning(f"Restauration audio échouée {audio_path} -> {dst}: {e}")
318: 
319:             tracking_path = ResultsArchiver.find_analysis_file(project_name, v, TRACKING_SUFFIX)
320:             if tracking_path:
321:                 dst = target_dir / f"{stem}{TRACKING_SUFFIX}"
322:                 try:
323:                     shutil.copy2(tracking_path, dst)
324:                 except Exception as e:
325:                     logging.warning(f"Restauration tracking échouée {tracking_path} -> {dst}: {e}")
326:     except Exception as e:
327:         logging.warning(f"Restauration des analyses archivées échouée (projet={project_name}): {e}")
328: 
329: def finalize_project(project_dir):
330:     """Copie un projet vers la destination finale et supprime la source."""
331:     try:
332:         project_name = project_dir.name
333:         logging.info(f"Finalisation du projet: {project_name}")
334:         print(f"Finalisation en cours pour '{project_name}'...")
335: 
336:         # 1) Archiver d'abord tous les artefacts d'analyse disponibles
337:         try:
338:             arch_summary = ResultsArchiver.archive_project_analysis(project_name)
339:             if arch_summary and not arch_summary.get("error"):
340:                 logging.info(
341:                     "Archivage des analyses terminé: %s",
342:                     json.dumps({k: arch_summary.get(k) for k in ("processed", "copied")}, ensure_ascii=False)
343:                 )
344:             else:
345:                 logging.warning(f"Archivage des analyses non effectué ou en erreur: {arch_summary}")
346:         except Exception as e:
347:             logging.warning(f"Erreur lors de l'archivage des analyses du projet '{project_name}': {e}")
348: 
349:         output_project_dir = OUTPUT_DIR / project_name
350: 
351:         if output_project_dir.exists():
352:             logging.warning(
353:                 f"Le dossier de destination '{output_project_dir}' existe déjà. Tentative de suppression avant copie."
354:             )
355:             _safe_rmtree(output_project_dir)
356:             if output_project_dir.exists():
357:                 alt_dir = _compute_alternative_output_dir(output_project_dir)
358:                 logging.warning(
359:                     f"Impossible de supprimer la destination existante. Bascule vers: '{alt_dir}'"
360:                 )
361:                 output_project_dir = alt_dir
362: 
363:         try:
364:             _copy_project_tree(project_dir, output_project_dir)
365:         except Exception as e:
366:             logging.error("Erreur lors de la copie du projet: %s", e, exc_info=True)
367:             raise
368:         logging.info(f"Projet '{project_name}' copié avec succès vers '{output_project_dir}'")
369: 
370:         _normalize_project_docs_structure(output_project_dir)
371: 
372:         if os.environ.get("RESTORE_ARCHIVES_TO_OUTPUT", "0") in ("1", "true", "True"):
373:             try:
374:                 restore_archived_analysis(project_name, output_project_dir)
375:             except Exception as e:
376:                 logging.warning(f"Restauration des analyses archivées échouée: {e}")
377: 
378:         # --- Suppression du dossier source (après archivage) ---
379:         try:
380:             project_dir.resolve().relative_to(config.ARCHIVES_DIR.resolve())
381:             logging.error(f"Refus de suppression: '{project_dir}' est sous ARCHIVES_DIR")
382:             return False
383:         except Exception:
384:             pass
385:         _safe_rmtree(project_dir)
386:         logging.info(f"Dossier source '{project_dir}' supprimé avec succès.")
387:         # --- FIN suppression ---
388: 
389:         # --- MODIFICATION: Suppression de la création du fichier metadata_final.json ---
390:         # --- FIN DE LA MODIFICATION ---
391: 
392:         logging.info(f"Finalisation terminée pour '{project_name}'")
393:         print(f"Finalisation terminée pour '{project_name}'")
394:         return True
395: 
396:     except Exception as e:
397:         logging.error(f"Erreur lors de la finalisation du projet {project_dir.name}: {e}", exc_info=True)
398:         return False
399: 
400: 
401: def main():
402:     """Fonction principale du script."""
403:     logging.info(f"--- Démarrage du script de Finalisation et Nettoyage ---")
404: 
405:     global OUTPUT_DIR
406:     try:
407:         OUTPUT_DIR = _select_output_dir(OUTPUT_DIR, BASE_DIR)
408:         logging.info(f"Le répertoire de destination est: {OUTPUT_DIR.resolve()}")
409:     except Exception as e:
410:         logging.critical(
411:             f"Impossible de préparer un répertoire de destination inscriptible. Erreur: {e}"
412:         )
413:         sys.exit(1)
414: 
415:     projects = find_projects_to_finalize()
416:     total_projects = len(projects)
417:     logging.info(f"{total_projects} projet(s) à finaliser")
418:     print(f"{total_projects} projet(s) à finaliser")
419: 
420:     if total_projects == 0:
421:         logging.info("Aucun projet à finaliser. Fin du script.")
422:         return
423: 
424:     successful_count = 0
425:     for project in projects:
426:         if finalize_project(project):
427:             successful_count += 1
428: 
429:     logging.info(f"--- Finalisation terminée ---")
430:     logging.info(f"Résumé: {successful_count}/{total_projects} projet(s) finalisé(s) et déplacé(s) avec succès.")
431: 
432:     if successful_count < total_projects:
433:         sys.exit(1)
434: 
435: 
436: if __name__ == "__main__":
437:     try:
438:         main()
439:     except Exception as e:
440:         logging.critical(f"Erreur critique non gérée dans le script de finalisation: {e}", exc_info=True)
441:         sys.exit(1)
```

## File: services/cache_service.py
```python
  1: """
  2: Cache Service
  3: Centralized caching service for improved performance.
  4: """
  5: 
  6: import logging
  7: import json
  8: import time
  9: import re
 10: from functools import lru_cache, wraps
 11: from typing import Dict, Any, Optional, Callable
 12: from pathlib import Path
 13: from flask_caching import Cache
 14: 
 15: from config.settings import config
 16: from services.workflow_state import get_workflow_state
 17: 
 18: logger = logging.getLogger(__name__)
 19: 
 20: _SAFE_STEP_KEY_PATTERN = re.compile(r'^[A-Za-z0-9_-]+$')
 21: 
 22: # Global cache instance (will be initialized by Flask app)
 23: cache_instance: Optional[Cache] = None
 24: 
 25: # Cache statistics
 26: cache_stats = {
 27:     "hits": 0,
 28:     "misses": 0,
 29:     "errors": 0,
 30:     "last_reset": time.time()
 31: }
 32: 
 33: 
 34: class CacheService:
 35:     """
 36:     Centralized caching service with performance tracking.
 37:     Provides intelligent caching for expensive operations.
 38:     """
 39:     
 40:     @staticmethod
 41:     def initialize(cache: Cache) -> None:
 42:         """
 43:         Initialize the cache service with Flask-Caching instance.
 44:         
 45:         Args:
 46:             cache: Flask-Caching instance
 47:         """
 48:         global cache_instance
 49:         cache_instance = cache
 50:         logger.info("Cache service initialized")
 51:     
 52:     @staticmethod
 53:     def get_cache_stats() -> Dict[str, Any]:
 54:         """
 55:         Get cache performance statistics.
 56:         
 57:         Returns:
 58:             Cache statistics dictionary
 59:         """
 60:         total_requests = cache_stats["hits"] + cache_stats["misses"]
 61:         hit_rate = (cache_stats["hits"] / total_requests * 100) if total_requests > 0 else 0
 62:         
 63:         return {
 64:             "hits": cache_stats["hits"],
 65:             "misses": cache_stats["misses"],
 66:             "errors": cache_stats["errors"],
 67:             "hit_rate_percent": round(hit_rate, 2),
 68:             "total_requests": total_requests,
 69:             "uptime_seconds": round(time.time() - cache_stats["last_reset"], 1)
 70:         }
 71:     
 72:     @staticmethod
 73:     def clear_cache() -> None:
 74:         """Clear all cached data."""
 75:         if cache_instance:
 76:             cache_instance.clear()
 77:             logger.info("Cache cleared")
 78:     
 79:     @staticmethod
 80:     def reset_stats() -> None:
 81:         """Reset cache statistics."""
 82:         global cache_stats
 83:         cache_stats = {
 84:             "hits": 0,
 85:             "misses": 0,
 86:             "errors": 0,
 87:             "last_reset": time.time()
 88:         }
 89:         logger.info("Cache statistics reset")
 90:     
 91:     @staticmethod
 92:     def cached_with_stats(timeout: int = 300, key_prefix: str = ""):
 93:         """
 94:         Decorator for caching with statistics tracking.
 95:         
 96:         Args:
 97:             timeout: Cache timeout in seconds
 98:             key_prefix: Prefix for cache keys
 99:             
100:         Returns:
101:             Decorator function
102:         """
103:         def decorator(func: Callable) -> Callable:
104:             @wraps(func)
105:             def wrapper(*args, **kwargs):
106:                 if not cache_instance:
107:                     cache_stats["errors"] += 1
108:                     return func(*args, **kwargs)
109:                 
110:                 # Generate cache key
111:                 cache_key = f"{key_prefix}:{func.__name__}:{hash(str(args) + str(kwargs))}"
112:                 
113:                 try:
114:                     # Try to get from cache
115:                     result = cache_instance.get(cache_key)
116:                     if result is not None:
117:                         cache_stats["hits"] += 1
118:                         return result
119:                     
120:                     # Cache miss - execute function
121:                     cache_stats["misses"] += 1
122:                     result = func(*args, **kwargs)
123:                     
124:                     # Store in cache
125:                     cache_instance.set(cache_key, result, timeout=timeout)
126:                     return result
127:                     
128:                 except Exception as e:
129:                     cache_stats["errors"] += 1
130:                     logger.error(f"Cache error for {func.__name__}: {e}")
131:                     return func(*args, **kwargs)
132:             
133:             return wrapper
134:         return decorator
135:     
136:     @staticmethod
137:     @lru_cache(maxsize=128)
138:     def get_video_metadata(video_path: str) -> Dict[str, Any]:
139:         """
140:         Get video metadata with caching to avoid repeated file reads.
141:         
142:         Args:
143:             video_path: Path to video file
144:             
145:         Returns:
146:             Video metadata dictionary
147:         """
148:         try:
149:             import cv2
150:             cap = cv2.VideoCapture(video_path)
151:             try:
152:                 if not cap.isOpened():
153:                     raise ValueError(f"Cannot open video: {video_path}")
154:                 
155:                 metadata = {
156:                     'frame_count': int(cap.get(cv2.CAP_PROP_FRAME_COUNT)),
157:                     'fps': cap.get(cv2.CAP_PROP_FPS),
158:                     'width': int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),
159:                     'height': int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)),
160:                     'duration_seconds': None
161:                 }
162:                 
163:                 # Calculate duration
164:                 if metadata['fps'] > 0 and metadata['frame_count'] > 0:
165:                     metadata['duration_seconds'] = metadata['frame_count'] / metadata['fps']
166:                 
167:                 return metadata
168:             finally:
169:                 cap.release()
170:                 
171:         except Exception as e:
172:             logger.error(f"Video metadata error for {video_path}: {e}")
173:             return {
174:                 'frame_count': 0,
175:                 'fps': 0,
176:                 'width': 0,
177:                 'height': 0,
178:                 'duration_seconds': None,
179:                 'error': str(e)
180:             }
181:     
182:     @staticmethod
183:     def get_cached_frontend_config() -> Dict[str, Any]:
184:         """
185:         Get cached frontend configuration.
186: 
187:         Returns:
188:             Frontend-safe configuration dictionary
189:         """
190:         try:
191:             # Try to use cache if available
192:             if cache_instance:
193:                 try:
194:                     cached_result = cache_instance.get("frontend_config")
195:                     if cached_result is not None:
196:                         cache_stats["hits"] += 1
197:                         logger.debug("Frontend config cache hit")
198:                         return cached_result
199:                 except Exception as cache_error:
200:                     logger.warning(f"Cache access failed, generating fresh config: {cache_error}")
201: 
202:             # Generate fresh config
203:             from config.workflow_commands import WorkflowCommandsConfig
204: 
205:             commands_config = WorkflowCommandsConfig().get_config()
206:             if not commands_config:
207:                 logger.error("WorkflowCommandsConfig is empty or not available")
208:                 cache_stats["errors"] += 1
209:                 return {}
210: 
211:             result: Dict[str, Any] = {}
212:             for step_key, step_data_orig in commands_config.items():
213:                 if not isinstance(step_key, str) or not _SAFE_STEP_KEY_PATTERN.match(step_key):
214:                     logger.error(
215:                         "Unsafe step_key detected in WorkflowCommandsConfig; skipping for frontend DOM safety: %r",
216:                         step_key,
217:                     )
218:                     continue
219:                 frontend_step_data: Dict[str, Any] = {}
220:                 for key, value in step_data_orig.items():
221:                     if key == "progress_patterns":
222:                         continue
223:                     if isinstance(value, Path):
224:                         frontend_step_data[key] = str(value)
225:                     elif key == "cmd" and isinstance(value, list):
226:                         frontend_step_data[key] = [str(item) for item in value]
227:                     elif key == "specific_logs" and isinstance(value, list):
228:                         safe_logs = []
229:                         for log_entry in value:
230:                             if not isinstance(log_entry, dict):
231:                                 continue
232:                             safe_entry = log_entry.copy()
233:                             if 'path' in safe_entry and isinstance(safe_entry['path'], Path):
234:                                 safe_entry['path'] = str(safe_entry['path'])
235:                             safe_logs.append(safe_entry)
236:                         frontend_step_data[key] = safe_logs
237:                     else:
238:                         frontend_step_data[key] = value
239:                 result[step_key] = frontend_step_data
240:             logger.debug(f"Generated fresh frontend config with {len(result)} steps")
241: 
242:             # Cache the result if cache is available
243:             if cache_instance:
244:                 try:
245:                     cache_instance.set("frontend_config", result, timeout=300)
246:                     cache_stats["misses"] += 1
247:                     logger.debug("Frontend config cached successfully")
248:                 except Exception as cache_error:
249:                     logger.warning(f"Failed to cache frontend config: {cache_error}")
250: 
251:             return result
252:         except Exception as e:
253:             logger.error(f"Frontend config cache error: {e}")
254:             cache_stats["errors"] += 1
255:             return {}
256:     
257:     @staticmethod
258:     def get_cached_log_content(step_key: str, log_index: int) -> Dict[str, Any]:
259:         """
260:         Get cached log file content.
261: 
262:         Args:
263:             step_key: Step identifier
264:             log_index: Log file index
265: 
266:         Returns:
267:             Log content dictionary
268: 
269:         Raises:
270:             ValueError: If step or log not found
271:         """
272:         try:
273:             # Try to use cache if available
274:             cache_key = f"log_content:{step_key}:{log_index}"
275:             if cache_instance:
276:                 cached_result = cache_instance.get(cache_key)
277:                 if cached_result is not None:
278:                     cache_stats["hits"] += 1
279:                     return cached_result
280: 
281:             from services.workflow_service import WorkflowService
282: 
283:             result = WorkflowService.get_step_log_file(step_key, log_index)
284: 
285:             # Cache the result if cache is available
286:             if cache_instance:
287:                 cache_instance.set(cache_key, result, timeout=60)
288:                 cache_stats["misses"] += 1
289: 
290:             return result
291:             
292:         except Exception as e:
293:             logger.error(f"Log content cache error for {step_key}/{log_index}: {e}")
294:             raise
295:     
296:     @staticmethod
297:     def get_cached_step_status(step_key: str) -> Dict[str, Any]:
298:         """
299:         Get cached step status to reduce computation.
300:         
301:         Args:
302:             step_key: Step identifier
303:             
304:         Returns:
305:             Step status dictionary
306:         """
307:         try:
308:             step_info = get_workflow_state().get_step_info(step_key)
309: 
310:             if not step_info:
311:                 raise ValueError(f"Step '{step_key}' not found")
312: 
313:             return step_info.copy()
314:             
315:         except Exception as e:
316:             logger.error(f"Step status cache error for {step_key}: {e}")
317:             raise
318:     
319:     @staticmethod
320:     def invalidate_step_cache(step_key: str) -> None:
321:         """
322:         Invalidate cache entries for a specific step.
323:         
324:         Args:
325:             step_key: Step identifier
326:         """
327:         if not cache_instance:
328:             return
329:         
330:         try:
331:             # Clear step-specific cache entries
332:             cache_keys_to_clear = [
333:                 f"step_status:{step_key}",
334:                 f"log_content:{step_key}"
335:             ]
336:             
337:             for key in cache_keys_to_clear:
338:                 cache_instance.delete(key)
339:             
340:             logger.debug(f"Invalidated cache for step {step_key}")
341:             
342:         except Exception as e:
343:             logger.error(f"Cache invalidation error for {step_key}: {e}")
344:     
345:     @staticmethod
346:     def warm_cache() -> None:
347:         """
348:         Pre-populate cache with commonly accessed data.
349:         """
350:         try:
351:             logger.info("Starting cache warm-up")
352:             
353:             # Warm up frontend config
354:             CacheService.get_cached_frontend_config()
355:             
356:             # Warm up step statuses
357:             workflow_state = get_workflow_state()
358:             step_keys = list((workflow_state.get_all_steps_info() or {}).keys())
359:             if not step_keys:
360:                 from config.workflow_commands import WorkflowCommandsConfig
361:                 step_keys = WorkflowCommandsConfig().get_all_step_keys()
362: 
363:             for step_key in step_keys:
364:                 try:
365:                     CacheService.get_cached_step_status(step_key)
366:                 except Exception:
367:                     pass  # Skip errors during warm-up
368:             
369:             logger.info("Cache warm-up completed")
370:             
371:         except Exception as e:
372:             logger.error(f"Cache warm-up error: {e}")
373:     
374:     @staticmethod
375:     def get_cache_size_estimate() -> Dict[str, Any]:
376:         """
377:         Get an estimate of cache memory usage.
378:         
379:         Returns:
380:             Cache size information
381:         """
382:         try:
383:             # This is a rough estimate since Flask-Caching doesn't provide direct size info
384:             stats = CacheService.get_cache_stats()
385:             
386:             # Estimate based on number of cached items
387:             estimated_items = stats["hits"] + stats["misses"]
388:             estimated_size_mb = estimated_items * 0.001  # Rough estimate: 1KB per item
389:             
390:             return {
391:                 "estimated_items": estimated_items,
392:                 "estimated_size_mb": round(estimated_size_mb, 2),
393:                 "cache_type": "SimpleCache",
394:                 "note": "Size estimates are approximate"
395:             }
396:             
397:         except Exception as e:
398:             logger.error(f"Cache size estimation error: {e}")
399:             return {
400:                 "estimated_items": 0,
401:                 "estimated_size_mb": 0,
402:                 "error": str(e)
403:             }
```

## File: services/csv_service.py
```python
  1: """
  2: CSV Service
  3: Centralized service for CSV monitoring functionality.
  4: """
  5: 
  6: import logging
  7: import os
  8: import json
  9: import re
 10: from datetime import datetime, timezone
 11: from typing import Dict, Any, Set, Optional, List
 12: import shutil
 13: import urllib.parse
 14: import html
 15: from config.settings import config
 16: from services.download_history_repository import download_history_repository
 17: 
 18: logger = logging.getLogger(__name__)
 19: 
 20: # Import WebhookService for external JSON source
 21: try:
 22:     from services.webhook_service import fetch_records as webhook_fetch_records, get_service_status as webhook_status
 23:     WEBHOOK_SERVICE_AVAILABLE = True
 24: except Exception:
 25:     WEBHOOK_SERVICE_AVAILABLE = False
 26:     def webhook_status():
 27:         return {"available": False, "error": "WebhookService not available"}
 28: 
 29: # Note: CSV monitoring state and downloads tracking are now managed in app_new.py
 30: # This service acts as an interface to those global variables
 31: 
 32: LEGACY_DOWNLOAD_HISTORY_FILE = config.BASE_PATH_SCRIPTS / "download_history.json"
 33: _SHARED_GROUP = config.DOWNLOAD_HISTORY_SHARED_GROUP
 34: _SHARED_FILE_MODE = 0o664
 35: 
 36: # Optional in-memory cache for last known good history set to avoid bursts on transient read errors
 37: _LAST_KNOWN_HISTORY_SET: Set[str] = set()
 38: 
 39: 
 40: def _is_dropbox_url(url: str) -> bool:
 41:     """Return True if the URL belongs to Dropbox domains.
 42: 
 43:     Args:
 44:         url: URL string
 45: 
 46:     Returns:
 47:         bool: True if hostname matches known Dropbox hostnames
 48:     """
 49:     try:
 50:         from urllib.parse import urlparse
 51:         parsed = urlparse((url or "").strip())
 52:         host = (parsed.hostname or "").lower()
 53:         # Known Dropbox hostnames
 54:         return host in {"dropbox.com", "www.dropbox.com", "dl.dropboxusercontent.com"}
 55:     except Exception:
 56:         return False
 57: 
 58: 
 59: def _is_dropbox_proxy_url(url: str) -> bool:
 60:     """Return True if the URL looks like a worker/R2 proxy for Dropbox downloads."""
 61:     try:
 62:         u = (url or "").strip().lower()
 63:         return "/dropbox/" in u and ("workers.dev" in u or "worker" in u)
 64:     except Exception:
 65:         return False
 66: 
 67: 
 68: def _looks_like_archive_download(url: Optional[str], original_filename: Optional[str]) -> bool:
 69:     """Heuristic to avoid auto-downloading non-archive Dropbox links (e.g. png previews)."""
 70:     u = (url or "").strip().lower()
 71:     fn = (original_filename or "").strip().lower()
 72:     if fn.endswith('.zip'):
 73:         return True
 74:     if '/scl/fo/' in u:
 75:         return True
 76:     if u.endswith('.zip') or '.zip?' in u:
 77:         return True
 78:     return False
 79: 
 80: 
 81: class CSVService:
 82:     """
 83:     Centralized service for CSV monitoring functionality.
 84:     Handles CSV file monitoring and download tracking.
 85:     """
 86:     
 87:     _initialized = False
 88: 
 89:     @staticmethod
 90:     def initialize() -> None:
 91:         """Initialize the CSV service."""
 92:         if not CSVService._initialized:
 93:             CSVService._initialized = True
 94:             logger.info("CSV service initialized")
 95:             try:
 96:                 download_history_repository.initialize()
 97:             except Exception as e:
 98:                 logger.warning(f"History DB initialization failed: {e}")
 99: 
100:             try:
101:                 CSVService._migrate_legacy_history_json_to_sqlite_if_needed()
102:             except Exception as e:
103:                 logger.warning(f"Legacy history migration skipped due to error: {e}")
104: 
105:     @staticmethod
106:     def _migrate_legacy_history_json_to_sqlite_if_needed() -> Dict[str, Any]:
107:         try:
108:             if not LEGACY_DOWNLOAD_HISTORY_FILE.exists():
109:                 return {"status": "noop", "reason": "legacy_missing"}
110:             if download_history_repository.count() > 0:
111:                 return {"status": "noop", "reason": "db_not_empty"}
112: 
113:             items = CSVService._load_structured_history()
114:             if not items:
115:                 return {"status": "noop", "reason": "legacy_empty"}
116: 
117:             def _normalize_ts_for_db(ts: str) -> str:
118:                 if not ts:
119:                     return ""
120:                 raw = str(ts).strip()
121:                 if not raw:
122:                     return ""
123:                 try:
124:                     dt = datetime.fromisoformat(raw.replace('Z', '+00:00'))
125:                     if dt.tzinfo:
126:                         return dt.astimezone().strftime('%Y-%m-%d %H:%M:%S')
127:                 except Exception:
128:                     pass
129:                 return raw
130: 
131:             entries = []
132:             for item in items:
133:                 url = item.get('url')
134:                 ts = item.get('timestamp') or ''
135:                 if not url:
136:                     continue
137:                 entries.append((url, _normalize_ts_for_db(ts)))
138: 
139:             download_history_repository.upsert_many(entries)
140:             return {"status": "success", "total": len(entries)}
141:         except Exception as e:
142:             logger.warning(f"Legacy history migration failed: {e}")
143:             return {"status": "error", "message": str(e)}
144:     
145:     @staticmethod
146:     def get_monitor_status() -> Dict[str, Any]:
147:         """
148:         Get monitoring service status (CSV or Airtable).
149: 
150:         Returns:
151:             Monitor status dictionary with both CSV and Airtable information
152:         """
153:         # Import WorkflowState singleton
154:         from services.workflow_state import get_workflow_state
155:         
156:         workflow_state = get_workflow_state()
157:         monitor_status = workflow_state.get_csv_monitor_status()
158: 
159:         # Webhook is the single data source
160:         status = {
161:             "csv_monitor": monitor_status,
162:             "data_source": "webhook",
163:             "monitor_interval": config.WEBHOOK_MONITOR_INTERVAL,
164:             "webhook": webhook_status()
165:         }
166: 
167:         return status
168:     
169: 
170:     
171:     @staticmethod
172:     def get_download_history() -> Set[str]:
173:         """
174:         Load download history from file as a set of URLs (backward compatible).
175: 
176:         The underlying file may be:
177:         - A list of strings: ["url1", "url2", ...]
178:         - A list of objects: [{"url": "...", "timestamp": "YYYY-MM-DD HH:MM:SS"}, ...]
179: 
180:         Returns:
181:             Set[str]: URLs present in history
182:         """
183:         try:
184:             CSVService.initialize()
185:             global _LAST_KNOWN_HISTORY_SET
186:             urls = download_history_repository.get_urls()
187:             _LAST_KNOWN_HISTORY_SET = set(urls)
188:             return set(urls)
189:         except Exception as e:
190:             logger.error(f"Error loading download history: {e}")
191:             return set(_LAST_KNOWN_HISTORY_SET)
192: 
193:     @staticmethod
194:     def _parse_history_to_set(data: Any) -> Set[str]:
195:         """Convert history JSON content to a set of URLs supporting both formats."""
196:         if not isinstance(data, list):
197:             return set()
198:         if data and isinstance(data[0], dict):
199:             return {CSVService._normalize_url(str(item.get('url'))) for item in data if isinstance(item, dict) and item.get('url')}
200:         return {CSVService._normalize_url(str(u)) for u in data if isinstance(u, str)}
201: 
202:     @staticmethod
203:     def _normalize_url(url: str) -> str:
204:         """Normalize URLs to prevent duplicates due to minor variations.
205: 
206:         Rules:
207:         - Trim whitespace
208:         - Unescape HTML entities (e.g., '&amp;' -> '&')
209:         - Recursively decode double-encoded sequences (e.g., amp%3Bdl=0 -> &dl=0)
210:         - Lowercase scheme and hostname
211:         - Remove default ports (80 for http, 443 for https)
212:         - Sort query parameters by key and value
213:         - Remove empty query parameters
214:         - For Dropbox links, ensure a single dl=1 param and collapse duplicates
215:         - Remove trailing slashes for non-root paths
216:         - Percent-decode path and then re-encode safely
217:         """
218:         if not url:
219:             return ""
220:         try:
221:             raw = url.strip()
222:             # First, unescape HTML entities that may come from CSV/HTML sources
223:             # Example: '...&amp;dl=0' -> '...&dl=0'
224:             try:
225:                 raw = html.unescape(raw)
226:             except Exception:
227:                 pass
228:             
229:             # Pre-process: recursively decode double-encoded sequences that may cause parsing issues
230:             # Example: "amp%3Bdl=0&dl=1" becomes "&dl=0&dl=1", which is then properly parsed
231:             prev_url = None
232:             max_decode_iterations = 3
233:             iteration = 0
234:             while prev_url != raw and iteration < max_decode_iterations:
235:                 prev_url = raw
236:                 # Decode common double-encoded patterns (HTML entity codes)
237:                 if 'amp%3B' in raw or 'amp%3b' in raw:
238:                     raw = raw.replace('amp%3B', '&').replace('amp%3b', '&')
239:                 # Detect and clean malformed ampersands that appear before valid params
240:                 # Pattern: "?amp%3Bdl=0&dl=1" -> "?dl=1"
241:                 if '%3B' in raw or '%3b' in raw:
242:                     # Try URL decode once to catch other double-encoded params
243:                     try:
244:                         decoded = urllib.parse.unquote(raw)
245:                         # Only accept if it still looks like a valid URL
246:                         if '://' in decoded:
247:                             raw = decoded
248:                     except Exception:
249:                         pass
250:                 iteration += 1
251:             
252:             parsed = urllib.parse.urlsplit(raw)
253:             scheme = (parsed.scheme or '').lower()
254:             netloc = (parsed.hostname or '').lower()
255:             port = parsed.port
256:             # Preserve username/password if any
257:             if parsed.username or parsed.password:
258:                 auth = ''
259:                 if parsed.username:
260:                     auth += urllib.parse.quote(parsed.username)
261:                 if parsed.password:
262:                     auth += f":{urllib.parse.quote(parsed.password)}"
263:                 netloc = f"{auth}@{netloc}" if auth else netloc
264: 
265:             # Drop default ports
266:             if port and not (
267:                 (scheme == 'http' and port == 80) or (scheme == 'https' and port == 443)
268:             ):
269:                 netloc = f"{netloc}:{port}"
270: 
271:             # Normalize path: decode, strip, remove duplicate slashes
272:             path = urllib.parse.unquote(parsed.path or '')
273:             if '//' in path:
274:                 while '//' in path:
275:                     path = path.replace('//', '/')
276:             # Remove trailing slash unless root
277:             if path.endswith('/') and path != '/':
278:                 path = path[:-1]
279:             # Re-encode path safely
280:             path = urllib.parse.quote(path, safe='/-._~')
281: 
282:             # Normalize query params
283:             q = urllib.parse.parse_qsl(parsed.query or '', keep_blank_values=False)
284:             # Special handling for Dropbox: force dl=1
285:             host_lower = (parsed.hostname or '').lower() if parsed.hostname else ''
286:             is_dropbox = host_lower.endswith('dropbox.com') or host_lower == 'dl.dropboxusercontent.com'
287:             filtered = []
288:             seen = set()
289:             for k, v in q:
290:                 key = k.strip()
291:                 val = v.strip()
292:                 # Skip completely empty params
293:                 if not key:
294:                     continue
295:                 # Skip params with empty values (except legitimate ones like rlkey)
296:                 if not val and key.lower() not in ('rlkey',):
297:                     # For Dropbox dl param, empty value is invalid
298:                     if is_dropbox and key.lower() == 'dl':
299:                         continue
300:                 # collapse duplicates
301:                 tup = (key, val)
302:                 if tup in seen:
303:                     continue
304:                 seen.add(tup)
305:                 # We will handle dl param below for Dropbox
306:                 if is_dropbox and key.lower() == 'dl':
307:                     continue
308:                 filtered.append((key, val))
309: 
310:             if is_dropbox:
311:                 filtered.append(('dl', '1'))
312: 
313:             # Sort for determinism
314:             filtered.sort(key=lambda kv: (kv[0].lower(), kv[1]))
315:             query = urllib.parse.urlencode(filtered, doseq=True)
316: 
317:             # Fragment is not relevant for downloads; drop it
318:             normalized = urllib.parse.urlunsplit((scheme, netloc, path, query, ''))
319:             return normalized
320:         except Exception:
321:             return url.strip()
322: 
323:     @staticmethod
324:     def _load_structured_history() -> List[Dict[str, str]]:
325:         """Load the history as a list of {url, timestamp} objects (best-effort).
326: 
327:         Returns:
328:             List[Dict[str, str]]: Each item contains 'url' and 'timestamp' (string)
329:         """
330:         try:
331:             if not LEGACY_DOWNLOAD_HISTORY_FILE.exists():
332:                 return []
333:             with open(LEGACY_DOWNLOAD_HISTORY_FILE, 'r', encoding='utf-8') as f:
334:                 data = json.load(f)
335:             if not isinstance(data, list):
336:                 return []
337:             # If already structured
338:             if data and isinstance(data[0], dict):
339:                 result: List[Dict[str, str]] = []
340:                 for item in data:
341:                     if isinstance(item, dict) and item.get('url'):
342:                         ts = str(item.get('timestamp')) if item.get('timestamp') else None
343:                         result.append({
344:                             'url': CSVService._normalize_url(str(item.get('url'))),
345:                             'timestamp': ts if ts else ''
346:                         })
347:                 return result
348:             # Flat list of URLs -> convert to objects with empty timestamp
349:             result = []
350:             for u in data:
351:                 if isinstance(u, str) and u:
352:                     result.append({'url': CSVService._normalize_url(u), 'timestamp': ''})
353:             return result
354:         except Exception as e:
355:             logger.error(f"Error loading structured download history: {e}")
356:             return []
357: 
358:     @staticmethod
359:     def _now_ts_str() -> str:
360:         """Return current timestamp as 'YYYY-MM-DD HH:MM:SS' in LOCAL time."""
361:         # Use system local timezone
362:         return datetime.now().astimezone().strftime('%Y-%m-%d %H:%M:%S')
363: 
364:     @staticmethod
365:     def migrate_history_to_local_time() -> Dict[str, Any]:
366:         """
367:         Convert existing download_history.json timestamps from UTC to local time.
368: 
369:         Assumes stored timestamps are in 'YYYY-MM-DD HH:MM:SS' and represent UTC.
370:         For each, convert to local timezone and re-save chronologically.
371: 
372:         Returns:
373:             dict with migration summary
374:         """
375:         try:
376:             items = CSVService._load_structured_history()
377:             if not items:
378:                 return {"status": "noop", "updated": 0}
379: 
380:             updated = 0
381:             converted: List[Dict[str, str]] = []
382:             for item in items:
383:                 url = item.get('url')
384:                 ts = item.get('timestamp') or ''
385:                 new_ts = ts
386:                 if ts:
387:                     try:
388:                         # Parse as UTC naive and set tzinfo=UTC, then convert to local
389:                         dt_utc = datetime.strptime(ts, '%Y-%m-%d %H:%M:%S').replace(tzinfo=timezone.utc)
390:                         dt_local = dt_utc.astimezone()  # system local tz
391:                         new_ts = dt_local.strftime('%Y-%m-%d %H:%M:%S')
392:                         if new_ts != ts:
393:                             updated += 1
394:                     except Exception:
395:                         # Best-effort: try ISO parse
396:                         try:
397:                             dt_any = datetime.fromisoformat(ts.replace('Z', '+00:00'))
398:                             dt_local = (dt_any if dt_any.tzinfo else dt_any.replace(tzinfo=timezone.utc)).astimezone()
399:                             new_ts = dt_local.strftime('%Y-%m-%d %H:%M:%S')
400:                             if new_ts != ts:
401:                                 updated += 1
402:                         except Exception:
403:                             # Leave as is if unparseable
404:                             new_ts = ts
405:                 converted.append({'url': url, 'timestamp': new_ts})
406: 
407:             # Sort chronologically, then by URL
408:             def _sort_key(item: Dict[str, str]):
409:                 ts = item.get('timestamp') or '9999-12-31 23:59:59'
410:                 return (ts, item.get('url') or '')
411: 
412:             converted.sort(key=_sort_key)
413: 
414:             with open(LEGACY_DOWNLOAD_HISTORY_FILE, 'w', encoding='utf-8') as f:
415:                 json.dump(converted, f, indent=2, ensure_ascii=False)
416: 
417:             return {"status": "success", "updated": updated, "total": len(converted)}
418:         except Exception as e:
419:             logger.error(f"Error migrating history to local time: {e}")
420:             return {"status": "error", "message": str(e)}
421:     
422:     @staticmethod
423:     def _ensure_shared_permissions(target):
424:         """
425:         Apply shared group/permission settings to the target file if configured.
426:         """
427:         if not target or not target.exists():
428:             return
429:         if _SHARED_GROUP:
430:             try:
431:                 shutil.chown(target, group=_SHARED_GROUP)
432:             except Exception as chown_err:
433:                 logger.warning(f"Unable to assign shared group '{_SHARED_GROUP}' to {target}: {chown_err}")
434:         try:
435:             os.chmod(target, _SHARED_FILE_MODE)
436:         except Exception as chmod_err:
437:             logger.warning(f"Unable to set shared permissions on {target}: {chmod_err}")
438: 
439:     @staticmethod
440:     def save_download_history(history_set: Set[str]) -> None:
441:         """
442:         Save download history to file, as a chronologically sorted list of
443:         objects {url, timestamp}. Backward compatible with callers passing a set.
444: 
445:         Rules:
446:         - Preserve existing timestamps for URLs already present.
447:         - Assign current timestamp to new URLs.
448:         - Only persist URLs present in history_set (reflects clear/overwrite).
449:         - Sort by timestamp ASC, then URL for deterministic order.
450:         """
451:         try:
452:             ts_by_url = download_history_repository.get_ts_by_url()
453:             normalized_set = {CSVService._normalize_url(u) for u in history_set}
454: 
455:             entries = []
456:             for url in sorted(normalized_set):
457:                 ts = ts_by_url.get(url) or CSVService._now_ts_str()
458:                 entries.append((url, ts))
459: 
460:             def _sort_key(item):
461:                 ts = item[1] or '9999-12-31 23:59:59'
462:                 return (ts, item[0] or '')
463: 
464:             entries.sort(key=_sort_key)
465:             download_history_repository.replace_all(entries)
466: 
467:             global _LAST_KNOWN_HISTORY_SET
468:             _LAST_KNOWN_HISTORY_SET = set(normalized_set)
469:         except Exception as e:
470:             logger.error(f"Error saving download history: {e}")
471:     
472:     @staticmethod
473:     def add_to_download_history(url: str) -> bool:
474:         """
475:         Add URL to download history.
476:         
477:         Args:
478:             url: URL to add to history
479:             
480:         Returns:
481:             True if successful, False otherwise
482:         """
483:         return CSVService.add_to_download_history_with_timestamp(url, None)
484: 
485:     @staticmethod
486:     def add_to_download_history_with_timestamp(url: str, timestamp_str: Optional[str]) -> bool:
487:         """
488:         Add URL to download history with a provided timestamp (if any).
489: 
490:         Preserves the earliest timestamp seen for a URL. If no valid timestamp is
491:         provided, uses current time.
492: 
493:         Args:
494:             url: URL to add
495:             timestamp_str: Optional timestamp string 'YYYY-MM-DD HH:MM:SS' or ISO; best-effort parsed
496: 
497:         Returns:
498:             True if persisted successfully, else False
499:         """
500:         try:
501:             ts_norm = None
502:             if timestamp_str:
503:                 try:
504:                     dt = datetime.fromisoformat(str(timestamp_str).replace('Z', '+00:00'))
505:                     ts_norm = dt.strftime('%Y-%m-%d %H:%M:%S')
506:                 except Exception:
507:                     try:
508:                         dt = datetime.strptime(str(timestamp_str), '%Y-%m-%d %H:%M:%S')
509:                         ts_norm = dt.strftime('%Y-%m-%d %H:%M:%S')
510:                     except Exception:
511:                         ts_norm = str(timestamp_str)
512:             if not ts_norm:
513:                 ts_norm = CSVService._now_ts_str()
514: 
515:             norm_url = CSVService._normalize_url(url)
516:             download_history_repository.upsert(norm_url, ts_norm)
517: 
518:             global _LAST_KNOWN_HISTORY_SET
519:             _LAST_KNOWN_HISTORY_SET.add(norm_url)
520:             return True
521:         except Exception as e:
522:             logger.error(f"Error adding to download history with timestamp: {e}")
523:             return False
524:     
525:     @staticmethod
526:     def is_url_downloaded(url: str) -> bool:
527:         """
528:         Check if URL has been downloaded before.
529:         
530:         Args:
531:             url: URL to check
532:             
533:         Returns:
534:             True if URL was previously downloaded
535:         """
536:         history = CSVService.get_download_history()
537:         return CSVService._normalize_url(url) in history
538:     
539:     @staticmethod
540:     def get_csv_downloads_status() -> Dict[str, Any]:
541:         """
542:         Get status of CSV downloads.
543: 
544:         Returns:
545:             CSV downloads status dictionary
546:         """
547:         # Import WorkflowState singleton
548:         from services.workflow_state import get_workflow_state
549:         
550:         workflow_state = get_workflow_state()
551:         active_downloads = workflow_state.get_active_csv_downloads_dict()
552:         recent_statuses = workflow_state.get_kept_csv_downloads_list()
553: 
554:         return {
555:             "active_downloads": active_downloads,
556:             "recent_statuses": recent_statuses,
557:             "total_active": len(active_downloads),
558:             "timestamp": datetime.now(timezone.utc).isoformat()
559:         }
560:     
561:     @staticmethod
562:     def add_csv_download(download_id: str, download_info: Dict[str, Any]) -> None:
563:         """
564:         Add a CSV download to tracking.
565: 
566:         Args:
567:             download_id: Unique download identifier
568:             download_info: Download information dictionary
569:         """
570:         # Import WorkflowState singleton
571:         from services.workflow_state import get_workflow_state
572:         
573:         workflow_state = get_workflow_state()
574:         download_data = {
575:             **download_info,
576:             "start_time": datetime.now(timezone.utc).isoformat()
577:         }
578:         workflow_state.add_csv_download(download_id, download_data)
579: 
580:         logger.info(f"CSV download added: {download_id}")
581:     
582:     @staticmethod
583:     def update_csv_download(download_id: str, status: str, **kwargs) -> None:
584:         """
585:         Update CSV download status.
586: 
587:         Args:
588:             download_id: Download identifier
589:             status: New status
590:             **kwargs: Additional fields to update
591:         """
592:         # Import WorkflowState singleton
593:         from services.workflow_state import get_workflow_state
594:         
595:         workflow_state = get_workflow_state()
596:         
597:         # Update download status with individual parameters
598:         progress = kwargs.get('progress')
599:         message = kwargs.get('message')
600:         filename = kwargs.get('filename')
601:         
602:         workflow_state.update_csv_download(
603:             download_id=download_id,
604:             status=status,
605:             progress=progress,
606:             message=message,
607:             filename=filename
608:         )
609: 
610:         # If download completed, move to history
611:         if status in ["completed", "failed", "cancelled", "unknown_error"]:
612:             workflow_state.move_csv_download_to_history(download_id)
613: 
614:         logger.debug(f"CSV download updated: {download_id} -> {status}")
615:     
616:     @staticmethod
617:     def remove_csv_download(download_id: str) -> None:
618:         """
619:         Remove CSV download from tracking.
620: 
621:         Args:
622:             download_id: Download identifier
623:         """
624:         # Import WorkflowState singleton
625:         from services.workflow_state import get_workflow_state
626:         
627:         workflow_state = get_workflow_state()
628:         workflow_state.remove_csv_download(download_id)
629: 
630:         logger.info(f"CSV download removed: {download_id}")
631:     
632:     @staticmethod
633:     def rewrite_dropbox_to_dl_host(url_str: str) -> str:
634:         """Rewrite a www.dropbox.com URL to dl.dropboxusercontent.com preserving path/query.
635: 
636:         Example:
637:         https://www.dropbox.com/scl/fo/<id>?rlkey=...&dl=1 ->
638:         https://dl.dropboxusercontent.com/scl/fo/<id>?rlkey=...&dl=1
639: 
640:         Args:
641:             url_str: Original Dropbox URL
642: 
643:         Returns:
644:             Rewritten URL string
645:         """
646:         try:
647:             parsed = urllib.parse.urlsplit(url_str)
648:             host = parsed.hostname or ""
649:             # Do NOT rewrite folder share links ('/scl/fo/') which are not served by dl.dropboxusercontent.com
650:             if (host.endswith("dropbox.com") and host != "dl.dropboxusercontent.com"
651:                     and not (parsed.path or '').lower().startswith('/scl/fo/')):
652:                 new_netloc = "dl.dropboxusercontent.com"
653:                 # preserve port if any (rare)
654:                 if parsed.port and parsed.port not in (80, 443):
655:                     new_netloc = f"{new_netloc}:{parsed.port}"
656:                 return urllib.parse.urlunsplit((parsed.scheme or "https", new_netloc, parsed.path, parsed.query, ""))
657:             return url_str
658:         except Exception:
659:             return url_str
660: 
661:     @staticmethod
662:     def _check_csv_for_downloads() -> None:
663:         """
664:         Check for new downloads using Webhook as the single data source.
665:         """
666:         try:
667:             # Import here to avoid circular imports
668:             from app_new import execute_csv_download_worker
669:             from services.workflow_state import get_workflow_state
670:             import threading
671:             import os
672: 
673:             # Fetch data from Webhook (single data source)
674:             if not WEBHOOK_SERVICE_AVAILABLE:
675:                 logger.error("WebhookService not available - monitoring disabled")
676:                 return
677: 
678:             logger.debug("Fetching data from Webhook")
679:             data_rows = webhook_fetch_records()
680:             source_type = "WEBHOOK"
681: 
682:             if data_rows is None:
683:                 logger.warning(f"Could not fetch data from {source_type}")
684:                 return
685: 
686:             # Get current download history (normalized URLs)
687:             download_history = CSVService.get_download_history()
688: 
689:             workflow_state = get_workflow_state()
690:             active_downloads = workflow_state.get_active_csv_downloads_dict()
691:             kept_downloads = workflow_state.get_kept_csv_downloads_list()
692: 
693:             tracked_urls: Set[str] = set()
694:             try:
695:                 candidates = list(active_downloads.values()) + list(kept_downloads)
696:                 for download in candidates:
697:                     if not isinstance(download, dict):
698:                         continue
699:                     status = str(download.get('status') or '').strip().lower()
700:                     if status in ('failed', 'cancelled', 'unknown_error'):
701:                         continue
702:                     raw_url = (download.get('original_url') or download.get('url') or '').strip()
703:                     if not raw_url:
704:                         continue
705:                     norm_existing = CSVService._normalize_url(raw_url)
706:                     if norm_existing:
707:                         tracked_urls.add(norm_existing)
708:             except Exception:
709:                 tracked_urls = set()
710: 
711:             def _is_url_already_tracked(norm_primary: Optional[str], norm_fallback: Optional[str]) -> bool:
712:                 if norm_primary and norm_primary in tracked_urls:
713:                     return True
714:                 if norm_fallback and norm_fallback in tracked_urls:
715:                     return True
716:                 return False
717: 
718:             total_rows = len(data_rows)
719:             skipped_missing_url = 0
720:             skipped_already_in_history = 0
721:             skipped_already_tracked = 0
722: 
723:             # Check for new URLs
724:             new_downloads = 0
725:             # Optional dry-run to avoid real downloads (useful for tests/CI):
726:             dry_run = os.environ.get('DRY_RUN_DOWNLOADS', 'false').lower() in ('true', '1')
727: 
728:             handled_in_this_pass: Set[str] = set()
729: 
730:             for row in data_rows:
731:                 url = row.get('url')
732:                 fallback_url = row.get('fallback_url')
733:                 original_filename = row.get('original_filename')
734:                 provider = row.get('provider')
735:                 timestamp_str = row.get('timestamp')
736: 
737:                 norm_url = CSVService._normalize_url(url) if url else None
738:                 norm_fallback_url = CSVService._normalize_url(fallback_url) if fallback_url else None
739: 
740:                 if norm_url and norm_url in handled_in_this_pass:
741:                     continue
742:                 if norm_fallback_url and norm_fallback_url in handled_in_this_pass:
743:                     continue
744: 
745:                 already_in_history = (
746:                     (norm_url and norm_url in download_history)
747:                     or (norm_fallback_url and norm_fallback_url in download_history)
748:                 )
749:                 if not norm_url:
750:                     skipped_missing_url += 1
751:                     continue
752:                 if _is_url_already_tracked(norm_url, norm_fallback_url):
753:                     skipped_already_tracked += 1
754:                     handled_in_this_pass.add(norm_url)
755:                     if norm_fallback_url:
756:                         handled_in_this_pass.add(norm_fallback_url)
757:                     continue
758:                 if already_in_history:
759:                     skipped_already_in_history += 1
760:                     # Common case: preferred URL removed from history, but fallback URL still present.
761:                     # This is expected behavior to prevent re-downloads across URL variants.
762:                     if (
763:                         norm_fallback_url
764:                         and norm_fallback_url in download_history
765:                         and norm_url not in download_history
766:                     ):
767:                         logger.debug(
768:                             f"{source_type} MONITOR: Skipping URL because fallback is already in download history "
769:                             f"(preferred={norm_url}, fallback={norm_fallback_url})"
770:                         )
771:                     continue
772: 
773:                 try:
774:                     parsed_primary = urllib.parse.urlsplit(url or '')
775:                     scheme_primary = (parsed_primary.scheme or '').lower()
776:                     if scheme_primary and scheme_primary not in ('http', 'https'):
777:                         logger.debug(
778:                             f"{source_type} MONITOR: Ignoring unsupported URL scheme '{scheme_primary}': {url}"
779:                         )
780:                         handled_in_this_pass.add(norm_url)
781:                         if norm_fallback_url:
782:                             handled_in_this_pass.add(norm_fallback_url)
783:                         continue
784:                 except Exception:
785:                     # If URL parsing fails, treat it as non-eligible.
786:                     logger.debug(
787:                         f"{source_type} MONITOR: Ignoring invalid URL (parse error): {url}"
788:                     )
789:                     handled_in_this_pass.add(norm_url)
790:                     if norm_fallback_url:
791:                         handled_in_this_pass.add(norm_fallback_url)
792:                     continue
793: 
794:                 url_lower = (url or '').lower()
795:                 provider_lower = str(provider or '').strip().lower()
796: 
797:                 # Determine URL type for UI hints / routing
798:                 url_type = (
799:                     str(row.get('url_type') or '').strip().lower()
800:                     or (
801:                         'fromsmash' if 'fromsmash.com' in url_lower else (
802:                             'swisstransfer' if 'swisstransfer.com' in url_lower else (
803:                                 'dropbox' if (_is_dropbox_url(url) or _is_dropbox_proxy_url(url) or provider_lower == 'dropbox') else 'external'
804:                             )
805:                         )
806:                     )
807:                 )
808: 
809:                 is_dropbox_like = (
810:                     url_type == 'dropbox'
811:                     or provider_lower == 'dropbox'
812:                     or _is_dropbox_url(url)
813:                     or _is_dropbox_proxy_url(url)
814:                 )
815: 
816:                 # Auto-download is intentionally restricted:
817:                 # - Prevents large backlog downloads from legacy entries.
818:                 # - Focuses on the new webhook schema (original_filename / fallback_url) and R2 proxy URLs.
819:                 has_new_schema_hints = bool(
820:                     (original_filename and str(original_filename).strip())
821:                     or (fallback_url and str(fallback_url).strip())
822:                     or _is_dropbox_proxy_url(url)
823:                 )
824:                 auto_download_allowed = (
825:                     is_dropbox_like
826:                     and _looks_like_archive_download(url, original_filename)
827:                     and has_new_schema_hints
828:                 )
829: 
830:                 if auto_download_allowed:
831:                     logger.info(
832:                         f"{source_type} MONITOR: New eligible URL detected: {url} (timestamp: {timestamp_str}) [type={url_type}]"
833:                     )
834:                     # Dropbox-like: proceed with auto-download (unless dry-run)
835:                     if dry_run:
836:                         logger.info(
837:                             f"[DRY RUN] Would start Dropbox download for URL: {url} (timestamp: {timestamp_str})"
838:                         )
839:                         CSVService.add_to_download_history_with_timestamp(norm_url, timestamp_str)
840:                         download_history.add(norm_url)
841:                         handled_in_this_pass.add(norm_url)
842:                         if norm_fallback_url:
843:                             CSVService.add_to_download_history_with_timestamp(norm_fallback_url, timestamp_str)
844:                             download_history.add(norm_fallback_url)
845:                             handled_in_this_pass.add(norm_fallback_url)
846:                         new_downloads += 1
847:                     else:
848:                         download_thread = threading.Thread(
849:                             target=execute_csv_download_worker,
850:                             args=(url, timestamp_str, fallback_url, original_filename),
851:                             name=f"Download-{str(timestamp_str).replace('/', '').replace(' ', '_').replace(':', '')}"
852:                         )
853:                         download_thread.daemon = True
854:                         download_thread.start()
855:                         handled_in_this_pass.add(norm_url)
856:                         if norm_fallback_url:
857:                             handled_in_this_pass.add(norm_fallback_url)
858:                         new_downloads += 1
859:                 else:
860:                     # Non-eligible link: ignore it (no UI entry, no history write) to keep auto-download Dropbox-only.
861:                     logger.debug(
862:                         f"{source_type} MONITOR: Ignoring non-eligible URL (auto-download disabled): {url} "
863:                         f"(timestamp: {timestamp_str}) [type={url_type}]"
864:                     )
865:                     handled_in_this_pass.add(norm_url)
866:                     if norm_fallback_url:
867:                         handled_in_this_pass.add(norm_fallback_url)
868: 
869:             if new_downloads > 0:
870:                 logger.info(f"{source_type} MONITOR: {new_downloads} new download(s) started")
871:             if new_downloads == 0:
872:                 logger.debug(
873:                     f"{source_type} MONITOR: No new items (rows={total_rows}, "
874:                     f"skipped_in_history={skipped_already_in_history}, skipped_tracked={skipped_already_tracked}, "
875:                     f"skipped_missing_url={skipped_missing_url})"
876:                 )
877: 
878:         except Exception as e:
879:             logger.error(f"Error checking for downloads: {e}", exc_info=True)
880: 
881:     @staticmethod
882:     def _normalize_and_deduplicate_history() -> None:
883:         """Normalize and deduplicate the persisted history file.
884: 
885:         Preserves earliest timestamp per logical URL.
886:         """
887:         try:
888:             existing = CSVService._load_structured_history()
889:             if not existing:
890:                 return
891:             # Build minimal map url->timestamp keeping earliest timestamp (lexicographically smaller)
892:             ts_by_url: Dict[str, str] = {}
893:             for item in existing:
894:                 url = CSVService._normalize_url(item.get('url') or '')
895:                 ts = item.get('timestamp') or ''
896:                 if not url:
897:                     continue
898:                 if url in ts_by_url:
899:                     prev = ts_by_url[url]
900:                     ts_by_url[url] = min(prev or ts, ts or prev)
901:                 else:
902:                     ts_by_url[url] = ts
903:             CSVService.save_download_history(set(ts_by_url.keys()))
904:         except Exception as e:
905:             logger.warning(f"Failed to normalize/deduplicate history: {e}")
906: 
907:     @staticmethod
908:     def clear_download_history() -> Dict[str, str]:
909:         """
910:         Clear download history.
911:         
912:         Returns:
913:             Result dictionary with status and message
914:         """
915:         try:
916:             CSVService.initialize()
917:             download_history_repository.delete_all()
918:             global _LAST_KNOWN_HISTORY_SET
919:             _LAST_KNOWN_HISTORY_SET = set()
920:             return {
921:                 "status": "success",
922:                 "message": "Download history cleared"
923:             }
924:         except Exception as e:
925:             logger.error(f"Error clearing download history: {e}")
926:             return {
927:                 "status": "error",
928:                 "message": f"Failed to clear download history: {str(e)}"
929:             }
930:     
931:     @staticmethod
932:     def get_statistics() -> Dict[str, Any]:
933:         """
934:         Get CSV service statistics.
935:         
936:         Returns:
937:             Statistics dictionary
938:         """
939:         history = CSVService.get_download_history()
940:         downloads_status = CSVService.get_csv_downloads_status()
941:         monitor_status = CSVService.get_monitor_status()
942: 
943:         return {
944:             "download_history_count": len(history),
945:             "active_downloads": downloads_status["total_active"],
946:             "monitor_status": monitor_status["csv_monitor"]["status"],
947:             "timestamp": datetime.now(timezone.utc).isoformat()
948:         }
```

## File: services/filesystem_service.py
```python
  1: #!/usr/bin/env python3
  2: # -*- coding: utf-8 -*-
  3: """
  4: FilesystemService
  5: 
  6: Service layer responsible for safe filesystem-related operations required by the
  7: frontend (searching cache folders and opening them locally on the host).
  8: 
  9: All business logic is centralized here per project standards.
 10: """
 11: from __future__ import annotations
 12: 
 13: import logging
 14: import os
 15: import re
 16: import shlex
 17: import subprocess
 18: from dataclasses import dataclass
 19: from pathlib import Path
 20: from typing import List, Optional, Tuple, Dict
 21: from datetime import datetime, date
 22: 
 23: from config.settings import config
 24: 
 25: logger = logging.getLogger(__name__)
 26: 
 27: CACHE_ROOT = Path(config.CACHE_ROOT_DIR)
 28: 
 29: 
 30: @dataclass
 31: class CacheSearchResult:
 32:     """Result for a cache folder search.
 33: 
 34:     Attributes:
 35:         number: The numeric identifier searched (as string to preserve formatting)
 36:         matches: List of absolute paths to matching folders
 37:         best_match: The best match path if determinable, else None
 38:     """
 39:     number: str
 40:     matches: List[str]
 41:     best_match: Optional[str]
 42: 
 43: 
 44: class FilesystemService:
 45:     """Service providing safe filesystem utilities for the app.
 46: 
 47:     This class intentionally restricts operations to known safe directories and
 48:     validates inputs to prevent path traversal or unintended access.
 49:     """
 50: 
 51:     @staticmethod
 52:     def _ensure_cache_root() -> None:
 53:         """Ensure the cache root exists, log a warning if not."""
 54:         if not CACHE_ROOT.exists():
 55:             logger.warning("CACHE_ROOT does not exist: %s", CACHE_ROOT)
 56: 
 57:     @staticmethod
 58:     def find_cache_folder_by_number(number: str) -> CacheSearchResult:
 59:         """Search for folders inside /mnt/cache that start with the given number.
 60: 
 61:         The expected folder format is for example: "115 Camille". We will match
 62:         case-insensitively any directory whose basename starts with "{number}" and
 63:         is followed by optional separators/spaces and text.
 64: 
 65:         Args:
 66:             number: Numeric string provided by the user (e.g., "115"). Non-digits are ignored.
 67: 
 68:         Returns:
 69:             CacheSearchResult with all matches and the best_match if exactly one clear match exists.
 70:         """
 71:         FilesystemService._ensure_cache_root()
 72: 
 73:         sanitized = re.sub(r"\D+", "", number or "")
 74:         if not sanitized:
 75:             logger.debug("Invalid number provided for cache search: %r", number)
 76:             return CacheSearchResult(number=number or "", matches=[], best_match=None)
 77: 
 78:         if not CACHE_ROOT.exists() or not CACHE_ROOT.is_dir():
 79:             return CacheSearchResult(number=sanitized, matches=[], best_match=None)
 80: 
 81:         regex = re.compile(rf"^{re.escape(sanitized)}[\s_-]?.*", re.IGNORECASE)
 82: 
 83:         matches: List[str] = []
 84:         try:
 85:             for entry in CACHE_ROOT.iterdir():
 86:                 try:
 87:                     if entry.is_dir() and regex.match(entry.name):
 88:                         matches.append(str(entry.resolve()))
 89:                 except PermissionError:
 90:                     continue
 91:         except Exception as e:
 92:             logger.error("Error while scanning cache root: %s", e)
 93:             return CacheSearchResult(number=sanitized, matches=[], best_match=None)
 94: 
 95:         best: Optional[str] = None
 96:         if len(matches) == 1:
 97:             best = matches[0]
 98:         else:
 99:             exact_regex = re.compile(rf"^{re.escape(sanitized)}\b.*", re.IGNORECASE)
100:             exact_matches = [m for m in matches if exact_regex.search(Path(m).name)]
101:             if len(exact_matches) >= 1:
102:                 exact_matches.sort(key=lambda p: (len(Path(p).name), Path(p).name.lower()))
103:                 best = exact_matches[0]
104: 
105:         return CacheSearchResult(number=sanitized, matches=matches, best_match=best)
106: 
107:     @staticmethod
108:     def open_path_in_explorer(abs_path: str, select_parent: bool = False) -> Tuple[bool, str]:
109:         """Open a given absolute path in the system's file explorer (server-side).
110: 
111:         IMPORTANT: This runs on the server where Flask is hosted. It assumes a desktop
112:         environment is available (e.g., the app is used locally). On headless servers,
113:         this will likely fail; we handle and return a clear error message.
114: 
115:         Args:
116:             abs_path: Absolute path to open.
117:             select_parent: If True, open the parent directory and try to preselect the target
118:                 folder in the file manager when supported (nautilus/dolphin/thunar/nemo). If not
119:                 supported, fall back to opening the parent directory.
120: 
121:         Returns:
122:             (success, message) tuple.
123:         """
124:         if config.DISABLE_EXPLORER_OPEN:
125:             logger.info("Explorer opening is disabled by configuration")
126:             return False, "Ouverture explorateur désactivée par configuration."
127:         if not config.DEBUG and not config.ENABLE_EXPLORER_OPEN:
128:             logger.info("Explorer opening is disabled in production/headless mode")
129:             return False, "Ouverture explorateur désactivée en production/headless."
130: 
131:         is_headless = not (os.environ.get("DISPLAY") or os.environ.get("WAYLAND_DISPLAY"))
132:         if is_headless and not config.ENABLE_EXPLORER_OPEN:
133:             logger.info("Explorer opening is disabled in headless mode")
134:             return False, "Ouverture explorateur désactivée en mode headless."
135:         try:
136:             path = Path(abs_path).resolve()
137:         except Exception:
138:             return False, "Chemin invalide."
139: 
140:         try:
141:             path.relative_to(CACHE_ROOT)
142:         except ValueError:
143:             return False, f"Chemin en dehors de {str(CACHE_ROOT)} non autorisé."
144: 
145:         if not path.exists():
146:             return False, "Le dossier n'existe pas."
147:         if not path.is_dir():
148:             return False, "Le chemin n'est pas un dossier."
149: 
150:         candidates: List[List[str]] = []
151: 
152:         if select_parent:
153:             parent = path.parent if path.parent.exists() else CACHE_ROOT
154:             candidates.extend([
155:                 ["nautilus", "--no-desktop", "--browser", "--select", str(path)],
156:                 ["nemo", "--no-desktop", "--browser", "--select", str(path)],
157:                 ["dolphin", "--select", str(path)],
158:                 ["thunar", "--select", str(path)],
159:                 ["xdg-open", str(parent)],
160:                 ["gio", "open", str(parent)],
161:                 ["nautilus", str(parent)],
162:                 ["nemo", str(parent)],
163:                 ["thunar", str(parent)],
164:                 ["dolphin", str(parent)],
165:             ])
166:         else:
167:             candidates.extend([
168:                 ["xdg-open", str(path)],
169:                 ["gio", "open", str(path)],
170:                 ["nautilus", str(path)],
171:                 ["nemo", str(path)],
172:                 ["thunar", str(path)],
173:                 ["dolphin", str(path)],
174:             ])
175: 
176:         last_error: Optional[str] = None
177:         for cmd in candidates:
178:             try:
179:                 subprocess.Popen(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
180:                 logger.info(
181:                     "Opened path in explorer using %s (select_parent=%s): %s",
182:                     cmd[0], select_parent, str(path)
183:                 )
184:                 return True, "Dossier ouvert dans l'explorateur."
185:             except FileNotFoundError:
186:                 last_error = f"Commande introuvable: {cmd[0]}"
187:                 continue
188:             except Exception as e:
189:                 last_error = str(e)
190:                 continue
191: 
192:         return False, last_error or "Impossible d'ouvrir l'explorateur sur ce système."
193: 
194:     @staticmethod
195:     def list_today_cache_folders() -> List[Dict[str, Optional[str]]]:
196:         """List folders created/modified today under CACHE_ROOT with extracted numeric prefix.
197: 
198:         We use the folder's modification time (mtime) to approximate creation date across platforms,
199:         and filter only directories whose mtime date equals today's local date. For each folder, we
200:         extract a leading numeric sequence if present (e.g., "115 Camille" -> "115").
201: 
202:         Returns:
203:             A list of dicts with keys: {"path": str, "name": str, "number": Optional[str], "mtime": str}
204:             where mtime is ISO 8601 string for display/logging purposes.
205:         """
206:         FilesystemService._ensure_cache_root()
207: 
208:         results: List[Dict[str, Optional[str]]] = []
209:         if not CACHE_ROOT.exists() or not CACHE_ROOT.is_dir():
210:             return results
211: 
212:         today = date.today()
213:         number_regex = re.compile(r"^(\d+)")
214: 
215:         try:
216:             for entry in CACHE_ROOT.iterdir():
217:                 try:
218:                     if not entry.is_dir():
219:                         continue
220:                     stat = entry.stat()
221:                     mtime_dt = datetime.fromtimestamp(stat.st_mtime)
222:                     if mtime_dt.date() != today:
223:                         continue
224:                     name = entry.name
225:                     m = number_regex.match(name)
226:                     number = m.group(1) if m else None
227:                     if number:
228:                         results.append({
229:                             "path": str(entry.resolve()),
230:                             "name": name,
231:                             "number": number,
232:                             "mtime": mtime_dt.isoformat()
233:                         })
234:                 except PermissionError:
235:                     continue
236:                 except Exception as e:
237:                     logger.warning("Skipping entry due to error: %s", e)
238:                     continue
239:         except Exception as e:
240:             logger.error("Error while listing today's cache folders: %s", e)
241:             return []
242: 
243:         results.sort(key=lambda x: x.get("mtime", ""), reverse=True)
244:         return results
245: 
246:     @staticmethod
247:     def sanitize_filename(filename_str: Optional[str], max_length: int = 230) -> str:
248:         """Sanitize filename for safe filesystem use.
249: 
250:         Args:
251:             filename_str: Original filename string (can be None)
252:             max_length: Maximum filename length (default: 230)
253: 
254:         Returns:
255:             Sanitized filename string
256:         """
257:         if filename_str is None:
258:             logger.warning("sanitize_filename: filename_str was None, using default 'fichier_nom_absent'.")
259:             filename_str = "fichier_nom_absent"
260:         
261:         s = str(filename_str).strip().replace(' ', '_')
262:         s = re.sub(r'(?u)[^-\w.]', '', s)
263:         
264:         if not s:
265:             s = "fichier_sans_nom_valide"
266:             logger.warning(f"sanitize_filename: Sanitized filename was empty for input '{filename_str}', using '{s}'.")
267:         
268:         if len(s) > max_length:
269:             original_full_name_for_log = s
270:             name_part, ext_part = os.path.splitext(s)
271:             max_name_len = max_length - len(ext_part) - (1 if ext_part else 0)
272:             if max_name_len < 1:
273:                 s = s[:max_length]
274:             else:
275:                 s = name_part[:max_name_len] + ext_part
276:             logger.info(f"sanitize_filename: Filename '{original_full_name_for_log}' truncated to '{s}' (max_length: {max_length}).")
277:         
278:         return s
279: 
280:     @staticmethod
281:     def format_bytes_human(n_bytes: int) -> str:
282:         """Return a human readable size string for bytes.
283: 
284:         Args:
285:             n_bytes: Number of bytes
286: 
287:         Returns:
288:             Formatted size string (e.g., '213.0KB', '151.5MB')
289:         """
290:         try:
291:             if n_bytes is None or n_bytes < 0:
292:                 return "0B"
293:             if n_bytes < 1024:
294:                 return f"{n_bytes}B"
295:             kb = n_bytes / 1024.0
296:             if kb < 1024.0:
297:                 return f"{kb:.1f}KB"
298:             mb = kb / 1024.0
299:             return f"{mb:.1f}MB"
300:         except Exception:
301:             return f"{n_bytes}B"
302: 
303:     @staticmethod
304:     def find_videos_for_tracking(base_path: Path, keyword: str = None, subdir: str = None) -> List[str]:
305:         """Find video files that don't have corresponding JSON tracking results.
306: 
307:         Robust scan under 'projets_extraits/' for common video extensions.
308:         A video is considered already processed ONLY if an exact sibling JSON
309:         with the same stem exists (e.g., 'video.mp4' -> 'video.json').
310:         Files like '*_audio.json' are ignored for this check.
311: 
312:         Args:
313:             base_path: Base path to search under (typically BASE_PATH_SCRIPTS / 'projets_extraits')
314:             keyword: Optional keyword filter (informational, not enforced)
315:             subdir: Optional subdirectory filter (informational, not enforced)
316: 
317:         Returns:
318:             List of absolute paths to video files needing tracking
319:         """
320:         search_base = Path(base_path) / "projets_extraits" if "projets_extraits" not in str(base_path) else Path(base_path)
321:         logger.info(
322:             f"Searching for videos for tracking in {search_base} (filter info: keyword='{keyword}', subdir='{subdir}')"
323:         )
324:         
325:         videos_to_process = []
326:         video_extensions = (".mp4", ".avi", ".mov", ".mkv", ".webm")
327: 
328:         found_videos = 0
329:         already_processed = 0
330:         
331:         try:
332:             for video_file in search_base.rglob("*"):
333:                 if not video_file.is_file():
334:                     continue
335:                 if video_file.suffix.lower() not in video_extensions:
336:                     continue
337:                 
338:                 found_videos += 1
339:                 json_file = video_file.with_suffix('.json')
340:                 
341:                 if json_file.exists():
342:                     already_processed += 1
343:                     continue
344:                 
345:                 videos_to_process.append(str(video_file.resolve()))
346:                 logger.debug(f"Video to process found: {video_file}")
347: 
348:             logger.info(
349:                 f"Videos detected: {found_videos}, already processed (exact JSON sibling): {already_processed}, to process: {len(videos_to_process)}"
350:             )
351:             
352:             if len(videos_to_process) == 0 and found_videos > 0 and already_processed == 0:
353:                 logger.warning(
354:                     "No <stem>.json found but videos exist. Check write permissions and Step5 output paths."
355:                 )
356:         except Exception as e:
357:             logger.error(f"Error scanning for videos: {e}")
358:         
359:         return videos_to_process
```

## File: services/workflow_state.py
```python
  1: #!/usr/bin/env python3
  2: # -*- coding: utf-8 -*-
  3: """
  4: Workflow State Management Service
  5: 
  6: Centralized state management for workflow execution with thread-safety.
  7: Replaces global variables with a clean, testable interface.
  8: """
  9: 
 10: import threading
 11: from collections import deque
 12: from datetime import datetime, timezone
 13: from typing import Dict, Any, Optional, List, Set
 14: from pathlib import Path
 15: import logging
 16: 
 17: logger = logging.getLogger(__name__)
 18: 
 19: 
 20: class WorkflowState:
 21:     """Centralized workflow state management with thread-safety."""
 22:     
 23:     def __init__(self):
 24:         self._lock = threading.RLock()
 25: 
 26:         self._process_info: Dict[str, Dict[str, Any]] = {}
 27:         self._sequence_running = False
 28:         self._sequence_outcome = {
 29:             "status": "never_run",
 30:             "type": None,
 31:             "message": None,
 32:             "timestamp": None
 33:         }
 34:         self._active_csv_downloads: Dict[str, Dict[str, Any]] = {}
 35:         self._kept_csv_downloads = deque(maxlen=20)
 36:         self._csv_monitor_status = {
 37:             "status": "stopped",
 38:             "last_check": None,
 39:             "error": None
 40:         }
 41:         
 42:         logger.info("WorkflowState initialized")
 43:     
 44:     def initialize_step(self, step_key: str) -> None:
 45:         with self._lock:
 46:             self._process_info[step_key] = {
 47:                 'status': 'idle',
 48:                 'log': deque(maxlen=300),
 49:                 'return_code': None,
 50:                 'process': None,
 51:                 'progress_current': 0,
 52:                 'progress_total': 0,
 53:                 'progress_text': '',
 54:                 'start_time_epoch': None,
 55:                 'duration_str': None
 56:             }
 57:             logger.debug(f"Initialized state for {step_key}")
 58:     
 59:     def initialize_all_steps(self, step_keys: List[str]) -> None:
 60:         with self._lock:
 61:             for step_key in step_keys:
 62:                 self.initialize_step(step_key)
 63:             logger.info(f"Initialized state for {len(step_keys)} steps")
 64:     
 65:     def get_step_info(self, step_key: str) -> Dict[str, Any]:
 66:         with self._lock:
 67:             if step_key not in self._process_info:
 68:                 logger.warning(f"Step {step_key} not initialized, returning empty dict")
 69:                 return {}
 70:             
 71:             # Deep copy to prevent external mutations
 72:             info = self._process_info[step_key].copy()
 73:             # Convert deque to list for JSON serialization
 74:             if 'log' in info and isinstance(info['log'], deque):
 75:                 info['log'] = list(info['log'])
 76:             return info
 77:     
 78:     def get_all_steps_info(self) -> Dict[str, Dict[str, Any]]:
 79:         with self._lock:
 80:             return {
 81:                 step_key: self.get_step_info(step_key)
 82:                 for step_key in self._process_info.keys()
 83:             }
 84:     
 85:     def update_step_status(self, step_key: str, status: str) -> None:
 86:         with self._lock:
 87:             if step_key in self._process_info:
 88:                 self._process_info[step_key]['status'] = status
 89:                 logger.debug(f"{step_key} status updated to: {status}")
 90:     
 91:     def update_step_progress(self, step_key: str, current: int, total: int, text: str = '') -> None:
 92:         with self._lock:
 93:             if step_key in self._process_info:
 94:                 info = self._process_info[step_key]
 95:                 info['progress_current'] = current
 96:                 info['progress_total'] = total
 97:                 info['progress_text'] = text
 98:     
 99:     def append_step_log(self, step_key: str, message: str) -> None:
100:         with self._lock:
101:             if step_key in self._process_info:
102:                 self._process_info[step_key]['log'].append(message)
103:     
104:     def clear_step_log(self, step_key: str) -> None:
105:         with self._lock:
106:             if step_key in self._process_info:
107:                 self._process_info[step_key]['log'].clear()
108:     
109:     def update_step_info(self, step_key: str, **kwargs) -> None:
110:         with self._lock:
111:             if step_key in self._process_info:
112:                 self._process_info[step_key].update(kwargs)
113:                 logger.debug(f"{step_key} updated with: {list(kwargs.keys())}")
114:     
115:     def get_step_status(self, step_key: str) -> Optional[str]:
116:         with self._lock:
117:             if step_key in self._process_info:
118:                 return self._process_info[step_key]['status']
119:             return None
120:     
121:     def is_step_running(self, step_key: str) -> bool:
122:         status = self.get_step_status(step_key)
123:         return status in ['running', 'starting', 'initiated']
124:     
125:     def is_any_step_running(self) -> bool:
126:         with self._lock:
127:             return any(
128:                 info['status'] in ['running', 'starting', 'initiated']
129:                 for info in self._process_info.values()
130:             )
131:     
132:     def set_step_process(self, step_key: str, process: Any) -> None:
133:         with self._lock:
134:             if step_key in self._process_info:
135:                 self._process_info[step_key]['process'] = process
136:     
137:     def get_step_process(self, step_key: str) -> Optional[Any]:
138:         with self._lock:
139:             if step_key in self._process_info:
140:                 return self._process_info[step_key].get('process')
141:             return None
142:     
143:     def get_step_field(self, step_key: str, field_name: str, default: Any = None) -> Any:
144:         with self._lock:
145:             if step_key in self._process_info:
146:                 return self._process_info[step_key].get(field_name, default)
147:             return default
148:     
149:     def set_step_field(self, step_key: str, field_name: str, value: Any) -> None:
150:         with self._lock:
151:             if step_key in self._process_info:
152:                 self._process_info[step_key][field_name] = value
153:     
154:     def get_step_log_deque(self, step_key: str) -> Optional[deque]:
155:         if step_key in self._process_info:
156:             return self._process_info[step_key]['log']
157:         return None
158:     
159:     def is_sequence_running(self) -> bool:
160:         with self._lock:
161:             return self._sequence_running
162:     
163:     def start_sequence(self, sequence_type: str) -> bool:
164:         with self._lock:
165:             if self._sequence_running:
166:                 logger.warning(f"Cannot start {sequence_type} sequence: already running")
167:                 return False
168:             
169:             self._sequence_running = True
170:             self._sequence_outcome = {
171:                 "status": f"running_{sequence_type.lower()}",
172:                 "type": sequence_type,
173:                 "message": None,
174:                 "timestamp": datetime.now(timezone.utc).isoformat()
175:             }
176:             logger.info(f"{sequence_type} sequence started")
177:             return True
178:     
179:     def complete_sequence(self, success: bool, message: str = None, sequence_type: str = None) -> None:
180:         with self._lock:
181:             self._sequence_running = False
182:             status = "success" if success else "error"
183:             self._sequence_outcome = {
184:                 "status": status,
185:                 "type": sequence_type or self._sequence_outcome.get("type"),
186:                 "message": message,
187:                 "timestamp": datetime.now(timezone.utc).isoformat()
188:             }
189:             logger.info(f"Sequence completed: {status}")
190:     
191:     def get_sequence_outcome(self) -> Dict[str, Any]:
192:         with self._lock:
193:             return self._sequence_outcome.copy()
194:     
195:     def add_csv_download(self, download_id: str, download_info: Dict[str, Any]) -> None:
196:         with self._lock:
197:             self._active_csv_downloads[download_id] = download_info.copy()
198:             logger.debug(f"CSV download added: {download_id}")
199:     
200:     def update_csv_download(self, download_id: str, status: str, 
201:                            progress: int = None, message: str = None, 
202:                            filename: str = None) -> None:
203:         with self._lock:
204:             if download_id in self._active_csv_downloads:
205:                 download = self._active_csv_downloads[download_id]
206:                 download['status'] = status
207:                 if progress is not None:
208:                     download['progress'] = progress
209:                 if message is not None:
210:                     download['message'] = message
211:                 if filename is not None:
212:                     download['filename'] = filename
213:     
214:     def remove_csv_download(self, download_id: str, keep_in_history: bool = True) -> None:
215:         with self._lock:
216:             if download_id in self._active_csv_downloads:
217:                 download = self._active_csv_downloads.pop(download_id)
218:                 if keep_in_history:
219:                     self._kept_csv_downloads.append(download)
220:                 logger.debug(f"CSV download removed: {download_id}")
221:     
222:     def get_csv_downloads_status(self) -> Dict[str, Any]:
223:         with self._lock:
224:             active = [d.copy() for d in self._active_csv_downloads.values()]
225:             kept = list(self._kept_csv_downloads)
226:             
227:             return {
228:                 "active": active,
229:                 "kept": kept,
230:                 "total_active": len(active),
231:                 "total_kept": len(kept)
232:             }
233:     
234:     def get_active_csv_downloads_dict(self) -> Dict[str, Dict[str, Any]]:
235:         with self._lock:
236:             return {k: v.copy() for k, v in self._active_csv_downloads.items()}
237:     
238:     def get_kept_csv_downloads_list(self) -> List[Dict[str, Any]]:
239:         with self._lock:
240:             return list(self._kept_csv_downloads)
241:     
242:     def move_csv_download_to_history(self, download_id: str) -> None:
243:         with self._lock:
244:             if download_id in self._active_csv_downloads:
245:                 download = self._active_csv_downloads.pop(download_id)
246:                 self._kept_csv_downloads.append(download)
247:                 logger.debug(f"CSV download moved to history: {download_id}")
248:     
249:     def get_csv_monitor_status(self) -> Dict[str, Any]:
250:         with self._lock:
251:             return self._csv_monitor_status.copy()
252:     
253:     def update_csv_monitor_status(self, status: str, last_check: str = None, error: str = None) -> None:
254:         with self._lock:
255:             self._csv_monitor_status['status'] = status
256:             if last_check is not None:
257:                 self._csv_monitor_status['last_check'] = last_check
258:             if error is not None:
259:                 self._csv_monitor_status['error'] = error
260:     
261:     def reset_all(self) -> None:
262:         with self._lock:
263:             self._process_info.clear()
264:             self._sequence_running = False
265:             self._sequence_outcome = {
266:                 "status": "never_run",
267:                 "type": None,
268:                 "message": None,
269:                 "timestamp": None
270:             }
271:             self._active_csv_downloads.clear()
272:             self._kept_csv_downloads.clear()
273:             self._csv_monitor_status = {
274:                 "status": "stopped",
275:                 "last_check": None,
276:                 "error": None
277:             }
278:             logger.info("WorkflowState reset to initial values")
279:     
280:     def get_summary(self) -> Dict[str, Any]:
281:         with self._lock:
282:             return {
283:                 "steps_count": len(self._process_info),
284:                 "running_steps": [
285:                     key for key, info in self._process_info.items()
286:                     if info['status'] in ['running', 'starting']
287:                 ],
288:                 "sequence_running": self._sequence_running,
289:                 "sequence_outcome": self._sequence_outcome.copy(),
290:                 "active_downloads": len(self._active_csv_downloads),
291:                 "csv_monitor_status": self._csv_monitor_status['status']
292:             }
293: 
294: 
295: _workflow_state: Optional[WorkflowState] = None
296: _state_lock = threading.Lock()
297: 
298: 
299: def get_workflow_state() -> WorkflowState:
300:     global _workflow_state
301:     
302:     if _workflow_state is None:
303:         with _state_lock:
304:             if _workflow_state is None:
305:                 _workflow_state = WorkflowState()
306:     
307:     return _workflow_state
308: 
309: 
310: def reset_workflow_state() -> None:
311:     global _workflow_state
312:     
313:     with _state_lock:
314:         if _workflow_state is not None:
315:             _workflow_state.reset_all()
316:         _workflow_state = None
```

## File: static/css/components/controls.css
```css
  1: /* ===== UNIFIED CONTROLS SECTION ===== */
  2: .unified-controls-section {
  3:     background: linear-gradient(135deg, var(--bg-card) 0%, rgba(121, 134, 203, 0.02) 100%);
  4:     border: 1px solid var(--border-color);
  5:     border-radius: 16px;
  6:     padding: 30px;
  7:     margin: 30px auto 40px auto;
  8:     max-width: 900px;
  9:     box-shadow: 0 6px 20px rgba(0, 0, 0, 0.08);
 10:     display: flex;
 11:     flex-direction: column;
 12:     gap: 25px;
 13:     align-items: center;
 14:     position: relative;
 15:     overflow: hidden;
 16: }
 17: 
 18: .unified-controls-section::before {
 19:     content: '';
 20:     position: absolute;
 21:     top: 0;
 22:     left: 0;
 23:     right: 0;
 24:     height: 3px;
 25:     background: linear-gradient(90deg, var(--accent-primary), var(--accent-secondary), var(--orange));
 26:     border-radius: 16px 16px 0 0;
 27: }
 28: 
 29: .topbar-affix {
 30:     position: fixed;
 31:     top: 0;
 32:     left: 0;
 33:     right: 0;
 34:     z-index: 40;
 35:     display: flex;
 36:     flex-direction: column;
 37:     align-items: center;
 38: }
 39: 
 40: /* Sticky Topbar variant */
 41: .unified-controls--topbar {
 42:     position: relative;
 43:     z-index: 30; /* above logs panel (20) */
 44:     border-radius: 0 0 16px 16px;
 45:     margin: 0 0 16px 0;
 46:     padding: 12px 16px;
 47:     max-width: unset;
 48:     width: 100%;
 49:     backdrop-filter: saturate(120%) blur(4px);
 50: }
 51: 
 52: .unified-controls--topbar::before {
 53:     height: 2px;
 54:     border-radius: 0;
 55: }
 56: 
 57: .unified-controls--topbar .workflow-controls,
 58: .unified-controls--topbar .utility-widgets {
 59:     width: 100%;
 60:     display: flex;
 61:     align-items: center;
 62:     gap: 10px;
 63:     flex-wrap: wrap;
 64: }
 65: 
 66: .unified-controls--topbar .workflow-controls {
 67:     justify-content: flex-start;
 68: }
 69: 
 70: .unified-controls--topbar .utility-widgets {
 71:     justify-content: flex-end;
 72: }
 73: 
 74: /* Sequence controls group */
 75: .sequence-controls { display: inline-flex; align-items: center; gap: 10px; flex-wrap: wrap; }
 76: 
 77: .control-group {
 78:     display: inline-flex;
 79:     flex-wrap: wrap;
 80:     gap: 8px;
 81:     align-items: center;
 82: }
 83: 
 84: .control-group--primary {
 85:     padding: 4px 12px;
 86:     background: var(--bg-tertiary);
 87:     border-radius: 10px;
 88:     box-shadow: 0 6px 20px rgba(0, 0, 0, 0.18);
 89: }
 90: 
 91: .control-group--secondary {
 92:     gap: 6px;
 93:     padding: 4px 10px;
 94:     background: color-mix(in oklab, var(--bg-card) 80%, rgba(0, 0, 0, 0.6) 20%);
 95:     border-radius: 10px;
 96:     border: 1px solid var(--border-color);
 97: }
 98: 
 99: #run-all-steps-button {
100:     background: linear-gradient(135deg, var(--accent-primary), var(--accent-secondary));
101:     font-size: 1.05em;
102:     padding: 14px 24px;
103:     min-height: var(--touch-target-min);
104:     border-radius: 999px;
105:     color: var(--bg-dark);
106:     font-weight: 600;
107:     box-shadow: 0 4px 12px rgba(94, 114, 228, 0.4);
108:     border: none;
109: }
110: 
111: #run-all-steps-button:hover {
112:     box-shadow: 0 6px 14px rgba(94, 114, 228, 0.55);
113:     transform: translateY(-1px);
114: }
115: 
116: .control-group--secondary button {
117:     background: transparent;
118:     border: 1px solid var(--border-bright);
119:     color: var(--text-primary);
120:     min-height: var(--touch-target-min);
121:     border-radius: 999px;
122:     padding: 10px 18px;
123:     font-weight: 500;
124: }
125: 
126: .control-group--secondary button:hover {
127:     border-color: var(--accent-primary);
128:     color: var(--accent-primary);
129: }
130: 
131: /* Settings toggle button */
132: .settings-toggle {
133:   background: var(--bg-secondary);
134:   border: 1px solid var(--border-color);
135:   color: var(--text-primary);
136:   padding: 8px 12px;
137:   border-radius: 8px;
138:   cursor: pointer;
139:   font-size: 14px;
140:   margin-left: auto; /* push to the right */
141: }
142: 
143: /* Collapsible settings panel */
144: .settings-panel {
145:   display: flex;
146:   flex-direction: column;
147:   gap: 16px;
148:   overflow: hidden;
149:   max-height: 0;
150:   opacity: 0;
151:   transition: max-height 0.25s ease, opacity 0.25s ease, padding-top 0.25s ease;
152:   padding-top: 0;
153: }
154: 
155: .settings-panel.open {
156:   max-height: 500px;
157:   opacity: 1;
158:   padding-top: 8px;
159: }
160: 
161: /* --- Harmonized styles for settings actions --- */
162: .settings-panel button,
163: .settings-panel .compact-toggle label,
164: .settings-panel .btn-like-switch {
165:   background: var(--bg-secondary);
166:   border: 1px solid var(--border-color);
167:   color: var(--text-primary);
168:   padding: 8px 12px;
169:   border-radius: 8px;
170:   cursor: pointer;
171:   font-size: 14px;
172:   display: inline-flex;
173:   align-items: center;
174:   gap: 8px;
175:   transition: background-color 0.2s ease, border-color 0.2s ease, color 0.2s ease, box-shadow 0.2s ease, transform 0.15s ease;
176:   position: relative; /* for ripple */
177:   overflow: hidden;   /* clip ripple */
178: }
179: 
180: .settings-panel button:disabled {
181:   opacity: 0.6;
182:   cursor: not-allowed;
183: }
184: 
185: .settings-panel .compact-toggle input[type="checkbox"],
186: .settings-panel .btn-like-switch input[type="checkbox"] {
187:   margin-left: 6px;
188: }
189: 
190: /* Ensure upload button matches harmonized style even if other rules apply */
191: .settings-panel #upload-button {
192:   background: var(--bg-secondary);
193:   border: 1px solid var(--border-color);
194:   color: var(--text-primary);
195:   padding: 8px 12px;
196:   border-radius: 8px;
197:   font-size: 14px;
198: }
199: 
200: /* Hover and focus styles (align with Stats/Theme/Cinematic interactions) */
201: .settings-panel button:hover,
202: .settings-panel .compact-toggle label:hover,
203: .settings-panel .btn-like-switch:hover,
204: .settings-panel .settings-action:hover,
205: .settings-panel a.settings-action:hover {
206:   background: var(--bg-card);
207:   border-color: var(--accent-primary);
208:   color: var(--accent-primary);
209:   box-shadow: 0 2px 8px rgba(0,0,0,0.12);
210:   transform: translateY(-1px);
211: }
212: 
213: .settings-row {
214:   display: flex;
215:   flex-wrap: wrap;
216:   gap: 12px;
217:   align-items: center;
218: }
219: 
220: .settings-row--stacked {
221:   gap: 18px;
222:   align-items: stretch;
223: }
224: 
225: .settings-block {
226:   background: color-mix(in oklab, var(--bg-card) 90%, transparent);
227:   border: 1px solid var(--border-color);
228:   border-radius: 10px;
229:   padding: 12px 14px;
230:   flex: 1 1 220px;
231:   min-width: 200px;
232:   transition: border-color 0.2s ease, box-shadow 0.2s ease;
233: }
234: 
235: .settings-block:focus-within,
236: .settings-block:hover {
237:   border-color: var(--accent-primary);
238:   box-shadow: 0 6px 18px rgba(0,0,0,0.12);
239: }
240: 
241: .settings-section {
242:   width: 100%;
243:   border-bottom: 1px solid color-mix(in oklab, var(--border-color) 80%, transparent);
244:   padding-bottom: 12px;
245: }
246: 
247: .settings-section:last-child {
248:   border-bottom: none;
249:   padding-bottom: 4px;
250: }
251: 
252: .settings-title {
253:   font-size: 0.85em;
254:   font-weight: 600;
255:   letter-spacing: 0.05em;
256:   text-transform: uppercase;
257:   color: var(--text-secondary);
258:   margin: 0 0 10px 0;
259: }
260: 
261: .settings-panel button:focus-visible,
262: .settings-panel .compact-toggle label:focus-visible,
263: .settings-panel .btn-like-switch:focus-visible,
264: .settings-panel .settings-action:focus-visible,
265: .settings-panel a.settings-action:focus-visible {
266:   outline: none;
267:   box-shadow: 0 0 0 3px color-mix(in oklab, var(--accent-primary) 35%, transparent);
268:   border-color: var(--accent-primary);
269: }
270: 
271: /* Ripple feedback on click (settings panel scope) */
272: .settings-panel button::after,
273: .settings-panel .btn-like-switch::after,
274: .settings-panel .compact-toggle label::after {
275:   content: '';
276:   position: absolute;
277:   top: 50%;
278:   left: 50%;
279:   width: 0;
280:   height: 0;
281:   border-radius: 50%;
282:   background: rgba(255,255,255,0.25);
283:   transform: translate(-50%, -50%);
284:   opacity: 0;
285:   transition: width 0.6s, height 0.6s, opacity 0.6s;
286: }
287: 
288: .settings-panel button:active::after,
289: .settings-panel .btn-like-switch:active::after,
290: .settings-panel .compact-toggle label:active::after {
291:   width: 220px;
292:   height: 220px;
293:   opacity: 1;
294:   transition: 0s;
295: }
296: 
297: /* Utility: settings-action base style (for future controls) */
298: .settings-panel .settings-action,
299: .settings-panel a.settings-action {
300:   background: var(--bg-secondary);
301:   border: 1px solid var(--border-color);
302:   color: var(--text-primary);
303:   padding: 8px 12px;
304:   border-radius: 8px;
305:   cursor: pointer;
306:   font-size: 14px;
307:   display: inline-flex;
308:   align-items: center;
309:   gap: 8px;
310:   text-decoration: none;
311:   transition: background-color 0.2s ease, border-color 0.2s ease, color 0.2s ease, box-shadow 0.2s ease, transform 0.15s ease;
312: }
313: 
314: /* Compact pill styles for widgets inside topbar */
315: .utility-widgets .auto-scroll-widget,
316: .utility-widgets .sound-control-widget,
317: .utility-widgets #upload-widget {
318:     display: inline-flex;
319:     align-items: center;
320:     gap: 8px;
321:     padding: 8px 10px;
322:     border: 1px solid var(--border-color);
323:     border-radius: 10px;
324:     background: var(--bg-elevated, var(--bg-card));
325: }
326: 
327: .auto-scroll-header, .sound-control-header { display: inline-flex; align-items: center; gap: 6px; }
328: .auto-scroll-title, .sound-control-title { font-size: 12px; color: var(--text-secondary); }
329: .auto-scroll-status, .sound-control-status { font-size: 12px; color: var(--text-secondary); }
330: 
331: .auto-scroll-switch, .sound-control-switch { position: relative; display: inline-flex; align-items: center; }
332: .auto-scroll-switch input, .sound-control-switch input { cursor: pointer; }
333: 
334: .upload-button { 
335:     background: var(--bg-secondary);
336:     border: 1px solid var(--border-color);
337:     color: var(--text-primary);
338:     padding: 6px 12px;
339:     border-radius: 8px;
340:     cursor: pointer;
341:     font-size: 13px;
342: }
343: 
344: /* Inline button-like switches used in topbar (for auto-scroll and sound) */
345: .btn-like-switch {
346:   display: inline-flex;
347:   align-items: center;
348:   gap: 8px;
349:   background: var(--bg-secondary);
350:   border: 1px solid var(--border-color);
351:   color: var(--text-primary);
352:   padding: 8px 12px;
353:   border-radius: 8px;
354:   cursor: pointer;
355:   font-size: 14px;
356:   user-select: none;
357: }
358: 
359: .btn-like-switch input[type="checkbox"] {
360:   position: static;
361:   width: auto;
362:   height: auto;
363:   opacity: 1;
364:   margin-left: 6px;
365: }
366: 
367: /* Align upload widget container with button baseline */
368: .upload-widget-inline { display: inline-flex; align-items: center; }
369: 
370: /* Responsive tweaks */
371: @media (max-width: 900px) {
372:   .unified-controls--topbar .workflow-controls { order: 1; }
373:   .unified-controls--topbar .utility-widgets { order: 2; justify-content: flex-start; }
374: }
375: 
376: @media (max-width: 720px) {
377:   .settings-row {
378:     flex-direction: column;
379:     align-items: stretch;
380:   }
381:   .settings-block {
382:     width: 100%;
383:   }
384:   .advanced-controls .advanced-row {
385:     flex-direction: column;
386:     align-items: stretch;
387:   }
388: }
```

## File: static/css/components/logs.css
```css
  1: .logs-column {
  2:     position: relative;
  3:     background: color-mix(in oklab, var(--bg-card) 92%, transparent);
  4:     border: 1px solid color-mix(in oklab, var(--border-color) 75%, transparent);
  5:     border-radius: var(--pipeline-card-radius);
  6:     box-shadow:
  7:         0 10px 26px rgba(0,0,0,0.22),
  8:         0 0 0 1px rgb(var(--accent-primary-rgb) / 0.12);
  9: }
 10: 
 11: .logs-column > * {
 12:     position: relative;
 13:     z-index: 1;
 14: }
 15: 
 16: .logs-column::before {
 17:     content: '';
 18:     position: absolute;
 19:     left: 12px;
 20:     top: 0;
 21:     bottom: 0;
 22:     width: var(--pipeline-axis-width);
 23:     background: color-mix(in oklab, var(--accent-primary) 45%, transparent);
 24:     border-radius: 999px;
 25:     opacity: 0.9;
 26:     pointer-events: none;
 27: }
 28: 
 29: .logs-column::after {
 30:     content: '';
 31:     position: absolute;
 32:     left: 0;
 33:     top: 0;
 34:     bottom: 0;
 35:     width: 36px;
 36:     background: radial-gradient(
 37:         circle at 12px 30%,
 38:         rgb(var(--accent-primary-rgb) / 0.22) 0%,
 39:         rgb(var(--accent-primary-rgb) / 0.10) 40%,
 40:         rgb(var(--accent-primary-rgb) / 0.00) 70%
 41:     );
 42:     pointer-events: none;
 43: }
 44: 
 45: .log-panel-header {
 46:     display: flex;
 47:     flex-direction: column;
 48:     align-items: stretch;
 49:     gap: 10px;
 50:     padding-bottom: 10px;
 51:     margin-bottom: 15px;
 52:     border-bottom: 1px solid color-mix(in oklab, var(--border-color) 80%, transparent);
 53:     color: var(--accent-primary);
 54:     font-size: 1.3em;
 55:     font-weight: 500;
 56:     flex-shrink: 0;
 57:     transition: color var(--motion-duration-medium) var(--motion-ease-standard);
 58: }
 59: 
 60: .log-panel-header-main {
 61:     display: flex;
 62:     justify-content: space-between;
 63:     align-items: center;
 64:     gap: 12px;
 65: }
 66: 
 67: .log-panel-subheader {
 68:     display: flex;
 69:     flex-wrap: wrap;
 70:     align-items: baseline;
 71:     gap: 10px;
 72:     font-size: 0.85em;
 73:     color: var(--text-secondary);
 74: }
 75: 
 76: #log-panel-context-step {
 77:     color: var(--text-primary);
 78:     font-weight: 500;
 79: }
 80: 
 81: .log-panel-specific-buttons {
 82:     display: flex;
 83:     flex-wrap: wrap;
 84:     gap: 10px;
 85: }
 86: 
 87: .log-panel-specific-buttons .specific-log-button {
 88:     display: inline-flex;
 89:     align-items: center;
 90:     justify-content: center;
 91:     gap: 8px;
 92:     border-radius: 999px;
 93:     padding: 10px 18px;
 94:     background: color-mix(in oklab, var(--bg-card) 80%, transparent);
 95:     border: 1px solid color-mix(in oklab, var(--border-color) 80%, transparent);
 96:     color: var(--text-secondary);
 97:     cursor: pointer;
 98:     transition:
 99:         color var(--motion-duration-fast) var(--motion-ease-standard),
100:         background var(--motion-duration-fast) var(--motion-ease-standard),
101:         border-color var(--motion-duration-fast) var(--motion-ease-standard),
102:         transform var(--motion-duration-fast) var(--motion-ease-standard);
103: }
104: 
105: .log-panel-specific-buttons .specific-log-button:hover {
106:     color: var(--text-primary);
107:     background: color-mix(in oklab, var(--bg-card) 70%, var(--accent-primary) 10%);
108:     border-color: rgb(var(--accent-primary-rgb) / 0.35);
109: }
110: 
111: .log-panel-specific-buttons .specific-log-button:active {
112:     transform: scale(0.98);
113: }
114: 
115: #close-log-panel {
116:     display: inline-flex;
117:     align-items: center;
118:     justify-content: center;
119:     width: 44px;
120:     height: 44px;
121:     background: color-mix(in oklab, var(--bg-card) 80%, transparent);
122:     border: 1px solid color-mix(in oklab, var(--border-color) 80%, transparent);
123:     border-radius: 999px;
124:     color: var(--text-secondary);
125:     font-size: 1.45em;
126:     cursor: pointer;
127:     transition:
128:         color var(--motion-duration-fast) var(--motion-ease-standard),
129:         background var(--motion-duration-fast) var(--motion-ease-standard),
130:         border-color var(--motion-duration-fast) var(--motion-ease-standard),
131:         transform var(--motion-duration-fast) var(--motion-ease-standard);
132: }
133: 
134: #close-log-panel:hover {
135:     color: var(--text-primary);
136:     background: color-mix(in oklab, var(--bg-card) 70%, var(--accent-primary) 10%);
137:     border-color: rgb(var(--accent-primary-rgb) / 0.35);
138: }
139: 
140: #close-log-panel:active {
141:     transform: scale(0.96);
142: }
143: 
144: .log-container { border: 1px solid color-mix(in oklab, var(--border-color) 80%, transparent); border-radius: calc(var(--pipeline-card-radius) - 8px); margin-bottom: 20px; flex-shrink: 0; display: flex; flex-direction: column; background: color-mix(in oklab, var(--bg-card) 94%, transparent);}
145: .log-header { background-color: color-mix(in oklab, var(--bg-card) 78%, black 10%); padding: 10px 15px; font-weight: 600; border-bottom: 1px solid color-mix(in oklab, var(--border-color) 80%, transparent); border-radius: calc(var(--pipeline-card-radius) - 8px) calc(var(--pipeline-card-radius) - 8px) 0 0; color: var(--text-secondary); }
146: 
147: .log-output,
148: .specific-log-output {
149:     background-color: var(--log-bg);
150:     color: var(--text-bright);
151:     padding: 15px;
152:     font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace;
153:     font-size: 1em;
154:     line-height: 1.45;
155:     max-height: 300px;
156:     overflow-y: auto;
157:     white-space: pre-wrap;
158:     word-break: break-all;
159:     border-radius: 0 0 8px 8px;
160:     flex-grow: 1;
161: }
162: 
163: /* Enhanced Log Styling - Different log types */
164: .log-line {
165:     display: block;
166:     margin: 2px 0;
167:     padding: 2px 6px;
168:     border-radius: 3px;
169:     position: relative;
170: }
171: 
172: .log-line.log-success {
173:     background-color: rgb(var(--status-success-rgb) / 0.12);
174:     border-left: 3px solid var(--status-success);
175:     color: color-mix(in oklab, var(--status-success) 55%, var(--text-primary));
176: }
177: 
178: .log-line.log-warning {
179:     background-color: rgb(var(--status-warning-rgb) / 0.12);
180:     border-left: 3px solid var(--status-warning);
181:     color: color-mix(in oklab, var(--status-warning) 55%, var(--text-primary));
182: }
183: 
184: .log-line.log-error {
185:     background-color: rgb(var(--status-error-rgb) / 0.12);
186:     border-left: 3px solid var(--status-error);
187:     color: color-mix(in oklab, var(--status-error) 55%, var(--text-primary));
188:     font-weight: 500;
189: }
190: 
191: .log-line.log-info {
192:     background-color: rgb(var(--status-running-rgb) / 0.12);
193:     border-left: 3px solid var(--status-running);
194:     color: color-mix(in oklab, var(--status-running) 55%, var(--text-primary));
195: }
196: 
197: .log-line.log-debug {
198:     background-color: rgba(158, 158, 158, 0.1);
199:     border-left: 3px solid #9e9e9e;
200:     color: #616161;
201:     font-size: 0.85em;
202: }
203: 
204: .log-line.log-command {
205:     background-color: rgba(156, 39, 176, 0.1);
206:     border-left: 3px solid #9c27b0;
207:     color: #7b1fa2;
208:     font-weight: 500;
209: }
210: 
211: .log-line.log-progress {
212:     background-color: rgb(var(--accent-primary-rgb) / 0.10);
213:     border-left: 3px solid var(--accent-primary);
214:     color: color-mix(in oklab, var(--accent-primary) 70%, var(--text-primary));
215: }
216: 
217: /* Log icons for better visual distinction */
218: .log-line::before {
219:     content: '';
220:     display: inline-block;
221:     width: 12px;
222:     height: 12px;
223:     margin-right: 8px;
224:     border-radius: 50%;
225:     vertical-align: middle;
226: }
227: 
228: .log-line.log-success::before {
229:     background-color: var(--status-success);
230:     content: '✓';
231:     color: white;
232:     font-size: 8px;
233:     text-align: center;
234:     line-height: 12px;
235:     font-weight: bold;
236: }
237: 
238: .log-line.log-warning::before {
239:     background-color: var(--status-warning);
240:     content: '⚠';
241:     color: white;
242:     font-size: 8px;
243:     text-align: center;
244:     line-height: 12px;
245: }
246: 
247: .log-line.log-error::before {
248:     background-color: var(--status-error);
249:     content: '✕';
250:     color: white;
251:     font-size: 8px;
252:     text-align: center;
253:     line-height: 12px;
254:     font-weight: bold;
255: }
256: 
257: .log-line.log-info::before {
258:     background-color: var(--status-running);
259:     content: 'ℹ';
260:     color: white;
261:     font-size: 8px;
262:     text-align: center;
263:     line-height: 12px;
264: }
265: 
266: .log-line.log-debug::before {
267:     background-color: #9e9e9e;
268:     content: '•';
269:     color: white;
270:     font-size: 10px;
271:     text-align: center;
272:     line-height: 12px;
273: }
274: 
275: .log-line.log-command::before {
276:     background-color: #9c27b0;
277:     content: '$';
278:     color: white;
279:     font-size: 8px;
280:     text-align: center;
281:     line-height: 12px;
282:     font-weight: bold;
283: }
284: 
285: .log-line.log-progress::before {
286:     background-color: var(--accent-primary);
287:     content: '⟳';
288:     color: white;
289:     font-size: 8px;
290:     text-align: center;
291:     line-height: 12px;
292: }
293: 
294: /* Dark mode adjustments for log styling */
295: @media (prefers-color-scheme: dark) {
296:     .log-line.log-success {
297:         background-color: rgb(var(--status-success-rgb) / 0.15);
298:         color: color-mix(in oklab, var(--status-success) 60%, var(--text-primary));
299:     }
300: 
301:     .log-line.log-warning {
302:         background-color: rgb(var(--status-warning-rgb) / 0.15);
303:         color: color-mix(in oklab, var(--status-warning) 60%, var(--text-primary));
304:     }
305: 
306:     .log-line.log-error {
307:         background-color: rgb(var(--status-error-rgb) / 0.15);
308:         color: color-mix(in oklab, var(--status-error) 60%, var(--text-primary));
309:     }
310: 
311:     .log-line.log-info {
312:         background-color: rgb(var(--status-running-rgb) / 0.15);
313:         color: color-mix(in oklab, var(--status-running) 60%, var(--text-primary));
314:     }
315: 
316:     .log-line.log-debug {
317:         background-color: rgba(158, 158, 158, 0.15);
318:         color: #bdbdbd;
319:     }
320: 
321:     .log-line.log-command {
322:         background-color: rgba(156, 39, 176, 0.15);
323:         color: #ba68c8;
324:     }
325: 
326:     .log-line.log-progress {
327:         background-color: rgb(var(--accent-primary-rgb) / 0.15);
328:         color: color-mix(in oklab, var(--accent-primary) 70%, var(--text-primary));
329:     }
330: }
331: 
332: /* Hover effects for better interactivity */
333: .log-line:hover {
334:     background-color: color-mix(in oklab, rgb(var(--accent-primary-rgb) / 0.10) 40%, transparent);
335:     transition: background-color var(--motion-duration-fast) var(--motion-ease-standard);
336: }
337: 
338: /* Improved spacing and readability */
339: .log-output .log-line:first-child {
340:     margin-top: 0;
341: }
342: 
343: .log-output .log-line:last-child {
344:     margin-bottom: 0;
345: }
346: .log-output:empty:before {
347:     content: "En attente de logs...";
348:     color: var(--text-secondary);
349:     font-style: italic;
350: }
351: .specific-log-output:empty:before {
352:     content: "Aucun log spécifique chargé.";
353:     color: var(--text-secondary);
354:     font-style: italic;
355: }
356: 
357: .specific-log-controls-wrapper { margin-top: 15px; }
358: .specific-log-controls-wrapper h4 { font-weight: 500; color: var(--text-secondary); margin-bottom: 10px; font-size: 1em;}
359: 
360: .specific-log-path { font-size: 0.8em; color: var(--text-muted); margin-bottom: 8px; word-break: break-all; padding: 5px 15px;}
361: 
362: .log-table { width: 100%; border-collapse: collapse; font-size: 0.9em; margin-top: 5px; }
363: .log-table th, .log-table td { border: 1px solid var(--border-color); padding: 8px; text-align: left; }
364: .log-table th { background-color: color-mix(in oklab, var(--bg-card) 88%, black 8%); color: var(--text-primary); }
365: .log-table tr:nth-child(even) { background-color: color-mix(in oklab, var(--bg-card) 85%, black 10%); }
```

## File: static/css/components/steps.css
```css
  1: .step {
  2:     background-color: var(--bg-card);
  3:     border: 1px solid var(--border-color);
  4:     border-left: 3px solid var(--border-color);
  5:     padding: 25px;
  6:     margin-bottom: 25px;
  7:     border-radius: 12px;
  8:     box-shadow: 0 5px 15px rgba(0,0,0,0.2);
  9:     width: 100%;
 10:     max-width: 700px;
 11:     transition:
 12:         transform 0.4s ease-out,
 13:         opacity 0.4s ease-in-out,
 14:         border-color 0.3s ease,
 15:         border-left-color 0.3s ease,
 16:         margin-bottom 0.4s ease-in-out,
 17:         padding 0.4s ease-in-out,
 18:         box-shadow 0.3s ease;
 19:     scroll-margin-top: 0; /* Désactivé pour permettre un centrage parfait */
 20: }
 21: 
 22: .step[data-status="running"],
 23: .step[data-status="starting"],
 24: .step[data-status="initiated"] {
 25:     border-left-color: var(--status-running);
 26: }
 27: 
 28: .step[data-status="completed"],
 29: .step[data-status="success"] {
 30:     border-left-color: var(--status-success);
 31: }
 32: 
 33: .step[data-status="failed"],
 34: .step[data-status="error"],
 35: .step[data-status="cancelled"] {
 36:     border-left-color: var(--status-error);
 37: }
 38: 
 39: .step[data-status="warning"],
 40: .step[data-status="paused"] {
 41:     border-left-color: var(--status-warning);
 42: }
 43: 
 44: /* Compact mode reduces padding/margins and typography to fit viewport */
 45: .workflow-wrapper.compact-mode:not(.logs-active) .step {
 46:     max-width: 100%;
 47:     /* Reduce bottom padding to limit empty space below last controls/log buttons */
 48:     padding: 14px 14px 10px; /* top right bottom left */
 49:     margin-bottom: 0; /* handled by grid gap */
 50:     box-shadow: 0 2px 6px rgba(0,0,0,0.12); /* softer shadow to reduce visual noise */
 51:     align-self: start; /* avoid vertical stretch inside grid on desktop */
 52: }
 53: 
 54: /* Smooth reappearance after closing logs panel - now only affects opacity/transform */
 55: .workflow-wrapper.logs-leaving .steps-column .step {
 56:     will-change: opacity, transform;
 57: }
 58: 
 59: /* Utility class applied via JS for deterministic transitions */
 60: .step.steps-hidden {
 61:     opacity: 0 !important;
 62:     transform: translateY(8px) scale(0.97) !important;
 63:     pointer-events: none;
 64: }
 65: 
 66: /* Staggered delays in compact mode - no longer needed since layout is stable */
 67: .workflow-wrapper.compact-mode.logs-leaving .steps-column .step {
 68:     transition-timing-function: cubic-bezier(0.2, 0.6, 0.2, 1);
 69:     transition-duration: 0.4s;
 70: }
 71: .step:hover:not(.active-for-log-panel) {
 72:      transform: translateY(-4px);
 73:      box-shadow: 0 8px 18px rgba(0,0,0,0.28);
 74: }
 75:  .step.active-for-log-panel {
 76:     border: 2px solid var(--accent-primary);
 77:     box-shadow: 0 8px 25px rgba(121, 134, 203, 0.3), 0 0 0 1px var(--accent-primary);
 78:     transform: translateY(-2px);
 79: }
 80: .step.custom-sequence-selected {
 81:     border: 2px solid var(--orange);
 82:     box-shadow: 0 0 15px var(--orange);
 83: }
 84: 
 85: .step-header-content {
 86:     display: flex;
 87:     justify-content: space-between;
 88:     align-items: center;
 89:     border-bottom: 1px solid var(--border-color);
 90:     padding-bottom: 10px;
 91:     margin-bottom: 20px;
 92: }
 93: 
 94: .step-title-group {
 95:     display: flex;
 96:     align-items: center;
 97:     gap: 12px;
 98:     flex-wrap: wrap;
 99: }
100: 
101: .step h2 {
102:     margin-top: 0;
103:     font-size: 1.5em;
104:     color: var(--accent-primary);
105:     display: flex;
106:     align-items: center;
107:     font-weight: 500;
108:     border-bottom: none;
109:     padding-bottom: 0;
110:     margin-bottom: 0;
111: }
112: .step h2 .step-icon { margin-right: 12px; font-size: 1.6em; }
113: 
114: /* Icon accent on hover for better affordance */
115: .step:hover h2 .step-icon {
116:     color: var(--accent-primary);
117:     transform: translateY(-1px);
118:     transition: color 0.2s ease, transform 0.2s ease;
119: }
120: 
121: .workflow-wrapper.compact-mode:not(.logs-active) .step h2 {
122:     font-size: 1.05em; /* slightly smaller titles in compact */
123: }
124: .workflow-wrapper.compact-mode:not(.logs-active) .step h2 .step-icon {
125:     font-size: 1.1em;
126:     margin-right: 6px;
127: }
128: 
129: .step-state-chip {
130:     display: inline-flex;
131:     align-items: center;
132:     gap: 6px;
133:     padding: 4px 12px;
134:     border-radius: 999px;
135:     font-size: 0.78em;
136:     font-weight: 600;
137:     text-transform: capitalize;
138:     border: 1px solid transparent;
139:     transition: background-color 0.2s ease, color 0.2s ease, border-color 0.2s ease, box-shadow 0.2s ease;
140:     letter-spacing: 0.02em;
141: }
142: 
143: .step-state-chip span {
144:     font-size: 0.9em;
145: }
146: 
147: .state-running {
148:     background: color-mix(in oklab, var(--status-running) 18%, transparent);
149:     color: var(--status-running);
150:     border-color: color-mix(in oklab, var(--status-running) 35%, transparent);
151:     box-shadow: 0 0 12px color-mix(in oklab, var(--status-running) 16%, transparent);
152: }
153: 
154: .state-success {
155:     background: color-mix(in oklab, var(--status-success) 18%, transparent);
156:     color: var(--status-success);
157:     border-color: color-mix(in oklab, var(--status-success) 35%, transparent);
158: }
159: 
160: .state-error {
161:     background: color-mix(in oklab, var(--status-error) 18%, transparent);
162:     color: var(--status-error);
163:     border-color: color-mix(in oklab, var(--status-error) 35%, transparent);
164: }
165: 
166: .state-warning {
167:     background: color-mix(in oklab, var(--status-warning) 18%, transparent);
168:     color: var(--status-warning);
169:     border-color: color-mix(in oklab, var(--status-warning) 35%, transparent);
170: }
171: 
172: .state-idle {
173:     background: color-mix(in oklab, var(--status-idle) 18%, transparent);
174:     color: var(--status-idle);
175:     border-color: color-mix(in oklab, var(--status-idle) 35%, transparent);
176: }
177: 
178: /* Hover tuning for processing/active steps: keep subtle, avoid large motion */
179: .steps-column .step[data-status="running"]:hover,
180: .steps-column .step[data-status="starting"]:hover,
181: .steps-column .step[data-status="initiated"]:hover,
182: .steps-column .step.active-for-log-panel:hover {
183:     transform: translateY(-2px) scale(1.005);
184:     box-shadow: 0 10px 24px rgba(121, 134, 203, 0.22), 0 2px 8px rgba(0,0,0,0.2);
185: }
186: 
187: /* Respect reduced motion for hover effects */
188: @media (prefers-reduced-motion: reduce) {
189:     .step:hover:not(.active-for-log-panel),
190:     .steps-column .step[data-status="running"]:hover,
191:     .steps-column .step[data-status="starting"]:hover,
192:     .steps-column .step[data-status="initiated"]:hover,
193:     .steps-column .step.active-for-log-panel:hover {
194:         transform: none !important;
195:         box-shadow: 0 6px 16px rgba(0,0,0,0.18);
196:     }
197: }
198: 
199: .step-selection-control {
200:     display: flex;
201:     align-items: center;
202: }
203: .step-selection-control input[type="checkbox"] {
204:     width: 20px;
205:     height: 20px;
206:     margin-right: 8px;
207:     cursor: pointer;
208: }
209: .step-selection-order-number {
210:     font-size: 1em;
211:     font-weight: bold;
212:     color: var(--orange);
213:     min-width: 20px;
214:     text-align: center;
215: }
216: 
217: .step-controls button,
218: .specific-log-button {
219:     padding: 12px 22px;
220:     margin-right: 12px;
221:     margin-bottom: 12px;
222:     font-size: 0.95em;
223:     cursor: pointer;
224:     border: none;
225:     border-radius: 25px;
226:     color: var(--bg-dark);
227:     transition: background-color 0.2s ease, transform 0.1s ease, box-shadow 0.2s ease;
228:     font-weight: 600;
229:     box-shadow: 0 2px 5px rgba(0,0,0,0.15);
230:     min-height: var(--touch-target-min);
231: }
232: .workflow-wrapper.compact-mode:not(.logs-active) .step-controls button,
233: .workflow-wrapper.compact-mode:not(.logs-active) .specific-log-button {
234:     padding: var(--button-compact-padding);
235:     font-size: 0.86em;
236:     margin-right: 6px;
237:     /* Reduce vertical gap below the last row of buttons */
238:     margin-bottom: 3px;
239:     border-radius: 16px;
240: }
241: 
242: /* Remove extra bottom gap under the final log button in compact mode */
243: .workflow-wrapper.compact-mode:not(.logs-active) .specific-log-button:last-child {
244:     margin-bottom: 0;
245: }
246: .step-controls button:hover, .specific-log-button:hover {
247:      box-shadow: 0 4px 8px rgba(0,0,0,0.2);
248: }
249: .step-controls button:active, .specific-log-button:active { transform: scale(0.97); }
250: 
251: .run-button { background-color: var(--green); color: white; }
252: .run-button:hover { background-color: #5cb85c; }
253: .run-button:disabled,
254: .cancel-button:disabled,
255: .specific-log-button:disabled {
256:     background: color-mix(in oklab, var(--bg-tertiary) 80%, black 8%);
257:     color: var(--text-muted);
258:     border-color: var(--border-color);
259: }
260: 
261: .cancel-button { background-color: var(--red); color: white;}
262: .cancel-button:hover { background-color: #d9534f; }
263: /* Universal focus-visible for action buttons (keyboard accessibility) */
264: .run-button:focus-visible,
265: .cancel-button:focus-visible,
266: .specific-log-button:focus-visible {
267:     outline: none;
268:     box-shadow: 0 0 0 3px color-mix(in oklab, var(--accent-primary) 35%, transparent);
269:     border-color: var(--accent-primary);
270: }
271: 
272: .specific-log-button { background-color: var(--yellow); color: #333; }
273: .specific-log-button:hover { background-color: #ffeb3b; }
274: 
275: .status-line { margin-top: 15px; margin-bottom:10px; font-weight: bold; font-size: 1.05em; }
276: .workflow-wrapper.compact-mode:not(.logs-active) .status-line {
277:     margin-top: 6px;
278:     margin-bottom: 4px;
279:     font-size: 0.9em;
280: }
281: .status-line {
282:     display: flex;
283:     align-items: center;
284:     gap: 10px;
285:     flex-wrap: wrap;
286: }
287: .status-badge {
288:     display: inline-flex;
289:     align-items: center;
290:     padding: 4px 10px;
291:     border-radius: 999px;
292:     font-weight: 600;
293:     font-size: 0.85em;
294:     letter-spacing: 0.01em;
295:     text-transform: capitalize;
296:     background: color-mix(in oklab, var(--status-idle) 18%, transparent);
297:     color: var(--status-idle);
298:     border: 1px solid color-mix(in oklab, var(--status-idle) 35%, transparent);
299: }
300: .status-running,
301: .status-starting,
302: .status-initiated {
303:     background: color-mix(in oklab, var(--status-running) 18%, transparent);
304:     color: var(--status-running);
305:     border-color: color-mix(in oklab, var(--status-running) 35%, transparent);
306:     animation: pulseStatus 1.8s infinite ease-in-out;
307: }
308: .status-success,
309: .status-completed {
310:     background: color-mix(in oklab, var(--status-success) 18%, transparent);
311:     color: var(--status-success);
312:     border-color: color-mix(in oklab, var(--status-success) 35%, transparent);
313: }
314: .status-error,
315: .status-failed,
316: .status-cancelled {
317:     background: color-mix(in oklab, var(--status-error) 18%, transparent);
318:     color: var(--status-error);
319:     border-color: color-mix(in oklab, var(--status-error) 35%, transparent);
320: }
321: .status-warning,
322: .status-paused {
323:     background: color-mix(in oklab, var(--status-warning) 18%, transparent);
324:     color: var(--status-warning);
325:     border-color: color-mix(in oklab, var(--status-warning) 35%, transparent);
326: }
327: .status-idle {
328:     background: color-mix(in oklab, var(--status-idle) 18%, transparent);
329:     color: var(--status-idle);
330: }
331: .status-line span.timer {
332:     font-weight: normal;
333:     font-size: 0.9em;
334:     color: var(--text-secondary);
335:     margin-left: 10px;
336: }
337: 
338: .step-progress-container {
339:     width: 100%;
340:     margin-top:10px;
341: }
342: .workflow-wrapper.compact-mode:not(.logs-active) .step-progress-container { margin-top: 6px; }
343: .progress-bar-wrapper {
344:     background-color: var(--border-color);
345:     border-radius: 8px;
346:     padding: 3px;
347: }
348: .progress-bar-step {
349:     width: 0%;
350:     height: 18px;
351:     background-color: var(--blue);
352:     border-radius: 5px;
353:     text-align: center;
354:     line-height: 18px;
355:     color: white;
356:     font-size: 0.85em;
357:     font-weight: bold;
358:     /* Smoother width transition */
359:     transition: width 0.4s cubic-bezier(0.4, 0, 0.2, 1), background-color 0.3s ease;
360:     position: relative;
361:     overflow: hidden; /* for shine effect */
362: }
363: .workflow-wrapper.compact-mode:not(.logs-active) .progress-bar-step {
364:     height: 12px;
365:     line-height: 12px;
366:     font-size: 0.76em;
367: }
368: .progress-bar-step[data-active="true"]::after {
369:     content: '';
370:     position: absolute;
371:     top: 0;
372:     left: -100%;
373:     width: 100%;
374:     height: 100%;
375:     background: linear-gradient(90deg, transparent, rgba(255,255,255,0.22), transparent);
376:     animation: progressShine 2.0s infinite;
377: }
378: .progress-text-step {
379:     text-align: center;
380:     margin-top: 4px;
381:     font-size: 0.85em;
382:     color: var(--text-secondary);
383:     white-space: pre-line;
384:     overflow: hidden;
385:     text-overflow: ellipsis;
386:     max-height: 3.6em;
387:     line-height: 1.2em;
388: }
389: .workflow-wrapper.compact-mode:not(.logs-active) .progress-text-step {
390:     font-size: 0.76em;
391: }
392: 
393: /* Subtle pulse for current filename while processing */
394: .progress-text-step[data-processing="true"] {
395:     animation: textPulse 1.5s ease-in-out infinite;
396: }
397: 
398: /* Reduce extra spacing around specific log controls to avoid empty space under buttons */
399: .workflow-wrapper.compact-mode:not(.logs-active) .step .specific-log-controls-wrapper {
400:     margin-top: 8px; /* was 15px in logs.css; tighter in compact mode */
401:     margin-bottom: 0;
402: }
403: .workflow-wrapper.compact-mode:not(.logs-active) .step .specific-log-controls-wrapper h4 {
404:     margin-bottom: 6px; /* override logs.css (10px) to reduce vertical gap */
405: }
406: .workflow-wrapper.compact-mode:not(.logs-active) .step .specific-log-controls-wrapper > div {
407:     /* the immediate container of buttons; ensure no unintended bottom margin */
408:     margin-bottom: 0;
409: }
410: 
411: /* ========== Micro-interactions for steps ========== */
412: /* Non-active steps get a very subtle halo when any step is running */
413: .workflow-wrapper.any-step-running .steps-column .step:not(.active-for-log-panel)[data-status="idle"],
414: .workflow-wrapper.any-step-running .steps-column .step:not(.active-for-log-panel)[data-status="completed"],
415: .workflow-wrapper.any-step-running .steps-column .step:not(.active-for-log-panel)[data-status="failed"] {
416:     position: relative;
417: }
418: 
419: .workflow-wrapper.any-step-running .steps-column .step:not(.active-for-log-panel)[data-status="idle"]::after,
420: .workflow-wrapper.any-step-running .steps-column .step:not(.active-for-log-panel)[data-status="completed"]::after,
421: .workflow-wrapper.any-step-running .steps-column .step:not(.active-for-log-panel)[data-status="failed"]::after {
422:     content: '';
423:     position: absolute;
424:     inset: 0;
425:     border-radius: 12px;
426:     pointer-events: none;
427:     box-shadow: 0 0 0 2px rgba(121, 134, 203, 0.35);
428:     opacity: 0.22;
429:     animation: subtleOpacityPulse 2.8s ease-in-out infinite;
430: }
431: 
432: /* Active/processing step: breathing effect to indicate ongoing work */
433: .steps-column .step.active-for-log-panel,
434: .steps-column .step[data-status="running"],
435: .steps-column .step[data-status="starting"],
436: .steps-column .step[data-status="initiated"] {
437:     will-change: transform;
438:     animation: cardBreath 3.2s ease-in-out infinite;
439: }
440: 
441: /* Respect reduced motion preferences */
442: @media (prefers-reduced-motion: reduce) {
443:     .workflow-wrapper.any-step-running .steps-column .step:not(.active-for-log-panel)[data-status]::after,
444:     .steps-column .step.active-for-log-panel,
445:     .steps-column .step[data-status="running"],
446:     .steps-column .step[data-status="starting"],
447:     .steps-column .step[data-status="initiated"] {
448:         animation: none !important;
449:     }
450: }
451: 
452: /* Override transform reset from layout.css to allow hover lift on idle steps */
453: .workflow-wrapper:not(.logs-active) .steps-column .step:hover:not(.active-for-log-panel) {
454:     transform: translateY(-4px);
455:     box-shadow: 0 8px 18px rgba(0,0,0,0.28);
456: }
457: 
458: .workflow-wrapper.compact-mode .steps-column {
459:     display: flex;
460:     flex-direction: column;
461:     align-items: center;
462:     gap: var(--pipeline-gap);
463: }
464: 
465: .workflow-pipeline {
466:     width: 100%;
467: }
468: 
469: .pipeline-timeline {
470:     position: relative;
471:     width: 100%;
472:     max-width: var(--pipeline-card-max-width);
473:     margin: 0 auto;
474:     display: flex;
475:     flex-direction: column;
476:     gap: var(--pipeline-gap);
477: }
478: 
479: .timeline-scroll-spacer {
480:     width: 100%;
481:     height: calc(100vh - var(--topbar-height));
482:     min-height: 520px;
483: }
484: 
485: .timeline-row {
486:     display: grid;
487:     grid-template-columns: var(--pipeline-rail-column-width) minmax(0, 1fr);
488:     column-gap: var(--pipeline-rail-gap);
489:     align-items: stretch;
490: }
491: 
492: .timeline-rail-column {
493:     position: relative;
494:     display: flex;
495:     align-items: center;
496:     justify-content: center;
497:     padding: var(--pipeline-card-padding) 0;
498: }
499: 
500: .timeline-cards-column {
501:     display: flex;
502:     flex-direction: column;
503: }
504: 
505: .timeline-axis {
506:     position: absolute;
507:     left: calc((var(--pipeline-rail-column-width) / 2) - (var(--pipeline-axis-width) / 2));
508:     top: 0;
509:     bottom: 0;
510:     width: var(--pipeline-axis-width);
511:     background: color-mix(in oklab, var(--accent-primary) 45%, transparent);
512:     border-radius: 999px;
513:     opacity: 0.9;
514:     pointer-events: none;
515:     z-index: 0;
516: }
517: 
518: .timeline-step {
519:     position: relative;
520:     z-index: 2;
521:     width: 100%;
522:     max-width: var(--pipeline-card-max-width);
523:     margin: 0;
524:     border-left: none;
525:     padding: 0;
526:     border-radius: var(--pipeline-card-radius);
527:     background: color-mix(in oklab, var(--bg-card) 92%, transparent);
528:     border: 1px solid color-mix(in oklab, var(--border-color) 75%, transparent);
529:     box-shadow: 0 10px 26px rgba(0,0,0,0.22);
530:     scroll-margin-top: 0;
531:     transition:
532:         transform var(--motion-duration-slow) var(--motion-ease-standard),
533:         box-shadow var(--motion-duration-medium) var(--motion-ease-standard),
534:         background var(--motion-duration-medium) var(--motion-ease-standard),
535:         border-color var(--motion-duration-medium) var(--motion-ease-standard);
536: }
537: 
538: .timeline-step:hover:not(.active-for-log-panel) {
539:     transform: translateY(-6px) scale(1.008);
540:     box-shadow:
541:         0 20px 40px rgba(0,0,0,0.25),
542:         0 0 0 1px rgba(var(--accent-primary-rgb), 0.2);
543: }
544: 
545: .timeline-step:focus-within:not(.active-for-log-panel) {
546:     box-shadow:
547:         0 16px 34px rgba(0,0,0,0.24),
548:         0 0 0 2px rgba(var(--accent-primary-rgb), 0.26);
549:     transform: translateY(-2px);
550: }
551: 
552: .timeline-step[data-status="running"],
553: .timeline-step[data-status="starting"],
554: .timeline-step[data-status="initiated"] {
555:     background: color-mix(in oklab, var(--status-running) 12%, var(--bg-card));
556:     border-color: color-mix(in oklab, var(--status-running) 65%, var(--border-color));
557:     box-shadow: 0 10px 30px rgba(var(--status-running-rgb), 0.25);
558: }
559: 
560: .timeline-step[data-status="failed"],
561: .timeline-step[data-status="error"],
562: .timeline-step[data-status="cancelled"] {
563:     background: color-mix(in oklab, var(--status-error) 10%, var(--bg-card));
564:     border-color: color-mix(in oklab, var(--status-error) 55%, var(--border-color));
565: }
566: 
567: .timeline-step[data-status="completed"],
568: .timeline-step[data-status="success"] {
569:     background: color-mix(in oklab, var(--status-success) 10%, var(--bg-card));
570:     border-color: color-mix(in oklab, var(--status-success) 55%, var(--border-color));
571: }
572: 
573: .timeline-step[data-status="warning"],
574: .timeline-step[data-status="paused"],
575: .timeline-step[data-status="pending"] {
576:     background: color-mix(in oklab, var(--status-warning) 10%, var(--bg-card));
577:     border-color: color-mix(in oklab, var(--status-warning) 55%, var(--border-color));
578: }
579: 
580: .timeline-node {
581:     position: relative;
582:     z-index: 2;
583:     width: var(--pipeline-node-dot-size);
584:     height: var(--pipeline-node-dot-size);
585:     border-radius: 50%;
586:     background: color-mix(in oklab, var(--bg-dark) 72%, transparent);
587:     border: var(--pipeline-node-border-width) solid var(--pipeline-color-idle);
588:     box-shadow: 0 0 12px rgba(0,0,0,0.25);
589:     transition:
590:         transform var(--motion-duration-fast) var(--motion-ease-standard),
591:         background var(--motion-duration-medium) var(--motion-ease-standard),
592:         border-color var(--motion-duration-medium) var(--motion-ease-standard),
593:         box-shadow var(--motion-duration-medium) var(--motion-ease-standard);
594: }
595: 
596: .timeline-step:hover:not(.active-for-log-panel) .timeline-node {
597:     transform: scale(1.08);
598: }
599: 
600: .timeline-step.is-selected {
601:     border-color: color-mix(in oklab, var(--accent-primary) 65%, var(--border-color));
602:     box-shadow:
603:         0 16px 34px rgba(0,0,0,0.24),
604:         0 0 0 2px rgba(var(--accent-primary-rgb), 0.26);
605: }
606: 
607: .timeline-step[data-status="running"] .timeline-node,
608: .timeline-step[data-status="starting"] .timeline-node,
609: .timeline-step[data-status="initiated"] .timeline-node {
610:     border-color: var(--pipeline-color-running);
611:     box-shadow: 0 0 16px color-mix(in oklab, var(--pipeline-color-running) 45%, transparent);
612: }
613: 
614: .timeline-step[data-status="completed"] .timeline-node,
615: .timeline-step[data-status="success"] .timeline-node {
616:     border-color: var(--pipeline-color-success);
617:     box-shadow: 0 0 16px color-mix(in oklab, var(--pipeline-color-success) 45%, transparent);
618: }
619: 
620: .timeline-step[data-status="failed"] .timeline-node,
621: .timeline-step[data-status="error"] .timeline-node,
622: .timeline-step[data-status="cancelled"] .timeline-node {
623:     border-color: var(--pipeline-color-error);
624:     box-shadow: 0 0 16px color-mix(in oklab, var(--pipeline-color-error) 45%, transparent);
625: }
626: 
627: .timeline-content {
628:     display: grid;
629:     gap: 0.85rem;
630:     padding: var(--pipeline-card-padding);
631: }
632: 
633: .timeline-head {
634:     display: flex;
635:     justify-content: space-between;
636:     align-items: center;
637:     gap: 12px;
638:     padding-bottom: 10px;
639:     border-bottom: 1px solid color-mix(in oklab, var(--border-color) 80%, transparent);
640: }
641: 
642: .node-actions {
643:     display: flex;
644:     flex-wrap: wrap;
645:     align-items: center;
646:     gap: 10px;
647: }
648: 
649: .timeline-step .step-controls button,
650: .timeline-step .specific-log-button {
651:     border-radius: 999px;
652:     padding: 10px 18px;
653:     margin: 0;
654: }
655: 
656: .timeline-step .specific-log-controls-wrapper {
657:     margin-top: 4px;
658: }
659: 
660: .steps-column .step[data-status="initiated"] {
661:     animation: none !important;
662: }
663: 
664: @media (prefers-reduced-motion: reduce) {
665:     .timeline-step {
666:         transition: none !important;
667:     }
668:     .timeline-step:hover:not(.active-for-log-panel) {
669:         transform: none;
670:     }
671:     .timeline-axis {
672:         transition: none !important;
673:     }
674:     .timeline-node {
675:         transition: none !important;
676:     }
677: }
678: 
679: @media (max-width: 860px) {
680:     .workflow-pipeline {
681:         --pipeline-node-size: 68px;
682:         --pipeline-card-padding: 1.25rem;
683:         --pipeline-gap: 1.6rem;
684:         --pipeline-rail-column-width: 84px;
685:     }
686: 
687:     .timeline-step:hover:not(.active-for-log-panel) {
688:         transform: translateY(-4px) scale(1.006);
689:     }
690: }
691: 
692: @media (max-width: 720px) {
693:     .workflow-pipeline {
694:         --pipeline-rail-column-width: 72px;
695:         --pipeline-rail-gap: 0.85rem;
696:     }
697: }
698: 
699: @media (max-width: 620px) {
700:     .timeline-axis {
701:         display: none;
702:     }
703: 
704:     .timeline-step {
705:         grid-template-columns: 1fr;
706:     }
707: 
708:     .timeline-rail {
709:         flex-direction: row;
710:         justify-content: flex-start;
711:         margin-bottom: 10px;
712:         gap: 12px;
713:     }
714: 
715:     .timeline-head {
716:         flex-direction: column;
717:         align-items: flex-start;
718:     }
719: 
720:     .step-selection-control {
721:         align-self: flex-end;
722:     }
723: 
724:     .node-actions {
725:         gap: 8px;
726:         align-items: stretch;
727:     }
728: 
729:     .timeline-step .step-controls button,
730:     .timeline-step .specific-log-button {
731:         width: 100%;
732:         justify-content: center;
733:     }
734: }
735: 
736: @media (max-width: 520px) {
737:     .workflow-pipeline {
738:         --pipeline-node-size: 60px;
739:         --pipeline-card-padding: 1rem;
740:         --pipeline-gap: 1.25rem;
741:     }
742: }
743: 
744: .step-details-panel {
745:     background: color-mix(in oklab, var(--bg-card) 92%, transparent);
746:     border: 1px solid color-mix(in oklab, var(--border-color) 75%, transparent);
747:     border-radius: var(--pipeline-card-radius);
748:     box-shadow: -5px 0 18px rgba(0,0,0,0.25);
749:     overflow: hidden;
750: }
751: 
752: .step-details-header {
753:     display: flex;
754:     align-items: center;
755:     justify-content: space-between;
756:     gap: 12px;
757:     padding: 14px 14px 10px;
758:     border-bottom: 1px solid color-mix(in oklab, var(--border-color) 80%, transparent);
759: }
760: 
761: .step-details-title {
762:     font-weight: 700;
763:     letter-spacing: 0.01em;
764:     color: var(--text-primary);
765: }
766: 
767: .step-details-close {
768:     width: 38px;
769:     height: 38px;
770:     border-radius: 10px;
771:     border: 1px solid color-mix(in oklab, var(--border-color) 80%, transparent);
772:     background: color-mix(in oklab, var(--bg-tertiary) 40%, transparent);
773:     color: var(--text-primary);
774:     cursor: pointer;
775:     transition: background var(--motion-duration-medium) var(--motion-ease-standard);
776: }
777: 
778: .step-details-close:hover {
779:     background: color-mix(in oklab, var(--accent-primary) 18%, var(--bg-tertiary));
780: }
781: 
782: .step-details-body {
783:     padding: 14px;
784:     display: grid;
785:     gap: 12px;
786: }
787: 
788: .step-details-meta {
789:     display: flex;
790:     align-items: center;
791:     flex-wrap: wrap;
792:     gap: 10px;
793: }
794: 
795: .step-details-progress {
796:     background: color-mix(in oklab, var(--bg-dark) 45%, transparent);
797:     border: 1px solid color-mix(in oklab, var(--border-color) 70%, transparent);
798:     border-radius: 12px;
799:     padding: 10px;
800: }
801: 
802: .step-details-actions {
803:     display: flex;
804:     flex-direction: column;
805:     gap: 10px;
806: }
807: 
808: .step-details-actions .run-button,
809: .step-details-actions .cancel-button,
810: .step-details-actions .step-details-open-logs {
811:     width: 100%;
812:     justify-content: center;
813:     margin: 0;
814: }
815: 
816: .step-details-open-logs {
817:     background-color: color-mix(in oklab, var(--accent-primary) 60%, var(--bg-tertiary));
818:     color: white;
819: }
820: 
821: .step-details-open-logs:hover {
822:     background-color: color-mix(in oklab, var(--accent-primary) 72%, var(--bg-tertiary));
823: }
```

## File: static/css/components/workflow-buttons.css
```css
  1: /* Workflow controls within unified section */
  2: .workflow-controls {
  3:     display: flex;
  4:     justify-content: center;
  5:     gap: 15px;
  6:     flex-wrap: wrap;
  7:     width: 100%;
  8:     padding-top: 10px;
  9:     border-top: 1px solid rgba(121, 134, 203, 0.1);
 10: }
 11: 
 12: /* ===== WORKFLOW CONTROL BUTTONS STYLING ===== */
 13: #run-all-steps-button, #run-custom-sequence-button, #clear-custom-sequence-button {
 14:     background-color: var(--accent-secondary);
 15:     color: white;
 16:     padding: 15px 30px;
 17:     font-size: 1.1em;
 18:     font-weight: 500;
 19:     border-radius: 30px;
 20:     border: none;
 21:     cursor: pointer;
 22:     transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
 23:     box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
 24:     position: relative;
 25:     overflow: hidden;
 26:     min-width: 200px;
 27:     white-space: nowrap;
 28: }
 29: 
 30: /* Add subtle shine effect on hover */
 31: #run-all-steps-button::before, #run-custom-sequence-button::before, #clear-custom-sequence-button::before {
 32:     content: '';
 33:     position: absolute;
 34:     top: 0;
 35:     left: -100%;
 36:     width: 100%;
 37:     height: 100%;
 38:     background: linear-gradient(90deg, transparent, rgba(255, 255, 255, 0.2), transparent);
 39:     transition: left 0.5s ease;
 40: }
 41: 
 42: #run-all-steps-button:hover::before, #run-custom-sequence-button:hover::before, #clear-custom-sequence-button:hover::before {
 43:     left: 100%;
 44: }
 45: #run-all-steps-button:hover, #run-custom-sequence-button:hover, #clear-custom-sequence-button:hover {
 46:     transform: translateY(-3px);
 47:     box-shadow: 0 6px 20px rgba(0, 0, 0, 0.25);
 48:     filter: brightness(1.1);
 49: }
 50: 
 51: #run-all-steps-button:active, #run-custom-sequence-button:active, #clear-custom-sequence-button:active {
 52:     transform: translateY(-1px);
 53:     box-shadow: 0 2px 8px rgba(0, 0, 0, 0.2);
 54:     transition: all 0.1s ease;
 55: }
 56: /* Individual button color schemes */
 57: #run-all-steps-button {
 58:     background: linear-gradient(135deg, var(--accent-secondary) 0%, rgba(121, 134, 203, 0.9) 100%);
 59:     border: 1px solid rgba(121, 134, 203, 0.3);
 60: }
 61: 
 62: #run-custom-sequence-button {
 63:     background: linear-gradient(135deg, var(--orange) 0%, rgba(255, 152, 0, 0.9) 100%);
 64:     border: 1px solid rgba(255, 152, 0, 0.3);
 65: }
 66: 
 67: #clear-custom-sequence-button {
 68:     background: linear-gradient(135deg, var(--text-secondary) 0%, rgba(108, 117, 125, 0.9) 100%);
 69:     border: 1px solid rgba(108, 117, 125, 0.3);
 70: }
 71: 
 72: #run-all-steps-button:disabled, #run-custom-sequence-button:disabled, #clear-custom-sequence-button:disabled {
 73:     background: linear-gradient(135deg, #555 0%, #444 100%) !important;
 74:     color: #999 !important;
 75:     cursor: not-allowed !important;
 76:     box-shadow: none !important;
 77:     transform: none !important;
 78:     filter: none !important;
 79:     border: 1px solid #666 !important;
 80:     opacity: 0.6;
 81: }
 82: 
 83: .global-progress-affix {
 84:     width: min(720px, 100%);
 85:     padding: 12px 18px 10px 18px;
 86:     border-radius: 14px;
 87:     backdrop-filter: saturate(120%) blur(6px);
 88:     background: color-mix(in oklab, var(--bg-card) 88%, rgba(255,255,255,0.08));
 89:     box-shadow: 0 12px 30px rgba(0, 0, 0, 0.35);
 90:     border: 1px solid color-mix(in oklab, var(--border-color) 70%, transparent);
 91:     display: none;
 92:     flex-direction: column;
 93:     gap: 6px;
 94:     margin: 12px auto 0 auto;
 95: }
 96: 
 97: .global-progress-container {
 98:     width: 100%;
 99:     background-color: color-mix(in oklab, var(--border-color) 80%, rgba(0,0,0,0.15));
100:     border-radius: 999px;
101:     padding: 4px;
102:     display: none;
103:     box-shadow: inset 0 2px 6px rgba(0,0,0,0.25);
104: }
105: 
106: #global-progress-bar {
107:     width: 0%;
108:     height: 18px;
109:     background: linear-gradient(90deg, var(--accent-primary), var(--accent-secondary));
110:     border-radius: 999px;
111:     text-align: center;
112:     line-height: 18px;
113:     color: white;
114:     font-weight: 600;
115:     font-size: 0.85em;
116:     transition: width 0.3s var(--motion-ease-standard), background 0.3s ease, color 0.3s ease;
117: }
118: 
119: #global-progress-text {
120:     text-align: center;
121:     font-size: 0.85em;
122:     color: var(--text-secondary);
123:     display: none;
124: }
125: 
126: @media (max-width: 768px) {
127:     .global-progress-affix {
128:         width: 100%;
129:         padding: 12px 14px 10px 14px;
130:         margin: 10px 0 0 0;
131:     }
132: }
```

## File: static/css/base.css
```css
 1: html {
 2:     scroll-behavior: smooth;
 3: }
 4: 
 5: @media (prefers-reduced-motion: reduce) {
 6:     html {
 7:         scroll-behavior: auto;
 8:     }
 9: 
10:     *,
11:     *::before,
12:     *::after {
13:         transition: none !important;
14:         animation: none !important;
15:     }
16: }
17: 
18: body {
19:     font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
20:     margin: 0;
21:     background-color: var(--bg-dark);
22:     color: var(--text-primary);
23:     line-height: 1.6;
24:     display: flex;
25:     flex-direction: column;
26:     align-items: center;
27:     padding-top: calc(var(--topbar-height) + 20px);
28:     padding-left: 20px;
29:     min-height: 100vh;
30:     overflow-x: hidden;
31: }
32: 
33: :where(a, button, input, select, textarea, summary):focus-visible {
34:     outline: 2px solid color-mix(in oklab, var(--accent-primary) 45%, transparent);
35:     outline-offset: 2px;
36: }
37: 
38: button:disabled,
39: input[type="button"]:disabled,
40: input[type="submit"]:disabled,
41: input[type="checkbox"]:disabled,
42: input[type="radio"]:disabled {
43:     opacity: 0.5;
44:     cursor: not-allowed;
45:     box-shadow: none !important;
46:     filter: saturate(85%);
47: }
```

## File: static/css/variables.css
```css
 1: :root {
 2:     /* Backgrounds */
 3:     --bg-dark: #1e1e2f;
 4:     --bg-card: #2c2c3e;
 5:     --bg-secondary: #2c2c3e; /* Secondary background for widgets */
 6:     --bg-tertiary: #3a3a4e; /* Tertiary background for disabled elements */
 7:     --bg-hover: rgba(121, 134, 203, 0.1);
 8:     --log-bg: #161625;
 9: 
10:     /* Text */
11:     --text-primary: #e0e0e0;
12:     --text-secondary: #a0a0b0;
13:     --text-muted: #707080; /* Muted text for disabled states */
14:     --text-bright: #f0f0f0;
15:     --log-text: #c0c0d0;
16:     --font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
17: 
18:     /* UI Elements */
19:     --border-color: #39394d;
20:     --border-color-translucent: #39394d88;
21:     --border-bright: #4a4a5e;
22:     --touch-target-min: 44px;
23:     --button-compact-padding: 12px;
24:     --topbar-height: 68px;
25: 
26:     --motion-duration-fast: 120ms;
27:     --motion-duration-medium: 220ms;
28:     --motion-duration-slow: 420ms;
29:     --motion-ease-standard: cubic-bezier(0.4, 0, 0.2, 1);
30:     --motion-ease-emphasized: cubic-bezier(0.2, 0.8, 0.2, 1);
31: 
32:     /* Standard Colors */
33:     --accent-primary: #7986cb;
34:     --accent-secondary: #ff8a65;
35:     --green: #66bb6a;
36:     --red: #ef5350;
37:     --yellow: #ffee58;
38:     --blue: #42a5f5;
39:     --orange: #ffb74d;
40: 
41:     /* Status Colors (Hex) */
42:     --status-running: #4dabf7;
43:     --status-success: #4caf50;
44:     --status-error: #e53935;
45:     --status-warning: #ff9800;
46:     --status-idle: #9e9e9e;
47: 
48:     /* RGB Values (for rgba usage) */
49:     --accent-primary-rgb: 121 134 203;
50:     --accent-secondary-rgb: 255 138 101;
51:     --status-running-rgb: 77 171 247;
52:     --status-success-rgb: 76 175 80;
53:     --status-error-rgb: 229 57 53;
54:     --status-warning-rgb: 255 152 0;
55:     --status-idle-rgb: 158 158 158;
56: 
57:     /* Pipeline Visualization Variables */
58:     --pipeline-node-size: 80px;
59:     --pipeline-connector-width: 3px;
60:     --pipeline-gap: 2rem;
61:     --pipeline-card-radius: 20px;
62:     --pipeline-card-padding: 1.5rem;
63:     --pipeline-card-max-width: 980px;
64:     --pipeline-node-dot-size: 18px;
65:     --pipeline-node-border-width: 3px;
66:     --pipeline-rail-column-width: 110px;
67:     --pipeline-rail-gap: 1.25rem;
68:     --pipeline-axis-width: 4px;
69: 
70:     /* Pipeline Status Colors mapped to Status Colors */
71:     --pipeline-color-idle: var(--status-idle);
72:     --pipeline-color-running: var(--status-running);
73:     --pipeline-color-success: var(--status-success);
74:     --pipeline-color-error: var(--status-error);
75: }
```

## File: static/state/AppState.js
```javascript
  1: class AppState {
  2:     constructor() {
  3:         this.state = {
  4:             pollingIntervals: {},
  5:             
  6:             activeStepKeyForLogsPanel: null,
  7:             isAnySequenceRunning: false,
  8:             focusedElementBeforePopup: null,
  9:             ui: {
 10:                 compactMode: false
 11:             },
 12:             
 13:             stepTimers: {},
 14:             selectedStepsOrder: [],
 15:             
 16:             processInfo: {},
 17:             
 18:             performanceMetrics: {
 19:                 apiResponseTimes: [],
 20:                 errorCounts: {},
 21:                 lastUpdate: null
 22:             },
 23:             
 24:             cacheStats: {
 25:                 hits: 0,
 26:                 misses: 0,
 27:                 hitRate: 0
 28:             }
 29:         };
 30:         
 31:         this.listeners = new Set();
 32:         this.isDestroyed = false;
 33:         
 34:         this.stateChangeCount = 0;
 35:         this.lastStateChange = Date.now();
 36:         
 37:         console.debug('[AppState] Initialized with immutable state management');
 38:     }
 39:     
 40:     getState() {
 41:         if (this.isDestroyed) {
 42:             console.warn('[AppState] Attempted to access destroyed state');
 43:             return {};
 44:         }
 45:         return this._deepClone(this.state);
 46:     }
 47:     
 48:     getStateProperty(path) {
 49:         if (this.isDestroyed) return undefined;
 50:         
 51:         return path.split('.').reduce((obj, key) => {
 52:             return obj && obj[key] !== undefined ? obj[key] : undefined;
 53:         }, this.state);
 54:     }
 55:     
 56:     setState(updates, source = 'unknown') {
 57:         if (this.isDestroyed) {
 58:             console.warn('[AppState] Attempted to update destroyed state');
 59:             return;
 60:         }
 61:         
 62:         const oldState = this._deepClone(this.state);
 63:         const newState = this._mergeDeep(this.state, updates);
 64:         
 65:         if (this._stateChanged(oldState, newState)) {
 66:             this.state = newState;
 67:             this.stateChangeCount++;
 68:             this.lastStateChange = Date.now();
 69:             
 70:             console.debug(`[AppState] State updated from ${source}:`, updates);
 71:             
 72:             this._notifyListeners(newState, oldState, source);
 73:         }
 74:     }
 75:     
 76:     subscribe(listener) {
 77:         if (typeof listener !== 'function') {
 78:             throw new Error('[AppState] Listener must be a function');
 79:         }
 80:         
 81:         this.listeners.add(listener);
 82:         
 83:         return () => {
 84:             this.listeners.delete(listener);
 85:         };
 86:     }
 87:     
 88:     /**
 89:      * Subscribe to specific state property changes.
 90:      * @param {string} path - Dot-notation path to property
 91:      * @param {Function} listener - Callback function (newValue, oldValue) => void
 92:      * @returns {Function} Unsubscribe function
 93:      */
 94:     subscribeToProperty(path, listener) {
 95:         const propertyListener = (newState, oldState) => {
 96:             const newValue = this._getPropertyByPath(newState, path);
 97:             const oldValue = this._getPropertyByPath(oldState, path);
 98:             
 99:             if (newValue !== oldValue) {
100:                 listener(newValue, oldValue);
101:             }
102:         };
103:         
104:         return this.subscribe(propertyListener);
105:     }
106:     
107:     batchUpdate(updateFn, source = 'batch') {
108:         const originalNotify = this._notifyListeners;
109:         const updates = [];
110:         
111:         this._notifyListeners = (newState, oldState, updateSource) => {
112:             updates.push({ newState, oldState, source: updateSource });
113:         };
114:         
115:         try {
116:             updateFn();
117:         } finally {
118:             this._notifyListeners = originalNotify;
119:             
120:             if (updates.length > 0) {
121:                 const finalUpdate = updates[updates.length - 1];
122:                 this._notifyListeners(finalUpdate.newState, updates[0].oldState, source);
123:             }
124:         }
125:     }
126:     
127:     reset() {
128:         const initialState = {
129:             pollingIntervals: {},
130:             activeStepKeyForLogsPanel: null,
131:             isAnySequenceRunning: false,
132:             focusedElementBeforePopup: null,
133:             ui: {
134:                 compactMode: false
135:             },
136:             stepTimers: {},
137:             selectedStepsOrder: [],
138: 
139:             processInfo: {},
140:             performanceMetrics: {
141:                 apiResponseTimes: [],
142:                 errorCounts: {},
143:                 lastUpdate: null
144:             },
145:             cacheStats: {
146:                 hits: 0,
147:                 misses: 0,
148:                 hitRate: 0
149:             }
150:         };
151:         
152:         this.setState(initialState, 'reset');
153:         console.info('[AppState] State reset to initial values');
154:     }
155:     
156:     getStats() {
157:         return {
158:             listenerCount: this.listeners.size,
159:             stateChangeCount: this.stateChangeCount,
160:             lastStateChange: this.lastStateChange,
161:             isDestroyed: this.isDestroyed,
162:             stateSize: JSON.stringify(this.state).length
163:         };
164:     }
165:     
166:     destroy() {
167:         console.info('[AppState] Destroying state manager');
168:         
169:         this.listeners.clear();
170:         this.state = {};
171:         this.isDestroyed = true;
172:     }
173:     
174:     _deepClone(obj) {
175:         if (typeof structuredClone === 'function') {
176:             try {
177:                 return structuredClone(obj);
178:             } catch (error) {
179:                 console.warn('[AppState] structuredClone failed, falling back to manual clone:', error);
180:             }
181:         }
182: 
183:         if (obj === null || typeof obj !== 'object') {
184:             return obj;
185:         }
186: 
187:         if (obj instanceof Date) {
188:             return new Date(obj.getTime());
189:         }
190: 
191:         if (Array.isArray(obj)) {
192:             return obj.map(item => this._deepClone(item));
193:         }
194: 
195:         const cloned = {};
196:         for (const key in obj) {
197:             if (Object.prototype.hasOwnProperty.call(obj, key)) {
198:                 cloned[key] = this._deepClone(obj[key]);
199:             }
200:         }
201:         return cloned;
202:     }
203:     
204:     _mergeDeep(target, source) {
205:         const result = this._deepClone(target);
206:         
207:         for (const key in source) {
208:             if (source.hasOwnProperty(key)) {
209:                 if (source[key] && typeof source[key] === 'object' && !Array.isArray(source[key])) {
210:                     result[key] = this._mergeDeep(result[key] || {}, source[key]);
211:                 } else {
212:                     result[key] = source[key];
213:                 }
214:             }
215:         }
216:         
217:         return result;
218:     }
219:     
220:     _stateChanged(oldState, newState) {
221:         return !this._areValuesEqual(oldState, newState);
222:     }
223: 
224:     _areValuesEqual(a, b, visited = new WeakMap()) {
225:         if (Object.is(a, b)) {
226:             return true;
227:         }
228: 
229:         if (typeof a !== typeof b) {
230:             return false;
231:         }
232: 
233:         if (a === null || b === null) {
234:             return false;
235:         }
236: 
237:         if (typeof a !== 'object') {
238:             return false;
239:         }
240: 
241:         if (visited.has(a) && visited.get(a) === b) {
242:             return true;
243:         }
244:         visited.set(a, b);
245: 
246:         const aKeys = Object.keys(a);
247:         const bKeys = Object.keys(b);
248:         if (aKeys.length !== bKeys.length) {
249:             return false;
250:         }
251: 
252:         for (const key of aKeys) {
253:             if (!Object.prototype.hasOwnProperty.call(b, key)) {
254:                 return false;
255:             }
256:             if (!this._areValuesEqual(a[key], b[key], visited)) {
257:                 return false;
258:             }
259:         }
260: 
261:         return true;
262:     }
263:     
264:     _getPropertyByPath(obj, path) {
265:         return path.split('.').reduce((current, key) => {
266:             return current && current[key] !== undefined ? current[key] : undefined;
267:         }, obj);
268:     }
269:     
270:     _notifyListeners(newState, oldState, source) {
271:         this.listeners.forEach(listener => {
272:             try {
273:                 listener(newState, oldState, source);
274:             } catch (error) {
275:                 console.error('[AppState] Listener error:', error);
276:             }
277:         });
278:     }
279: }
280: 
281: export const appState = new AppState();
282: 
283: window.addEventListener('beforeunload', () => {
284:     appState.destroy();
285: });
286: 
287: if (window.location.hostname === 'localhost' || window.location.hostname === '127.0.0.1') {
288:     window.appState = appState;
289:     
290:     appState.subscribe((newState, oldState, source) => {
291:         console.debug(`[AppState] Change from ${source}:`, {
292:             newState: newState,
293:             oldState: oldState
294:         });
295:     });
296: }
297: 
298: export default appState;
```

## File: static/utils/PollingManager.js
```javascript
  1: /**
  2:  * Centralized polling management utility for workflow_mediapipe frontend.
  3:  * 
  4:  * This module provides safe interval management with automatic cleanup
  5:  * to prevent memory leaks and ensure proper resource management.
  6:  */
  7: 
  8: class PollingManager {
  9:     /**
 10:      * Initialize the polling manager.
 11:      */
 12:     constructor() {
 13:         this.intervals = new Map();
 14:         this.timeouts = new Map();
 15:         this.isDestroyed = false;
 16:         this.pendingResumes = new Map();
 17:         this.errorCounts = new Map();
 18:         this.maxErrorCount = 5;
 19:         
 20:         this._bindCleanupEvents();
 21:         
 22:         console.debug('PollingManager initialized');
 23:     }
 24: 
 25:     /**
 26:      * Start polling with a given callback function.
 27:      * 
 28:      * @param {string} name - Unique name for this polling operation
 29:      * @param {Function} callback - Function to call on each interval
 30:      * @param {number} interval - Interval in milliseconds
 31:      * @param {Object} options - Additional options
 32:      * @param {boolean} options.immediate - Whether to call callback immediately
 33:      * @param {number} options.maxErrors - Maximum consecutive errors before stopping
 34:      * @returns {string|null} - Polling ID or null if manager is destroyed
 35:      */
 36:     startPolling(name, callback, interval, options = {}) {
 37:         if (this.isDestroyed) {
 38:             console.warn('PollingManager: Cannot start polling, manager is destroyed');
 39:             return null;
 40:         }
 41: 
 42:         this.stopPolling(name);
 43: 
 44:         const {
 45:             immediate = false,
 46:             maxErrors = this.maxErrorCount
 47:         } = options;
 48: 
 49:         this.errorCounts.set(name, 0);
 50: 
 51:         const wrappedCallback = async () => {
 52:             if (this.isDestroyed) {
 53:                 return;
 54:             }
 55: 
 56:             try {
 57:                 const result = await callback();
 58:                 this.errorCounts.set(name, 0);
 59: 
 60:                 if (typeof result === 'number' && result > 0) {
 61:                     const existing = this.intervals.get(name);
 62:                     if (existing) {
 63:                         clearInterval(existing.id);
 64:                         this.intervals.delete(name);
 65:                     }
 66:                     if (this.pendingResumes.has(name)) {
 67:                         clearTimeout(this.pendingResumes.get(name));
 68:                     }
 69:                     const resumeId = setTimeout(() => {
 70:                         if (!this.isDestroyed && !this.intervals.has(name)) {
 71:                             const newIntervalId = setInterval(wrappedCallback, interval);
 72:                             this.intervals.set(name, {
 73:                                 id: newIntervalId,
 74:                                 callback: wrappedCallback,
 75:                                 interval: interval,
 76:                                 startTime: Date.now()
 77:                             });
 78:                             this.pendingResumes.delete(name);
 79:                             console.debug(`Resumed polling: ${name} after ${result}ms backoff`);
 80:                         }
 81:                     }, result);
 82:                     this.pendingResumes.set(name, resumeId);
 83:                     return;
 84:                 }
 85:             } catch (error) {
 86:                 const errorCount = (this.errorCounts.get(name) || 0) + 1;
 87:                 this.errorCounts.set(name, errorCount);
 88:                 
 89:                 console.error(`Polling error in ${name} (attempt ${errorCount}):`, error);
 90:                 
 91:                 if (errorCount >= maxErrors) {
 92:                     console.error(`Stopping polling ${name} due to ${errorCount} consecutive errors`);
 93:                     this.stopPolling(name);
 94:                     
 95:                     this._dispatchPollingError(name, error, errorCount);
 96:                 }
 97:             }
 98:         };
 99: 
100:         if (immediate) {
101:             wrappedCallback();
102:         }
103: 
104:         const intervalId = setInterval(wrappedCallback, interval);
105:         this.intervals.set(name, {
106:             id: intervalId,
107:             callback: wrappedCallback,
108:             interval: interval,
109:             startTime: Date.now()
110:         });
111: 
112:         console.debug(`Started polling: ${name} (interval: ${interval}ms)`);
113:         return name;
114:     }
115: 
116:     /**
117:      * Stop a specific polling operation.
118:      * 
119:      * @param {string} name - Name of the polling operation to stop
120:      * @returns {boolean} - True if polling was stopped, false if not found
121:      */
122:     stopPolling(name) {
123:         const pollingInfo = this.intervals.get(name);
124:         if (pollingInfo) {
125:             clearInterval(pollingInfo.id);
126:             this.intervals.delete(name);
127:             this.errorCounts.delete(name);
128:             if (this.pendingResumes.has(name)) {
129:                 clearTimeout(this.pendingResumes.get(name));
130:                 this.pendingResumes.delete(name);
131:             }
132:             
133:             const duration = Date.now() - pollingInfo.startTime;
134:             console.debug(`Stopped polling: ${name} (ran for ${duration}ms)`);
135:             return true;
136:         }
137:         return false;
138:     }
139: 
140:     /**
141:      * Schedule a one-time delayed execution.
142:      * 
143:      * @param {string} name - Unique name for this timeout
144:      * @param {Function} callback - Function to call after delay
145:      * @param {number} delay - Delay in milliseconds
146:      * @returns {string|null} - Timeout ID or null if manager is destroyed
147:      */
148:     setTimeout(name, callback, delay) {
149:         if (this.isDestroyed) {
150:             console.warn('PollingManager: Cannot set timeout, manager is destroyed');
151:             return null;
152:         }
153: 
154:         this.clearTimeout(name);
155: 
156:         const wrappedCallback = () => {
157:             if (this.isDestroyed) {
158:                 return;
159:             }
160: 
161:             try {
162:                 callback();
163:             } catch (error) {
164:                 console.error(`Timeout callback error in ${name}:`, error);
165:             } finally {
166:                 this.timeouts.delete(name);
167:             }
168:         };
169: 
170:         const timeoutId = setTimeout(wrappedCallback, delay);
171:         this.timeouts.set(name, {
172:             id: timeoutId,
173:             callback: wrappedCallback,
174:             delay: delay,
175:             startTime: Date.now()
176:         });
177: 
178:         console.debug(`Set timeout: ${name} (delay: ${delay}ms)`);
179:         return name;
180:     }
181: 
182:     /**
183:      * Clear a specific timeout.
184:      * 
185:      * @param {string} name - Name of the timeout to clear
186:      * @returns {boolean} - True if timeout was cleared, false if not found
187:      */
188:     clearTimeout(name) {
189:         const timeoutInfo = this.timeouts.get(name);
190:         if (timeoutInfo) {
191:             clearTimeout(timeoutInfo.id);
192:             this.timeouts.delete(name);
193:             
194:             console.debug(`Cleared timeout: ${name}`);
195:             return true;
196:         }
197:         return false;
198:     }
199: 
200:     /**
201:      * Get information about active polling operations.
202:      * 
203:      * @returns {Object} - Information about active operations
204:      */
205:     getActiveOperations() {
206:         const now = Date.now();
207:         
208:         return {
209:             intervals: Array.from(this.intervals.entries()).map(([name, info]) => ({
210:                 name,
211:                 interval: info.interval,
212:                 runningTime: now - info.startTime,
213:                 errorCount: this.errorCounts.get(name) || 0
214:             })),
215:             timeouts: Array.from(this.timeouts.entries()).map(([name, info]) => ({
216:                 name,
217:                 delay: info.delay,
218:                 timeRemaining: Math.max(0, (info.startTime + info.delay) - now)
219:             })),
220:             totalIntervals: this.intervals.size,
221:             totalTimeouts: this.timeouts.size
222:         };
223:     }
224: 
225:     /**
226:      * Get summarized statistics for monitoring consumers (PerformanceMonitor).
227:      *
228:      * @returns {Object}
229:      */
230:     getStats() {
231:         const ops = this.getActiveOperations();
232:         return {
233:             totalIntervals: ops.totalIntervals,
234:             totalTimeouts: ops.totalTimeouts,
235:             intervals: ops.intervals,
236:             timeouts: ops.timeouts
237:         };
238:     }
239: 
240:     /**
241:      * Destroy the polling manager and clean up all resources.
242:      */
243:     destroy() {
244:         if (this.isDestroyed) {
245:             return;
246:         }
247: 
248:         console.debug('Destroying PollingManager...');
249: 
250:         this.intervals.forEach((info, name) => {
251:             clearInterval(info.id);
252:             console.debug(`Cleaned up interval: ${name}`);
253:         });
254:         this.intervals.clear();
255: 
256:         this.timeouts.forEach((info, name) => {
257:             clearTimeout(info.id);
258:             console.debug(`Cleaned up timeout: ${name}`);
259:         });
260:         this.timeouts.clear();
261: 
262:         this.errorCounts.clear();
263: 
264:         this.pendingResumes.forEach((timeoutId, name) => {
265:             clearTimeout(timeoutId);
266:             console.debug(`Cleared pending resume: ${name}`);
267:         });
268:         this.pendingResumes.clear();
269: 
270:         this.isDestroyed = true;
271:         console.debug('PollingManager destroyed');
272:     }
273: 
274:     /**
275:      * Bind cleanup events to prevent memory leaks.
276:      * @private
277:      */
278:     _bindCleanupEvents() {
279:         window.addEventListener('beforeunload', () => {
280:             this.destroy();
281:         });
282: 
283:         window.addEventListener('pagehide', () => {
284:             this.destroy();
285:         });
286: 
287:         document.addEventListener('visibilitychange', () => {
288:             if (document.hidden) {
289:                 console.debug('Page hidden, polling continues in background');
290:             }
291:         });
292:     }
293: 
294:     /**
295:      * Dispatch a custom event for polling errors.
296:      * @private
297:      */
298:     _dispatchPollingError(name, error, errorCount) {
299:         const event = new CustomEvent('pollingError', {
300:             detail: {
301:                 name,
302:                 error,
303:                 errorCount,
304:                 timestamp: new Date().toISOString()
305:             }
306:         });
307:         
308:         window.dispatchEvent(event);
309:     }
310: }
311: 
312: // Create and export global polling manager instance
313: const pollingManager = new PollingManager();
314: 
315: // Export for use in other modules
316: export { PollingManager, pollingManager };
317: 
318: window.pollingManager = pollingManager;
```

## File: static/scrollManager.js
```javascript
  1: /**
  2:  * Scroll Manager Module
  3:  * Handles automatic scrolling to active workflow steps with smooth animations
  4:  * and intelligent viewport positioning.
  5:  */
  6: 
  7: import * as dom from './domElements.js';
  8: 
  9: 
 10: const SCROLL_CONFIG = {
 11:     behavior: 'smooth',
 12:     block: 'center',
 13:     inline: 'nearest',
 14:     topOffset: 100,
 15:     minScrollDistance: 50,
 16:     scrollDelay: 150,
 17:     topbarHeight: 68, // Hauteur de la topbar depuis variables.css
 18:     bottomMargin: 40   // Marge inférieure pour éviter le débordement
 19: };
 20: 
 21: /**
 22:  * Checks if an element is currently visible in the viewport
 23:  * @param {HTMLElement} element - The element to check
 24:  * @returns {boolean} True if element is fully or partially visible
 25:  */
 26: function isElementInViewport(element) {
 27:     if (!element) return false;
 28:     
 29:     const rect = element.getBoundingClientRect();
 30:     const windowHeight = window.innerHeight || document.documentElement.clientHeight;
 31:     const windowWidth = window.innerWidth || document.documentElement.clientWidth;
 32:     
 33:     return (
 34:         rect.top >= 0 &&
 35:         rect.left >= 0 &&
 36:         rect.bottom <= windowHeight &&
 37:         rect.right <= windowWidth
 38:     );
 39: }
 40: 
 41: /**
 42:  * Checks if an element is partially visible in the viewport
 43:  * @param {HTMLElement} element - The element to check
 44:  * @returns {boolean} True if element is at least partially visible
 45:  */
 46: function isElementPartiallyVisible(element) {
 47:     if (!element) return false;
 48:     
 49:     const rect = element.getBoundingClientRect();
 50:     const windowHeight = window.innerHeight || document.documentElement.clientHeight;
 51:     const windowWidth = window.innerWidth || document.documentElement.clientWidth;
 52:     
 53:     return (
 54:         rect.bottom > 0 &&
 55:         rect.right > 0 &&
 56:         rect.top < windowHeight &&
 57:         rect.left < windowWidth
 58:     );
 59: }
 60: 
 61: /**
 62:  * Calculates the optimal scroll position for an element with perfect centering
 63:  * @param {HTMLElement} element - The target element
 64:  * @returns {number} The optimal scroll top position
 65:  */
 66: function calculateOptimalScrollPosition(element) {
 67:     if (!element) return 0;
 68:     
 69:     const rect = element.getBoundingClientRect();
 70:     const currentScrollTop = window.pageYOffset || document.documentElement.scrollTop;
 71:     const windowHeight = window.innerHeight || document.documentElement.clientHeight;
 72:     
 73:     // Zone visible effective : topbar + margin inférieure
 74:     const effectiveViewportHeight = windowHeight - SCROLL_CONFIG.topbarHeight - SCROLL_CONFIG.bottomMargin;
 75:     const elementTop = rect.top + currentScrollTop;
 76:     const elementHeight = rect.height;
 77:     
 78:     // Centrage agressif dans la zone effective (pas de contraintes min/max)
 79:     const viewportCenter = SCROLL_CONFIG.topbarHeight + (effectiveViewportHeight / 2);
 80:     const targetScrollTop = elementTop + (elementHeight / 2) - viewportCenter;
 81:     
 82:     // Contrainte simple : ne pas aller en négatif
 83:     const finalScrollTop = Math.max(0, targetScrollTop);
 84:     
 85:     console.log('[SCROLL] Position calculation:', {
 86:         elementTop,
 87:         elementHeight,
 88:         windowHeight,
 89:         effectiveViewportHeight,
 90:         viewportCenter,
 91:         targetScrollTop,
 92:         finalScrollTop
 93:     });
 94:     
 95:     return finalScrollTop;
 96: }
 97: 
 98: /**
 99:  * Determines if scrolling is necessary based on current element visibility
100:  * @param {HTMLElement} element - The target element
101:  * @returns {boolean} True if scrolling should be performed
102:  */
103: function shouldScroll(element) {
104:     if (!element) return false;
105:     
106:     // Pour les séquences, toujours autoriser le scroll pour garantir le repositionnement
107:     console.log('[SCROLL] shouldScroll: allowing scroll for sequence positioning');
108:     return true;
109: }
110: 
111: /**
112:  * Smoothly scrolls to bring the target element into optimal view
113:  * @param {HTMLElement} element - The element to scroll to
114:  * @param {Object} options - Additional scroll options
115:  */
116: function scrollToElement(element, options = {}) {
117:     if (!element) {
118:         console.warn('[SCROLL] No element provided for scrolling');
119:         return;
120:     }
121:     
122:     const config = { ...SCROLL_CONFIG, ...options };
123:     
124:     if (!shouldScroll(element)) {
125:         return;
126:     }
127:     
128:     console.log(`[SCROLL] Scrolling to element: ${element.id || element.className}`);
129: 
130:     const behavior = config.behavior === 'smooth' ? 'smooth' : 'auto';
131:     const targetScrollTop = calculateOptimalScrollPosition(element);
132:     window.scrollTo({
133:         top: targetScrollTop,
134:         behavior
135:     });
136: }
137: 
138: /**
139:  * Scrolls to the active workflow step with a delay to allow UI transitions
140:  * @param {string} stepKey - The key of the step to scroll to
141:  * @param {Object} options - Additional options for scrolling
142:  */
143: export function scrollToActiveStep(stepKey, options = {}) {
144:     if (!stepKey) {
145:         console.warn('[SCROLL] No stepKey provided for scrollToActiveStep');
146:         return;
147:     }
148:     
149:     const stepElement = document.getElementById(`step-${stepKey}`);
150:     if (!stepElement) {
151:         console.warn(`[SCROLL] Step element not found: step-${stepKey}`);
152:         return;
153:     }
154:     
155:     const config = { ...SCROLL_CONFIG, ...options };
156:     
157:     setTimeout(() => {
158:         scrollToElement(stepElement, config);
159:     }, config.scrollDelay);
160: }
161: 
162: /**
163:  * Scrolls to a step immediately without delay (for manual triggers)
164:  * @param {string} stepKey - The key of the step to scroll to
165:  * @param {Object} options - Additional options for scrolling
166:  */
167: export function scrollToStepImmediate(stepKey, options = {}) {
168:     if (!stepKey) return;
169:     
170:     const stepElement = document.getElementById(`step-${stepKey}`);
171:     if (!stepElement) return;
172:     
173:     scrollToElement(stepElement, { ...SCROLL_CONFIG, ...options });
174: }
175: 
176: /**
177:  * Scrolls to a step with forced repositioning for sequences (ignores current position)
178:  * @param {string} stepKey - The key of the step to scroll to
179:  * @param {Object} options - Additional options for scrolling
180:  */
181: export function scrollToStepForced(stepKey, options = {}) {
182:     if (!stepKey) {
183:         console.warn('[SCROLL] No stepKey provided for scrollToStepForced');
184:         return;
185:     }
186:     
187:     const stepElement = document.getElementById(`step-${stepKey}`);
188:     if (!stepElement) {
189:         console.warn(`[SCROLL] Step element not found: step-${stepKey}`);
190:         return;
191:     }
192:     
193:     const config = { ...SCROLL_CONFIG, ...options };
194:     
195:     // Forcer le scroll avec scrollIntoView direct (plus efficace pour les grids)
196:     console.log(`[SCROLL] Forced scrolling to step: ${stepKey}`);
197:     
198:     try {
199:         // Utiliser scrollIntoView avec block 'center' pour forcer le positionnement
200:         stepElement.scrollIntoView({
201:             behavior: config.behavior,
202:             block: 'center',
203:             inline: 'nearest'
204:         });
205:         
206:         console.log(`[SCROLL] Applied scrollIntoView with block: center`);
207:         
208:         // Backup : forcer avec window.scrollTo si scrollIntoView ne fonctionne pas
209:         setTimeout(() => {
210:             const rect = stepElement.getBoundingClientRect();
211:             const currentScrollTop = window.pageYOffset || document.documentElement.scrollTop;
212:             const windowHeight = window.innerHeight || document.documentElement.clientHeight;
213:             
214:             // Calcul simple : centrer l'élément dans le viewport
215:             const elementCenter = rect.top + currentScrollTop + (rect.height / 2);
216:             const viewportCenter = windowHeight / 2;
217:             const targetScrollTop = elementCenter - viewportCenter;
218:             
219:             console.log(`[SCROLL] Backup scroll calculation:`, {
220:                 rectTop: rect.top,
221:                 currentScrollTop,
222:                 windowHeight,
223:                 elementCenter,
224:                 viewportCenter,
225:                 targetScrollTop
226:             });
227:             
228:             window.scrollTo({
229:                 top: Math.max(0, targetScrollTop),
230:                 behavior: 'instant' // instant pour le backup
231:             });
232:         }, 50);
233:         
234:     } catch (error) {
235:         console.warn('[SCROLL] scrollIntoView failed, using manual calculation:', error);
236:         
237:         // Fallback manuel
238:         const optimalScrollTop = calculateOptimalScrollPosition(stepElement);
239:         window.scrollTo({
240:             top: optimalScrollTop,
241:             behavior: config.behavior
242:         });
243:     }
244: }
245: 
246: /**
247:  * Checks if auto-scroll should be enabled based on user preferences and context
248:  * @returns {boolean} True if auto-scroll should be active
249:  */
250: export function isAutoScrollEnabled() {
251:     const userPreference = localStorage.getItem('workflow-auto-scroll');
252:     if (userPreference === 'disabled') {
253:         return false;
254:     }
255: 
256:     if (userPreference === 'enabled') {
257:         return true;
258:     }
259: 
260:     const isLogsActive = dom.workflowWrapper && dom.workflowWrapper.classList.contains('logs-active');
261:     return isLogsActive;
262: }
263: 
264: /**
265:  * Enables or disables auto-scroll functionality
266:  * @param {boolean} enabled - Whether to enable auto-scroll
267:  */
268: export function setAutoScrollEnabled(enabled) {
269:     localStorage.setItem('workflow-auto-scroll', enabled ? 'enabled' : 'disabled');
270:     console.log(`[SCROLL] Auto-scroll ${enabled ? 'enabled' : 'disabled'}`);
271: }
272: 
273: /**
274:  * Checks if auto-scroll for sequences should be enabled based on user preferences
275:  * @returns {boolean} True if sequence auto-scroll should be active
276:  */
277: export function isSequenceAutoScrollEnabled() {
278:     const sequencePreference = localStorage.getItem('workflow-sequence-auto-scroll');
279:     if (sequencePreference === 'disabled') {
280:         return false;
281:     }
282:     
283:     if (sequencePreference === 'enabled') {
284:         return true;
285:     }
286:     
287:     // Par défaut, activer l'auto-scroll pour les séquences
288:     return true;
289: }
290: 
291: /**
292:  * Enables or disables auto-scroll for sequences specifically
293:  * @param {boolean} enabled - Whether to enable sequence auto-scroll
294:  */
295: export function setSequenceAutoScrollEnabled(enabled) {
296:     localStorage.setItem('workflow-sequence-auto-scroll', enabled ? 'enabled' : 'disabled');
297:     console.log(`[SCROLL] Sequence auto-scroll ${enabled ? 'enabled' : 'disabled'}`);
298: }
299: 
300: /**
301:  * Scrolls to a step with ultra-aggressive repositioning (instant scroll)
302:  * @param {string} stepKey - The key of the step to scroll to
303:  * @param {Object} options - Additional options for scrolling
304:  */
305: export function scrollToStepUltraAggressive(stepKey, options = {}) {
306:     if (!stepKey) {
307:         console.warn('[SCROLL] No stepKey provided for scrollToStepUltraAggressive');
308:         return;
309:     }
310:     
311:     const stepElement = document.getElementById(`step-${stepKey}`);
312:     if (!stepElement) {
313:         console.warn(`[SCROLL] Step element not found: step-${stepKey}`);
314:         return;
315:     }
316:     
317:     console.log(`[SCROLL] Ultra-aggressive scrolling to step: ${stepKey}`);
318:     
319:     // Scroll instantané avec scrollIntoView
320:     stepElement.scrollIntoView({
321:         behavior: 'instant',
322:         block: 'center',
323:         inline: 'nearest'
324:     });
325:     
326:     // Forcer un second scroll immédiat après
327:     setTimeout(() => {
328:         const rect = stepElement.getBoundingClientRect();
329:         const windowHeight = window.innerHeight || document.documentElement.clientHeight;
330:         const currentScrollTop = window.pageYOffset || document.documentElement.scrollTop;
331:         
332:         // Calcul ultra-simple : centrer parfaitement
333:         const elementCenter = rect.top + currentScrollTop + (rect.height / 2);
334:         const viewportCenter = (windowHeight / 2);
335:         const targetScrollTop = elementCenter - viewportCenter;
336:         
337:         console.log(`[SCROLL] Ultra-aggressive calculation:`, {
338:             rectTop: rect.top,
339:             rectHeight: rect.height,
340:             elementCenter,
341:             viewportCenter,
342:             targetScrollTop
343:         });
344:         
345:         window.scrollTo({
346:             top: Math.max(0, targetScrollTop),
347:             behavior: 'instant'
348:         });
349:     }, 10);
350: }
351: 
352: /**
353:  * Scrolls to a step with absolute forced repositioning (ignores all CSS and layout factors)
354:  * @param {string} stepKey - The key of the step to scroll to
355:  * @param {Object} options - Additional options for scrolling
356:  */
357: export function scrollToStepAbsolute(stepKey, options = {}) {
358:     if (!stepKey) {
359:         console.warn('[SCROLL] No stepKey provided for scrollToStepAbsolute');
360:         return;
361:     }
362:     
363:     const stepElement = document.getElementById(`step-${stepKey}`);
364:     if (!stepElement) {
365:         console.warn(`[SCROLL] Step element not found: step-${stepKey}`);
366:         return;
367:     }
368:     
369:     console.log(`[SCROLL] Absolute forced scrolling to step: ${stepKey}`);
370:     
371:     // Forcer un scroll absolu en calculant la position exacte
372:     const rect = stepElement.getBoundingClientRect();
373:     const currentScrollTop = window.pageYOffset || document.documentElement.scrollTop;
374:     const windowHeight = window.innerHeight || document.documentElement.clientHeight;
375:     
376:     // Calcul absolu : centrer l'élément parfaitement au milieu du viewport
377:     const elementAbsoluteTop = rect.top + currentScrollTop;
378:     const elementCenter = elementAbsoluteTop + (rect.height / 2);
379:     const viewportCenter = windowHeight / 2;
380:     const absoluteScrollTop = elementCenter - viewportCenter;
381:     
382:     console.log(`[SCROLL] Absolute calculation:`, {
383:         rectTop: rect.top,
384:         rectHeight: rect.height,
385:         currentScrollTop,
386:         elementAbsoluteTop,
387:         elementCenter,
388:         viewportCenter,
389:         absoluteScrollTop
390:     });
391:     
392:     // Appliquer le scroll absolu instantané
393:     window.scrollTo({
394:         top: Math.max(0, absoluteScrollTop),
395:         behavior: 'instant'
396:     });
397:     
398:     // Forcer un second scroll après un court delay pour contrer toute animation CSS
399:     setTimeout(() => {
400:         window.scrollTo({
401:             top: Math.max(0, absoluteScrollTop),
402:             behavior: 'instant'
403:         });
404:         console.log(`[SCROLL] Applied absolute scroll to: ${Math.max(0, absoluteScrollTop)}px`);
405:     }, 5);
406: }
407: 
408: export { SCROLL_CONFIG };
```

## File: static/state.js
```javascript
  1: // Import new immutable state management
  2: import { appState } from './state/AppState.js';
  3: 
  4: export const PROCESS_INFO_CLIENT = new Proxy({}, {
  5:     get(_target, prop) {
  6:         if (typeof prop !== 'string') return undefined;
  7:         return appState.getStateProperty(`processInfo.${prop}`);
  8:     },
  9:     set(_target, prop, value) {
 10:         if (typeof prop !== 'string') return false;
 11:         appState.setState({ processInfo: { [prop]: value } }, 'process_info_update');
 12:         return true;
 13:     },
 14:     ownKeys() {
 15:         const root = appState.getStateProperty('processInfo') || {};
 16:         return Reflect.ownKeys(root);
 17:     },
 18:     getOwnPropertyDescriptor(_target, prop) {
 19:         const root = appState.getStateProperty('processInfo') || {};
 20:         if (Object.prototype.hasOwnProperty.call(root, prop)) {
 21:             return { enumerable: true, configurable: true };
 22:         }
 23:         return undefined;
 24:     }
 25: });
 26: 
 27: // Legacy exports for backward compatibility (deprecated - use appState instead)
 28: export let pollingIntervals = {};
 29: export let activeStepKeyForLogsPanel = null;
 30: export let stepTimers = {};
 31: export let selectedStepsOrder = [];
 32: export let isAnySequenceRunning = false;
 33: export let focusedElementBeforePopup = null;
 34: 
 35: 
 36: // --- MODIFICATION: La liste des étapes est mise à jour pour correspondre au backend ---
 37: export const REMOTE_SEQUENCE_STEP_KEYS = [
 38:     "STEP1",
 39:     "STEP2",
 40:     "STEP3",
 41:     "STEP4",
 42:     "STEP5",
 43:     "STEP6",
 44:     "STEP7"
 45: ];
 46: 
 47: // Modern state management functions using AppState
 48: export function setActiveStepKeyForLogs(key) {
 49:     activeStepKeyForLogsPanel = key; // Legacy
 50:     appState.setState({ activeStepKeyForLogsPanel: key }, 'setActiveStepKeyForLogs');
 51: }
 52: export function getActiveStepKeyForLogs() {
 53:     return appState.getStateProperty('activeStepKeyForLogsPanel') || activeStepKeyForLogsPanel;
 54: }
 55: 
 56: export function addStepTimer(stepKey, timerData) {
 57:     stepTimers[stepKey] = timerData; // Legacy
 58:     appState.setState({
 59:         stepTimers: { ...appState.getStateProperty('stepTimers'), [stepKey]: timerData }
 60:     }, 'addStepTimer');
 61: }
 62: export function getStepTimer(stepKey) {
 63:     return appState.getStateProperty(`stepTimers.${stepKey}`) || stepTimers[stepKey];
 64: }
 65: export function clearStepTimerInterval(stepKey) {
 66:     const timer = getStepTimer(stepKey);
 67:     if (timer && timer.intervalId) {
 68:         clearInterval(timer.intervalId);
 69:         const updatedTimer = { ...timer, intervalId: null };
 70:         addStepTimer(stepKey, updatedTimer);
 71:     }
 72: }
 73: export function deleteStepTimer(stepKey) {
 74:     if (getStepTimer(stepKey)) {
 75:         clearStepTimerInterval(stepKey);
 76:         delete stepTimers[stepKey]; // Legacy
 77:         const currentTimers = appState.getStateProperty('stepTimers') || {};
 78:         const { [stepKey]: removed, ...remainingTimers } = currentTimers;
 79:         appState.setState({ stepTimers: remainingTimers }, 'deleteStepTimer');
 80:     }
 81: }
 82: 
 83: export function setSelectedStepsOrder(order) {
 84:     selectedStepsOrder = order; // Legacy
 85:     appState.setState({ selectedStepsOrder: order }, 'setSelectedStepsOrder');
 86: }
 87: export function getSelectedStepsOrder() {
 88:     return appState.getStateProperty('selectedStepsOrder') || selectedStepsOrder;
 89: }
 90: 
 91: export function setIsAnySequenceRunning(running) {
 92:     isAnySequenceRunning = running; // Legacy
 93:     appState.setState({ isAnySequenceRunning: running }, 'setIsAnySequenceRunning');
 94: }
 95: export function getIsAnySequenceRunning() {
 96:     return appState.getStateProperty('isAnySequenceRunning') || isAnySequenceRunning;
 97: }
 98: 
 99: export function setFocusedElementBeforePopup(element) {
100:     focusedElementBeforePopup = element; // Legacy
101:     appState.setState({ focusedElementBeforePopup: element }, 'setFocusedElementBeforePopup');
102: }
103: export function getFocusedElementBeforePopup() {
104:     return appState.getStateProperty('focusedElementBeforePopup') || focusedElementBeforePopup;
105: }
106: 
107: export function setAutoModeLogPanelOpened(opened) {
108:     appState.setState({ ui: { autoModeLogPanelOpened: !!opened } }, 'setAutoModeLogPanelOpened');
109: }
110: 
111: export function getAutoModeLogPanelOpened() {
112:     return !!appState.getStateProperty('ui.autoModeLogPanelOpened');
113: }
114: 
115: export function addPollingInterval(stepKey, id) {
116:     pollingIntervals[stepKey] = id; // Legacy
117:     appState.setState({
118:         pollingIntervals: { ...appState.getStateProperty('pollingIntervals'), [stepKey]: id }
119:     }, 'addPollingInterval');
120: }
121: export function clearPollingInterval(stepKey) {
122:     if (pollingIntervals[stepKey]) {
123:         clearInterval(pollingIntervals[stepKey]);
124:         delete pollingIntervals[stepKey]; // Legacy
125:     }
126:     const currentIntervals = appState.getStateProperty('pollingIntervals') || {};
127:     const { [stepKey]: removed, ...remainingIntervals } = currentIntervals;
128:     appState.setState({ pollingIntervals: remainingIntervals }, 'clearPollingInterval');
129: }
130: export function getPollingInterval(stepKey) {
131:     return appState.getStateProperty(`pollingIntervals.${stepKey}`) || pollingIntervals[stepKey];
132: }
133: 
134: 
135: 
136: // Export the appState for direct access to modern state management
137: export { appState };
```

## File: static/utils.js
```javascript
 1: import { getNotificationsArea } from './domElements.js';
 2: 
 3: export function formatElapsedTime(startTime) {
 4:     if (!startTime) return "";
 5:     const now = new Date();
 6:     let seconds = Math.floor((now - startTime) / 1000);
 7:     let minutes = Math.floor(seconds / 60);
 8:     let hours = Math.floor(minutes / 60);
 9:     seconds %= 60;
10:     minutes %= 60;
11:     let timeStr = "";
12:     if (hours > 0) timeStr += `${hours}h `;
13:     if (minutes > 0 || hours > 0) timeStr += `${minutes}m `;
14:     timeStr += `${seconds}s`;
15:     return timeStr;
16: }
17: 
18: export function showNotification(message, type = 'info') { // type can be 'info', 'success', 'error', 'warning'
19:     const notificationsArea = getNotificationsArea();
20:     if (!notificationsArea) return;
21:     const notif = document.createElement('div');
22:     notif.className = `notification ${type}`;
23:     notif.textContent = message;
24:     notificationsArea.appendChild(notif);
25:     setTimeout(() => {
26:         notif.remove();
27:     }, 5000);
28: }
29: 
30: // Web Notifications API helpers
31: export async function ensureBrowserNotificationsPermission() {
32:     if (!('Notification' in window)) {
33:         console.warn('[Notifications] Browser does not support Notification API');
34:         return false;
35:     }
36:     if (Notification.permission === 'granted') return true;
37:     if (Notification.permission === 'denied') return false;
38:     try {
39:         const perm = await Notification.requestPermission();
40:         return perm === 'granted';
41:     } catch (e) {
42:         console.warn('[Notifications] Permission request failed:', e);
43:         return false;
44:     }
45: }
46: 
47: export async function sendBrowserNotification(title, body, options = {}) {
48:     try {
49:         const ok = await ensureBrowserNotificationsPermission();
50:         if (!ok) return false;
51:         const notif = new Notification(title || 'Notification', { body: body || '', ...options });
52:         setTimeout(() => notif && notif.close && notif.close(), 8000);
53:         return true;
54:     } catch (e) {
55:         console.debug('[Notifications] Fallback to UI banner due to error:', e);
56:         showNotification(`${title || 'Notification'}: ${body || ''}`, 'info');
57:         return false;
58:     }
59: }
```

## File: workflow_scripts/step5/run_tracking_manager.py
```python
  1: #!/usr/bin/env python3
  2: # -*- coding: utf-8 -*-
  3: import os, sys, json, argparse, subprocess, threading, time, logging
  4: from pathlib import Path
  5: from collections import OrderedDict, deque
  6: from datetime import datetime
  7: from typing import Mapping, Optional
  8: 
  9: try:
 10:     from dotenv import load_dotenv
 11: except ImportError:
 12:     load_dotenv = None
 13: 
 14: 
 15: class _EnvConfig:
 16:     def __init__(self, environ: Mapping[str, str]):
 17:         self._environ = environ
 18: 
 19:     def get_optional_str(self, key: str) -> Optional[str]:
 20:         raw = self._environ.get(key)
 21:         if raw is None:
 22:             return None
 23:         value = str(raw).strip()
 24:         return value if value else None
 25: 
 26:     def get_str(self, key: str, default: str = "") -> str:
 27:         value = self.get_optional_str(key)
 28:         return value if value is not None else default
 29: 
 30:     def get_int(self, key: str, default: int) -> int:
 31:         raw = self._environ.get(key)
 32:         if raw is None:
 33:             return default
 34:         try:
 35:             return int(str(raw).strip())
 36:         except Exception:
 37:             return default
 38: 
 39:     def get_bool(self, key: str, default: bool) -> bool:
 40:         raw = self._environ.get(key)
 41:         if raw is None:
 42:             return default
 43: 
 44:         normalized = str(raw).strip().lower()
 45:         if normalized in {"1", "true", "yes", "y", "on"}:
 46:             return True
 47:         if normalized in {"0", "false", "no", "n", "off"}:
 48:             return False
 49:         return default
 50: 
 51:     def get_csv_list(self, key: str) -> list[str]:
 52:         raw = self.get_str(key, default="").strip().lower()
 53:         return [p.strip() for p in raw.split(",") if p.strip()]
 54: 
 55:     def snapshot(self, keys: list[str]) -> dict[str, Optional[str]]:
 56:         return {k: self.get_optional_str(k) for k in keys}
 57: 
 58: 
 59: def _load_env_file():
 60:     """Load the project .env even if python-dotenv is unavailable."""
 61:     env_path = Path(__file__).resolve().parent.parent.parent / ".env"
 62:     if not env_path.exists():
 63:         return
 64:     if load_dotenv:
 65:         load_dotenv(env_path)
 66:         return
 67: 
 68:     for raw_line in env_path.read_text(encoding="utf-8").splitlines():
 69:         line = raw_line.strip()
 70:         if not line or line.startswith("#") or "=" not in line:
 71:             continue
 72:         key, value = line.split("=", 1)
 73:         key = key.strip()
 74:         if not key:
 75:             continue
 76:         cleaned = value.strip().strip('"').strip("'")
 77:         os.environ.setdefault(key, cleaned)
 78: 
 79: 
 80: _load_env_file()
 81: 
 82: 
 83: ENV = _EnvConfig(os.environ)
 84: step5_enable_object_detection = ENV.get_bool("STEP5_ENABLE_OBJECT_DETECTION", default=True)
 85: 
 86: def _log_env_snapshot():
 87:     relevant_keys = [
 88:         "STEP5_ENABLE_GPU",
 89:         "STEP5_GPU_ENGINES",
 90:         "STEP5_TRACKING_ENGINE",
 91:         "TRACKING_DISABLE_GPU",
 92:         "TRACKING_CPU_WORKERS",
 93:         "STEP5_GPU_FALLBACK_AUTO",
 94:         "STEP5_ENABLE_OBJECT_DETECTION",
 95:         "INSIGHTFACE_HOME",
 96:     ]
 97:     snapshot = ENV.snapshot(relevant_keys)
 98:     logging.info(f"[EnvSnapshot] {snapshot}")
 99: 
100: 
101: # --- CONFIGURATIONS GLOBALES ---
102: BASE_DIR = Path(__file__).resolve().parent.parent.parent
103: 
104: try:
105:     sys.path.insert(0, str(BASE_DIR))
106:     from config.settings import config as app_config
107:     TRACKING_ENV_PYTHON = app_config.get_venv_python("tracking_env")
108:     EOS_ENV_PYTHON = app_config.get_venv_python("eos_env")
109:     INSIGHTFACE_ENV_PYTHON = app_config.get_venv_python("insightface_env")
110: except (ImportError, AttributeError):
111:     TRACKING_ENV_PYTHON = BASE_DIR / "tracking_env" / "bin" / "python"
112:     EOS_ENV_PYTHON = BASE_DIR / "eos_env" / "bin" / "python"
113:     INSIGHTFACE_ENV_PYTHON = BASE_DIR / "insightface_env" / "bin" / "python"
114: 
115: TRACKING_ENV_PYTHON = Path(TRACKING_ENV_PYTHON)
116: EOS_ENV_PYTHON = Path(EOS_ENV_PYTHON)
117: INSIGHTFACE_ENV_PYTHON = Path(INSIGHTFACE_ENV_PYTHON)
118: 
119: WORKER_SCRIPT = Path(__file__).parent / "process_video_worker.py"
120: 
121: TRACKING_ENGINE = None
122: 
123: TF_GPU_ENV_PYTHON = ENV.get_str("STEP5_TF_GPU_ENV_PYTHON", "").strip()
124: if TF_GPU_ENV_PYTHON:
125:     TF_GPU_ENV_PYTHON = Path(TF_GPU_ENV_PYTHON)
126: 
127: CUDA_LIB_SUBDIRS = [
128:     "cublas/lib",
129:     "cuda_runtime/lib",
130:     "cuda_nvrtc/lib",
131:     "cufft/lib",
132:     "curand/lib",
133:     "cusolver/lib",
134:     "cusparse/lib",
135:     "cudnn/lib",
136:     "nvjitlink/lib",
137: ]
138: 
139: SYSTEM_CUDA_DEFAULTS = [
140:     "/usr/local/cuda-12.4",
141:     "/usr/local/cuda-12",
142:     "/usr/local/cuda-11.8",
143:     "/usr/local/cuda",
144: ]
145: 
146: 
147: def _build_venv_cuda_paths(python_exe: Path) -> list[str]:
148:     """Return CUDA library directories bundled in a venv (for ONNX Runtime CUDA provider)."""
149:     try:
150:         venv_dir = Path(python_exe).expanduser().parent.parent
151:     except Exception:
152:         return []
153: 
154:     python_version = f"python{sys.version_info.major}.{sys.version_info.minor}"
155:     site_packages = venv_dir / "lib" / python_version / "site-packages"
156:     nvidia_dir = site_packages / "nvidia"
157:     if not nvidia_dir.exists():
158:         return []
159: 
160:     paths = []
161:     for sub in CUDA_LIB_SUBDIRS:
162:         candidate = nvidia_dir / sub
163:         if candidate.exists():
164:             paths.append(str(candidate))
165:     return paths
166: 
167: 
168: def _discover_system_cuda_lib_paths(env: _EnvConfig) -> list[str]:
169:     """
170:     Detect CUDA libraries available on the host for engines that bundle their own interpreter (InsightFace).
171:     Priority order:
172:       1. Explicit STEP5_CUDA_LIB_PATH (colon-separated).
173:       2. STEP5_CUDA_HOME / CUDA_HOME lib64 folders.
174:       3. Common /usr/local/cuda-* lib64 folders.
175:       4. /usr/lib/x86_64-linux-gnu in case distro ships the libs there.
176:     """
177:     explicit_paths = env.get_str("STEP5_CUDA_LIB_PATH", "").strip()
178:     if explicit_paths:
179:         resolved = [
180:             str(Path(p.strip()).expanduser())
181:             for p in explicit_paths.split(":")
182:             if p.strip() and Path(p.strip()).expanduser().exists()
183:         ]
184:         if resolved:
185:             return resolved
186: 
187:     candidates: list[Path] = []
188:     for env_var in ("STEP5_CUDA_HOME", "CUDA_HOME"):
189:         value = env.get_str(env_var, "").strip()
190:         if value:
191:             candidates.append(Path(value).expanduser())
192:     for default_path in SYSTEM_CUDA_DEFAULTS:
193:         candidates.append(Path(default_path))
194: 
195:     discovered: list[str] = []
196:     for base_dir in candidates:
197:         if not base_dir.exists():
198:             continue
199:         for sub in ("lib64", "targets/x86_64-linux/lib"):
200:             candidate = base_dir / sub
201:             if candidate.exists():
202:                 discovered.append(str(candidate))
203:     debian_cuda = Path("/usr/lib/x86_64-linux-gnu")
204:     if debian_cuda.exists():
205:         expected = ["libcufft.so.11", "libcublas.so.12"]
206:         if all((debian_cuda / lib_name).exists() for lib_name in expected):
207:             discovered.append(str(debian_cuda))
208:     return discovered
209: 
210: 
211: # --- CONFIGURATION DU LOGGER ---
212: LOG_DIR = BASE_DIR / "logs" / "step5"
213: LOG_DIR.mkdir(parents=True, exist_ok=True)
214: log_file = LOG_DIR / f"manager_tracking_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
215: logging.basicConfig(level=logging.INFO, format='%(asctime)s - [MANAGER] - %(message)s',
216:                     handlers=[logging.FileHandler(log_file, 'w', 'utf-8'), logging.StreamHandler(sys.stdout)])
217: 
218: WORKER_CONFIG_TEMPLATE = {
219:     "mp_landmarker_num_faces": 5, "mp_landmarker_min_face_detection_confidence": 0.5,
220:     "mp_landmarker_min_face_presence_confidence": 0.3, "mp_landmarker_min_tracking_confidence": 0.5,
221:     "mp_landmarker_output_blendshapes": True, "enable_object_detection": step5_enable_object_detection,
222:     "object_score_threshold": 0.5, "object_max_results": 5, "mp_max_distance_tracking": 70,
223:     "mp_frames_unseen_deregister": 7, "speaking_detection_jaw_open_threshold": 0.08,
224:     "object_detector_model": ENV.get_str('STEP5_OBJECT_DETECTOR_MODEL', 'efficientdet_lite2'),
225:     "object_detector_model_path": ENV.get_optional_str('STEP5_OBJECT_DETECTOR_MODEL_PATH'),
226: }
227: 
228: CPU_OPTIMIZED_CONFIG = {
229:     "mp_landmarker_min_face_detection_confidence": 0.3,
230:     "mp_landmarker_min_face_presence_confidence": 0.2,
231:     "mp_landmarker_min_tracking_confidence": 0.3,
232:     "object_score_threshold": 0.4,
233:     "mp_max_distance_tracking": 80,
234: }
235: 
236: 
237: _DEFAULT_SUBPROCESS_ENV: dict[str, str] = {
238:     'OMP_NUM_THREADS': '1',
239:     'OPENBLAS_NUM_THREADS': '1',
240:     'MKL_NUM_THREADS': '1',
241:     'NUMEXPR_NUM_THREADS': '1',
242:     'TF_CPP_MIN_LOG_LEVEL': '2',
243:     'GLOG_minloglevel': '2',
244:     'ABSL_LOGGING_MIN_LOG_LEVEL': '2',
245: }
246: 
247: 
248: def _collect_cuda_lib_paths(worker_python: Path, engine_norm: str) -> list[str]:
249:     cuda_paths: list[str] = []
250:     cuda_paths_worker = _build_venv_cuda_paths(worker_python)
251:     cuda_paths_tracking = _build_venv_cuda_paths(TRACKING_ENV_PYTHON)
252:     if cuda_paths_worker:
253:         cuda_paths.extend(cuda_paths_worker)
254:     elif cuda_paths_tracking:
255:         cuda_paths.extend(cuda_paths_tracking)
256: 
257:     if engine_norm == "insightface":
258:         system_cuda_paths = _discover_system_cuda_lib_paths(ENV)
259:         for path in system_cuda_paths:
260:             if path not in cuda_paths:
261:                 cuda_paths.append(path)
262:     return cuda_paths
263: 
264: 
265: def _apply_ld_library_path(env: dict[str, str], extra_paths: list[str]) -> None:
266:     if not extra_paths:
267:         return
268:     extra_ld_path = ":".join(extra_paths)
269:     existing_ld_path = env.get("LD_LIBRARY_PATH", "")
270:     env["LD_LIBRARY_PATH"] = (
271:         f"{extra_ld_path}:{existing_ld_path}" if existing_ld_path else extra_ld_path
272:     )
273: 
274: 
275: def _build_subprocess_env(worker_python: Path, engine_norm: str) -> dict[str, str]:
276:     env = os.environ.copy()
277:     cuda_paths = _collect_cuda_lib_paths(worker_python, engine_norm)
278:     if cuda_paths:
279:         _apply_ld_library_path(env, cuda_paths)
280:         logging.info("[MANAGER] Injected CUDA library paths for ONNX Runtime")
281: 
282:     for key, value in _DEFAULT_SUBPROCESS_ENV.items():
283:         env.setdefault(key, value)
284:     return env
285: 
286: 
287: def log_reader_thread(process, video_name, progress_map, lock):
288:     if process.stdout:
289:         for line in iter(process.stdout.readline, ''):
290:             line = line.strip()
291:             logging.info(f"[{video_name}] {line}")
292:             if line.startswith("[Progression]|"):
293:                 try:
294:                     _, percent, _, _ = line.split('|')
295:                     with lock:
296:                         progress_map[video_name] = f"{int(percent)}%"
297:                     try:
298:                         print(f"{video_name}: {int(percent)}%", flush=True)
299:                     except Exception:
300:                         pass
301:                 except:
302:                     pass
303:         process.stdout.close()
304: 
305: 
306: def monitor_progress(processes, progress_map, lock, total_jobs_to_run):
307:     while len(processes) < total_jobs_to_run or any(p.poll() is None for p in processes.values()):
308:         time.sleep(1)
309:         with lock:
310:             progress_copy = progress_map.copy()
311:         progress_parts = [f"{name}: {status}" for name, status in sorted(progress_copy.items())]
312:         if progress_parts: print(f"[Progression-MultiLine]{' || '.join(progress_parts)}", flush=True)
313:     time.sleep(1)
314: 
315: 
316: def launch_worker_process(video_path, use_gpu, internal_workers=1, tracking_engine=None):
317:     video_name = Path(video_path).name
318:     worker_type_log = "GPU" if use_gpu else f"CPU (x{internal_workers})"
319:     logging.info(f"Préparation du job {worker_type_log} pour: {video_name}")
320: 
321:     config = WORKER_CONFIG_TEMPLATE.copy()
322: 
323:     engine_norm = (str(tracking_engine).strip().lower() if tracking_engine is not None else "")
324:     if engine_norm in {"mediapipe", "mediapipe_landmarker"}:
325:         engine_norm = ""
326: 
327:     gpu_capable_engines = {"openseeface", "opencv_yunet_pyfeat", "insightface"}
328:     
329:     if engine_norm and engine_norm not in gpu_capable_engines:
330:         if use_gpu:
331:             logging.info(f"Engine {engine_norm} not GPU-capable, forcing CPU mode")
332:         use_gpu = False
333: 
334:     if not use_gpu:
335:         config.update(CPU_OPTIMIZED_CONFIG)
336:         config["mp_num_workers_internal"] = internal_workers
337: 
338:         if internal_workers > 1:
339:             worker_script = "process_video_worker_multiprocessing.py"
340:             logging.info(f"Using multiprocessing worker with {internal_workers} processes")
341:         else:
342:             worker_script = "process_video_worker.py"
343:             logging.info("Using single-threaded CPU worker")
344: 
345:         logging.info(f"Applied CPU optimizations: lower confidence thresholds for better detection rate")
346:     else:
347:         worker_script = "process_video_worker.py"
348:         logging.info("Using GPU worker with sequential processing")
349:         if step5_enable_object_detection and internal_workers > 1:
350:             config["mp_num_workers_internal"] = internal_workers
351: 
352:     models_dir_path = Path(__file__).resolve().parent / "models"
353:     worker_script_path = Path(__file__).resolve().parent / worker_script
354: 
355:     worker_python_override_eos = ENV.get_optional_str("STEP5_EOS_ENV_PYTHON")
356:     worker_python_override_insightface = ENV.get_optional_str("STEP5_INSIGHTFACE_ENV_PYTHON")
357:     worker_python = TRACKING_ENV_PYTHON
358:     
359:     if engine_norm == "eos":
360:         worker_python = Path(worker_python_override_eos) if worker_python_override_eos else EOS_ENV_PYTHON
361:         if not worker_python.exists():
362:             raise RuntimeError(
363:                 f"EOS engine selected but python interpreter not found: {worker_python}. "
364:                 "Create eos_env or set STEP5_EOS_ENV_PYTHON to the eos_env python path."
365:             )
366:     elif engine_norm == "insightface":
367:         if not use_gpu:
368:             raise RuntimeError(
369:                 "InsightFace engine is GPU-only. Enable STEP5_ENABLE_GPU=1 and include 'insightface' in STEP5_GPU_ENGINES."
370:             )
371: 
372:         worker_python = (
373:             Path(worker_python_override_insightface)
374:             if worker_python_override_insightface
375:             else INSIGHTFACE_ENV_PYTHON
376:         )
377:         if not worker_python.exists():
378:             raise RuntimeError(
379:                 f"InsightFace engine selected but python interpreter not found: {worker_python}. "
380:                 "Create insightface_env or set STEP5_INSIGHTFACE_ENV_PYTHON to the insightface_env python path."
381:             )
382:     elif use_gpu and not engine_norm:
383:         if TF_GPU_ENV_PYTHON and isinstance(TF_GPU_ENV_PYTHON, Path):
384:             if TF_GPU_ENV_PYTHON.exists():
385:                 worker_python = TF_GPU_ENV_PYTHON
386:                 logging.info(f"Using TensorFlow GPU interpreter for MediaPipe: {worker_python}")
387:             else:
388:                 logging.warning(
389:                     "STEP5_TF_GPU_ENV_PYTHON is set but the interpreter was not found "
390:                     f"({TF_GPU_ENV_PYTHON}). Falling back to tracking_env."
391:                 )
392: 
393:     command_args = [str(worker_python), str(worker_script_path), video_path, "--models_dir", str(models_dir_path)]
394:     if use_gpu: command_args.append("--use_gpu")
395: 
396:     if engine_norm:
397:         command_args.extend(["--tracking_engine", engine_norm])
398: 
399:     for key, value in config.items():
400:         if key == "mp_num_workers_internal" and use_gpu and not (step5_enable_object_detection and internal_workers > 1):
401:             continue
402:         if isinstance(value, bool):
403:             if value:
404:                 command_args.append(f"--{key}")
405:             continue
406:         if value is not None:
407:             command_args.extend([f"--{key}", str(value)])
408: 
409:     if (not use_gpu) and internal_workers > 1:
410:         command_args.extend(["--chunk_size", "0"])  # 0 = adaptive in worker
411: 
412:     try:
413:         env = _build_subprocess_env(Path(worker_python), engine_norm)
414: 
415:         p = subprocess.Popen(
416:             command_args,
417:             stdout=subprocess.PIPE,
418:             stderr=subprocess.STDOUT,
419:             text=True,
420:             encoding='utf-8',
421:             errors='replace',
422:             env=env,
423:         )
424:         return p
425:     except Exception as e:
426:         logging.error(f"ERREUR LANCEMENT de {video_name}: {e}")
427:         return None
428: 
429: 
430: def run_job_and_monitor(job_info, processes, progress_map, lock):
431:     video_path = job_info['path']
432:     video_name = Path(video_path).name
433:     internal_workers = job_info.get('cpu_internal_workers', 15) if (not job_info['use_gpu'] or step5_enable_object_detection) else 1
434:     tracking_engine = job_info.get('tracking_engine')
435:     p = launch_worker_process(video_path, job_info['use_gpu'], internal_workers, tracking_engine=tracking_engine)
436: 
437:     if p:
438:         with lock:
439:             processes[video_name] = p
440:             progress_map[video_name] = "Démarrage..."
441: 
442:         reader_thread = threading.Thread(target=log_reader_thread, args=(p, video_name, progress_map, lock),
443:                                          daemon=True)
444:         reader_thread.start()
445:         p.wait()
446:         reader_thread.join(timeout=1)
447: 
448:         try:
449:             if p.returncode == 0:
450:                 print(f"[Gestionnaire] Succès pour {video_name}", flush=True)
451:             else:
452:                 print(f"[Gestionnaire] Échec pour {video_name}", flush=True)
453:         except Exception:
454:             pass
455: 
456: 
457: 
458: def resource_worker_loop(resource_name, use_gpu, videos_deque, deque_lock, processes, progress_map, lock):
459:     """Continuously pull videos from the shared deque and process them on the given resource.
460: 
461:     Args:
462:         resource_name (str): Human-readable resource label (e.g., 'GPU' or 'CPU').
463:         use_gpu (bool): Whether to use GPU for the worker process.
464:         videos_deque (collections.deque): Shared queue of video paths to process.
465:         deque_lock (threading.Lock): Lock protecting access to the shared deque.
466:         processes (OrderedDict): Shared mapping of video_name -> subprocess.Popen.
467:         progress_map (OrderedDict): Shared mapping of video_name -> progress string.
468:         lock (threading.Lock): Lock protecting shared maps.
469:     """
470:     while True:
471:         with deque_lock:
472:             if videos_deque:
473:                 video_path = videos_deque.popleft()
474:             else:
475:                 break
476:         video_name = Path(video_path).name
477:         logging.info(f"[Scheduler] Assigning {video_name} to {resource_name}")
478:         run_job_and_monitor(
479:             {
480:                 'path': video_path,
481:                 'use_gpu': use_gpu,
482:                 'cpu_internal_workers': CPU_INTERNAL_WORKERS,
483:                 'tracking_engine': TRACKING_ENGINE,
484:             },
485:             processes,
486:             progress_map,
487:             lock,
488:         )
489: 
490: 
491: def main():
492:     parser = argparse.ArgumentParser(description="Gestionnaire de tracking parallèle intelligent.")
493:     parser.add_argument("--videos_json_path", required=True, help="Chemin JSON des vidéos.")
494:     parser.add_argument("--disable_gpu", action="store_true", help="Désactiver le worker GPU et utiliser uniquement le CPU")
495:     parser.add_argument("--cpu_internal_workers", type=int, default=15, help="Nombre de workers internes CPU par vidéo")
496:     parser.add_argument(
497:         "--tracking_engine",
498:         default=None,
499:         help="Moteur de tracking: mediapipe_landmarker (défaut), opencv_haar, opencv_yunet, opencv_yunet_pyfeat, openseeface, eos, insightface",
500:     )
501:     args = parser.parse_args()
502:     try:
503:         if ENV.get_bool('TRACKING_DISABLE_GPU', default=False):
504:             args.disable_gpu = True
505: 
506:         args.cpu_internal_workers = ENV.get_int('TRACKING_CPU_WORKERS', args.cpu_internal_workers)
507: 
508:         if args.tracking_engine is None:
509:             raw_engine = ENV.get_optional_str('STEP5_TRACKING_ENGINE')
510:             if raw_engine:
511:                 args.tracking_engine = raw_engine
512:     except Exception:
513:         pass
514: 
515:     _engine_norm = (str(args.tracking_engine).strip().lower() if args.tracking_engine is not None else "")
516:     
517:     gpu_enabled_global = ENV.get_str('STEP5_ENABLE_GPU', '0').strip() == '1'
518:     gpu_engines_str = ENV.get_str('STEP5_GPU_ENGINES', '').strip().lower()
519:     gpu_engines = [e.strip() for e in gpu_engines_str.split(',') if e.strip()]
520:     _log_env_snapshot()
521:     
522:     engine_normalized = _engine_norm if _engine_norm else "mediapipe_landmarker"
523:     if engine_normalized in {"mediapipe", "mediapipe_landmarker"}:
524:         engine_normalized = "mediapipe_landmarker"
525:     
526:     engine_supports_gpu = False
527:     if gpu_enabled_global and not args.disable_gpu:
528:         if engine_normalized == "insightface":
529:             if engine_normalized in gpu_engines or 'all' in gpu_engines:
530:                 engine_supports_gpu = True
531:                 logging.info(f"GPU mode requested for engine: {engine_normalized}")
532:                 
533:                 try:
534:                     sys.path.insert(0, str(Path(__file__).parent.parent.parent))
535:                     from config.settings import Config
536:                     gpu_status = Config.check_gpu_availability()
537:                     
538:                     if not gpu_status['available']:
539:                         logging.warning(f"GPU requested but unavailable: {gpu_status['reason']}")
540:                         fallback_auto = ENV.get_str('STEP5_GPU_FALLBACK_AUTO', '1').strip() == '1'
541:                         if fallback_auto:
542:                             logging.info("Auto-fallback to CPU enabled")
543:                             args.disable_gpu = True
544:                             engine_supports_gpu = False
545:                         else:
546:                             logging.error("GPU fallback disabled, aborting")
547:                             raise RuntimeError(f"GPU unavailable: {gpu_status['reason']}")
548:                     else:
549:                         logging.info(f"GPU validation passed: VRAM {gpu_status['vram_free_gb']:.1f} Go free, CUDA {gpu_status.get('cuda_version', 'N/A')}")
550:                 except ImportError as e:
551:                     logging.error(f"Failed to import config.settings: {e}")
552:                     args.disable_gpu = True
553:                     engine_supports_gpu = False
554:                 except Exception as e:
555:                     logging.error(f"GPU validation failed: {e}")
556:                     args.disable_gpu = True
557:                     engine_supports_gpu = False
558:             else:
559:                 logging.warning(f"InsightFace not listed in STEP5_GPU_ENGINES ({gpu_engines_str}), forcing CPU-only mode")
560:                 args.disable_gpu = True
561:         else:
562:             logging.info(f"GPU mode is reserved for InsightFace only. Engine '{engine_normalized}' will run in CPU-only mode.")
563:             args.disable_gpu = True
564:     
565:     if _engine_norm in {"opencv_haar", "opencv_yunet", "eos"}:
566:         if not args.disable_gpu:
567:             args.disable_gpu = True
568:             logging.info(f"Engine {_engine_norm} does not support GPU, forcing CPU-only mode")
569:     
570:     if not args.disable_gpu and engine_supports_gpu:
571:         logging.info(f"✓ GPU mode ENABLED for {_engine_norm or 'mediapipe_landmarker'}")
572:     else:
573:         logging.info(f"CPU-only mode (GPU disabled or not supported for this engine)")
574: 
575:     if _engine_norm == "insightface" and (args.disable_gpu or not engine_supports_gpu):
576:         logging.error(
577:             "InsightFace engine is GPU-only, but GPU mode is disabled or not authorized. "
578:             "Set STEP5_ENABLE_GPU=1 and include 'insightface' in STEP5_GPU_ENGINES."
579:         )
580:         sys.exit(1)
581: 
582:     with open(args.videos_json_path, 'r') as f:
583:         data = json.load(f)
584:     
585:     if isinstance(data, list):
586:         videos_list = data
587:     elif isinstance(data, dict) and 'videos' in data:
588:         logging.info("Legacy videos JSON schema detected; using 'videos' key.")
589:         videos_list = data['videos']
590:     else:
591:         logging.error(f"Invalid JSON format in {args.videos_json_path}. Expected a list or object with 'videos' key.")
592:         sys.exit(1)
593:     
594:     videos_to_process = deque(videos_list)
595:     if not videos_to_process: logging.info("Aucune vidéo à traiter."); return
596: 
597:     logging.info(f"--- DÉMARRAGE DU GESTIONNAIRE DE TRACKING (Planificateur Dynamique GPU/CPU) ---")
598:     total_jobs = len(videos_to_process)
599:     logging.info(f"Vidéos à traiter: {total_jobs}")
600:     global CPU_INTERNAL_WORKERS
601:     CPU_INTERNAL_WORKERS = max(1, int(args.cpu_internal_workers))
602: 
603:     global TRACKING_ENGINE
604:     TRACKING_ENGINE = args.tracking_engine
605:     if args.disable_gpu:
606:         logging.info("Mode FULL CPU activé: le worker GPU est désactivé")
607: 
608:     processes, video_progress_map, progress_lock = OrderedDict(), OrderedDict(), threading.Lock()
609: 
610:     monitor_thread = threading.Thread(target=monitor_progress,
611:                                       args=(processes, video_progress_map, progress_lock, total_jobs), daemon=True)
612:     monitor_thread.start()
613: 
614:     deque_lock = threading.Lock()
615: 
616:     threads = []
617:     if not args.disable_gpu:
618:         gpu_thread = threading.Thread(
619:             target=resource_worker_loop,
620:             args=("GPU", True, videos_to_process, deque_lock, processes, video_progress_map, progress_lock),
621:             daemon=True,
622:         )
623:         threads.append(gpu_thread)
624:     if _engine_norm != "insightface":
625:         cpu_thread = threading.Thread(
626:             target=resource_worker_loop,
627:             args=("CPU", False, videos_to_process, deque_lock, processes, video_progress_map, progress_lock),
628:             daemon=True,
629:         )
630:         threads.append(cpu_thread)
631:     else:
632:         logging.info("InsightFace is GPU-only: CPU worker thread disabled")
633: 
634:     if args.disable_gpu:
635:         workers_label = "CPU seul"
636:     else:
637:         workers_label = "GPU seul" if _engine_norm == "insightface" else "GPU et CPU"
638:     logging.info("Lancement des workers: " + workers_label)
639:     for t in threads:
640:         t.start()
641: 
642:     for t in threads:
643:         t.join()
644: 
645:     monitor_thread.join(timeout=2)
646: 
647:     success_count = sum(1 for p in processes.values() if p.returncode == 0)
648:     logging.info(f"--- FIN DU GESTIONNAIRE ---")
649:     logging.info(f"Traitement terminé. {success_count}/{total_jobs} jobs réussis.")
650:     if success_count < total_jobs: sys.exit(1)
651: 
652: 
653: if __name__ == "__main__":
654:     main()
```

## File: config/settings.py
```python
  1: """
  2: Centralized configuration management for workflow_mediapipe.
  3: 
  4: This module provides environment-based configuration management
  5: following the project's development guidelines.
  6: """
  7: 
  8: import os
  9: import logging
 10: import subprocess
 11: from pathlib import Path
 12: from dataclasses import dataclass, field
 13: from typing import Optional, List
 14: 
 15: logger = logging.getLogger(__name__)
 16: 
 17: 
 18: def _parse_bool(raw: Optional[str], default: bool) -> bool:
 19:     if raw is None:
 20:         return default
 21:     return raw.strip().lower() in {"1", "true", "yes", "y", "on"}
 22: 
 23: 
 24: def _parse_optional_int(raw: Optional[str]) -> Optional[int]:
 25:     if raw is None:
 26:         return None
 27:     raw = raw.strip()
 28:     if not raw:
 29:         return None
 30:     try:
 31:         return int(raw)
 32:     except Exception:
 33:         return None
 34: 
 35: 
 36: def _parse_optional_positive_int(raw: Optional[str]) -> Optional[int]:
 37:     value = _parse_optional_int(raw)
 38:     if value is None:
 39:         return None
 40:     if value <= 0:
 41:         return None
 42:     return value
 43: 
 44: 
 45: def _parse_csv_list(raw: Optional[str]) -> List[str]:
 46:     if raw is None:
 47:         return []
 48:     parts = [p.strip() for p in raw.split(",")]
 49:     return [p for p in parts if p]
 50: 
 51: 
 52: @dataclass
 53: class Config:
 54:     """
 55:     Centralized configuration class for the workflow_mediapipe application.
 56:     
 57:     All configuration values are loaded from environment variables with
 58:     sensible defaults to maintain backward compatibility.
 59:     """
 60:     
 61:     # Flask Application Settings
 62:     SECRET_KEY: str = os.environ.get('FLASK_SECRET_KEY', 'dev-key-change-in-production')
 63:     DEBUG: bool = os.environ.get('DEBUG', 'false').lower() == 'true'
 64:     HOST: str = os.environ.get('FLASK_HOST', '0.0.0.0')
 65:     PORT: int = int(os.environ.get('FLASK_PORT', '5000'))
 66:     
 67:     # Security Tokens (loaded from environment)
 68:     INTERNAL_WORKER_TOKEN: Optional[str] = os.environ.get('INTERNAL_WORKER_COMMS_TOKEN')
 69:     RENDER_REGISTER_TOKEN: Optional[str] = os.environ.get('RENDER_REGISTER_TOKEN')
 70:     
 71:     # Webhook JSON Source (single data source for monitoring)
 72:     WEBHOOK_JSON_URL: str = os.environ.get(
 73:         'WEBHOOK_JSON_URL',
 74:         'https://webhook.kidpixel.fr/data/webhook_links.json'
 75:     )
 76:     WEBHOOK_TIMEOUT: int = int(os.environ.get('WEBHOOK_TIMEOUT', '10'))
 77:     WEBHOOK_CACHE_TTL: int = int(os.environ.get('WEBHOOK_CACHE_TTL', '60'))
 78:     WEBHOOK_MONITOR_INTERVAL: int = int(os.environ.get('WEBHOOK_MONITOR_INTERVAL', '15'))
 79:     
 80:     # Directory Configuration
 81:     BASE_PATH_SCRIPTS: Path = Path(os.environ.get(
 82:         'BASE_PATH_SCRIPTS_ENV', 
 83:         os.path.dirname(os.path.abspath(__file__ + '/../'))
 84:     ))
 85:     CACHE_ROOT_DIR: Path = Path(os.environ.get('CACHE_ROOT_DIR', '/mnt/cache'))
 86:     LOCAL_DOWNLOADS_DIR: Path = Path(os.environ.get(
 87:         'LOCAL_DOWNLOADS_DIR', 
 88:         Path.home() / 'Téléchargements'
 89:     ))
 90:     DISABLE_EXPLORER_OPEN: bool = _parse_bool(os.environ.get('DISABLE_EXPLORER_OPEN'), default=False)
 91:     ENABLE_EXPLORER_OPEN: bool = _parse_bool(os.environ.get('ENABLE_EXPLORER_OPEN'), default=False)
 92:     DOWNLOAD_HISTORY_SHARED_GROUP: Optional[str] = os.environ.get('DOWNLOAD_HISTORY_SHARED_GROUP')
 93:     DOWNLOAD_HISTORY_DB_PATH: Path = Path(os.environ.get('DOWNLOAD_HISTORY_DB_PATH', ''))
 94:     # LOGS_DIR is normalized in __post_init__ to be absolute under BASE_PATH_SCRIPTS by default.
 95:     # If LOGS_DIR is set in env and is relative, it will be resolved against BASE_PATH_SCRIPTS.
 96:     LOGS_DIR: Path = Path(os.environ.get('LOGS_DIR', ''))
 97:     # Virtual environments base directory (defaults to project root if not set)
 98:     VENV_BASE_DIR: Optional[Path] = Path(os.environ.get('VENV_BASE_DIR', '')) if os.environ.get('VENV_BASE_DIR') else None
 99:     # Projects directory for visualization/timeline features
100:     PROJECTS_DIR: Path = Path(os.environ.get('PROJECTS_DIR', '')) if os.environ.get('PROJECTS_DIR') else None
101:     # Archives directory for persistent analysis results (timeline)
102:     ARCHIVES_DIR: Path = Path(os.environ.get('ARCHIVES_DIR', '')) if os.environ.get('ARCHIVES_DIR') else None
103:     
104:     # Python Environment Configuration
105:     PYTHON_VENV_EXE: str = os.environ.get('PYTHON_VENV_EXE_ENV', '')
106:     
107:     # Processing Configuration
108:     MAX_CPU_WORKERS: int = int(os.environ.get(
109:         'MAX_CPU_WORKERS', 
110:         str(max(1, os.cpu_count() - 2 if os.cpu_count() else 2))
111:     ))
112:     
113:     # Polling Intervals (in milliseconds for frontend, seconds for backend)
114:     POLLING_INTERVAL: int = int(os.environ.get('POLLING_INTERVAL', '1000'))
115:     LOCAL_DOWNLOAD_POLLING_INTERVAL: int = int(os.environ.get('LOCAL_DOWNLOAD_POLLING_INTERVAL', '3000'))
116: 
117:     SYSTEM_MONITOR_POLLING_INTERVAL: int = int(os.environ.get('SYSTEM_MONITOR_POLLING_INTERVAL', '5000'))
118:     
119:     # MediaPipe Configuration
120:     MP_LANDMARKER_MIN_DETECTION_CONFIDENCE: float = float(os.environ.get(
121:         'MP_LANDMARKER_MIN_DETECTION_CONFIDENCE', '0.5'
122:     ))
123:     MP_LANDMARKER_MIN_TRACKING_CONFIDENCE: float = float(os.environ.get(
124:         'MP_LANDMARKER_MIN_TRACKING_CONFIDENCE', '0.5'
125:     ))
126:     
127:     # GPU Configuration
128:     ENABLE_GPU_MONITORING: bool = os.environ.get('ENABLE_GPU_MONITORING', 'true').lower() == 'true'
129:     
130:     # Lemonfox API Configuration (STEP4 alternative)
131:     LEMONFOX_API_KEY: Optional[str] = os.environ.get('LEMONFOX_API_KEY')
132:     LEMONFOX_TIMEOUT_SEC: int = int(os.environ.get('LEMONFOX_TIMEOUT_SEC', '300'))
133:     LEMONFOX_EU_DEFAULT: bool = os.environ.get('LEMONFOX_EU_DEFAULT', '0') == '1'
134: 
135:     LEMONFOX_DEFAULT_LANGUAGE: Optional[str] = os.environ.get("LEMONFOX_DEFAULT_LANGUAGE")
136:     LEMONFOX_DEFAULT_PROMPT: Optional[str] = os.environ.get("LEMONFOX_DEFAULT_PROMPT")
137:     LEMONFOX_SPEAKER_LABELS_DEFAULT: bool = _parse_bool(
138:         os.environ.get("LEMONFOX_SPEAKER_LABELS_DEFAULT"),
139:         default=True,
140:     )
141:     LEMONFOX_DEFAULT_MIN_SPEAKERS: Optional[int] = _parse_optional_int(
142:         os.environ.get("LEMONFOX_DEFAULT_MIN_SPEAKERS")
143:     )
144:     LEMONFOX_DEFAULT_MAX_SPEAKERS: Optional[int] = _parse_optional_int(
145:         os.environ.get("LEMONFOX_DEFAULT_MAX_SPEAKERS")
146:     )
147:     LEMONFOX_TIMESTAMP_GRANULARITIES: List[str] = field(
148:         default_factory=lambda: _parse_csv_list(os.environ.get("LEMONFOX_TIMESTAMP_GRANULARITIES", "word"))
149:     )
150:     LEMONFOX_SPEECH_GAP_FILL_SEC: float = float(os.environ.get("LEMONFOX_SPEECH_GAP_FILL_SEC", "0.15"))
151:     LEMONFOX_SPEECH_MIN_ON_SEC: float = float(os.environ.get("LEMONFOX_SPEECH_MIN_ON_SEC", "0.0"))
152:     LEMONFOX_MAX_UPLOAD_MB: Optional[int] = _parse_optional_positive_int(
153:         os.environ.get("LEMONFOX_MAX_UPLOAD_MB")
154:     )
155:     LEMONFOX_ENABLE_TRANSCODE: bool = _parse_bool(
156:         os.environ.get("LEMONFOX_ENABLE_TRANSCODE"),
157:         default=False,
158:     )
159:     LEMONFOX_TRANSCODE_AUDIO_CODEC: str = os.environ.get("LEMONFOX_TRANSCODE_AUDIO_CODEC", "aac")
160:     LEMONFOX_TRANSCODE_BITRATE_KBPS: int = int(os.environ.get("LEMONFOX_TRANSCODE_BITRATE_KBPS", "96"))
161: 
162:     STEP4_USE_LEMONFOX: bool = os.environ.get('STEP4_USE_LEMONFOX', '0') == '1'
163:     
164:     # STEP5 Object Detection Configuration
165:     # Model selection for fallback object detection when face detection fails (MediaPipe only)
166:     STEP5_OBJECT_DETECTOR_MODEL: str = os.environ.get(
167:         'STEP5_OBJECT_DETECTOR_MODEL',
168:         'efficientdet_lite2'  # Default: current baseline, backward compatible
169:     )
170:     STEP5_OBJECT_DETECTOR_MODEL_PATH: Optional[str] = os.environ.get('STEP5_OBJECT_DETECTOR_MODEL_PATH')
171:     STEP5_ENABLE_OBJECT_DETECTION: bool = os.environ.get('STEP5_ENABLE_OBJECT_DETECTION', '0') == '1'
172:     
173:     # STEP5 Performance Optimizations (opencv_yunet_pyfeat)
174:     STEP5_ENABLE_PROFILING: bool = os.environ.get('STEP5_ENABLE_PROFILING', '0') == '1'
175:     STEP5_ONNX_INTRA_OP_THREADS: int = int(os.environ.get('STEP5_ONNX_INTRA_OP_THREADS', '2'))
176:     STEP5_ONNX_INTER_OP_THREADS: int = int(os.environ.get('STEP5_ONNX_INTER_OP_THREADS', '1'))
177:     STEP5_BLENDSHAPES_THROTTLE_N: int = int(os.environ.get('STEP5_BLENDSHAPES_THROTTLE_N', '1'))  # 1 = every frame (no throttling)
178:     STEP5_YUNET_MAX_WIDTH: int = int(os.environ.get('STEP5_YUNET_MAX_WIDTH', '640'))  # Max width for YuNet detection (coordinates rescaled)
179: 
180:     STEP5_OPENCV_MAX_FACES: Optional[int] = _parse_optional_positive_int(
181:         os.environ.get('STEP5_OPENCV_MAX_FACES')
182:     )
183:     STEP5_OPENCV_JAWOPEN_SCALE: float = float(os.environ.get('STEP5_OPENCV_JAWOPEN_SCALE', '1.0'))
184: 
185:     STEP5_MEDIAPIPE_MAX_FACES: Optional[int] = _parse_optional_positive_int(
186:         os.environ.get('STEP5_MEDIAPIPE_MAX_FACES')
187:     )
188:     STEP5_MEDIAPIPE_JAWOPEN_SCALE: float = float(os.environ.get('STEP5_MEDIAPIPE_JAWOPEN_SCALE', '1.0'))
189:     STEP5_MEDIAPIPE_MAX_WIDTH: Optional[int] = _parse_optional_positive_int(
190:         os.environ.get('STEP5_MEDIAPIPE_MAX_WIDTH')
191:     )
192: 
193:     # STEP5 OpenSeeFace Engine Configuration
194:     # Lightweight CPU tracking via ONNX Runtime (OpenSeeFace-style models)
195:     STEP5_OPENSEEFACE_MODELS_DIR: Optional[str] = os.environ.get('STEP5_OPENSEEFACE_MODELS_DIR')
196:     STEP5_OPENSEEFACE_MODEL_ID: int = int(os.environ.get('STEP5_OPENSEEFACE_MODEL_ID', '1'))
197:     STEP5_OPENSEEFACE_DETECTION_MODEL_PATH: Optional[str] = os.environ.get('STEP5_OPENSEEFACE_DETECTION_MODEL_PATH')
198:     STEP5_OPENSEEFACE_LANDMARK_MODEL_PATH: Optional[str] = os.environ.get('STEP5_OPENSEEFACE_LANDMARK_MODEL_PATH')
199:     STEP5_OPENSEEFACE_DETECT_EVERY_N: int = int(os.environ.get('STEP5_OPENSEEFACE_DETECT_EVERY_N', '1'))
200:     STEP5_OPENSEEFACE_MAX_WIDTH: int = int(
201:         os.environ.get('STEP5_OPENSEEFACE_MAX_WIDTH')
202:         or os.environ.get('STEP5_YUNET_MAX_WIDTH', '640')
203:     )
204:     STEP5_OPENSEEFACE_DETECTION_THRESHOLD: float = float(os.environ.get('STEP5_OPENSEEFACE_DETECTION_THRESHOLD', '0.6'))
205:     STEP5_OPENSEEFACE_MAX_FACES: int = int(os.environ.get('STEP5_OPENSEEFACE_MAX_FACES', '1'))
206:     STEP5_OPENSEEFACE_JAWOPEN_SCALE: float = float(os.environ.get('STEP5_OPENSEEFACE_JAWOPEN_SCALE', '1.0'))
207: 
208:     STEP5_EOS_ENV_PYTHON: Optional[str] = os.environ.get('STEP5_EOS_ENV_PYTHON')
209:     STEP5_EOS_MODELS_DIR: Optional[str] = os.environ.get('STEP5_EOS_MODELS_DIR')
210:     STEP5_EOS_SFM_MODEL_PATH: Optional[str] = os.environ.get('STEP5_EOS_SFM_MODEL_PATH')
211:     STEP5_EOS_EXPRESSION_BLENDSHAPES_PATH: Optional[str] = os.environ.get('STEP5_EOS_EXPRESSION_BLENDSHAPES_PATH')
212:     STEP5_EOS_LANDMARK_MAPPER_PATH: Optional[str] = os.environ.get('STEP5_EOS_LANDMARK_MAPPER_PATH')
213:     STEP5_EOS_EDGE_TOPOLOGY_PATH: Optional[str] = os.environ.get('STEP5_EOS_EDGE_TOPOLOGY_PATH')
214:     STEP5_EOS_MODEL_CONTOUR_PATH: Optional[str] = os.environ.get('STEP5_EOS_MODEL_CONTOUR_PATH')
215:     STEP5_EOS_CONTOUR_LANDMARKS_PATH: Optional[str] = os.environ.get('STEP5_EOS_CONTOUR_LANDMARKS_PATH')
216:     STEP5_EOS_FIT_EVERY_N: int = int(os.environ.get('STEP5_EOS_FIT_EVERY_N', '1'))
217:     STEP5_EOS_MAX_FACES: Optional[int] = _parse_optional_positive_int(
218:         os.environ.get('STEP5_EOS_MAX_FACES')
219:     )
220:     STEP5_EOS_MAX_WIDTH: int = int(os.environ.get('STEP5_EOS_MAX_WIDTH', '1280'))
221:     STEP5_EOS_JAWOPEN_SCALE: float = float(os.environ.get('STEP5_EOS_JAWOPEN_SCALE', '1.0'))
222:     
223:     def __post_init__(self):
224:         """Post-initialization to ensure paths are Path objects and create directories."""
225:         # Ensure all path attributes are Path objects
226:         if isinstance(self.BASE_PATH_SCRIPTS, str):
227:             self.BASE_PATH_SCRIPTS = Path(self.BASE_PATH_SCRIPTS)
228:         if isinstance(self.CACHE_ROOT_DIR, str):
229:             self.CACHE_ROOT_DIR = Path(self.CACHE_ROOT_DIR)
230:         if isinstance(self.LOCAL_DOWNLOADS_DIR, str):
231:             self.LOCAL_DOWNLOADS_DIR = Path(self.LOCAL_DOWNLOADS_DIR)
232:         if isinstance(self.LOGS_DIR, str):
233:             self.LOGS_DIR = Path(self.LOGS_DIR)
234: 
235:         if isinstance(self.DOWNLOAD_HISTORY_DB_PATH, str):
236:             self.DOWNLOAD_HISTORY_DB_PATH = Path(self.DOWNLOAD_HISTORY_DB_PATH)
237:         
238:         # Default VENV_BASE_DIR to BASE_PATH_SCRIPTS if not set
239:         if self.VENV_BASE_DIR is None or (isinstance(self.VENV_BASE_DIR, str) and not self.VENV_BASE_DIR):
240:             self.VENV_BASE_DIR = self.BASE_PATH_SCRIPTS
241:         elif isinstance(self.VENV_BASE_DIR, str):
242:             self.VENV_BASE_DIR = Path(self.VENV_BASE_DIR)
243: 
244:         # Resolve PYTHON_VENV_EXE via VENV_BASE_DIR logic.
245:         # If PYTHON_VENV_EXE_ENV is provided and is relative, resolve it against VENV_BASE_DIR.
246:         if not self.PYTHON_VENV_EXE:
247:             self.PYTHON_VENV_EXE = str(self.get_venv_python("env"))
248:         else:
249:             python_exe_path = Path(self.PYTHON_VENV_EXE)
250:             if not python_exe_path.is_absolute():
251:                 self.PYTHON_VENV_EXE = str((self.VENV_BASE_DIR / python_exe_path).resolve())
252:         
253:         # Normalize LOGS_DIR to avoid CWD-dependent side effects when importing config from step scripts.
254:         # Default to <BASE_PATH_SCRIPTS>/logs if not provided. If provided and relative, make it absolute
255:         # under BASE_PATH_SCRIPTS. This prevents accidental creation of logs under working directories
256:         # like 'projets_extraits/logs' when steps run with a different CWD.
257:         if (not str(self.LOGS_DIR)) or (str(self.LOGS_DIR).strip() == '.'):
258:             self.LOGS_DIR = (self.BASE_PATH_SCRIPTS / 'logs').resolve()
259:         elif not self.LOGS_DIR.is_absolute():
260:             self.LOGS_DIR = (self.BASE_PATH_SCRIPTS / self.LOGS_DIR).resolve()
261: 
262:         if (not str(self.DOWNLOAD_HISTORY_DB_PATH)) or (str(self.DOWNLOAD_HISTORY_DB_PATH).strip() == '.'):
263:             self.DOWNLOAD_HISTORY_DB_PATH = (self.BASE_PATH_SCRIPTS / 'download_history.sqlite3').resolve()
264:         elif not self.DOWNLOAD_HISTORY_DB_PATH.is_absolute():
265:             self.DOWNLOAD_HISTORY_DB_PATH = (self.BASE_PATH_SCRIPTS / self.DOWNLOAD_HISTORY_DB_PATH).resolve()
266: 
267:         if (not str(self.CACHE_ROOT_DIR)) or (str(self.CACHE_ROOT_DIR).strip() == '.'):
268:             self.CACHE_ROOT_DIR = Path('/mnt/cache')
269:         elif not self.CACHE_ROOT_DIR.is_absolute():
270:             self.CACHE_ROOT_DIR = (self.BASE_PATH_SCRIPTS / self.CACHE_ROOT_DIR).resolve()
271:         else:
272:             self.CACHE_ROOT_DIR = self.CACHE_ROOT_DIR.resolve()
273: 
274:         # Default PROJECTS_DIR if not set
275:         if self.PROJECTS_DIR is None or (isinstance(self.PROJECTS_DIR, str) and not self.PROJECTS_DIR):
276:             self.PROJECTS_DIR = self.BASE_PATH_SCRIPTS / 'projets_extraits'
277:         elif isinstance(self.PROJECTS_DIR, str):
278:             self.PROJECTS_DIR = Path(self.PROJECTS_DIR)
279:         # Default ARCHIVES_DIR if not set
280:         if self.ARCHIVES_DIR is None or (isinstance(self.ARCHIVES_DIR, str) and not self.ARCHIVES_DIR):
281:             self.ARCHIVES_DIR = self.BASE_PATH_SCRIPTS / 'archives'
282:         elif isinstance(self.ARCHIVES_DIR, str):
283:             self.ARCHIVES_DIR = Path(self.ARCHIVES_DIR)
284:             
285:         # Create necessary directories
286:         self._create_directories()
287:     
288:     def _create_directories(self) -> None:
289:         """Create necessary directories if they don't exist."""
290:         directories_to_create = [
291:             self.LOGS_DIR,
292:             self.LOGS_DIR / 'step1',
293:             self.LOGS_DIR / 'step2',
294:             self.LOGS_DIR / 'step3',
295:             self.LOGS_DIR / 'step4',
296:             self.LOGS_DIR / 'step5',
297:             self.LOGS_DIR / 'step6',
298:             self.LOGS_DIR / 'step7',
299:             # Ensure projects directory exists by default to avoid confusion
300:             self.PROJECTS_DIR,
301:             # Ensure archives directory exists
302:             self.ARCHIVES_DIR,
303:         ]
304:         
305:         for directory in directories_to_create:
306:             try:
307:                 directory.mkdir(parents=True, exist_ok=True)
308:                 logger.debug(f"Ensured directory exists: {directory}")
309:             except Exception as e:
310:                 logger.error(f"Failed to create directory {directory}: {e}")
311:     
312:     def validate(self, strict: bool = None) -> bool:
313:         """
314:         Validate the configuration and ensure all required settings are present.
315: 
316:         Args:
317:             strict: If None, uses DEBUG mode to determine strictness.
318:                    If True, raises errors. If False, logs warnings.
319: 
320:         Returns:
321:             bool: True if configuration is valid
322: 
323:         Raises:
324:             ValueError: If required configuration is missing or invalid and strict=True
325:         """
326:         if strict is None:
327:             strict = not self.DEBUG  # Strict in production, lenient in development
328: 
329:         errors = []
330:         warnings = []
331: 
332:         # Security validation
333:         if not self.INTERNAL_WORKER_TOKEN:
334:             msg = "INTERNAL_WORKER_COMMS_TOKEN environment variable is required"
335:             if strict:
336:                 errors.append(msg)
337:             else:
338:                 warnings.append(msg)
339:                 # Set development default
340:                 self.INTERNAL_WORKER_TOKEN = "dev-internal-worker-token"
341: 
342:         if not self.RENDER_REGISTER_TOKEN:
343:             msg = "RENDER_REGISTER_TOKEN environment variable is required"
344:             if strict:
345:                 errors.append(msg)
346:             else:
347:                 warnings.append(msg)
348:                 # Set development default
349:                 self.RENDER_REGISTER_TOKEN = "dev-render-register-token"
350: 
351:         # Production security checks
352:         if not self.DEBUG and self.SECRET_KEY in ['dev-key-change-in-production', 'dev-secret-key-change-in-production-12345678901234567890']:
353:             errors.append("FLASK_SECRET_KEY must be changed in production (DEBUG=false)")
354: 
355:         # Webhook validation (single data source)
356:         if not self.WEBHOOK_JSON_URL:
357:             msg = "WEBHOOK_JSON_URL must be set"
358:             if strict:
359:                 errors.append(msg)
360:             else:
361:                 warnings.append(msg)
362:         if self.WEBHOOK_TIMEOUT <= 0:
363:             warnings.append("WEBHOOK_TIMEOUT should be > 0; using default")
364:         if self.WEBHOOK_CACHE_TTL < 0:
365:             warnings.append("WEBHOOK_CACHE_TTL should be >= 0; using default")
366:         
367:         # Path validation
368:         if not self.BASE_PATH_SCRIPTS.exists():
369:             warnings.append(f"Base scripts path does not exist: {self.BASE_PATH_SCRIPTS}")
370:         
371:         if not self.LOCAL_DOWNLOADS_DIR.exists():
372:             warnings.append(f"Downloads directory does not exist: {self.LOCAL_DOWNLOADS_DIR}")
373:         
374:         # Python executable validation
375:         python_exe_path = Path(self.PYTHON_VENV_EXE)
376:         if not python_exe_path.exists():
377:             warnings.append(f"Python executable not found: {python_exe_path}")
378:         
379:         # Log warnings
380:         for warning in warnings:
381:             logger.warning(warning)
382:         
383:         # Log warnings
384:         if warnings:
385:             for warning in warnings:
386:                 logger.warning(f"Configuration warning: {warning}")
387:             if not strict:
388:                 logger.warning("Using development defaults - NOT SUITABLE FOR PRODUCTION")
389: 
390:         # Raise errors if any
391:         if errors:
392:             error_msg = f"Configuration validation failed: {'; '.join(errors)}"
393:             logger.error(error_msg)
394:             raise ValueError(error_msg)
395: 
396:         if warnings and not strict:
397:             logger.info("Configuration validation completed with warnings (development mode)")
398:         else:
399:             logger.info("Configuration validation successful")
400:         return True
401:     
402:     def get_venv_path(self, venv_name: str) -> Path:
403:         """
404:         Get the path to a virtual environment.
405:         
406:         Args:
407:             venv_name: Name of the virtual environment (e.g., 'env', 'audio_env', 'tracking_env')
408:             
409:         Returns:
410:             Path object to the virtual environment directory
411:         """
412:         return self.VENV_BASE_DIR / venv_name
413:     
414:     def get_venv_python(self, venv_name: str) -> Path:
415:         """
416:         Get the path to the Python executable in a virtual environment.
417:         
418:         Args:
419:             venv_name: Name of the virtual environment
420:             
421:         Returns:
422:             Path object to the Python executable
423:         """
424:         return self.get_venv_path(venv_name) / "bin" / "python"
425:     
426:     def get_allowed_base_paths(self) -> List[Path]:
427:         """
428:         Get list of allowed base paths for file operations.
429:         
430:         Returns:
431:             List of Path objects representing allowed base directories
432:         """
433:         return [
434:             self.BASE_PATH_SCRIPTS,
435:             self.LOCAL_DOWNLOADS_DIR,
436:             self.LOGS_DIR,
437:             self.BASE_PATH_SCRIPTS / 'workflow_scripts',
438:             self.BASE_PATH_SCRIPTS / 'static',
439:             self.BASE_PATH_SCRIPTS / 'templates',
440:             self.BASE_PATH_SCRIPTS / 'utils',
441:         ]
442:     
443:     def to_dict(self) -> dict:
444:         """
445:         Convert configuration to dictionary for serialization.
446:         
447:         Returns:
448:             Dictionary representation of configuration (excluding sensitive data)
449:         """
450:         config_dict = {}
451:         for key, value in self.__dict__.items():
452:             # Exclude sensitive information
453:             if 'TOKEN' in key or 'SECRET' in key:
454:                 config_dict[key] = '***HIDDEN***' if value else None
455:             elif isinstance(value, Path):
456:                 config_dict[key] = str(value)
457:             else:
458:                 config_dict[key] = value
459:         
460:         return config_dict
461:     
462:     @staticmethod
463:     def check_gpu_availability() -> dict:
464:         """
465:         Vérifier la disponibilité GPU pour STEP5 (MediaPipe + OpenSeeFace).
466:         
467:         Returns:
468:             dict: {
469:                 'available': bool,
470:                 'reason': str (si non disponible),
471:                 'vram_total_gb': float (si disponible),
472:                 'vram_free_gb': float (si disponible),
473:                 'cuda_version': str (si disponible),
474:                 'onnx_cuda': bool,
475:                 'tensorflow_gpu': bool
476:             }
477:         """
478:         result = {
479:             'available': False,
480:             'reason': '',
481:             'onnx_cuda': False,
482:             'tensorflow_gpu': False
483:         }
484:         
485:         # Check PyTorch CUDA (indicateur général)
486:         try:
487:             import torch
488:             if not torch.cuda.is_available():
489:                 result['reason'] = 'CUDA not available (PyTorch check)'
490:                 return result
491:             
492:             # Vérifier VRAM
493:             vram_total = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # Go
494:             vram_free = (torch.cuda.mem_get_info()[0]) / (1024**3)  # Go
495:             
496:             result['vram_total_gb'] = round(vram_total, 2)
497:             result['vram_free_gb'] = round(vram_free, 2)
498:             result['cuda_version'] = torch.version.cuda
499:             
500:             # Vérifier VRAM minimale (2 Go libres recommandés)
501:             if vram_free < 1.5:
502:                 result['reason'] = f'VRAM insuffisante ({vram_free:.1f} Go libres < 1.5 Go)'
503:                 return result
504:         
505:         except ImportError:
506:             result['reason'] = 'PyTorch not installed in tracking_env'
507:             return result
508:         except Exception as e:
509:             result['reason'] = f'PyTorch CUDA check failed: {e}'
510:             return result
511:         
512:         # Check ONNXRuntime CUDA provider (requis pour OpenSeeFace / InsightFace)
513:         try:
514:             import onnxruntime as ort
515:             if 'CUDAExecutionProvider' in ort.get_available_providers():
516:                 result['onnx_cuda'] = True
517:         except ImportError:
518:             pass
519:         except Exception as e:
520:             logger.warning(f"ONNXRuntime check failed: {e}")
521: 
522:         # Optional external ONNXRuntime CUDA check (useful when ORT GPU lives in a dedicated venv)
523:         if not result.get('onnx_cuda'):
524:             ort_gpu_python = os.environ.get('STEP5_INSIGHTFACE_ENV_PYTHON', '').strip()
525:             if ort_gpu_python:
526:                 try:
527:                     ort_check_code = (
528:                         "import sys\n"
529:                         "import onnxruntime as ort\n"
530:                         "providers = ort.get_available_providers()\n"
531:                         "sys.stdout.write('1' if 'CUDAExecutionProvider' in providers else '0')"
532:                     )
533:                     completed = subprocess.run(
534:                         [ort_gpu_python, "-c", ort_check_code],
535:                         capture_output=True,
536:                         text=True,
537:                         timeout=15,
538:                     )
539:                     if completed.returncode == 0:
540:                         result['onnx_cuda'] = completed.stdout.strip() == "1"
541:                     else:
542:                         logger.warning(
543:                             "ONNXRuntime GPU check failed (external env returned code %s): %s",
544:                             completed.returncode,
545:                             completed.stderr.strip(),
546:                         )
547:                 except FileNotFoundError:
548:                     logger.warning(
549:                         "STEP5_INSIGHTFACE_ENV_PYTHON '%s' introuvable pour la vérification GPU ONNXRuntime",
550:                         ort_gpu_python,
551:                     )
552:                 except subprocess.TimeoutExpired:
553:                     logger.warning("ONNXRuntime GPU check timed out via STEP5_INSIGHTFACE_ENV_PYTHON")
554:                 except Exception as exc:
555:                     logger.warning(f"ONNXRuntime GPU check failed via STEP5_INSIGHTFACE_ENV_PYTHON: {exc}")
556:         
557:         # Déterminer disponibilité finale (InsightFace s'appuie uniquement sur ONNX Runtime GPU)
558:         if result['onnx_cuda']:
559:             result['available'] = True
560:         else:
561:             result['reason'] = 'ONNXRuntime GPU indisponible (installer onnxruntime-gpu)'
562:         
563:         return result
564:     
565:     @staticmethod
566:     def is_step5_gpu_enabled() -> bool:
567:         """
568:         Vérifier si le mode GPU STEP5 est activé via configuration.
569:         
570:         Returns:
571:             bool: True si STEP5_ENABLE_GPU=1
572:         """
573:         return _parse_bool(os.environ.get('STEP5_ENABLE_GPU'), default=False)
574:     
575:     @staticmethod
576:     def get_step5_gpu_engines() -> List[str]:
577:         """
578:         Récupérer la liste des moteurs STEP5 autorisés à utiliser le GPU.
579:         
580:         Returns:
581:             List[str]: ['mediapipe_landmarker', 'openseeface', ...]
582:         """
583:         engines_str = os.environ.get('STEP5_GPU_ENGINES', '')
584:         engines = _parse_csv_list(engines_str)
585:         # Normaliser les noms
586:         return [e.strip().lower() for e in engines if e.strip()]
587:     
588:     @staticmethod
589:     def get_step5_gpu_max_vram_mb() -> int:
590:         """
591:         Récupérer la limite VRAM maximale pour STEP5 GPU (Mo).
592:         
593:         Returns:
594:             int: Limite en Mo (défaut: 2048)
595:         """
596:         return _parse_optional_positive_int(
597:             os.environ.get('STEP5_GPU_MAX_VRAM_MB')
598:         ) or 2048
599: 
600: 
601: # Global configuration instance
602: config = Config()
```

## File: static/css/layout.css
```css
  1: .workflow-wrapper {
  2:     display: flex;
  3:     width: 100%;
  4:     max-width: 1600px;
  5:     flex-grow: 1;
  6: }
  7: 
  8: /* Local Downloads panel animation (class-based for max browser compatibility) */
  9: .local-downloads-section {
 10:     opacity: 0;
 11:     transform: translateX(30px) scale(0.98);
 12:     transition: opacity 0.4s cubic-bezier(0.25, 0.46, 0.45, 0.94),
 13:                 transform 0.4s cubic-bezier(0.25, 0.46, 0.45, 0.94);
 14: }
 15: 
 16: .local-downloads-section.downloads-visible {
 17:     opacity: 1;
 18:     transform: translateX(0) scale(1);
 19: }
 20: 
 21: @media (prefers-reduced-motion: reduce) {
 22:     .local-downloads-section { transition: none !important; transform: none !important; }
 23: }
 24: 
 25: /* Keep vertical scrollbar always present to avoid layout jank during height changes */
 26: html, body { overflow-y: scroll; }
 27: 
 28: .steps-column {
 29:     flex: 1 1 100%;
 30:     padding: 10px 20px;
 31:     display: flex;
 32:     flex-direction: column;
 33:     align-items: center;
 34:     transition: flex-basis 0.5s ease-in-out, padding 0.5s ease-in-out, opacity 0.5s ease-in-out;
 35:     opacity: 1;
 36:     scroll-margin-top: 0;
 37: }
 38: 
 39: /* Compact mode: grid layout (multiple cards per row, no large vertical gaps) */
 40: .workflow-wrapper.compact-mode .steps-column {
 41:     display: grid;
 42:     grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
 43:     grid-auto-flow: row dense; /* backfill holes to reduce empty spaces between rows */
 44:     gap: 16px 16px; /* row gap, column gap */
 45:     align-items: start; /* cards height adapt to content by default */
 46:     align-content: start; /* pack rows at the top to avoid large vertical gaps between groups */
 47:     justify-items: center; /* center cards if column wider than minmax */
 48:     padding: 10px 20px;
 49:     transition: opacity 0.5s ease-in-out;
 50: }
 51: 
 52: /* Desktop optimization: uniformize row heights to avoid subtle misalignment */
 53: @media (min-width: 1200px) {
 54:   .workflow-wrapper.compact-mode .steps-column {
 55:     align-items: start;
 56:   }
 57: }
 58: 
 59: /* (removed flex-specific sizing for steps; grid handles column widths) */
 60: 
 61: .workflow-wrapper.logs-active .steps-column {
 62:     flex-basis: 48%;
 63:     align-items: stretch;
 64:     opacity: 1;
 65: }
 66: 
 67: /* COMPACT MODE: Overlay logs panel to avoid grid reflow and saccades */
 68: .workflow-wrapper.compact-mode .logs-column {
 69:     position: fixed;
 70:     right: 16px;
 71:     /* Align approximately below unified controls; adjust if needed */
 72:     top: 120px;
 73:     width: min(50vw, 800px);
 74:     height: calc(100vh - 140px);
 75:     z-index: 20;
 76:     box-shadow: -5px 0 15px rgba(0,0,0,0.25);
 77: }
 78: .workflow-wrapper.compact-mode .logs-column {
 79:     transition: opacity 0.3s ease, transform 0.3s ease;
 80: }
 81: .workflow-wrapper.compact-mode:not(.logs-active) .logs-column {
 82:     opacity: 0;
 83:     pointer-events: none;
 84:     transform: translateX(30px);
 85: }
 86: .workflow-wrapper.compact-mode.logs-active .logs-column {
 87:     opacity: 1;
 88:     pointer-events: auto;
 89:     transform: translateX(0);
 90: }
 91: 
 92: /* In compact mode, keep steps column width unchanged when logs open (no flex-basis change) */
 93: .workflow-wrapper.compact-mode.logs-active .steps-column {
 94:     flex-basis: auto;
 95:     /* Reserve space for the fixed logs panel so steps never go underneath it */
 96:     margin-right: min(50vw, 800px);
 97:     padding-right: 16px; /* small gutter between steps and logs */
 98:     min-height: calc(100vh - 140px); /* ensure sticky active card has space */
 99:     /* Subtle parallax/brightness for depth while logs overlay is visible */
100:     transform: scale(0.985);
101:     filter: brightness(0.95);
102:     transition: transform 0.35s ease, filter 0.35s ease;
103: }
104: 
105: /* Keep the active step visually pinned relative to the logs panel (consistent vertical position) */
106: .workflow-wrapper.compact-mode.logs-active .steps-column .step.active-for-log-panel {
107:     position: sticky;
108:     top: 120px; /* align with .logs-column top for visual consistency */
109:     z-index: 1; /* ensure above siblings during transitions */
110:     /* Span full width across grid */
111:     grid-column: 1 / -1;
112: }
113: 
114: /* Entering phase: hide non-active steps to avoid saccade before panel is fully active */
115: .workflow-wrapper.compact-mode.logs-entering .steps-column .step:not(.active-for-log-panel) {
116:     opacity: 0 !important;
117:     transform: translateY(8px) scale(0.97) !important;
118: }
119: .workflow-wrapper.compact-mode.logs-active .steps-column .step:not(.active-for-log-panel) {
120:     display: none !important; /* fully remove from layout to keep stable height/scrollbar */
121: }
122: .workflow-wrapper.logs-active .steps-column .step.active-for-log-panel {
123:     opacity: 1;
124:     transform: translateY(0) scale(1);
125:     pointer-events: auto;
126:     transition: opacity 0.3s ease-in-out, transform 0.3s ease-in-out;
127: }
128: 
129: /* Ensure explicit visible state when logs panel is closed */
130: .workflow-wrapper:not(.logs-active) .steps-column .step {
131:     opacity: 1;
132:     transform: translateY(0) scale(1);
133:     transition: opacity 0.4s ease-in-out, transform 0.4s ease-in-out;
134: }
135: 
136: .logs-column {
137:     flex: 0 0 0%;
138:     width: 0%;
139:     opacity: 0;
140:     transform: translateX(30px);
141:     background-color: var(--bg-card);
142:     border-left: 1px solid var(--border-color);
143:     padding: 0;
144:     overflow-y: auto;
145:     transition: flex-basis 0.5s ease-in-out, width 0.5s ease-in-out, opacity 0.4s 0.1s ease-in-out, transform 0.5s ease-in-out, padding 0.5s ease-in-out;
146:     display: flex;
147:     flex-direction: column;
148:     box-shadow: -5px 0 15px rgba(0,0,0,0.2);
149:     max-height: calc(100vh - 40px);
150: }
151: 
152: .workflow-wrapper.logs-active .logs-column {
153:     flex-basis: 50%;
154:     width: 50%;
155:     opacity: 1;
156:     transform: translateX(0);
157:     padding: 20px 20px 20px 52px;
158:  }
159: 
160: .workflow-wrapper.compact-mode .step-details-panel {
161:     position: fixed;
162:     right: 16px;
163:     top: 120px;
164:     width: min(30vw, 420px);
165:     height: calc(100vh - 140px);
166:     z-index: 19;
167:     opacity: 0;
168:     pointer-events: none;
169:     transform: translateX(30px);
170:     transition: opacity 0.3s ease, transform 0.3s ease;
171:  }
172: 
173: .workflow-wrapper.compact-mode.details-active .step-details-panel {
174:     opacity: 1;
175:     pointer-events: auto;
176:     transform: translateX(0);
177:  }
178: 
179: .workflow-wrapper.compact-mode.details-active .steps-column {
180:     margin-right: min(30vw, 420px);
181:     padding-right: 16px;
182:  }
183: 
184: .workflow-wrapper.compact-mode.logs-active.details-active .steps-column {
185:     margin-right: min(50vw, 800px);
186:     padding-right: 16px;
187:  }
188: 
189: .workflow-wrapper.compact-mode.logs-active .step-details-panel {
190:     opacity: 0;
191:     pointer-events: none;
192:     transform: translateX(30px);
193:  }
194: 
195: /* Unified controls section compatibility with slideshow mode */
196: .workflow-wrapper.slideshow-mode ~ .unified-controls-section,
197: .unified-controls-section + .workflow-wrapper.slideshow-mode {
198:     /* Ensure proper spacing and visibility during slideshow */
199:     z-index: 1;
200:     position: relative;
201:  }
```

## File: static/apiService.js
```javascript
  1: // --- START OF REFACTORED apiService.js ---
  2: 
  3: import { POLLING_INTERVAL } from './constants.js';
  4: import * as ui from './uiUpdater.js';
  5: import * as dom from './domElements.js';
  6: import { appState } from './state/AppState.js';
  7: import { showNotification, sendBrowserNotification } from './utils.js';
  8: import { soundEvents } from './soundManager.js';
  9: import { pollingManager } from './utils/PollingManager.js';
 10: import { errorHandler } from './utils/ErrorHandler.js';
 11: 
 12: 
 13: /**
 14:  * Fetch helper that toggles a loading state on a button during the request.
 15:  * It adds data-loading="true" and disables the button while the fetch runs.
 16:  * @param {string} url
 17:  * @param {RequestInit} options
 18:  * @param {HTMLElement|string|null} buttonElOrId - element or element id
 19:  * @returns {Promise<any>} parsed JSON response (or throws on network/error)
 20:  */
 21: export async function fetchWithLoadingState(url, options = {}, buttonElOrId = null) {
 22:     let btn = null;
 23:     if (typeof buttonElOrId === 'string') {
 24:         btn = document.getElementById(buttonElOrId);
 25:     } else if (buttonElOrId && buttonElOrId.nodeType === 1) {
 26:         btn = buttonElOrId;
 27:     }
 28: 
 29:     try {
 30:         if (btn) {
 31:             btn.setAttribute('data-loading', 'true');
 32:             btn.disabled = true;
 33:         }
 34:         const response = await fetch(url, options);
 35:         const data = await response.json().catch(() => ({}));
 36:         if (!response.ok) {
 37:             throw new Error((data && data.message) || `Erreur HTTP ${response.status}`);
 38:         }
 39:         return data;
 40:     } finally {
 41:         if (btn) {
 42:             btn.removeAttribute('data-loading');
 43:             btn.disabled = false;
 44:         }
 45:     }
 46: }
 47: 
 48: /**
 49:  * Centralized function to handle UI updates and state changes when a step fails.
 50:  * This avoids code duplication in runStepAPI and performPoll.
 51:  * @param {string} stepKey - The key of the step that failed.
 52:  * @param {Error} error - The error object.
 53:  * @param {string} errorSource - A string indicating where the error occurred (e.g., 'Lancement', 'Polling').
 54:  */
 55: function handleStepFailure(stepKey, error, errorSource) {
 56:     const errorMessage = error.message || 'Erreur inconnue.';
 57:     console.error(`[API handleStepFailure] Erreur ${errorSource} pour ${stepKey}:`, error);
 58: 
 59:     if (errorMessage.includes('Étape inconnue')) {
 60:         console.warn(`[API handleStepFailure] Étape '${stepKey}' n'est pas reconnue.`);
 61:     }
 62: 
 63:     soundEvents.errorEvent();
 64: 
 65:     showNotification(`Erreur ${errorSource} ${stepKey}: ${errorMessage}`, 'error');
 66: 
 67:     const statusEl = document.getElementById(`status-${stepKey}`);
 68:     if (statusEl) {
 69:         statusEl.textContent = `Erreur: ${errorMessage.substring(0, 50)}`;
 70:         statusEl.className = 'status-badge status-failed';
 71:     }
 72: 
 73:     if (appState.getStateProperty('activeStepKeyForLogsPanel') === stepKey) {
 74:         ui.updateMainLogOutputUI(`<i>Erreur d'initiation: ${errorMessage}</i>`);
 75:     }
 76: 
 77:     const runButton = document.querySelector(`.run-button[data-step="${stepKey}"]`);
 78:     if (runButton) {
 79:         runButton.disabled = !!appState.getStateProperty('isAnySequenceRunning');
 80:     }
 81:     const cancelButton = document.querySelector(`.cancel-button[data-step="${stepKey}"]`);
 82:     if (cancelButton) {
 83:         cancelButton.disabled = true;
 84:     }
 85: 
 86:     ui.stopStepTimer(stepKey);
 87:     const progressBar = document.getElementById(`progress-bar-${stepKey}`);
 88:     if (progressBar) {
 89:         progressBar.style.backgroundColor = 'var(--red)';
 90:     }
 91: 
 92:     stopPollingAPI(stepKey);
 93: }
 94: 
 95: 
 96: export async function runStepAPI(stepKey) {
 97:     // --- UI setup (unchanged) ---
 98:     ui.resetStepTimerDisplay(stepKey);
 99:     const statusEl = document.getElementById(`status-${stepKey}`);
100:     if(statusEl) { statusEl.textContent = 'Initiation...'; statusEl.className = 'status-badge status-initiated'; }
101:     const runButton = document.querySelector(`.run-button[data-step="${stepKey}"]`);
102:     if(runButton) runButton.disabled = true;
103:     const cancelButton = document.querySelector(`.cancel-button[data-step="${stepKey}"]`);
104:     if(cancelButton) cancelButton.disabled = false;
105: 
106:     if (appState.getStateProperty('activeStepKeyForLogsPanel') === stepKey) {
107:         ui.updateMainLogOutputUI('<i>Initiation du processus...</i>');
108:     }
109: 
110:     try {
111:         const data = await fetchWithLoadingState(`/run/${stepKey}`, { method: 'POST' }, runButton);
112:         console.log(`[API runStepAPI] Réponse pour ${stepKey}:`, data);
113: 
114:         if (data.status === 'initiated') {
115:             const statusEl = document.getElementById(`status-${stepKey}`);
116:             if(statusEl) { statusEl.textContent = 'Lancé'; statusEl.className = 'status-badge status-starting'; }
117:             ui.startStepTimer(stepKey);
118:             console.log(`[API runStepAPI] Appel de startPollingAPI pour ${stepKey}`);
119:             startPollingAPI(stepKey);
120:             return true;
121:         } else {
122:             throw new Error(data.message || `Réponse invalide du serveur pour le lancement de ${stepKey}.`);
123:         }
124:     } catch (error) {
125:         handleStepFailure(stepKey, error, 'Lancement');
126:         return false;
127:     }
128: }
129: 
130: function appendItalicLineToMainLog(panelEl, message) {
131:     if (!panelEl) return;
132:     const br = document.createElement('br');
133:     const i = document.createElement('i');
134:     i.textContent = String(message ?? '');
135:     panelEl.appendChild(br);
136:     panelEl.appendChild(i);
137: }
138: 
139: export async function cancelStepAPI(stepKey) {
140:     if (appState.getStateProperty('activeStepKeyForLogsPanel') === stepKey) {
141:         appendItalicLineToMainLog(dom.mainLogOutputPanel, 'Annulation en cours...');
142:     }
143: 
144:     try {
145:         const cancelUrl = `/cancel/${stepKey}`;
146:         const fullUrl = new URL(cancelUrl, window.location.origin).href;
147:         console.log(`[CANCEL DEBUG] Attempting to cancel ${stepKey}:`);
148:         console.log(`  - Relative URL: ${cancelUrl}`);
149:         console.log(`  - Full URL: ${fullUrl}`);
150:         console.log(`  - Current origin: ${window.location.origin}`);
151:         console.log(`  - Current port: ${window.location.port}`);
152: 
153:         const cancelButton = document.querySelector(`.cancel-button[data-step="${stepKey}"]`);
154:         const data = await fetchWithLoadingState(cancelUrl, { method: 'POST' }, cancelButton);
155:         console.log(`[CANCEL DEBUG] Response received (ok):`, data);
156:         showNotification(data.message || "Annulation demandée", 'info');
157: 
158:         if (appState.getStateProperty('activeStepKeyForLogsPanel') === stepKey) {
159:             appendItalicLineToMainLog(dom.mainLogOutputPanel, data.message || 'Annulation demandée');
160:         }
161:     } catch (error) {
162:         console.error(`Erreur annulation ${stepKey}:`, error);
163: 
164:         errorHandler.handleApiError(`cancel/${stepKey}`, error, { stepKey });
165: 
166:         if (appState.getStateProperty('activeStepKeyForLogsPanel') === stepKey) {
167:             appendItalicLineToMainLog(dom.mainLogOutputPanel, `Erreur communication pour annulation: ${error.toString()}`);
168:         }
169:     }
170: }
171: 
172: export function startPollingAPI(stepKey, isAutoModeHighFrequency = false) {
173:     stopPollingAPI(stepKey);
174: 
175:     const pollingInterval = isAutoModeHighFrequency ? 200 : POLLING_INTERVAL;
176:     console.log(`[API startPollingAPI] 🚀 Polling démarré pour ${stepKey}. Intervalle: ${pollingInterval}ms ${isAutoModeHighFrequency ? '(AutoMode high-frequency)' : '(normal)'}`);
177: 
178:     const performPoll = async () => {
179:         try {
180:             const pollStartTime = performance.now();
181:             console.log(`[API POLL] Fetching status for ${stepKey} at ${new Date().toISOString()}`);
182: 
183:             const response = await fetch(`/status/${stepKey}`);
184:             if (!response.ok) {
185:                 console.warn(`[API performPoll] Erreur ${response.status} lors du polling pour ${stepKey}. Arrêt du polling.`);
186:                 stopPollingAPI(stepKey);
187:                 if (!appState.getStateProperty('isAnySequenceRunning')) {
188:                     handleStepFailure(stepKey, new Error(`Erreur statut (${response.status})`), 'Polling');
189:                 }
190:                 return;
191:             }
192:             const data = await response.json();
193: 
194:             const pollEndTime = performance.now();
195:             const statusEmoji = data.status === 'running' ? '🔄' : data.status === 'completed' ? '✅' : data.status === 'failed' ? '❌' : '⚪';
196:             console.log(`[API POLL RESPONSE] ${statusEmoji} ${stepKey} (${(pollEndTime - pollStartTime).toFixed(2)}ms): status="${data.status}", progress=${data.progress_current}/${data.progress_total}, return_code=${data.return_code}`);
197: 
198:             const previousStatus = appState.getStateProperty(`processInfo.${stepKey}.status`) || 'unknown';
199:             appState.setState({ processInfo: { [stepKey]: data } }, 'process_info_polled');
200: 
201:             if (typeof data.is_any_sequence_running === 'boolean') {
202:                 appState.setState({ isAnySequenceRunning: data.is_any_sequence_running }, 'sequence_running_polled');
203:             }
204: 
205:             ui.updateStepCardUI(stepKey, data);
206: 
207:             const workflowWrapper = typeof dom.getWorkflowWrapper === 'function' ? dom.getWorkflowWrapper() : dom.workflowWrapper;
208:             if (appState.getStateProperty('activeStepKeyForLogsPanel') === stepKey && workflowWrapper && workflowWrapper.classList.contains('logs-active')) {
209:                 ui.updateMainLogOutputUI(data.log.join(''));
210:             }
211:             const isTerminal = ['completed', 'failed'].includes(data.status);
212:             if (isTerminal && previousStatus !== data.status) {
213:                 const title = data.status === 'completed' ? '✅ Étape terminée' : '❌ Étape en erreur';
214:                 const body = `${stepKey} — statut: ${data.status}`;
215:                 sendBrowserNotification(title, body).catch(() => {});
216:             }
217: 
218:             const shouldStopPolling = isTerminal ||
219:                                     (data.status === 'idle' && !appState.getStateProperty('autoModeEnabled'));
220: 
221:             if (shouldStopPolling) {
222:                 console.log(`[API performPoll] Statut final '${data.status}' pour ${stepKey}. Arrêt du polling.`);
223:                 stopPollingAPI(stepKey);
224:             } else if (data.status === 'idle' && appState.getStateProperty('autoModeEnabled')) {
225:                 console.log(`[API performPoll] ⚪ ${stepKey} idle mais AutoMode actif - maintien du polling pour détecter les transitions`);
226:             }
227:         } catch (error) {
228:             console.error(`[API performPoll] Erreur CATCH polling ${stepKey}:`, error);
229:             stopPollingAPI(stepKey);
230:             if (!appState.getStateProperty('isAnySequenceRunning')) {
231:                 handleStepFailure(stepKey, error, 'Polling');
232:             }
233:         }
234:     };
235: 
236:     const pollingId = pollingManager.startPolling(
237:         `step-${stepKey}`,
238:         performPoll,
239:         pollingInterval, // Use dynamic interval (200ms for AutoMode, 500ms for normal)
240:         { immediate: true, maxErrors: 3 }
241:     );
242: }
243: 
244: export function stopPollingAPI(stepKey) {
245:     pollingManager.stopPolling(`step-${stepKey}`);
246:     console.log(`[API stopPollingAPI] Polling arrêté pour ${stepKey}.`);
247: }
248: 
249: export async function fetchSpecificLogAPI(stepKey, logIndex, logName, buttonElOrId = null) {
250:     ui.updateSpecificLogUI(logName, null, "<i>Chargement...</i>");
251:     try {
252:         let data;
253:         const url = `/get_specific_log/${stepKey}/${logIndex}`;
254:         if (buttonElOrId) {
255:             data = await fetchWithLoadingState(url, {}, buttonElOrId);
256:         } else {
257:             const response = await fetch(url);
258:             data = await response.json();
259:             if (!response.ok) {
260:                 throw new Error(data.error || `Erreur HTTP ${response.status}`);
261:             }
262:         }
263:         ui.updateSpecificLogUI(logName, data.path, data.content);
264:     } catch (error) {
265:         console.error(`Erreur fetch log spécifique ${stepKey}/${logIndex}:`, error);
266:         ui.updateSpecificLogUI(logName, null, '', true, `Erreur de communication: ${error.toString()}`);
267:     }
268: }
269: 
270: 
271: export async function fetchInitialStatusAPI(stepKey) {
272:     try {
273:         const response = await fetch(`/status/${stepKey}`);
274:         if (!response.ok) {
275:             console.warn(`Initial status fetch failed for ${stepKey}: ${response.status}. Using fallback.`);
276:             appState.setState({
277:                 processInfo: {
278:                     [stepKey]: appState.getStateProperty(`processInfo.${stepKey}`) || {
279:                         status: 'idle', log: [], progress_current: 0, progress_total: 0, progress_text: '',
280:                         is_any_sequence_running: false
281:                     }
282:                 }
283:             }, 'process_info_initial_fallback');
284:         } else {
285:             const data = await response.json();
286:             appState.setState({ processInfo: { [stepKey]: data } }, 'process_info_initial');
287: 
288:             if (typeof data.is_any_sequence_running === 'boolean') {
289:                 appState.setState({ isAnySequenceRunning: data.is_any_sequence_running }, 'sequence_running_initial');
290:             }
291:         }
292: 
293:         const stepInfo = appState.getStateProperty(`processInfo.${stepKey}`) || {
294:             status: 'idle', log: [], progress_current: 0, progress_total: 0, progress_text: '',
295:             is_any_sequence_running: false
296:         };
297: 
298:         if (stepKey === 'clear_disk_cache') {
299:             ui.updateClearCacheGlobalButtonState(stepInfo.status);
300:         } else {
301:             ui.updateStepCardUI(stepKey, stepInfo);
302:         }
303:         
304:         if (['running', 'starting', 'initiated'].includes(stepInfo.status)) {
305:             console.log(`[API fetchInitialStatusAPI] Étape ${stepKey} en cours au démarrage. Lancement du polling.`);
306:             startPollingAPI(stepKey);
307:         }
308: 
309:     } catch (err) {
310:         console.error(`Erreur CATCH fetchInitialStatusAPI pour ${stepKey}:`, err);
311:         const fallbackData = appState.getStateProperty(`processInfo.${stepKey}`) || {
312:             status: 'idle', log: [], progress_current: 0, progress_total: 0, progress_text: '',
313:             is_any_sequence_running: false
314:         };
315:         if (stepKey === 'clear_disk_cache') {
316:             ui.updateClearCacheGlobalButtonState(fallbackData.status);
317:         } else {
318:             ui.updateStepCardUI(stepKey, fallbackData);
319:         }
320:     }
321: }
322: 
323: export async function fetchLocalDownloadsStatusAPI() {
324:     try {
325:         const response = await fetch('/api/csv_downloads_status');
326:         if (!response.ok) {
327:             console.warn(`Erreur lors de la récupération du statut des téléchargements CSV: ${response.status}`);
328:             return [];
329:         }
330:         return await response.json();
331:     } catch (error) {
332:         console.error("Erreur réseau fetchLocalDownloadsStatusAPI:", error);
333:         return [];
334:     }
335: }
336: 
337: 
338: 
339: export async function fetchCSVMonitorStatusAPI() {
340:     try {
341:         const response = await fetch('/api/csv_monitor_status');
342:         if (!response.ok) {
343:             throw new Error(`Erreur HTTP ${response.status}`);
344:         }
345:         return await response.json();
346:     } catch (error) {
347:         console.error("Erreur fetchCSVMonitorStatusAPI:", error);
348:         return {
349:             csv_monitor: { status: "error", last_check: null, error: "Impossible de récupérer le statut" },
350:             auto_mode_enabled: false,
351:             csv_url: "",
352:             check_interval: 15
353:         };
354:     }
355: }
```

## File: static/popupManager.js
```javascript
  1: import * as dom from './domElements.js';
  2: import { appState } from './state/AppState.js';
  3: import { DOMUpdateUtils } from './utils/DOMBatcher.js';
  4: 
  5: function handlePopupKeydown(event) {
  6:     const popupOverlay = event.currentTarget;
  7:     if (event.key === 'Escape') {
  8:         closePopupUI(popupOverlay);
  9:     }
 10:     if (event.key === 'Tab') {
 11:         const focusableElements = Array.from(popupOverlay.querySelectorAll('button, [href], input, select, textarea, [tabindex]:not([tabindex="-1"])')).filter(el => el.offsetParent !== null);
 12:         if (focusableElements.length === 0) return;
 13: 
 14:         const firstElement = focusableElements[0];
 15:         const lastElement = focusableElements[focusableElements.length - 1];
 16: 
 17:         if (event.shiftKey) {
 18:             if (document.activeElement === firstElement) {
 19:                 lastElement.focus();
 20:                 event.preventDefault();
 21:             }
 22:         } else {
 23:             if (document.activeElement === lastElement) {
 24:                 firstElement.focus();
 25:                 event.preventDefault();
 26:             }
 27:         }
 28:     }
 29: }
 30: 
 31: export function openPopupUI(popupOverlay) {
 32:     if (!popupOverlay) return;
 33: 
 34:     const currentFocused = document.activeElement;
 35:     if (currentFocused &&
 36:         currentFocused !== document.body &&
 37:         currentFocused !== document.documentElement &&
 38:         typeof currentFocused.focus === 'function' &&
 39:         currentFocused.nodeType === Node.ELEMENT_NODE) {
 40:         appState.setState({ focusedElementBeforePopup: currentFocused }, 'popup_focus_store');
 41:         console.debug('[POPUP] Stored focusable element:', {
 42:             tagName: currentFocused.tagName,
 43:             id: currentFocused.id || 'no-id',
 44:             className: currentFocused.className || 'no-class'
 45:         });
 46:     } else {
 47:         appState.setState({ focusedElementBeforePopup: null }, 'popup_focus_store');
 48:         console.debug('[POPUP] No valid focusable element to store');
 49:     }
 50: 
 51:     popupOverlay.style.display = 'flex';
 52:     popupOverlay.setAttribute('data-visible', 'true');
 53:     popupOverlay.setAttribute('aria-hidden', 'false');
 54:     const focusableElements = popupOverlay.querySelectorAll('button, [href], input, select, textarea, [tabindex]:not([tabindex="-1"])');
 55: 
 56:     let elementToFocus = null;
 57:     for (let i = 0; i < focusableElements.length; i++) {
 58:         const element = focusableElements[i];
 59:         if (element.offsetParent !== null && typeof element.focus === 'function') {
 60:             elementToFocus = element;
 61:             break;
 62:         }
 63:     }
 64: 
 65:     if (elementToFocus) {
 66:         try {
 67:             elementToFocus.focus();
 68:         } catch (error) {
 69:             console.warn('[POPUP] Failed to focus element in popup:', error);
 70:         }
 71:     }
 72: 
 73:     popupOverlay.addEventListener('keydown', handlePopupKeydown);
 74: }
 75: 
 76: export function closePopupUI(popupOverlay) {
 77:     if (!popupOverlay) return;
 78:     popupOverlay.removeAttribute('data-visible');
 79:     popupOverlay.setAttribute('aria-hidden', 'true');
 80:     popupOverlay.style.display = 'none';
 81:     popupOverlay.removeEventListener('keydown', handlePopupKeydown);
 82:     const prevFocused = appState.getStateProperty('focusedElementBeforePopup');
 83: 
 84:     if (prevFocused && typeof prevFocused.focus === 'function') {
 85:         try {
 86:             if (prevFocused.isConnected && document.hasFocus()) {
 87:                 if (document.contains(prevFocused) && prevFocused.offsetParent !== null) {
 88:                     prevFocused.focus();
 89:                 } else {
 90:                     console.debug('[POPUP] Previous focused element no longer focusable, skipping focus restoration');
 91:                 }
 92:             }
 93:         } catch (error) {
 94:             console.warn('[POPUP] Failed to restore focus to previous element:', error);
 95:         }
 96:     } else if (prevFocused) {
 97:         const elementInfo = {
 98:             tagName: prevFocused.tagName || 'unknown',
 99:             id: prevFocused.id || 'no-id',
100:             className: prevFocused.className || 'no-class',
101:             nodeType: prevFocused.nodeType || 'unknown',
102:             hasFocusMethod: typeof prevFocused.focus === 'function'
103:         };
104:         console.debug('[POPUP] Previous focused element is not focusable:', elementInfo);
105:     }
106: 
107:     appState.setState({ focusedElementBeforePopup: null }, 'popup_focus_clear');
108: }
109: 
110: function resolveSequenceSummaryElements() {
111:     const overlay = typeof dom.getSequenceSummaryPopupOverlay === 'function'
112:         ? dom.getSequenceSummaryPopupOverlay()
113:         : dom.sequenceSummaryPopupOverlay;
114:     const list = typeof dom.getSequenceSummaryList === 'function'
115:         ? dom.getSequenceSummaryList()
116:         : dom.sequenceSummaryList;
117:     return { overlay, list };
118: }
119: 
120: export function showSequenceSummaryUI(results, overallSuccess, sequenceName = "Séquence", overallDuration = null) {
121:     const { overlay, list } = resolveSequenceSummaryElements();
122:     if (!overlay || !list) {
123:         console.error("Éléments DOM pour la popup de résumé non trouvés!");
124:         return;
125:     }
126:     const summaryTitle = overlay.querySelector("h3");
127:     if (summaryTitle) summaryTitle.textContent = `Résumé: ${sequenceName}`;
128: 
129:     list.innerHTML = '';
130:     if (overallDuration && typeof overallDuration === 'string') {
131:         const totalItem = document.createElement('li');
132:         totalItem.style.fontWeight = 'bold';
133:         totalItem.style.marginBottom = '8px';
134:         totalItem.style.paddingBottom = '8px';
135:         totalItem.style.borderBottom = `1px solid var(--border-color)`;
136:         const safeDuration = DOMUpdateUtils.escapeHtml(overallDuration);
137:         totalItem.innerHTML = `<span class="status-icon" style="color:var(--accent-color);">⏱️</span> Durée totale: ${safeDuration}`;
138:         list.appendChild(totalItem);
139:     }
140:     results.forEach(result => {
141:         const listItem = document.createElement('li');
142:         const icon = result.success ? '<span class="status-icon status-completed" style="color:var(--green);">✔️</span>' : '<span class="status-icon status-failed" style="color:var(--red);">❌</span>';
143:         const safeName = DOMUpdateUtils.escapeHtml(String(result.name ?? ''));
144:         const safeDurationText = result.duration && result.duration !== "N/A" ? DOMUpdateUtils.escapeHtml(String(result.duration)) : "";
145:         const durationText = safeDurationText ? `<span class="duration">(${safeDurationText})</span>` : "";
146:         listItem.innerHTML = `${icon} ${safeName}: ${result.success ? 'Terminée avec succès' : 'Échouée ou annulée'} ${durationText}`;
147:         list.appendChild(listItem);
148:     });
149: 
150:     const overallStatusItem = document.createElement('li');
151:     overallStatusItem.style.fontWeight = 'bold';
152:     overallStatusItem.style.marginTop = '10px';
153:     overallStatusItem.style.paddingTop = '10px';
154:     overallStatusItem.style.borderTop = `1px solid var(--border-color)`;
155:     const safeSequenceName = DOMUpdateUtils.escapeHtml(String(sequenceName ?? 'Séquence'));
156:     if (overallSuccess) {
157:         overallStatusItem.innerHTML = `<span class="status-icon status-completed" style="color:var(--green);">🎉</span> ${safeSequenceName} terminée avec succès !`;
158:     } else {
159:         overallStatusItem.innerHTML = `<span class="status-icon status-failed" style="color:var(--red);">⚠️</span> ${safeSequenceName} a rencontré une ou plusieurs erreurs.`;
160:     }
161:     list.appendChild(overallStatusItem);
162:     openPopupUI(overlay);
163: }
164: 
165: export function showCustomSequenceConfirmUI() {
166:     dom.customSequenceConfirmList.innerHTML = '';
167:     const selectedStepsOrder = appState.getStateProperty('selectedStepsOrder') || [];
168:     selectedStepsOrder.forEach((stepKey, index) => {
169:         const stepElement = document.getElementById(`step-${stepKey}`);
170:         const stepName = stepElement ? stepElement.dataset.stepName : stepKey;
171:         const li = document.createElement('li');
172:         const safeStepName = DOMUpdateUtils.escapeHtml(String(stepName ?? ''));
173:         li.innerHTML = `<span class="order-prefix">${index + 1}.</span> ${safeStepName}`;
174:         dom.customSequenceConfirmList.appendChild(li);
175:     });
176:     openPopupUI(dom.customSequenceConfirmPopupOverlay);
177: }
```

## File: static/sequenceManager.js
```javascript
  1: import { POLLING_INTERVAL } from './constants.js';
  2: import * as ui from './uiUpdater.js';
  3: import { runStepAPI } from './apiService.js';
  4: import { showSequenceSummaryUI } from './popupManager.js';
  5: import { formatElapsedTime } from './utils.js';
  6: import { scrollToActiveStep, isSequenceAutoScrollEnabled } from './scrollManager.js';
  7: 
  8: import { appState } from './state/AppState.js';
  9: 
 10: import { soundEvents } from './soundManager.js';
 11: 
 12: /**
 13:  * Executes and tracks a single step within a sequence.
 14:  * This helper function encapsulates all logic for one step, from initiation to completion.
 15:  * @private
 16:  * @param {string} stepKey - The unique key for the step.
 17:  * @param {string} sequenceName - The name of the parent sequence.
 18:  * @param {number} currentStepNum - The step's number in the sequence (e.g., 1, 2, 3...).
 19:  * @param {number} totalSteps - The total number of steps in the sequence.
 20:  * @returns {Promise<object>} A promise that resolves to a result object: { name, success, duration }.
 21:  */
 22: async function _executeSingleStep(stepKey, sequenceName, currentStepNum, totalSteps) {
 23:     const stepConfig = ui.getStepsConfig()[stepKey];
 24:     const stepDisplayName = stepConfig ? stepConfig.display_name : stepKey;
 25: 
 26:     console.log(`[SEQ_MGR] ${sequenceName} - Step ${currentStepNum}/${totalSteps}: ${stepDisplayName} (${stepKey})`);
 27: 
 28:     ui.updateGlobalProgressUI(`${sequenceName} - Étape ${currentStepNum}/${totalSteps}: ${stepDisplayName}`,
 29:         Math.round(((currentStepNum - 1) / totalSteps) * 100)
 30:     );
 31: 
 32:     if (stepKey !== 'clear_disk_cache') {
 33:         ui.openLogPanelUI(stepKey, true);
 34:         ui.setActiveStepForLogPanelUI(stepKey);
 35:         
 36:         if (isSequenceAutoScrollEnabled()) {
 37:             setTimeout(() => {
 38:                 scrollToActiveStep(stepKey, { behavior: 'smooth', scrollDelay: 0 });
 39:             }, 0);
 40:         }
 41:     }
 42: 
 43:     const stepInitiated = await runStepAPI(stepKey);
 44:     if (!stepInitiated) {
 45:         console.error(`[SEQ_MGR] Initiation FAILED for ${stepKey}`);
 46:         ui.updateGlobalProgressUI(`ÉCHEC: L'étape "${stepDisplayName}" n'a pas pu être initiée. Séquence interrompue.`,
 47:             Math.round(((currentStepNum - 1) / totalSteps) * 100), true
 48:         );
 49:         return { name: stepDisplayName, success: false, duration: "N/A (échec initiation)" };
 50:     }
 51: 
 52:     ui.startStepTimer(stepKey);
 53:     console.log(`[SEQ_MGR] Started timer for ${stepKey}`);
 54: 
 55:     try {
 56:         ui.setActiveStepForLogPanelUI(stepKey);
 57:     } catch (e) {
 58:         console.debug('[SEQ_MGR] setActiveStepForLogPanelUI post-start failed (non-fatal):', e);
 59:     }
 60: 
 61:     let timerData = appState.getStateProperty(`stepTimers.${stepKey}`);
 62:     if (timerData && timerData.startTime) {
 63:         console.log(`[SEQ_MGR] Timer verified for ${stepKey}, start time:`, timerData.startTime);
 64:     } else {
 65:         console.error(`[SEQ_MGR] Timer NOT properly started for ${stepKey}:`, timerData);
 66:     }
 67: 
 68:     console.log(`[SEQ_MGR] Waiting for completion of ${stepKey}`);
 69:     const stepCompleted = await waitForStepCompletionInSequence(stepKey);
 70: 
 71:     ui.stopStepTimer(stepKey);
 72:     console.log(`[SEQ_MGR] Stopped timer for ${stepKey}`);
 73: 
 74:     timerData = appState.getStateProperty(`stepTimers.${stepKey}`);
 75:     const duration = (timerData?.elapsedTimeFormatted) || "N/A";
 76: 
 77:     console.log(`[SEQ_MGR] Timer data for ${stepKey}:`, {
 78:         timerData,
 79:         duration,
 80:         startTime: timerData?.startTime,
 81:         elapsedTimeFormatted: timerData?.elapsedTimeFormatted
 82:     });
 83: 
 84:     if (!stepCompleted) {
 85:         console.error(`[SEQ_MGR] Execution FAILED for ${stepKey}`);
 86: 
 87:         ui.updateGlobalProgressUI(`ÉCHEC: L'étape "${stepDisplayName}" a échoué. Séquence interrompue.`,
 88:             Math.round((currentStepNum / totalSteps) * 100), true
 89:         );
 90:         return { name: stepDisplayName, success: false, duration };
 91:     }
 92: 
 93:     console.log(`[SEQ_MGR] Step ${stepDisplayName} completed successfully.`);
 94: 
 95:     soundEvents.stepSuccess();
 96: 
 97:     return { name: stepDisplayName, success: true, duration };
 98: }
 99: 
100: export async function runStepSequence(stepsToExecute, sequenceName = "Séquence") {
101:     console.log(`[SEQ_MGR] Starting sequence: ${sequenceName} with steps:`, stepsToExecute);
102:     ui.updateGlobalUIForSequenceState(true);
103:     ui.updateGlobalProgressUI(`Démarrage de la ${sequenceName}...`, 0);
104: 
105:     const sequenceStart = Date.now();
106: 
107:     const sequenceResults = [];
108:     const totalStepsInThisSequence = stepsToExecute.length;
109:     const isAutoModeSequence = sequenceName === "AutoMode";
110:     let sequenceFailed = false;
111: 
112:     if (isAutoModeSequence) {
113:         appState.setState({ ui: { autoModeLogPanelOpened: false } }, 'auto_mode_sequence_reset');
114:     }
115: 
116:     for (let i = 0; i < stepsToExecute.length; i++) {
117:         const stepKey = stepsToExecute[i];
118:         const currentStepNum = i + 1;
119: 
120:         const result = await _executeSingleStep(stepKey, sequenceName, currentStepNum, totalStepsInThisSequence);
121:         sequenceResults.push(result);
122: 
123:         if (!result.success) {
124:             sequenceFailed = true;
125:             break; // Exit the loop immediately on failure
126:         }
127: 
128:         if (i < stepsToExecute.length - 1) {
129:             const nextStepKey = stepsToExecute[i + 1];
130:             if (nextStepKey && nextStepKey !== 'clear_disk_cache') {
131:                 try {
132:                     ui.openLogPanelUI(nextStepKey, true);
133:                     ui.setActiveStepForLogPanelUI(nextStepKey);
134:                     
135:                     if (isSequenceAutoScrollEnabled()) {
136:                         setTimeout(() => {
137:                             scrollToActiveStep(nextStepKey, { behavior: 'smooth', scrollDelay: 0 });
138:                         }, 0);
139:                     }
140:                 } catch (e) {
141:                     console.debug('[SEQ_MGR] Pre-focus next step failed (non-fatal):', e);
142:                 }
143:             }
144:             ui.updateGlobalProgressUI(`${sequenceName} - Étape ${currentStepNum}/${totalStepsInThisSequence}: ${result.name} terminée.`,
145:                 Math.round((currentStepNum / totalStepsInThisSequence) * 100)
146:             );
147:         }
148:     }
149: 
150:     console.log(`[SEQ_MGR] Sequence ${sequenceName} finished. sequenceFailed: ${sequenceFailed}`);
151: 
152:     if (sequenceFailed) {
153:     } else {
154:         ui.updateGlobalProgressUI(`${sequenceName} terminée avec succès ! 🎉`, 100);
155:         soundEvents.workflowCompletion();
156:     }
157: 
158:     if (sequenceResults.length > 0) {
159:         const overallDuration = formatElapsedTime(new Date(sequenceStart));
160:         showSequenceSummaryUI(sequenceResults, !sequenceFailed, sequenceName, overallDuration);
161:     } else {
162:         console.warn(`[SEQ_MGR] No results to show for sequence ${sequenceName}`);
163:     }
164: 
165:     ui.updateGlobalUIForSequenceState(false);
166:     if (isAutoModeSequence) {
167:         appState.setState({ ui: { autoModeLogPanelOpened: false } }, 'auto_mode_sequence_reset_end');
168:     }
169: }
170: 
171: function waitForStepCompletionInSequence(stepKey) {
172:     return new Promise((resolve) => {
173:         const intervalIdForLog = `wait_${stepKey}_${Date.now()}`;
174:         console.log(`[SEQ_MGR - ${intervalIdForLog}] Waiting for final status...`);
175: 
176:         const checkInterval = setInterval(() => {
177:             const data = appState.getStateProperty(`processInfo.${stepKey}`);
178: 
179:             if (!data) {
180:                 return;
181:             }
182: 
183:             if (data.status === 'completed') {
184:                 console.log(`[SEQ_MGR - ${intervalIdForLog}] Resolved as COMPLETED.`);
185:                 clearInterval(checkInterval);
186:                 resolve(true);
187:             } else if (data.status === 'failed' || data.return_code === -9) {
188:                 console.error(`[SEQ_MGR - ${intervalIdForLog}] Resolved as FAILED or CANCELLED.`);
189:                 clearInterval(checkInterval);
190:                 resolve(false);
191:             }
192:             }, POLLING_INTERVAL / 2);
193:     });
194: }
```

## File: app_new.py
```python
   1: import csv
   2: import html
   3: import json
   4: import logging
   5: import os
   6: import re
   7: import subprocess
   8: import sys
   9: import threading
  10: import time
  11: import atexit
  12: from collections import deque
  13: from pathlib import Path
  14: from datetime import datetime, timezone, timedelta
  15: import uuid
  16: 
  17: logging.basicConfig(
  18:     level=logging.INFO,
  19:     format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
  20: )
  21: logger = logging.getLogger(__name__)
  22: 
  23: import psutil
  24: import requests
  25: from flask import Flask, render_template, jsonify, request, send_from_directory
  26: from flask_caching import Cache
  27: 
  28: try:
  29:     from dotenv import load_dotenv
  30:     load_dotenv()
  31:     logger.info("Dotenv configuration loaded")
  32: except ImportError:
  33:     logger.warning("python-dotenv not available; relying on environment variables only")
  34: 
  35: from config.settings import config
  36: from config.security import SecurityConfig, require_internal_worker_token, require_render_register_token
  37: from config.workflow_commands import WorkflowCommandsConfig
  38: 
  39: from routes.api_routes import api_bp
  40: from routes.workflow_routes import workflow_bp
  41: from services.monitoring_service import MonitoringService
  42: from services.csv_service import CSVService
  43: from services.workflow_service import WorkflowService
  44: from services.cache_service import CacheService
  45: from services.performance_service import PerformanceService
  46: from services.filesystem_service import FilesystemService
  47: from services.download_service import DownloadService
  48: from services.workflow_state import get_workflow_state, reset_workflow_state
  49: import urllib.parse
  50: 
  51: try:
  52:     import pandas as pd
  53:     PANDAS_AVAILABLE = True
  54: except ImportError:
  55:     PANDAS_AVAILABLE = False
  56:     logger.warning("pandas not available; Excel files cannot be processed")
  57: 
  58: try:
  59:     import pynvml
  60:     if config.ENABLE_GPU_MONITORING:
  61:         PYNVML_AVAILABLE = True
  62:         pynvml.nvmlInit()
  63:         logger.info("GPU monitoring initialized successfully")
  64:     else:
  65:         PYNVML_AVAILABLE = False
  66:         logger.info("GPU monitoring disabled by configuration")
  67: except ImportError:
  68:     PYNVML_AVAILABLE = False
  69:     logger.warning("pynvml not available. GPU monitoring disabled.")
  70: except Exception as e:
  71:     PYNVML_AVAILABLE = False
  72:     logger.error(f"Failed to initialize GPU monitoring: {e}")
  73: 
  74: BASE_PATH_SCRIPTS = config.BASE_PATH_SCRIPTS
  75: PYTHON_VENV_EXE = config.PYTHON_VENV_EXE
  76: 
  77: PARALLEL_TRACKING_SCRIPT_PATH = BASE_PATH_SCRIPTS / "workflow_scripts" / "step5" / "run_tracking_manager.py"
  78: 
  79: os.environ['ROOT_SCAN_DIR'] = str(BASE_PATH_SCRIPTS / "projets_extraits")
  80: KEYWORD_FILTER_TRACKING_ENV = "Camille"
  81: os.environ['FOLDER_KEYWORD'] = KEYWORD_FILTER_TRACKING_ENV
  82: SUBDIR_FILTER_TRACKING_ENV = "docs"
  83: os.environ['SUBFOLDER_NAME'] = SUBDIR_FILTER_TRACKING_ENV
  84: os.environ.setdefault('TRACKING_DISABLE_GPU', '1')
  85: os.environ.setdefault('TRACKING_CPU_WORKERS', '15')
  86: 
  87: LOGS_BASE_DIR = BASE_PATH_SCRIPTS / "logs"
  88: os.makedirs(LOGS_BASE_DIR, exist_ok=True)
  89: 
  90: for step in range(1, 8):
  91:     os.makedirs(LOGS_BASE_DIR / f"step{step}", exist_ok=True)
  92: 
  93: STEP0_PREP_LOG_DIR = Path(os.environ.get('STEP0_PREP_LOG_DIR_ENV', str(LOGS_BASE_DIR / "step1")))
  94: MEDIA_ENCODER_LOGS_DIR = Path(os.environ.get('MEDIA_ENCODER_LOGS_DIR_ENV', str(LOGS_BASE_DIR / "step2")))
  95: SCENE_DETECT_LOG_DIR = Path(os.environ.get('SCENE_DETECT_LOG_DIR_ENV', str(LOGS_BASE_DIR / "step3")))
  96: AUDIO_ANALYSIS_LOG_DIR = Path(os.environ.get('AUDIO_ANALYSIS_LOG_DIR_ENV', str(LOGS_BASE_DIR / "step4")))
  97: 
  98: BASE_TRACKING_LOG_SEARCH_PATH = Path(os.environ.get('BASE_TRACKING_LOG_SEARCH_PATH_ENV', str(BASE_PATH_SCRIPTS)))
  99: BASE_TRACKING_PROGRESS_SEARCH_PATH = Path(os.environ.get('BASE_TRACKING_PROGRESS_SEARCH_PATH_ENV', str(BASE_PATH_SCRIPTS)))
 100: HF_AUTH_TOKEN_ENV = os.environ.get("HF_AUTH_TOKEN")
 101: 
 102: security_config = SecurityConfig()
 103: 
 104: RENDER_APP_CALLBACK_URL_ENV = os.environ.get("RENDER_APP_CALLBACK_URL")
 105: RENDER_APP_CALLBACK_TOKEN_ENV = os.environ.get("RENDER_APP_CALLBACK_TOKEN")
 106: RENDER_REGISTER_URL_ENDPOINT_ENV = os.environ.get("RENDER_REGISTER_URL_ENDPOINT")
 107: 
 108: RENDER_REGISTER_TOKEN_ENV = security_config.RENDER_REGISTER_TOKEN
 109: INTERNAL_WORKER_COMMS_TOKEN_ENV = security_config.INTERNAL_WORKER_TOKEN
 110: 
 111: WEBHOOK_MONITOR_INTERVAL = config.WEBHOOK_MONITOR_INTERVAL
 112: LOCAL_DOWNLOADS_DIR = config.LOCAL_DOWNLOADS_DIR
 113: DOWNLOAD_HISTORY_FILE = config.BASE_PATH_SCRIPTS / "download_history.json"
 114: 
 115: def create_app(config_class=None):
 116:     """
 117:     Create and configure Flask application with modular architecture.
 118: 
 119:     Args:
 120:         config_class: Configuration class to use (defaults to main config)
 121: 
 122:     Returns:
 123:         Configured Flask application
 124:     """
 125:     app = Flask(__name__)
 126: 
 127:     app.config.update({
 128:         'SECRET_KEY': config.SECRET_KEY,
 129:         'DEBUG': config.DEBUG,
 130:         'CACHE_TYPE': 'SimpleCache',
 131:         'CACHE_DEFAULT_TIMEOUT': 300,
 132:         'SEND_FILE_MAX_AGE_DEFAULT': 0
 133:     })
 134: 
 135:     import logging
 136:     logging.getLogger('werkzeug').setLevel(logging.WARNING)
 137: 
 138:     try:
 139:         config.validate()
 140:         logger.info("Application configuration validated successfully")
 141:     except ValueError as e:
 142:         logger.error(f"Configuration validation failed: {e}")
 143: 
 144:     cache = Cache(app)
 145: 
 146:     app._cache_service_initialized = False
 147:     app._services_initialized = False
 148:     app._cache_instance = cache
 149: 
 150:     app.register_blueprint(api_bp, url_prefix='/api')
 151:     app.register_blueprint(workflow_bp)
 152: 
 153:     logger.info("Flask application created with modular architecture")
 154:     return app
 155: 
 156: APP_FLASK = create_app()
 157: APP_LOGGER = APP_FLASK.logger
 158: 
 159: with APP_FLASK.app_context():
 160:     CACHE = APP_FLASK.extensions['cache']
 161: 
 162: KEYWORD_FILTER_TRACKING_ENV = os.environ.get('KEYWORD_FILTER_TRACKING_ENV', "Camille")
 163: SUBDIR_FILTER_TRACKING_ENV = os.environ.get('SUBDIR_FILTER_TRACKING_ENV', "docs")
 164: 
 165: if not HF_AUTH_TOKEN_ENV:
 166:     APP_LOGGER.warning("HF_AUTH_TOKEN environment variable not set. Step 4 (Audio Analysis) will fail if executed.")
 167: else:
 168:     APP_LOGGER.info("HF_AUTH_TOKEN environment variable found and will be used for Analyze Audio.")
 169: 
 170: try:
 171:     security_config.validate_tokens()
 172:     logger.info("Security tokens validated successfully")
 173:     if INTERNAL_WORKER_COMMS_TOKEN_ENV:
 174:         logger.info(f"CFG TOKEN: INTERNAL_WORKER_COMMS_TOKEN_ENV configured: '...{INTERNAL_WORKER_COMMS_TOKEN_ENV[-5:]}'")
 175:     if RENDER_REGISTER_TOKEN_ENV:
 176:         logger.info(f"CFG TOKEN: RENDER_REGISTER_TOKEN_ENV configured: '...{RENDER_REGISTER_TOKEN_ENV[-5:]}'")
 177: except ValueError as e:
 178:     logger.error(f"Security configuration error: {e}")
 179:     logger.error("Application will continue but some endpoints will be INSECURE")
 180: 
 181: workflow_commands_config = WorkflowCommandsConfig(
 182:     base_path=BASE_PATH_SCRIPTS,
 183:     hf_token=HF_AUTH_TOKEN_ENV
 184: )
 185: 
 186: workflow_state = get_workflow_state()
 187: workflow_state.initialize_all_steps(workflow_commands_config.get_all_step_keys())
 188: 
 189: 
 190: 
 191: _services_initialized = False
 192: _services_lock = threading.Lock()
 193: 
 194: def initialize_services():
 195:     """Initialize all services with proper configuration."""
 196:     global _services_initialized
 197: 
 198:     with _services_lock:
 199:         if _services_initialized:
 200:             logger.debug("Services already initialized, skipping")
 201:             return
 202: 
 203:         try:
 204:             with APP_FLASK.app_context():
 205:                 if not APP_FLASK._cache_service_initialized:
 206:                     CacheService.initialize(APP_FLASK._cache_instance)
 207:                     APP_FLASK._cache_service_initialized = True
 208:                     logger.info("CacheService initialized")
 209: 
 210:                 if not APP_FLASK._services_initialized:
 211:                     CSVService.initialize()
 212:                     WorkflowService.initialize(workflow_commands_config.get_config())
 213:                     PerformanceService.start_background_monitoring()
 214:                     APP_FLASK._services_initialized = True
 215:                     logger.info("All services initialized successfully")
 216: 
 217:             _services_initialized = True
 218: 
 219:         except Exception as e:
 220:             logger.error(f"Service initialization failed: {e}")
 221: 
 222: _app_initialized = False
 223: _app_init_lock = threading.Lock()
 224: 
 225: def init_app():
 226:     global _app_initialized
 227: 
 228:     with _app_init_lock:
 229:         if _app_initialized:
 230:             return APP_FLASK
 231: 
 232:         APP_LOGGER.handlers.clear()
 233: 
 234:         logs_dir = BASE_PATH_SCRIPTS / "logs"
 235:         logs_dir.mkdir(exist_ok=True)
 236: 
 237:         log_file_path = logs_dir / "app.log"
 238:         file_handler = logging.FileHandler(log_file_path, mode='w', encoding='utf-8')  # 'w' mode to start fresh each time
 239:         file_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(threadName)s - %(message)s [in %(pathname)s:%(lineno)d]')
 240:         file_handler.setFormatter(file_formatter)
 241: 
 242:         console_handler = logging.StreamHandler(sys.stdout)
 243:         console_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(threadName)s - %(message)s')
 244:         console_handler.setFormatter(console_formatter)
 245: 
 246:         APP_LOGGER.addHandler(file_handler)
 247:         APP_LOGGER.addHandler(console_handler)
 248:         APP_LOGGER.propagate = False
 249: 
 250:         is_debug_mode = os.environ.get("FLASK_DEBUG") == "1"
 251:         APP_LOGGER.setLevel(logging.DEBUG)
 252: 
 253:         APP_LOGGER.info(f"=== COMPREHENSIVE LOGGING INITIALIZED ===")
 254:         APP_LOGGER.info(f"Log file: {log_file_path}")
 255:         APP_LOGGER.info(f"Debug mode: {is_debug_mode}")
 256:         APP_LOGGER.info(f"Logger level: {APP_LOGGER.level}")
 257:         APP_LOGGER.info(f"=== STARTING APPLICATION ===")
 258: 
 259:         root_logger = logging.getLogger()
 260:         root_logger.setLevel(logging.DEBUG)
 261:         if not root_logger.handlers:
 262:             root_logger.addHandler(file_handler)
 263:             root_logger.addHandler(console_handler)
 264: 
 265:         APP_LOGGER.info("Workflow launcher initialized (Ubuntu profile)")
 266: 
 267:         os.makedirs(BASE_PATH_SCRIPTS / 'projets_extraits', exist_ok=True)
 268: 
 269:         if not HF_AUTH_TOKEN_ENV:
 270:             APP_LOGGER.warning("HF_AUTH_TOKEN non défini. L'étape 4 (Analyse Audio) nécessitera cette variable d'environnement.")
 271: 
 272:         if not INTERNAL_WORKER_COMMS_TOKEN_ENV:
 273:             APP_LOGGER.warning("INTERNAL_WORKER_COMMS_TOKEN non défini. API critiques INSECURES.")
 274:         else:
 275:             APP_LOGGER.info("INTERNAL_WORKER_COMMS_TOKEN configuré correctement.")
 276: 
 277:         if not RENDER_REGISTER_TOKEN_ENV:
 278:             APP_LOGGER.warning("RENDER_REGISTER_TOKEN non défini. Fonctionnalités de rendu INSECURES.")
 279:         else:
 280:             APP_LOGGER.info("RENDER_REGISTER_TOKEN configuré correctement.")
 281: 
 282:         initialize_services()
 283: 
 284:         if not getattr(APP_FLASK, "_polling_threads_started", False):
 285:             remote_poll_thread = threading.Thread(target=poll_remote_trigger, name="RemoteWorkflowPoller")
 286:             remote_poll_thread.daemon = True
 287:             remote_poll_thread.start()
 288: 
 289:             csv_monitor_thread = threading.Thread(target=csv_monitor_service, name="CSVMonitorService")
 290:             csv_monitor_thread.daemon = True
 291:             csv_monitor_thread.start()
 292: 
 293:             APP_FLASK._polling_threads_started = True
 294: 
 295:         APP_LOGGER.info("WEBHOOK MONITOR: Système de monitoring activé et prêt (Webhook uniquement).")
 296: 
 297:         _app_initialized = True
 298:         return APP_FLASK
 299: 
 300: initialize_services()
 301: 
 302: REMOTE_TRIGGER_URL = os.environ.get('REMOTE_TRIGGER_URL', "https://render-signal-server.onrender.com/api/check_trigger")
 303: REMOTE_POLLING_INTERVAL = int(os.environ.get('REMOTE_POLLING_INTERVAL', "15"))
 304: 
 305: REMOTE_SEQUENCE_STEP_KEYS = [
 306:     "STEP1",
 307:     "STEP2",
 308:     "STEP3",
 309:     "STEP4",
 310:     "STEP5",
 311:     "STEP6",
 312:     "STEP7"
 313: ]
 314: 
 315: if PYNVML_AVAILABLE:
 316:     atexit.register(pynvml.nvmlShutdown)
 317:     logger.info("pynvml shutdown hook registered")
 318: def format_duration_seconds(seconds_total: float) -> str:
 319:     if seconds_total is None or seconds_total < 0: return "N/A"
 320:     seconds_total = int(seconds_total)
 321:     hours, remainder = divmod(seconds_total, 3600)
 322:     minutes, seconds = divmod(remainder, 60)
 323:     time_str = ""
 324:     if hours > 0: time_str += f"{hours}h "
 325:     if minutes > 0 or hours > 0: time_str += f"{minutes}m "
 326:     time_str += f"{seconds}s"
 327:     return time_str.strip() if time_str else "0s"
 328: 
 329: def create_frontend_safe_config(config_dict: dict) -> dict:
 330:     frontend_config = {}
 331:     for step_key, step_data_orig in config_dict.items():
 332:         frontend_step_data = {}
 333:         for key, value in step_data_orig.items():
 334:             if key == "progress_patterns":
 335:                 pass
 336:             elif isinstance(value, Path):
 337:                 frontend_step_data[key] = str(value)
 338:             elif key == "cmd" and isinstance(value, list):
 339:                 frontend_step_data[key] = [str(item) for item in value]
 340:             elif key == "specific_logs" and isinstance(value, list):
 341:                 safe_logs = []
 342:                 for log_entry in value:
 343:                     safe_entry = log_entry.copy()
 344:                     if 'path' in safe_entry and isinstance(safe_entry['path'], Path):
 345:                         safe_entry['path'] = str(safe_entry['path'])
 346:                     safe_logs.append(safe_entry)
 347:                 frontend_step_data[key] = safe_logs
 348:             else:
 349:                 frontend_step_data[key] = value
 350:         frontend_config[step_key] = frontend_step_data
 351:     return frontend_config
 352: 
 353: def execute_csv_download_worker(dropbox_url, timestamp_str, fallback_url=None, original_filename=None):
 354:     LOCAL_DOWNLOADS_DIR.mkdir(parents=True, exist_ok=True)
 355:     
 356:     download_id = f"csv_{uuid.uuid4().hex[:8]}"
 357:     
 358:     download_info = {
 359:         'id': download_id,
 360:         'filename': 'Détermination en cours...',
 361:         'original_url': dropbox_url,
 362:         'url': dropbox_url,
 363:         'url_type': 'dropbox',
 364:         'status': 'pending',
 365:         'progress': 0,
 366:         'message': 'En attente de démarrage...',
 367:         'timestamp': datetime.now(),
 368:         'display_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
 369:         'csv_timestamp': timestamp_str
 370:     }
 371:     
 372:     CSVService.add_csv_download(download_id, download_info)
 373:     
 374:     def progress_callback(status, progress, message):
 375:         """Callback to update CSVService with download progress."""
 376:         update_kwargs = {
 377:             'progress': progress,
 378:             'message': message,
 379:             'display_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
 380:             'timestamp': datetime.now()
 381:         }
 382:         CSVService.update_csv_download(download_id, status, **update_kwargs)
 383:     
 384:     try:
 385:         urls_to_try = [dropbox_url]
 386:         if fallback_url and str(fallback_url).strip() and str(fallback_url).strip() != str(dropbox_url).strip():
 387:             urls_to_try.append(str(fallback_url).strip())
 388: 
 389:         forced_name = str(original_filename).strip() if original_filename else None
 390: 
 391:         result = None
 392:         last_attempt_url = dropbox_url
 393:         for attempt_url in urls_to_try:
 394:             last_attempt_url = attempt_url
 395:             result = DownloadService.download_dropbox_file(
 396:                 url=attempt_url,
 397:                 timestamp=timestamp_str,
 398:                 output_dir=LOCAL_DOWNLOADS_DIR,
 399:                 progress_callback=progress_callback,
 400:                 forced_filename=forced_name
 401:             )
 402:             if result and result.success:
 403:                 break
 404: 
 405:         if result and result.success:
 406:             CSVService.update_csv_download(
 407:                 download_id,
 408:                 'completed',
 409:                 progress=100,
 410:                 message=result.message,
 411:                 filename=result.filename,
 412:                 display_timestamp=datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
 413:                 timestamp=datetime.now()
 414:             )
 415:             
 416:             try:
 417:                 CSVService.add_to_download_history_with_timestamp(dropbox_url, timestamp_str)
 418:                 if fallback_url and str(fallback_url).strip():
 419:                     CSVService.add_to_download_history_with_timestamp(str(fallback_url).strip(), timestamp_str)
 420:             except Exception as e:
 421:                 APP_LOGGER.error(f"Error adding to download history: {e}")
 422:             
 423:             APP_LOGGER.info(f"CSV DOWNLOAD: File '{result.filename}' downloaded successfully ({result.size_bytes} bytes)")
 424:         else:
 425:             CSVService.update_csv_download(
 426:                 download_id,
 427:                 'failed',
 428:                 message=result.message if result else 'N/A',
 429:                 filename=result.filename if (result and result.filename) else 'N/A',
 430:                 display_timestamp=datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
 431:                 timestamp=datetime.now()
 432:             )
 433:             APP_LOGGER.error(
 434:                 f"CSV DOWNLOAD: Failed - {(result.message if result else 'N/A')} (last_url={last_attempt_url})"
 435:             )
 436:             
 437:     except Exception as e:
 438:         error_msg = f"Unexpected error: {str(e)}"
 439:         APP_LOGGER.error(f"CSV DOWNLOAD: {error_msg}", exc_info=True)
 440:         CSVService.update_csv_download(
 441:             download_id,
 442:             'failed',
 443:             message=error_msg,
 444:             display_timestamp=datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
 445:             timestamp=datetime.now()
 446:         )
 447:     
 448:     APP_LOGGER.info(f"CSV DOWNLOAD: Worker for {download_id} completed")
 449: 
 450: def csv_monitor_service():
 451:     """Service de monitoring Webhook qui s'exécute en arrière-plan."""
 452:     APP_LOGGER.info("WEBHOOK MONITOR: Service démarré.")
 453: 
 454:     from services.csv_service import CSVService
 455: 
 456:     while True:
 457:         try:
 458:             workflow_state.update_csv_monitor_status(
 459:                 status="checking",
 460:                 last_check=datetime.now().isoformat(),
 461:                 error=None
 462:             )
 463: 
 464:             try:
 465:                 CSVService._check_csv_for_downloads()
 466:                 APP_LOGGER.debug("Webhook monitor check completed successfully")
 467:             except Exception as check_error:
 468:                 APP_LOGGER.error(f"Webhook monitor check error: {check_error}")
 469:                 workflow_state.update_csv_monitor_status(
 470:                     status="error",
 471:                     last_check=datetime.now().isoformat(),
 472:                     error=str(check_error)
 473:                 )
 474:                 time.sleep(WEBHOOK_MONITOR_INTERVAL)
 475:                 continue
 476: 
 477:             workflow_state.update_csv_monitor_status(
 478:                 status="active",
 479:                 last_check=datetime.now().isoformat(),
 480:                 error=None
 481:             )
 482: 
 483:         except Exception as e:
 484:             error_msg = f"Erreur dans le service CSV monitor: {e}"
 485:             APP_LOGGER.error(error_msg, exc_info=True)
 486:             workflow_state.update_csv_monitor_status(
 487:                 status="error",
 488:                 last_check=datetime.now().isoformat(),
 489:                 error=error_msg
 490:             )
 491: 
 492:         time.sleep(WEBHOOK_MONITOR_INTERVAL)
 493: 
 494: 
 495: def run_process_async(step_key: str):
 496:     from services.workflow_service import WorkflowService
 497:     
 498:     APP_LOGGER.info(f"[RUN_PROCESS] Starting execution for {step_key}")
 499: 
 500:     projects_dir = os.path.join(BASE_PATH_SCRIPTS, 'projets_extraits')
 501:     os.makedirs(projects_dir, exist_ok=True)
 502: 
 503:     config = workflow_commands_config.get_step_config(step_key)
 504:     if not config:
 505:         APP_LOGGER.error(f"Invalid step_key: {step_key}")
 506:         return
 507:     workflow_state.update_step_status(step_key, 'starting')
 508:     workflow_state.clear_step_log(step_key)
 509:     workflow_state.append_step_log(step_key, f"--- Lancement de: {html.escape(config['display_name'])} ---\n")
 510:     workflow_state.update_step_info(
 511:         step_key,
 512:         return_code=None,
 513:         progress_current=0,
 514:         progress_total=0,
 515:         progress_text='',
 516:         start_time_epoch=time.time(),
 517:         duration_str=None
 518:     )
 519: 
 520: 
 521:     
 522:     cmd_str_list = [str(c) for c in config['cmd']]
 523:     temp_json_path_for_tracking = None
 524: 
 525:     if step_key == "STEP5":
 526:         workflow_state.append_step_log(step_key, "Préparation de l'étape de tracking : recherche des vidéos à traiter...\n")
 527:         try:
 528:             videos_to_process = WorkflowService.prepare_tracking_step(
 529:                 BASE_TRACKING_LOG_SEARCH_PATH,
 530:                 KEYWORD_FILTER_TRACKING_ENV,
 531:                 SUBDIR_FILTER_TRACKING_ENV
 532:             )
 533:             
 534:             if not videos_to_process:
 535:                 APP_LOGGER.info(f"{step_key}: No videos require tracking, completing immediately")
 536:                 workflow_state.append_step_log(step_key, "Toutes les vidéos candidates semblent déjà traitées (aucun .mp4/.mov/... sans .json trouvé). Étape terminée.\n")
 537:                 workflow_state.update_step_info(step_key, status='completed', return_code=0)
 538:                 start_time = workflow_state.get_step_field(step_key, 'start_time_epoch')
 539:                 workflow_state.set_step_field(step_key, 'duration_str', WorkflowService.calculate_step_duration(start_time))
 540:                 return
 541: 
 542:             temp_json_path_for_tracking = WorkflowService.create_tracking_temp_file(videos_to_process)
 543:             cmd_str_list.extend(["--videos_json_path", str(temp_json_path_for_tracking)])
 544:             workflow_state.append_step_log(step_key, f"{len(videos_to_process)} vidéo(s) ajoutée(s) au lot de traitement.\nLe script gestionnaire va maintenant prendre le relais.\n\n")
 545: 
 546:         except Exception as e_prep:
 547:             APP_LOGGER.error(f"{step_key}: Preparation failed - {e_prep}", exc_info=True)
 548:             error_msg = f"Erreur lors de la préparation de l'étape de tracking: {e_prep}"
 549:             workflow_state.append_step_log(step_key, html.escape(error_msg))
 550:             workflow_state.update_step_info(step_key, status='failed', return_code=-1)
 551:             return
 552: 
 553:     workflow_state.append_step_log(step_key, f"Commande: {html.escape(' '.join(cmd_str_list))}\n")
 554:     workflow_state.append_step_log(step_key, f"Dans: {html.escape(str(config['cwd']))}\n\n")
 555:     
 556:     step_progress_patterns = config.get("progress_patterns", {})
 557:     total_pattern_re = step_progress_patterns.get("total")
 558:     current_pattern_re = step_progress_patterns.get("current")
 559:     current_success_line_pattern_re = step_progress_patterns.get("current_success_line_pattern")
 560:     current_item_counter = 0
 561: 
 562:     try:
 563:         APP_LOGGER.info(f"[SUBPROCESS_DEBUG] {step_key} executing command: {cmd_str_list}")
 564:         APP_LOGGER.info(f"[SUBPROCESS_DEBUG] {step_key} working directory: {config['cwd']}")
 565: 
 566:         process_env = os.environ.copy()
 567:         process_env["PYTHONIOENCODING"] = "UTF-8"; process_env["PYTHONUTF8"] = "1"
 568: 
 569:         if step_key == "STEP3":
 570:             try:
 571:                 process_env["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
 572:             except Exception as _e:
 573:                 APP_LOGGER.warning(f"Unable to set PYTORCH_CUDA_ALLOC_CONF: {_e}")
 574: 
 575:         if step_key == "STEP4":
 576:             try:
 577:                 process_env["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:32"
 578:                 process_env["AUDIO_PARTIAL_SUCCESS_OK"] = "1"
 579:             except Exception as _e:
 580:                 APP_LOGGER.warning(f"Unable to set PYTORCH_CUDA_ALLOC_CONF for STEP4: {_e}")
 581: 
 582:         process = subprocess.Popen(
 583:             cmd_str_list, cwd=str(config['cwd']),
 584:             stdout=subprocess.PIPE, stderr=subprocess.STDOUT,
 585:             text=True, bufsize=1, encoding='utf-8', errors='replace', env=process_env
 586:         )
 587: 
 588:         APP_LOGGER.info(f"[SUBPROCESS_DEBUG] {step_key} subprocess started successfully (PID: {process.pid})")
 589: 
 590:         workflow_state.set_step_process(step_key, process)
 591:         workflow_state.update_step_status(step_key, 'running')
 592:         running_status_start_time = time.time()
 593: 
 594:         log_deque = workflow_state.get_step_log_deque(step_key)
 595: 
 596:         if process.stdout:
 597:             for line in iter(process.stdout.readline, ''):
 598:                 line_strip = line.strip()
 599: 
 600:                 if line_strip.startswith("[Progression-MultiLine]"):
 601:                     progress_data = line_strip.replace("[Progression-MultiLine]", "", 1)
 602:                     text_progress = progress_data.replace(" || ", "\n")
 603:                     workflow_state.set_step_field(step_key, 'progress_text', text_progress)
 604:                     continue
 605: 
 606:                 if '\r' in line or '\x1b[' in line or '\033[' in line:
 607:                     continue
 608: 
 609:                 if log_deque is not None:
 610:                     log_deque.append(html.escape(line))
 611:                 try:
 612:                     APP_LOGGER.debug(f"[{step_key}] SCRIPT_OUT: {line_strip}")
 613:                 except UnicodeEncodeError:
 614:                     APP_LOGGER.debug(f"[{step_key}] SCRIPT_OUT (ascii): {line_strip.encode('ascii', 'replace').decode('ascii')}")
 615: 
 616:                 if total_pattern_re:
 617:                     total_match = total_pattern_re.search(line_strip)
 618:                     if total_match:
 619:                         try:
 620:                             workflow_state.set_step_field(step_key, 'progress_total', int(total_match.group(1)))
 621:                             files_completed = workflow_state.get_step_field(step_key, 'files_completed')
 622:                             if files_completed is None or not isinstance(files_completed, int):
 623:                                 workflow_state.set_step_field(step_key, 'files_completed', 0)
 624:                         except (ValueError, IndexError):
 625:                             APP_LOGGER.warning(f"[{step_key}] ProgTotal parse error: {line_strip}")
 626: 
 627:                 if current_pattern_re:
 628:                     current_match = current_pattern_re.search(line_strip)
 629:                     if current_match:
 630:                         try:
 631:                             groups = current_match.groups()
 632: 
 633:                             if len(groups) >= 3 and groups[0].isdigit() and groups[1].isdigit():
 634:                                 current_num = int(groups[0])
 635:                                 total_num = int(groups[1])
 636:                                 filename = groups[2].strip()
 637:                                 workflow_state.set_step_field(step_key, 'progress_current', current_num)
 638:                                 if workflow_state.get_step_field(step_key, 'progress_total', 0) == 0:
 639:                                     workflow_state.set_step_field(step_key, 'progress_total', total_num)
 640:                                 if filename:
 641:                                     workflow_state.set_step_field(step_key, 'progress_text', html.escape(filename))
 642:                             elif len(groups) >= 1 and step_key == 'STEP3':
 643:                                 filename = groups[0].strip()
 644:                                 if filename:
 645:                                     workflow_state.set_step_field(step_key, 'progress_text', html.escape(filename))
 646:                                 progress_total = workflow_state.get_step_field(step_key, 'progress_total', 0)
 647:                                 if progress_total > 0:
 648:                                     files_completed = int(workflow_state.get_step_field(step_key, 'files_completed', max(0, int(workflow_state.get_step_field(step_key, 'progress_current', 0)))))
 649:                                     workflow_state.set_step_field(step_key, 'progress_current', min(progress_total, max(files_completed, 0) + 1))
 650:                                     workflow_state.set_step_field(step_key, 'progress_current_fractional', min(float(progress_total), float(workflow_state.get_step_field(step_key, 'progress_current', 0)) - 0.0 + 0.01))
 651:                             else:
 652:                                 filename = groups[0].strip() if len(groups) >= 1 else ""
 653:                                 percent = int(groups[1]) if len(groups) >= 2 and str(groups[1]).isdigit() else None
 654:                                 if filename:
 655:                                     workflow_state.set_step_field(step_key, 'progress_text', html.escape(filename))
 656:                                 progress_total = workflow_state.get_step_field(step_key, 'progress_total', 0)
 657:                                 if percent is not None and progress_total > 0:
 658:                                     files_completed = int(workflow_state.get_step_field(step_key, 'files_completed', max(0, int(workflow_state.get_step_field(step_key, 'progress_current', 0)))))
 659:                                     current_file_progress = max(0.0, min(0.99, percent / 100.0))
 660:                                     overall_progress = (files_completed + current_file_progress)
 661:                                     workflow_state.set_step_field(step_key, 'progress_current_fractional', max(0.0, min(float(progress_total), overall_progress)))
 662:                         except (ValueError, IndexError):
 663:                             APP_LOGGER.warning(f"[{step_key}] ProgCurrent parse error: {line_strip}")
 664: 
 665:                 internal_pattern_re = step_progress_patterns.get("internal")
 666:                 if internal_pattern_re:
 667:                     internal_match = internal_pattern_re.search(line_strip)
 668:                     if internal_match:
 669:                         try:
 670:                             groups = internal_match.groups()
 671: 
 672:                             if len(groups) >= 4:
 673:                                 current_batch = int(groups[0])
 674:                                 total_batches = int(groups[1])
 675:                                 percent = int(groups[2])
 676:                                 filename = groups[3].strip() if groups[3] else ""
 677:                             elif len(groups) >= 2:
 678:                                 filename = groups[0].strip() if groups[0] else ""
 679:                                 percent = int(groups[1])
 680:                                 current_batch = percent
 681:                                 total_batches = 100
 682:                             else:
 683:                                 continue
 684: 
 685:                             progress_total = workflow_state.get_step_field(step_key, 'progress_total', 0)
 686:                             if progress_total > 0:
 687:                                 files_completed = int(workflow_state.get_step_field(step_key, 'files_completed', max(0, int(workflow_state.get_step_field(step_key, 'progress_current', 0)))))
 688:                                 current_file_progress = max(0.0, min(0.99, percent / 100.0))
 689:                                 overall_progress_files = files_completed + current_file_progress
 690:                                 overall_progress_files = max(0.0, min(float(progress_total), overall_progress_files))
 691:                                 workflow_state.set_step_field(step_key, 'progress_current_fractional', overall_progress_files)
 692: 
 693:                             if filename:
 694:                                 workflow_state.set_step_field(step_key, 'progress_text', html.escape(f"{filename} ({percent}%)"))
 695: 
 696:                         except (ValueError, IndexError):
 697:                             APP_LOGGER.warning(f"[{step_key}] Internal progress parse error: {line_strip}")
 698: 
 699:                 internal_simple_re = step_progress_patterns.get("internal_simple")
 700:                 if internal_simple_re:
 701:                     internal_simple_match = internal_simple_re.search(line_strip)
 702:                     if internal_simple_match:
 703:                         try:
 704:                             batches = int(internal_simple_match.group(1))
 705:                             filename = internal_simple_match.group(2).strip() if internal_simple_match.group(2) else ""
 706:                             progress_total = workflow_state.get_step_field(step_key, 'progress_total', 0)
 707:                             if progress_total > 0:
 708:                                 files_completed = int(workflow_state.get_step_field(step_key, 'files_completed', max(0, int(workflow_state.get_step_field(step_key, 'progress_current', 0)))))
 709:                                 current_file_progress = 0.01
 710:                                 overall_progress_files = files_completed + current_file_progress
 711:                                 overall_progress_files = max(0.0, min(float(progress_total), overall_progress_files))
 712:                                 workflow_state.set_step_field(step_key, 'progress_current_fractional', overall_progress_files)
 713:                             if filename:
 714:                                 workflow_state.set_step_field(step_key, 'progress_text', html.escape(filename))
 715:                         except Exception:
 716:                             APP_LOGGER.warning(f"[{step_key}] Internal simple progress parse error: {line_strip}")
 717: 
 718:                 if current_success_line_pattern_re:
 719:                     success_match = current_success_line_pattern_re.search(line_strip)
 720:                     if success_match:
 721:                         current_item_counter += 1
 722:                         workflow_state.set_step_field(step_key, 'progress_current', current_item_counter)
 723:                         workflow_state.set_step_field(step_key, 'files_completed', current_item_counter)
 724:                         workflow_state.set_step_field(step_key, 'progress_current_fractional', None)
 725:                         if step_progress_patterns.get("current_item_text_from_success_line"):
 726:                             try:
 727:                                 workflow_state.set_step_field(step_key, 'progress_text', html.escape(success_match.group(1).strip()))
 728:                             except IndexError:
 729:                                 pass
 730:             
 731:             process.stdout.close()
 732:         subprocess_start_time = time.time()
 733:         process.wait()
 734:         subprocess_duration = time.time() - subprocess_start_time
 735: 
 736:         APP_LOGGER.info(f"[SUBPROCESS_DEBUG] {step_key} subprocess completed in {subprocess_duration:.2f} seconds (return_code: {process.returncode})")
 737: 
 738:         running_status_duration = time.time() - running_status_start_time
 739:         min_running_time = 0.6
 740: 
 741:         if running_status_duration < min_running_time:
 742:             sleep_time = min_running_time - running_status_duration
 743:             APP_LOGGER.info(f"[TIMING_FIX] {step_key} ensuring minimum running time: sleeping {sleep_time:.3f}s (total running time will be {min_running_time:.3f}s)")
 744:             time.sleep(sleep_time)
 745: 
 746:         workflow_state.update_step_info(
 747:             step_key,
 748:             return_code=process.returncode,
 749:             status='completed' if process.returncode == 0 else 'failed'
 750:         )
 751:         log_suffix = "terminé avec succès" if process.returncode == 0 else f"a échoué (code: {process.returncode})"
 752:         workflow_state.append_step_log(step_key, f"\n--- {html.escape(config['display_name'])} {log_suffix} ---")
 753: 
 754:         status = workflow_state.get_step_status(step_key)
 755:         progress_total = workflow_state.get_step_field(step_key, 'progress_total', 0)
 756:         progress_current = workflow_state.get_step_field(step_key, 'progress_current', 0)
 757:         if status == 'completed' and progress_total > 0 and progress_current < progress_total:
 758:             workflow_state.set_step_field(step_key, 'progress_current', progress_total)
 759:         progress_text = workflow_state.get_step_field(step_key, 'progress_text', '')
 760:         if not progress_text and status == 'completed':
 761:             workflow_state.set_step_field(step_key, 'progress_text', "Terminé")
 762:     except FileNotFoundError:
 763:         APP_LOGGER.error(f"[EARLY_RETURN_DEBUG] {step_key} failing - executable not found: {cmd_str_list[0] if cmd_str_list else 'N/A'}")
 764: 
 765:         error_msg = f"Erreur: Exécutable non trouvé pour {step_key}: {cmd_str_list[0]}"
 766:         workflow_state.append_step_log(step_key, html.escape(error_msg))
 767:         workflow_state.update_step_info(step_key, status='failed', return_code=-1)
 768:         APP_LOGGER.error(error_msg)
 769:     except Exception as e:
 770:         APP_LOGGER.error(f"[EARLY_RETURN_DEBUG] {step_key} failing - general exception: {e}")
 771: 
 772:         error_msg = f"Erreur exécution {step_key}: {str(e)}"
 773:         workflow_state.append_step_log(step_key, html.escape(error_msg))
 774:         workflow_state.update_step_info(step_key, status='failed', return_code=-1)
 775:         APP_LOGGER.error(f"Exception run_process_async pour {step_key}: {e}", exc_info=True)
 776:     finally:
 777:         start_time = workflow_state.get_step_field(step_key, 'start_time_epoch')
 778:         workflow_state.set_step_field(step_key, 'duration_str', WorkflowService.calculate_step_duration(start_time))
 779:         workflow_state.set_step_process(step_key, None)
 780:         if temp_json_path_for_tracking and temp_json_path_for_tracking.exists():
 781:             try:
 782:                 os.remove(temp_json_path_for_tracking)
 783:                 APP_LOGGER.info(f"Fichier temporaire de tracking '{temp_json_path_for_tracking.name}' supprimé.")
 784:             except Exception as e_clean:
 785:                 APP_LOGGER.error(f"Impossible de supprimer le fichier temporaire de tracking '{temp_json_path_for_tracking.name}': {e_clean}")
 786: 
 787: 
 788: def execute_step_sequence_worker(steps_to_run_list: list, sequence_type: str ="Custom"):
 789:     """Execute a sequence of workflow steps.
 790:     
 791:     This function has been migrated to use WorkflowState for sequence management.
 792:     
 793:     Args:
 794:         steps_to_run_list: List of step keys to execute in order
 795:         sequence_type: Type of sequence ('Full', 'Remote', 'Custom', etc.)
 796:     """
 797:     APP_LOGGER.info("🔥🔥🔥 [SEQUENCE_WORKER_TEST] UPDATED SEQUENCE WORKER WITH DEBUGGING IS RUNNING! 🔥🔥🔥")
 798:     
 799:     if sequence_type != "InternalPollingCheck" and workflow_state.is_sequence_running():
 800:         APP_LOGGER.warning(f"{sequence_type.upper()} SEQUENCE: Tentative de lancement alors qu'une séquence est déjà en cours.")
 801:         return
 802:     
 803:     if not workflow_state.start_sequence(sequence_type):
 804:         APP_LOGGER.warning(f"{sequence_type.upper()} SEQUENCE: Could not start - already running")
 805:         return
 806:     
 807:     APP_LOGGER.info(f"{sequence_type.upper()} SEQUENCE: Séquence démarrée.")
 808:     all_steps_succeeded = True; sequence_summary_data = []
 809:     try:
 810:         APP_LOGGER.info(f"{sequence_type.upper()} SEQUENCE: Thread démarré pour {len(steps_to_run_list)} étapes: {steps_to_run_list}")
 811: 
 812:         APP_LOGGER.info(f"[CACHE_CLEAR_TEST] *** UPDATED CODE IS RUNNING - CACHE CLEARED SUCCESSFULLY ***")
 813:         for i, step_key in enumerate(steps_to_run_list):
 814:             step_config = workflow_commands_config.get_step_config(step_key)
 815:             if not step_config:
 816:                 APP_LOGGER.error(f"{sequence_type.upper()} SEQUENCE: Clé invalide '{step_key}'. Interruption.")
 817:                 all_steps_succeeded = False; sequence_summary_data.append({"name": f"Étape Invalide ({html.escape(step_key)})", "status": "Erreur de config", "duration": "0s", "success": False}); break
 818:             step_display_name = step_config['display_name']
 819:             APP_LOGGER.info(f"{sequence_type.upper()} SEQUENCE: Lancement étape {i+1}/{len(steps_to_run_list)}: '{step_display_name}' ({step_key})")
 820:             
 821:             workflow_state.update_step_info(
 822:                 step_key,
 823:                 status='idle',
 824:                 progress_current=0,
 825:                 progress_total=0,
 826:                 progress_text='',
 827:                 start_time_epoch=None,
 828:                 duration_str=None
 829:             )
 830: 
 831:             current_status = workflow_state.get_step_status(step_key)
 832: 
 833:             try:
 834:                 run_process_async(step_key)
 835:                 final_status = workflow_state.get_step_status(step_key)
 836:                 return_code = workflow_state.get_step_field(step_key, 'return_code')
 837:                 APP_LOGGER.info(f"[SEQUENCE_DEBUG] run_process_async completed for {step_key} (final status: {final_status}, return_code: {return_code})")
 838:             except Exception as e:
 839:                 APP_LOGGER.error(f"[SEQUENCE_DEBUG] Exception in run_process_async for {step_key}: {e}", exc_info=True)
 840:                 workflow_state.update_step_info(step_key, status='failed', return_code=-1)
 841:             
 842:             step_info = workflow_state.get_step_info(step_key)
 843:             duration_str_step = step_info.get('duration_str', 'N/A')
 844:             if step_info['status'] == 'completed':
 845:                 sequence_summary_data.append({"name": html.escape(step_display_name), "status": "Réussie", "duration": duration_str_step, "success": True})
 846:             else:
 847:                 all_steps_succeeded = False
 848:                 sequence_summary_data.append({"name": html.escape(step_display_name), "status": f"Échouée ({step_info['status']})", "duration": duration_str_step, "success": False})
 849:                 APP_LOGGER.error(f"{sequence_type.upper()} SEQUENCE: Étape '{html.escape(step_display_name)}' Échouée. Interruption.")
 850:                 break 
 851:         final_overall_status_text = "Terminée avec succès" if all_steps_succeeded else "Terminée avec erreurs"
 852:         summary_log = [f"{s['name']}: {s['status']} ({s['duration']})" for s in sequence_summary_data]
 853:         full_summary_log_text = f"Séquence {sequence_type} {final_overall_status_text}. Détails: " + " | ".join(summary_log)
 854:         APP_LOGGER.info(f"{sequence_type.upper()} SEQUENCE: {full_summary_log_text}")
 855:         
 856:         workflow_state.complete_sequence(success=all_steps_succeeded, message=full_summary_log_text, sequence_type=sequence_type)
 857:     except Exception as e_seq:
 858:         APP_LOGGER.error(f"{sequence_type.upper()} SEQUENCE: Erreur inattendue dans le worker de séquence: {e_seq}", exc_info=True)
 859:         all_steps_succeeded = False
 860:         workflow_state.complete_sequence(success=False, message=f"Erreur critique durant la séquence: {e_seq}", sequence_type=sequence_type)
 861:     finally:
 862:         if workflow_state.is_sequence_running():
 863:             workflow_state.complete_sequence(success=False, message="Séquence terminée de façon inattendue", sequence_type=sequence_type)
 864:         APP_LOGGER.info(f"{sequence_type.upper()} SEQUENCE: Séquence terminée.")
 865: 
 866: def run_full_sequence_from_remote():
 867:     """Launch full sequence from remote trigger.
 868:     
 869:     Migrated to use WorkflowState for sequence management.
 870:     """
 871:     APP_LOGGER.info("REMOTE TRIGGER: Demande de lancement de la séquence complète reçue.")
 872:     
 873:     if workflow_state.is_sequence_running():
 874:         APP_LOGGER.warning("REMOTE TRIGGER: Séquence complète non lancée, une autre séquence est déjà en cours.")
 875:         return
 876:     
 877:     seq_thread = threading.Thread(target=execute_step_sequence_worker, args=(list(REMOTE_SEQUENCE_STEP_KEYS), "Remote"))
 878:     seq_thread.daemon = True
 879:     seq_thread.start()
 880: 
 881: def poll_remote_trigger():
 882:     """Poll remote trigger URL for pending commands.
 883:     
 884:     Migrated to use WorkflowState for sequence status checking.
 885:     """
 886:     APP_LOGGER.info(f"REMOTE POLLER: Démarré. Interrogation de {REMOTE_TRIGGER_URL} toutes les {REMOTE_POLLING_INTERVAL}s.")
 887:     
 888:     while True:
 889:         time.sleep(REMOTE_POLLING_INTERVAL)
 890:         
 891:         if workflow_state.is_sequence_running():
 892:             APP_LOGGER.debug("REMOTE POLLER: Une séquence est marquée comme en cours, attente.")
 893:             continue
 894:         
 895:         if workflow_state.is_any_step_running():
 896:             APP_LOGGER.debug("REMOTE POLLER: Une étape individuelle est active. Attente.")
 897:             continue
 898:         
 899:         try:
 900:             APP_LOGGER.debug(f"REMOTE POLLER: Vérification de {REMOTE_TRIGGER_URL}.")
 901:             response = requests.get(REMOTE_TRIGGER_URL, timeout=10)
 902:             response.raise_for_status()
 903:             data = response.json()
 904:             
 905:             if data.get('command_pending'):
 906:                 APP_LOGGER.info(f"REMOTE POLLER: Commande reçue! Payload: {data.get('payload')}")
 907:                 
 908:                 if not workflow_state.is_sequence_running():
 909:                     run_full_sequence_from_remote()
 910:                 else:
 911:                     APP_LOGGER.warning("REMOTE POLLER: Signal reçu, mais une séquence est déjà en cours.")
 912:             else:
 913:                 APP_LOGGER.debug("REMOTE POLLER: Aucune commande en attente.")
 914:         except requests.exceptions.RequestException as e:
 915:             APP_LOGGER.warning(f"REMOTE POLLER: Erreur communication {REMOTE_TRIGGER_URL}: {e}")
 916:         except Exception as e_poll:
 917:             APP_LOGGER.error(f"REMOTE POLLER: Erreur inattendue: {e_poll}", exc_info=True)
 918: 
 919: @APP_FLASK.route('/test-slideshow-fixes')
 920: def test_slideshow_fixes():
 921:     """Serve the slideshow fixes test page."""
 922:     APP_LOGGER.debug("Serving slideshow fixes test page")
 923: 
 924:     try:
 925:         with open('test_dom_slideshow_fixes.html', 'r', encoding='utf-8') as f:
 926:             test_content = f.read()
 927: 
 928:         return test_content, 200, {'Content-Type': 'text/html; charset=utf-8'}
 929: 
 930:     except Exception as e:
 931:         APP_LOGGER.error(f"Error serving test page: {e}")
 932:         return f"Error loading test page: {e}", 500
 933: 
 934: @APP_FLASK.route('/favicon.ico')
 935: def favicon():
 936:     """
 937:     Handle favicon.ico requests to prevent 404 errors in browser console.
 938:     Returns a 204 No Content response since we don't have a favicon file.
 939:     """
 940:     APP_LOGGER.debug("Favicon requested - returning 204 No Content")
 941:     return '', 204
 942: 
 943: def get_current_workflow_status_summary():
 944:     """Get comprehensive workflow status summary including CSV downloads.
 945:     
 946:     Migrated to use WorkflowState for all state access.
 947:     """
 948:     overall_status_code_val = "idle"
 949:     overall_status_text_display_val = "Prêt et en attente."
 950:     current_step_name_val = None
 951:     progress_current_val = 0
 952:     progress_total_val = 0
 953:     active_step_key_found = None
 954:     
 955:     is_sequence_globally_active = workflow_state.is_sequence_running()
 956:     
 957:     if is_sequence_globally_active:
 958:         sequence_outcome = workflow_state.get_sequence_outcome()
 959:         active_sequence_type = sequence_outcome.get("type", "Inconnue") if sequence_outcome.get("status", "").startswith("running_") else "Inconnue"
 960:         
 961:         for step_key_seq in REMOTE_SEQUENCE_STEP_KEYS:
 962:             step_status = workflow_state.get_step_status(step_key_seq)
 963:             if step_status in ['running', 'starting']:
 964:                 active_step_key_found = step_key_seq
 965:                 overall_status_code_val = f"sequence_running_{active_sequence_type.lower()}"
 966:                 break
 967:     
 968:     if not active_step_key_found:
 969:         all_steps_info = workflow_state.get_all_steps_info()
 970:         for step_key_ind, info_ind in all_steps_info.items():
 971:             if info_ind['status'] in ['running', 'starting']:
 972:                 active_step_key_found = step_key_ind
 973:                 overall_status_code_val = f"step_running_{step_key_ind}"
 974:                 break
 975:     
 976:     if active_step_key_found:
 977:         active_step_info = workflow_state.get_step_info(active_step_key_found)
 978:         active_step_config = workflow_commands_config.get_step_config(active_step_key_found)
 979:         current_step_name_val = active_step_config['display_name'] if active_step_config else 'Étape inconnue'
 980:         progress_current_val = active_step_info['progress_current']
 981:         progress_total_val = active_step_info['progress_total']
 982:         
 983:         if overall_status_code_val.startswith("sequence_running"):
 984:             seq_type_disp = overall_status_code_val.split("_")[-1].capitalize()
 985:             overall_status_text_display_val = f"Séquence {seq_type_disp} - En cours: {current_step_name_val}"
 986:         elif overall_status_code_val.startswith("step_running"):
 987:             overall_status_text_display_val = f"En cours: {current_step_name_val}"
 988:     else:
 989:         sequence_outcome = workflow_state.get_sequence_outcome()
 990:         if sequence_outcome and sequence_outcome.get("timestamp"):
 991:             last_seq_time_str = sequence_outcome["timestamp"]
 992:             if last_seq_time_str.endswith('Z'):
 993:                 last_seq_time_str = last_seq_time_str[:-1] + '+00:00'
 994:             try:
 995:                 last_seq_dt = datetime.fromisoformat(last_seq_time_str)
 996:                 if last_seq_dt.tzinfo is None:
 997:                     last_seq_dt = last_seq_dt.replace(tzinfo=timezone.utc)
 998:                 time_since_last_seq = datetime.now(timezone.utc) - last_seq_dt
 999:                 
1000:                 if time_since_last_seq < timedelta(hours=1):
1001:                     seq_type_display = sequence_outcome.get('type', 'N/A')
1002:                     if sequence_outcome.get("status") == "success":
1003:                         overall_status_code_val = "completed_success_recent"
1004:                         overall_status_text_display_val = f"Terminé avec succès (Séquence {seq_type_display})"
1005:                         current_step_name_val = sequence_outcome.get("message", "Détails non disponibles.")
1006:                     elif sequence_outcome.get("status") == "error":
1007:                         overall_status_code_val = "completed_error_recent"
1008:                         overall_status_text_display_val = f"Terminé avec erreur(s) (Séquence {seq_type_display})"
1009:                         current_step_name_val = sequence_outcome.get("message", "Détails non disponibles.")
1010:                     elif sequence_outcome.get("status", "").startswith("running_"):
1011:                         seq_type_running = sequence_outcome.get("type", "N/A")
1012:                         overall_status_code_val = f"sequence_starting_{seq_type_running.lower()}"
1013:                         overall_status_text_display_val = f"Démarrage Séquence {seq_type_running}..."
1014:                 else:
1015:                     overall_status_code_val = "idle_after_completion"
1016:                     overall_status_text_display_val = "Prêt (dernière opération terminée il y a >1h)."
1017:             except ValueError as e_ts:
1018:                 APP_LOGGER.warning(f"Error parsing sequence outcome timestamp '{sequence_outcome['timestamp']}': {e_ts}")
1019:                 overall_status_code_val = "idle_parse_error"
1020:                 overall_status_text_display_val = "Prêt (erreur parsing dernier statut)."
1021:         else:
1022:             overall_status_code_val = "idle_initial"
1023:             overall_status_text_display_val = "Prêt et en attente (jamais exécuté)."
1024: 
1025:     active_list_csv = workflow_state.get_active_csv_downloads_list()
1026:     kept_list_csv = workflow_state.get_kept_csv_downloads_list()
1027: 
1028:     all_csv_downloads_intermediate = active_list_csv + kept_list_csv
1029:     all_csv_downloads_intermediate.sort(key=lambda x: x.get('timestamp', datetime.min), reverse=True)
1030: 
1031:     recent_downloads_summary_val = []
1032:     for item_csv in all_csv_downloads_intermediate[:5]:
1033:         recent_downloads_summary_val.append({
1034:             "filename": item_csv.get('filename', 'N/A'),
1035:             "status": item_csv.get('status', 'N/A'),
1036:             "timestamp": item_csv.get('display_timestamp', 'N/A'),
1037:             "csv_timestamp": item_csv.get('csv_timestamp', 'N/A')
1038:         })
1039:     status_text_detail_val = datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')
1040: 
1041:     return {
1042:         "overall_status_code": overall_status_code_val,
1043:         "overall_status_text_display": overall_status_text_display_val,
1044:         "current_step_name": current_step_name_val,
1045:         "status_text_detail": status_text_detail_val,
1046:         "progress_current": progress_current_val,
1047:         "progress_total": progress_total_val,
1048:         "recent_downloads": recent_downloads_summary_val,
1049:         "last_updated_utc": datetime.now(timezone.utc).isoformat(),
1050:         "last_sequence_summary": workflow_state.get_sequence_outcome()
1051:     }
1052: 
1053: 
1054: if __name__ == '__main__':
1055:     init_app()
1056: 
1057:     APP_FLASK.run(
1058:         debug=config.DEBUG,
1059:         host=config.HOST,
1060:         port=config.PORT,
1061:         threaded=True,
1062:         use_reloader=False
1063:     )
```

## File: static/eventHandlers.js
```javascript
  1: import * as dom from './domElements.js';
  2: import * as ui from './uiUpdater.js';
  3: import * as api from './apiService.js';
  4: import { runStepSequence } from './sequenceManager.js';
  5: import { defaultSequenceableStepsKeys } from './constants.js';
  6: import { showNotification } from './utils.js';
  7: import { openPopupUI, closePopupUI, showCustomSequenceConfirmUI } from './popupManager.js';
  8: import { scrollToStepImmediate } from './scrollManager.js';
  9: import { soundEvents } from './soundManager.js';
 10: import { appState } from './state/AppState.js';
 11: 
 12: function getIsAnySequenceRunning() {
 13:     return !!appState.getStateProperty('isAnySequenceRunning');
 14: }
 15: 
 16: function getSelectedStepsOrder() {
 17:     return appState.getStateProperty('selectedStepsOrder') || [];
 18: }
 19: 
 20: function setSelectedStepsOrder(order) {
 21:     const safeOrder = Array.isArray(order) ? [...order] : [];
 22:     appState.setState({ selectedStepsOrder: safeOrder }, 'selected_steps_order_update');
 23: }
 24: 
 25: function resolveElement(getterFn, legacyValue = null) {
 26:     if (typeof getterFn === 'function') {
 27:         try {
 28:             return getterFn();
 29:         } catch (_) {
 30:             return legacyValue;
 31:         }
 32:     }
 33:     return legacyValue;
 34: }
 35: 
 36: function resolveCollection(getterFn, legacyValue = null) {
 37:     const resolved = resolveElement(getterFn, legacyValue);
 38:     if (!resolved) return [];
 39:     return Array.from(resolved);
 40: }
 41: 
 42: export function initializeEventHandlers() {
 43:     const closeLogButton = resolveElement(dom.getCloseLogPanelButton, dom.closeLogPanelButton);
 44:     if (closeLogButton) {
 45:         closeLogButton.addEventListener('click', ui.closeLogPanelUI);
 46:     }
 47: 
 48:     const runButtons = resolveCollection(dom.getAllRunButtons, dom.allRunButtons);
 49:     runButtons.forEach(button => {
 50:         button.addEventListener('click', async () => {
 51:             try {
 52:                 if (getIsAnySequenceRunning()) {
 53:                     showNotification("Une séquence est déjà en cours. Veuillez attendre sa fin.", 'warning');
 54:                     return;
 55:                 }
 56:                 const stepKey = button.dataset.step;
 57:                 ui.updateMainLogOutputUI('');
 58:                 const specificLogContainer = resolveElement(dom.getSpecificLogContainerPanel, dom.specificLogContainerPanel);
 59:                 if (specificLogContainer) specificLogContainer.style.display = 'none';
 60:                 ui.openLogPanelUI(stepKey);
 61: 
 62:                 scrollToStepImmediate(stepKey, { scrollDelay: 0 });
 63: 
 64:                 soundEvents.workflowStart();
 65: 
 66:                 await api.runStepAPI(stepKey);
 67:             } catch (error) {
 68:                 console.error('[EVENT] Error in run button handler:', error);
 69:                 showNotification("Erreur lors de l'exécution de l'étape", 'error');
 70:             }
 71:         });
 72:     });
 73: 
 74:     const cancelButtons = resolveCollection(dom.getAllCancelButtons, dom.allCancelButtons);
 75:     cancelButtons.forEach(button => {
 76:         button.addEventListener('click', async () => {
 77:             try {
 78:                 const stepKey = button.dataset.step;
 79:                 await api.cancelStepAPI(stepKey);
 80:             } catch (error) {
 81:                 console.error('[EVENT] Error in cancel button handler:', error);
 82:                 showNotification("Erreur lors de l'annulation de l'étape", 'error');
 83:             }
 84:         });
 85:     });
 86: 
 87:     const specificLogButtons = resolveCollection(dom.getAllSpecificLogButtons, dom.allSpecificLogButtons);
 88:     specificLogButtons.forEach(button => {
 89:         button.addEventListener('click', async () => {
 90:             const stepKey = button.dataset.step;
 91:             const logIndex = button.dataset.logIndex;
 92:             const workflowWrapper = resolveElement(dom.getWorkflowWrapper, dom.workflowWrapper);
 93: 
 94:             if (!workflowWrapper || !workflowWrapper.classList.contains('logs-active') || appState.getStateProperty('activeStepKeyForLogsPanel') !== stepKey) {
 95:                 ui.openLogPanelUI(stepKey);
 96:                 try {
 97:                     const statusResponse = await fetch(`/status/${stepKey}`);
 98:                     if (!statusResponse.ok) throw new Error(`Erreur statut: ${statusResponse.status}`);
 99:                     const statusData = await statusResponse.json();
100:                     ui.updateMainLogOutputUI(statusData.log ? statusData.log.join('') : '<i>Log principal non disponible.</i>');
101:                 } catch (error) {
102:                     console.error("Erreur chargement statut pour log panel:", error);
103:                     ui.updateMainLogOutputUI(`<i>Erreur chargement log principal: ${error.message}</i>`);
104:                 }
105:             }
106:             // Pass the clicked button to enable loading state (spinner + disabled)
107:             await api.fetchSpecificLogAPI(stepKey, logIndex, button.textContent.trim(), button);
108:         });
109:     });
110: 
111:     const runAllButton = resolveElement(dom.getRunAllButton, dom.runAllButton);
112:     if (runAllButton) {
113:         runAllButton.addEventListener('click', async () => {
114:             if (getIsAnySequenceRunning()) {
115:                 showNotification("Séquence déjà en cours.", 'warning'); return;
116:             }
117:             // Play workflow start sound for complete sequence 1-6
118:             soundEvents.workflowStart();
119:             await runStepSequence(defaultSequenceableStepsKeys, "Séquence 0-4");
120:         });
121:     }
122: 
123:     const customSequenceCheckboxes = resolveCollection(dom.getCustomSequenceCheckboxes, dom.customSequenceCheckboxes);
124:     customSequenceCheckboxes.forEach(checkbox => {
125:         checkbox.addEventListener('change', (event) => {
126:             const stepKey = event.target.dataset.stepKey;
127:             const stepCard = document.getElementById(`step-${stepKey}`);
128:             const orderNumberEl = document.getElementById(`order-${stepKey}`);
129:             let currentOrder = getSelectedStepsOrder();
130: 
131:             // Play checkbox interaction sound
132:             soundEvents.checkboxInteraction();
133: 
134:             if (event.target.checked) {
135:                 if (!currentOrder.includes(stepKey)) {
136:                     currentOrder.push(stepKey);
137:                     if (stepCard) stepCard.classList.add('custom-sequence-selected');
138:                 }
139:             } else {
140:                 const index = currentOrder.indexOf(stepKey);
141:                 if (index > -1) currentOrder.splice(index, 1);
142:                 if (stepCard) stepCard.classList.remove('custom-sequence-selected');
143:             }
144:             setSelectedStepsOrder(currentOrder);
145:             document.querySelectorAll('.step-selection-order-number').forEach(el => { el.textContent = ''; });
146:             getSelectedStepsOrder().forEach((sk, idx) => {
147:                 const orderEl = document.getElementById(`order-${sk}`);
148:                 if (orderEl) orderEl.textContent = idx + 1;
149:             });
150:             ui.updateCustomSequenceButtonsUI();
151:         });
152:     });
153: 
154:     const clearCustomSequenceButton = resolveElement(dom.getClearCustomSequenceButton, dom.clearCustomSequenceButton);
155:     if (clearCustomSequenceButton) {
156:         clearCustomSequenceButton.addEventListener('click', () => {
157:             setSelectedStepsOrder([]);
158:             customSequenceCheckboxes.forEach(cb => {
159:                 cb.checked = false;
160:                 const stepCard = document.getElementById(`step-${cb.dataset.stepKey}`);
161:                 if (stepCard) stepCard.classList.remove('custom-sequence-selected');
162:                 const orderEl = document.getElementById(`order-${cb.dataset.stepKey}`);
163:                 if (orderEl) orderEl.textContent = '';
164:             });
165:             ui.updateCustomSequenceButtonsUI();
166:         });
167:     }
168: 
169:     const runCustomSequenceButton = resolveElement(dom.getRunCustomSequenceButton, dom.runCustomSequenceButton);
170:     if (runCustomSequenceButton) {
171:         runCustomSequenceButton.addEventListener('click', () => {
172:             if (getSelectedStepsOrder().length === 0) {
173:                 showNotification("Veuillez sélectionner au moins une étape.", 'warning');
174:                 return;
175:             }
176:             if (getIsAnySequenceRunning()) {
177:                 showNotification("Une autre séquence est déjà en cours.", 'warning'); return;
178:             }
179:             showCustomSequenceConfirmUI();
180:         });
181:     }
182: 
183:     const confirmRunCustomSequenceButton = resolveElement(dom.getConfirmRunCustomSequenceButton, dom.confirmRunCustomSequenceButton);
184:     const customSequenceConfirmOverlay = resolveElement(dom.getCustomSequenceConfirmPopupOverlay, dom.customSequenceConfirmPopupOverlay);
185:     if (confirmRunCustomSequenceButton) {
186:         confirmRunCustomSequenceButton.addEventListener('click', async () => {
187:             closePopupUI(customSequenceConfirmOverlay);
188:             if (getIsAnySequenceRunning()) {
189:                 showNotification("Une autre séquence est déjà en cours.", 'warning'); return;
190:             }
191:             // Loading state on confirm button and disable run-custom while executing
192:             try {
193:                 confirmRunCustomSequenceButton.setAttribute('data-loading', 'true');
194:                 confirmRunCustomSequenceButton.disabled = true;
195:                 if (runCustomSequenceButton) runCustomSequenceButton.disabled = true;
196: 
197:                 // Play workflow start sound for custom sequence
198:                 soundEvents.workflowStart();
199:                 await runStepSequence(getSelectedStepsOrder(), "Séquence Personnalisée");
200:             } finally {
201:                 confirmRunCustomSequenceButton.removeAttribute('data-loading');
202:                 confirmRunCustomSequenceButton.disabled = false;
203:                 if (runCustomSequenceButton) runCustomSequenceButton.disabled = getIsAnySequenceRunning();
204:             }
205:         });
206:     }
207: 
208:     const cancelRunCustomSequenceButton = resolveElement(dom.getCancelRunCustomSequenceButton, dom.cancelRunCustomSequenceButton);
209:     if (cancelRunCustomSequenceButton) {
210:         cancelRunCustomSequenceButton.addEventListener('click', () => {
211:             closePopupUI(customSequenceConfirmOverlay);
212:         });
213:     }
214:     const closeSummaryPopupButton = resolveElement(dom.getCloseSummaryPopupButton, dom.closeSummaryPopupButton);
215:     const sequenceSummaryOverlay = resolveElement(dom.getSequenceSummaryPopupOverlay, dom.sequenceSummaryPopupOverlay);
216:     if (closeSummaryPopupButton) {
217:         closeSummaryPopupButton.addEventListener('click', () => {
218:             closePopupUI(sequenceSummaryOverlay);
219:         });
220:     }
221: 
222:     if (dom.getSoundToggle()) {
223:         import('./soundManager.js').then(({ isSoundEnabled, setSoundEnabled }) => {
224:             const isEnabled = isSoundEnabled();
225:             dom.getSoundToggle().checked = isEnabled;
226:             if (dom.getSoundStatus()) {
227:                 dom.getSoundStatus().textContent = isEnabled ? 'Activé' : 'Désactivé';
228:             }
229: 
230:             dom.getSoundToggle().addEventListener('change', (event) => {
231:                 const enabled = event.target.checked;
232:                 setSoundEnabled(enabled);
233:                 if (dom.getSoundStatus()) {
234:                     dom.getSoundStatus().textContent = enabled ? 'Activé' : 'Désactivé';
235:                 }
236:                 console.log(`[EVENT] Sound effects ${enabled ? 'enabled' : 'disabled'} by user`);
237:             });
238:         });
239:     }
240: }
```

## File: templates/index_new.html
```html
  1: <!DOCTYPE html>
  2: <html lang="fr">
  3: 
  4: <head>
  5:     <meta charset="UTF-8">
  6:     <meta name="viewport" content="width=device-width, initial-scale=1.0">
  7:     <title>Workflow Manager</title>
  8:     <link rel="stylesheet" href="{{ url_for('static', filename='css/variables.css') }}?v={{ cache_buster }}">
  9:     <link rel="stylesheet" href="{{ url_for('static', filename='css/themes.css') }}?v={{ cache_buster }}">
 10:     <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}?v={{ cache_buster }}">
 11:     <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}?v={{ cache_buster }}">
 12: 
 13:     <link rel="stylesheet" href="{{ url_for('static', filename='css/components/notifications.css') }}?v={{ cache_buster }}">
 14:     <link rel="stylesheet" href="{{ url_for('static', filename='css/components/controls.css') }}?v={{ cache_buster }}">
 15:     <link rel="stylesheet" href="{{ url_for('static', filename='css/components/steps.css') }}?v={{ cache_buster }}">
 16:     <link rel="stylesheet" href="{{ url_for('static', filename='css/components/logs.css') }}?v={{ cache_buster }}">
 17:     <link rel="stylesheet" href="{{ url_for('static', filename='css/components/workflow-buttons.css') }}?v={{ cache_buster }}">
 18:     <link rel="stylesheet" href="{{ url_for('static', filename='css/components/popups.css') }}?v={{ cache_buster }}">
 19:     <link rel="stylesheet" href="{{ url_for('static', filename='css/components/downloads.css') }}?v={{ cache_buster }}">
 20:     <link rel="stylesheet" href="{{ url_for('static', filename='css/components/widgets.css') }}?v={{ cache_buster }}">
 21:     <link rel="stylesheet" href="{{ url_for('static', filename='css/components/csv-workflow-prompt.css') }}?v={{ cache_buster }}">
 22:     <link rel="stylesheet" href="{{ url_for('static', filename='css/utils/animations.css') }}?v={{ cache_buster }}">
 23:     <link rel="stylesheet" href="{{ url_for('static', filename='css/features/responsive.css') }}?v={{ cache_buster }}">
 24: </head>
 25: 
 26: <body>
 27:     <div id="notifications-area" aria-live="assertive"></div>
 28: 
 29:     <div class="topbar-affix" id="topbar-affix">
 30:         <div class="unified-controls-section unified-controls--topbar" id="topbar-controls">
 31:             <div class="workflow-controls">
 32:                 <div class="sequence-controls">
 33:                     <div class="control-group control-group--primary" role="group" aria-label="Actions principales du workflow">
 34:                         <button id="run-all-steps-button">✨ Lancer le Workflow Complet (1-7)</button>
 35:                     </div>
 36:                     <div class="control-group control-group--secondary" role="group" aria-label="Actions secondaires du workflow">
 37:                         <button id="run-custom-sequence-button" disabled>🎯 Lancer Séquence Personnalisée</button>
 38:                         <button id="clear-custom-sequence-button" disabled>🗑️ Vider Séquence</button>
 39:                         <button id="toggle-local-downloads" class="downloads-toggle" aria-pressed="true" title="Afficher/Masquer les Téléchargements Locaux">📥 Téléchargements</button>
 40:                         <button id="settings-toggle" class="settings-toggle" aria-haspopup="true" aria-expanded="false" aria-controls="settings-panel" title="Afficher les options">⚙️ Settings</button>
 41:                     </div>
 42:                 </div>
 43:             </div>
 44: 
 45:             <div class="global-progress-affix" id="global-progress-affix">
 46:                 <div class="global-progress-container" id="global-progress-container">
 47:                     <div id="global-progress-bar" role="progressbar" aria-valuenow="0" aria-valuemin="0" aria-valuemax="100">0%</div>
 48:                 </div>
 49:                 <div id="global-progress-text" aria-live="polite"></div>
 50:             </div>
 51:             <div id="settings-panel" class="settings-panel" hidden>
 52:                 <section class="settings-section" aria-label="Affichage et préférences">
 53:                     <p class="settings-title">Affichage & Préférences</p>
 54:                     <div class="settings-row settings-row--stacked">
 55:                         <div class="theme-selector-container settings-block">
 56:                             <label for="theme-selector" class="theme-selector-label">Thème:</label>
 57:                             <select id="theme-selector" class="theme-selector" aria-label="Sélectionner un thème visuel">
 58:                             </select>
 59:                         </div>
 60: 
 61:                         <div id="sound-control-widget" class="sound-control-widget settings-block">
 62:                             <label class="btn-like-switch" for="sound-toggle" aria-label="Activer/Désactiver les effets sonores">
 63:                                 🔊 Effets Sonores
 64:                                 <input type="checkbox" id="sound-toggle" checked>
 65:                             </label>
 66:                         </div>
 67:                     </div>
 68:                 </section>
 69: 
 70:                         </div>
 71:         </div>
 72: 
 73:     </div>
 74: 
 75:     <div id="topbar-spacer" aria-hidden="true"></div>
 76: 
 77:     <div class="local-downloads-section card-like-section">
 78:         <h2>
 79:             <span class="section-icon">📥</span> Téléchargements Locaux
 80:         </h2>
 81:         <div class="local-downloads-list-container">
 82:             <ul id="local-downloads-list" class="status-list" aria-live="polite">
 83:                 <li class="placeholder">Aucune activité de téléchargement locale récente.</li>
 84:             </ul>
 85:         </div>
 86:     </div>
 87: 
 88:     <div class="workflow-wrapper compact-mode" id="workflow-wrapper">
 89:         <div class="steps-column" id="steps-column">
 90:             <section id="workflow-steps" class="workflow-pipeline" role="region" aria-label="Pipeline de traitement">
 91:                 <div class="pipeline-timeline" role="list">
 92:                     <div class="timeline-axis" aria-hidden="true"></div>
 93:                     {% for step_key, config in steps_config.items() %}
 94:                     {% if config %}
 95:                     <div class="timeline-row">
 96:                         <div class="timeline-rail-column" aria-hidden="true">
 97:                             <div class="timeline-node" data-step="{{ step_key }}"></div>
 98:                         </div>
 99:                         <div class="timeline-cards-column">
100:                             <div class="step timeline-step" id="step-{{ step_key }}" data-step-key="{{ step_key }}" data-step-name="{{ config.display_name }}" data-status="idle" role="listitem" tabindex="0" aria-controls="step-details-panel" aria-expanded="false">
101:                                 <div class="timeline-content">
102:                             <div class="timeline-head">
103:                                 <div class="step-title-group">
104:                                 <h2><span class="step-icon">{% if step_key == 'STEP1' %}🗜️{% elif step_key == 'STEP2' %}🔄{% elif step_key == 'STEP3' %}✂️{% elif step_key == 'STEP4' %}🔊{% elif step_key == 'STEP5' %}👀{% elif step_key == 'STEP6' %}🧩{% elif step_key == 'STEP7' %}📦{% else %}⚙️{% endif %}</span>{{ config.display_name }}</h2>
105:                                 <span class="step-state-chip state-idle" id="state-chip-{{ step_key }}" aria-live="polite">Prêt</span>
106:                                 </div>
107:                                 <div class="step-selection-control">
108:                                     <span class="step-selection-order-number" id="order-{{step_key}}"></span>
109:                                     <input type="checkbox" class="custom-sequence-checkbox" data-step-key="{{ step_key }}" title="Sélectionner pour séquence personnalisée" aria-label="Sélectionner {{ config.display_name }} pour séquence personnalisée">
110:                                 </div>
111:                             </div>
112:                             <div class="node-actions step-controls">
113:                                 <button class="run-button" data-step="{{ step_key }}">Lancer</button>
114:                                 <button class="cancel-button" data-step="{{ step_key }}" disabled>Annuler</button>
115:                             </div>
116:                             <div class="timeline-body">
117:                                 <div class="status-line">
118:                                     Statut:
119:                                     <span id="status-{{ step_key }}" class="status-badge status-idle" aria-live="polite">Prêt</span>
120:                                     <span class="timer" id="timer-{{ step_key }}"></span>
121:                                 </div>
122:                                 <div class="step-progress-container" id="progress-container-{{ step_key }}" style="display: none;">
123:                                     <div class="progress-bar-wrapper">
124:                                         <div class="progress-bar-step" id="progress-bar-{{ step_key }}" role="progressbar" aria-valuenow="0" aria-valuemin="0" aria-valuemax="100">0%</div>
125:                                     </div>
126:                                     <div class="progress-text-step" id="progress-text-{{ step_key }}" aria-live="polite"></div>
127:                                 </div>
128:                                 {% if config.specific_logs %}
129:                                 <div class="specific-log-controls-wrapper">
130:                                     <h4>Logs Spécifiques :</h4>
131:                                     <div id="log-panel-specific-buttons-container-{{ step_key }}">
132:                                         {% for log_conf in config.specific_logs %}
133:                                         <button class="specific-log-button" data-step="{{ step_key }}" data-log-index="{{ loop.index0 }}">{{ log_conf.name }}</button>
134:                                         {% endfor %}
135:                                     </div>
136:                                 </div>
137:                                 {% endif %}
138:                             </div>
139:                         </div>
140:                     </div>
141:                         </div>
142:                     </div>
143:                     {% endif %}
144:                     {% endfor %}
145:                 </div>
146:                 <div class="timeline-scroll-spacer" aria-hidden="true"></div>
147:                 <aside id="step-details-panel" class="step-details-panel" role="complementary" aria-label="Détails de l'étape" hidden>
148:                     <div class="step-details-header">
149:                         <div class="step-details-title" id="step-details-title">Détails</div>
150:                         <button id="close-step-details" class="step-details-close" type="button" aria-label="Fermer le panneau détails">×</button>
151:                     </div>
152:                     <div class="step-details-body">
153:                         <div class="step-details-meta">
154:                             <span id="step-details-status" class="status-badge status-idle" aria-live="polite">Prêt</span>
155:                             <span id="step-details-timer" class="timer"></span>
156:                         </div>
157:                         <div class="step-details-progress">
158:                             <div id="step-details-progress-text" class="progress-text-step" aria-live="polite"></div>
159:                         </div>
160:                         <div class="step-details-actions">
161:                             <button id="step-details-run" class="run-button" type="button" disabled>Lancer</button>
162:                             <button id="step-details-cancel" class="cancel-button" type="button" disabled>Annuler</button>
163:                             <button id="step-details-open-logs" class="step-details-open-logs" type="button" disabled>Logs</button>
164:                         </div>
165:                     </div>
166:                 </aside>
167:             </section>
168:         </div>
169:         <div class="logs-column" id="logs-column-global">
170:             <div class="log-panel-header">
171:                 <div class="log-panel-header-main">
172:                     <span id="log-panel-title">Logs</span>
173:                     <button id="close-log-panel" title="Fermer le panneau des logs" aria-label="Fermer le panneau des logs">×</button>
174:                 </div>
175:                 <div class="log-panel-subheader" id="log-panel-subheader" aria-live="polite">
176:                     <span id="log-panel-context-step">Aucune étape active</span>
177:                     <span id="log-panel-context-status"></span>
178:                     <span id="log-panel-context-timer"></span>
179:                 </div>
180:                 <div class="log-panel-specific-buttons" id="log-panel-specific-buttons-container" aria-label="Logs spécifiques"></div>
181:             </div>
182:             <div class="log-container" id="main-log-container-panel">
183:                 <div class="log-header">
184:                     Log Principal (<span id="current-step-log-name-panel">Aucune étape active</span>)
185:                 </div>
186:                 <div class="log-output" id="main-log-output-panel" aria-live="polite" aria-atomic="true"></div>
187:             </div>
188:             <div class="log-container" id="specific-log-container-panel" style="display:none;">
189:                 <div class="log-header" id="specific-log-header-text-panel">Log Spécifique</div>
190:                 <div class="specific-log-path" id="specific-log-path-info-panel"></div>
191:                 <div class="specific-log-output" id="specific-log-output-content-panel" aria-live="polite" aria-atomic="true"></div>
192:             </div>
193:         </div>
194:     </div>
195: 
196:     <div id="sequence-summary-popup-overlay" class="popup-overlay" role="dialog" aria-modal="true" aria-labelledby="sequence-summary-title">
197:         <div class="popup-content">
198:             <h3 id="sequence-summary-title">Résumé de la Séquence</h3>
199:             <ul id="sequence-summary-list" class="popup-list"></ul>
200:             <button id="close-summary-popup" class="popup-cancel-button">Fermer</button>
201:         </div>
202:     </div>
203:     <div id="custom-sequence-confirm-popup-overlay" class="popup-overlay" role="dialog" aria-modal="true" aria-labelledby="custom-sequence-confirm-title">
204:         <div class="popup-content">
205:             <h3 id="custom-sequence-confirm-title">Confirmer la Séquence Personnalisée</h3>
206:             <p>
207:                 Les étapes suivantes seront lancées dans cet ordre :
208:             </p>
209:             <ul id="custom-sequence-confirm-list" class="popup-list"></ul>
210:             <div class="popup-buttons">
211:                 <button id="confirm-run-custom-sequence-button" class="popup-confirm-button">Lancer la Séquence</button>
212:                 <button id="cancel-run-custom-sequence-button" class="popup-cancel-button">Annuler</button>
213:             </div>
214:         </div>
215:     </div>
216: 
217:     <div id="system-monitor-widget" class="system-monitor-widget">
218:         <div class="monitor-header">
219:             <span class="monitor-icon">💻</span>
220:             <span class="monitor-title">Moniteur Système</span>
221:             <button id="system-monitor-minimize" class="monitor-close" aria-label="Réduire le moniteur" title="Réduire (affichage compact)">×</button>
222:         </div>
223:         <div id="monitor-compact-line" class="monitor-compact-line" aria-hidden="true" style="display:none;">
224:             C: <span id="compact-cpu"></span> · R: <span id="compact-ram"></span> · G: <span id="compact-gpu"></span>
225:         </div>
226:         <div class="monitor-item">
227:             <span class="monitor-label">CPU</span>
228:             <div class="monitor-bar-container">
229:                 <div id="cpu-monitor-bar" class="monitor-bar"></div>
230:             </div>
231:             <span id="cpu-monitor-value" class="monitor-value">... %</span>
232:         </div>
233:         <div class="monitor-item">
234:             <span class="monitor-label">RAM</span>
235:             <div class="monitor-bar-container">
236:                 <div id="ram-monitor-bar" class="monitor-bar"></div>
237:             </div>
238:             <span id="ram-monitor-value" class="monitor-value">... %</span>
239:         </div>
240:         <div id="ram-monitor-details" class="monitor-details">... / ... GB</div>
241: 
242:         <div id="gpu-monitor-section" style="display: none;">
243:             <div class="monitor-item">
244:                 <span class="monitor-label">GPU</span>
245:                 <div class="monitor-bar-container">
246:                     <div id="gpu-monitor-bar" class="monitor-bar"></div>
247:                 </div>
248:                 <span id="gpu-monitor-value" class="monitor-value">... %</span>
249:             </div>
250:             <div id="gpu-monitor-details" class="monitor-details">... °C | ... / ... GB</div>
251:         </div>
252:         <div id="gpu-monitor-error" class="monitor-details" style="display: none; color: var(--red);"></div>
253:     </div>
254: 
255:     
256:     <script id="steps-config-data" type="application/json">{{ steps_config | tojson | safe }}</script>
257:     <script src="{{ url_for('static', filename='main.js') }}?v={{ cache_buster }}" type="module" defer></script>
258: </body>
259: 
260: </html>
```

## File: static/domElements.js
```javascript
  1: const _SAFE_STEP_KEY_PATTERN = /^[A-Za-z0-9_-]+$/;
  2: 
  3: function byId(id) {
  4:     return document.getElementById(id);
  5: }
  6: 
  7: function bySelectorAll(selector) {
  8:     return document.querySelectorAll(selector);
  9: }
 10: 
 11: export const getWorkflowWrapper = () => byId('workflow-wrapper');
 12: export const getStepsColumn = () => byId('steps-column');
 13: export const getLogsColumnGlobal = () => byId('logs-column-global');
 14: export const getLogPanelTitle = () => byId('log-panel-title');
 15: export const getLogPanelSubheader = () => byId('log-panel-subheader');
 16: export const getLogPanelContextStep = () => byId('log-panel-context-step');
 17: export const getLogPanelContextStatus = () => byId('log-panel-context-status');
 18: export const getLogPanelContextTimer = () => byId('log-panel-context-timer');
 19: export const getLogPanelSpecificButtonsContainer = () => byId('log-panel-specific-buttons-container');
 20: export const getMainLogContainerPanel = () => byId('main-log-container-panel');
 21: export const getMainLogOutputPanel = () => byId('main-log-output-panel');
 22: export const getCurrentStepLogNamePanel = () => byId('current-step-log-name-panel');
 23: export const getSpecificLogContainerPanel = () => byId('specific-log-container-panel');
 24: export const getSpecificLogHeaderTextPanel = () => byId('specific-log-header-text-panel');
 25: export const getSpecificLogPathInfoPanel = () => byId('specific-log-path-info-panel');
 26: export const getSpecificLogOutputContentPanel = () => byId('specific-log-output-content-panel');
 27: export const getRunAllButton = () => byId('run-all-steps-button');
 28: export const getTopbarAffix = () => byId('topbar-affix');
 29: export const getTopbarControls = () => byId('topbar-controls');
 30: export const getGlobalProgressAffix = () => byId('global-progress-affix');
 31: export const getGlobalProgressContainer = () => byId('global-progress-container');
 32: export const getGlobalProgressBar = () => byId('global-progress-bar');
 33: export const getGlobalProgressText = () => byId('global-progress-text');
 34: export const getSequenceSummaryPopupOverlay = () => byId('sequence-summary-popup-overlay');
 35: export const getSequenceSummaryList = () => byId('sequence-summary-list');
 36: export const getCloseSummaryPopupButton = () => byId('close-summary-popup');
 37: export const getRunCustomSequenceButton = () => byId('run-custom-sequence-button');
 38: export const getClearCustomSequenceButton = () => byId('clear-custom-sequence-button');
 39: export const getCustomSequenceCheckboxes = () => bySelectorAll('.custom-sequence-checkbox');
 40: export const getCustomSequenceConfirmPopupOverlay = () => byId('custom-sequence-confirm-popup-overlay');
 41: export const getCustomSequenceConfirmList = () => byId('custom-sequence-confirm-list');
 42: export const getConfirmRunCustomSequenceButton = () => byId('confirm-run-custom-sequence-button');
 43: export const getCancelRunCustomSequenceButton = () => byId('cancel-run-custom-sequence-button');
 44: export const getNotificationsArea = () => byId('notifications-area');
 45: 
 46: // Lazy DOM element getters to ensure elements are available when accessed
 47: export function getAllStepDivs() {
 48:     const elements = document.querySelectorAll('.step');
 49:     console.debug(`[DOM] getAllStepDivs found ${elements.length} elements`);
 50:     return elements;
 51: }
 52: 
 53: export function getAllRunButtons() {
 54:     const elements = document.querySelectorAll('.run-button');
 55:     console.debug(`[DOM] getAllRunButtons found ${elements.length} elements`);
 56:     return elements;
 57: }
 58: 
 59: export function getAllCancelButtons() {
 60:     const elements = document.querySelectorAll('.cancel-button');
 61:     console.debug(`[DOM] getAllCancelButtons found ${elements.length} elements`);
 62:     return elements;
 63: }
 64: 
 65: export function getAllSpecificLogButtons() {
 66:     const elements = document.querySelectorAll('.specific-log-button');
 67:     console.debug(`[DOM] getAllSpecificLogButtons found ${elements.length} elements`);
 68:     return elements;
 69: }
 70: 
 71: // Enhanced step element getter with validation
 72: export function getStepElement(stepKey) {
 73:     if (!stepKey) {
 74:         console.warn('[DOM] getStepElement called with invalid stepKey:', stepKey);
 75:         return null;
 76:     }
 77: 
 78:     if (!_SAFE_STEP_KEY_PATTERN.test(String(stepKey))) {
 79:         console.warn('[DOM] getStepElement called with unsafe stepKey (refusing to query by id):', stepKey);
 80:         return null;
 81:     }
 82: 
 83:     const element = document.getElementById(`step-${stepKey}`);
 84:     if (!element) {
 85:         console.warn(`[DOM] Step element not found: step-${stepKey}`);
 86:         console.debug('[DOM] Available step elements:',
 87:             Array.from(document.querySelectorAll('[id^="step-"]')).map(el => el.id));
 88:     }
 89: 
 90:     return element;
 91: }
 92: 
 93: // Validate DOM structure for debugging
 94: export function validateDOMStructure() {
 95:     const results = {
 96:         stepElements: getAllStepDivs().length,
 97:         runButtons: getAllRunButtons().length,
 98:         cancelButtons: getAllCancelButtons().length,
 99:         workflowWrapper: !!getWorkflowWrapper(),
100:         stepsColumn: !!getStepsColumn(),
101:         issues: []
102:     };
103: 
104:     // Check for common issues
105:     if (results.stepElements === 0) {
106:         results.issues.push('No step elements found (.step)');
107:     }
108: 
109:     if (!results.workflowWrapper) {
110:         results.issues.push('Workflow wrapper not found (#workflow-wrapper)');
111:     }
112: 
113:     if (!results.stepsColumn) {
114:         results.issues.push('Steps column not found (#steps-column)');
115:     }
116: 
117:     console.debug('[DOM] Structure validation:', results);
118:     return results;
119: }
120: 
121: // Legacy exports for backward compatibility (will be deprecated)
122: export const allStepDivs = getAllStepDivs();
123: export const allRunButtons = getAllRunButtons();
124: export const allCancelButtons = getAllCancelButtons();
125: export const allSpecificLogButtons = getAllSpecificLogButtons();
126: export const closeLogPanelButton = document.getElementById('close-log-panel');
127: 
128: export const localDownloadsList = document.getElementById('local-downloads-list');
129: 
130: 
131: 
132: // ÉLÉMENTS POUR LE CONTRÔLE SONORE
133: export const soundToggle = document.getElementById('sound-toggle');
134: export const soundStatus = document.getElementById('sound-status');
135: export const soundControlWidget = document.getElementById('sound-control-widget');
136: 
137: 
138: // ÉLÉMENTS POUR LE PANNEAU DE RÉGLAGES (top bar)
139: export const settingsToggle = document.getElementById('settings-toggle');
140: export const settingsPanel = document.getElementById('settings-panel');
141: 
142: // New getter functions for lazy DOM access
143: export const getCloseLogPanelButton = () => byId('close-log-panel');
144: export const getLocalDownloadsList = () => byId('local-downloads-list');
145: 
146: // ÉLÉMENTS POUR LE CONTRÔLE SONORE
147: export const getSoundToggle = () => byId('sound-toggle');
148: export const getSoundStatus = () => byId('sound-status');
149: export const getSoundControlWidget = () => byId('sound-control-widget');
150: 
151: // ÉLÉMENTS POUR LE PANNEAU DE RÉGLAGES (top bar)
152: export const getSettingsToggle = () => byId('settings-toggle');
153: export const getSettingsPanel = () => byId('settings-panel');
```

## File: static/main.js
```javascript
  1: import * as dom from './domElements.js';
  2: import * as ui from './uiUpdater.js';
  3: import * as api from './apiService.js';
  4: import { initializeEventHandlers } from './eventHandlers.js';
  5: import { POLLING_INTERVAL } from './constants.js';
  6: import { showNotification } from './utils.js';
  7: 
  8: window.showNotification = showNotification;
  9: import { showSequenceSummaryUI } from './popupManager.js';
 10: import { scrollToStepImmediate } from './scrollManager.js';
 11: 
 12: import { initializeSoundManager } from './soundManager.js';
 13: import { pollingManager } from './utils/PollingManager.js';
 14: import { errorHandler } from './utils/ErrorHandler.js';
 15: import { performanceMonitor } from './utils/PerformanceMonitor.js';
 16: import { domBatcher, DOMUpdateUtils } from './utils/DOMBatcher.js';
 17: 
 18: import { performanceOptimizer } from './utils/PerformanceOptimizer.js';
 19: import { appState } from './state/AppState.js';
 20: import { initializeCSVDownloadMonitor } from './csvDownloadMonitor.js';
 21: import { themeManager } from './themeManager.js';
 22: import { reportViewer } from './reportViewer.js';
 23: import { fetchWithLoadingState } from './apiService.js';
 24: 
 25: import { initializeStepDetailsPanel } from './stepDetailsPanel.js';
 26: 
 27: window.addEventListener('unhandledrejection', (event) => {
 28:     console.error('[MAIN] Unhandled promise rejection:', event.reason);
 29: 
 30:     // Check if this is the specific browser extension error we're trying to fix
 31:     if (event.reason && event.reason.message &&
 32:         event.reason.message.includes('message channel closed')) {
 33:         console.debug('[MAIN] Suppressing browser extension message channel error');
 34:         event.preventDefault(); // Prevent the error from appearing in console
 35:         return;
 36:     }
 37: });
 38: 
 39: function setupLocalDownloadsToggle() {
 40:     const section = document.querySelector('.local-downloads-section');
 41:     const btn = document.getElementById('toggle-local-downloads');
 42:     if (!section || !btn) return;
 43: 
 44:     let visible = true;
 45:     try {
 46:         const stored = localStorage.getItem('ui.localDownloadsVisible');
 47:         if (stored !== null) visible = stored === 'true';
 48:     } catch (_) {}
 49: 
 50:     if (visible) {
 51:         section.style.display = '';
 52:     } else {
 53:         section.style.display = 'none';
 54:     }
 55: 
 56:     applyLocalDownloadsVisibility(section, btn, visible);
 57: 
 58:     btn.addEventListener('click', () => {
 59:         visible = !(btn.getAttribute('aria-pressed') === 'true');
 60:         applyLocalDownloadsVisibility(section, btn, visible);
 61:         try { localStorage.setItem('ui.localDownloadsVisible', String(visible)); } catch (_) {}
 62:         appState.setState({ ui: { localDownloadsVisible: visible } }, 'downloads_visibility_toggle');
 63:         if (visible) {
 64:             btn.classList.remove('downloads-toggle--alert');
 65:             try { localStorage.removeItem('ui.localDownloadsAlertedOnce'); } catch (_) {}
 66:         }
 67:     });
 68: 
 69:     updateDownloadsToggleAlert(appState.getStateProperty('csvDownloads') || []);
 70: }
 71: 
 72: function applyLocalDownloadsVisibility(section, btn, visible) {
 73:     domBatcher.scheduleUpdate('downloads-visibility-toggle', () => {
 74:         if (visible) {
 75:             section.style.display = '';
 76:             section.classList.remove('minimized');
 77:             btn.setAttribute('aria-pressed', 'true');
 78:             btn.classList.remove('downloads-toggle--hidden');
 79:             // Focus and highlight the Downloads section title for accessibility feedback
 80:             requestAnimationFrame(() => {
 81:                 const title = section.querySelector('h2');
 82:                 if (title) {
 83:                     safeFocusAndHighlight(title);
 84:                 }
 85:             });
 86:         } else {
 87:             btn.setAttribute('aria-pressed', 'false');
 88:             btn.classList.add('downloads-toggle--hidden');
 89:             section.style.display = 'none';
 90:         }
 91:     });
 92: }
 93: 
 94: function safeFocusAndHighlight(el) {
 95:     if (!el || typeof el !== 'object') return;
 96:     try {
 97:         // Make sure element can be focused without altering tab order permanently
 98:         let removeTabIndex = false;
 99:         if (el !== document.body && el.tabIndex === -1) {
100:             // Already programmatically focusable
101:         } else if (el !== document.body && (el.getAttribute && el.getAttribute('tabindex') === null)) {
102:             el.setAttribute('tabindex', '-1');
103:             removeTabIndex = true;
104:         }
105:         el.focus && el.focus();
106:         // Apply highlight flash
107:         if (el.classList) {
108:             el.classList.add('section-focus-highlight');
109:             setTimeout(() => { try { el.classList.remove('section-focus-highlight'); } catch(_){} }, 450);
110:         }
111:         if (removeTabIndex) {
112:             setTimeout(() => { try { el.removeAttribute('tabindex'); } catch(_){} }, 500);
113:         }
114:     } catch(_){}
115: }
116: 
117: function updateDownloadsToggleAlert(downloads) {
118:     const btn = document.getElementById('toggle-local-downloads');
119:     const section = document.querySelector('.local-downloads-section');
120:     if (!btn || !section) return;
121: 
122:     const visible = btn.getAttribute('aria-pressed') === 'true';
123:     const list = Array.isArray(downloads) ? downloads : [];
124:     const inProgress = list.some(d => {
125:         const s = (d && d.status) ? String(d.status).toLowerCase() : '';
126:         return s === 'downloading' || s === 'starting' || s === 'pending';
127:     });
128: 
129:     if (!visible && inProgress) {
130:         btn.classList.add('downloads-toggle--alert');
131:         try {
132:             const alerted = localStorage.getItem('ui.localDownloadsAlertedOnce') === 'true';
133:             if (!alerted && typeof window.showNotification === 'function') {
134:                 window.showNotification('Téléchargements', 'Des téléchargements locaux sont en cours. Cliquez pour afficher.');
135:                 localStorage.setItem('ui.localDownloadsAlertedOnce', 'true');
136:             }
137:         } catch (_) {}
138:     } else {
139:         btn.classList.remove('downloads-toggle--alert');
140:         try { localStorage.removeItem('ui.localDownloadsAlertedOnce'); } catch (_) {}
141:     }
142: }
143: 
144: window.addEventListener('error', (event) => {
145:     console.error('[MAIN] Uncaught error:', event.error);
146:     errorHandler.handleApiError('uncaught-error', event.error);
147: });
148: 
149: let stepsConfigData = {};
150: try {
151:     stepsConfigData = JSON.parse(document.getElementById('steps-config-data').textContent);
152: } catch (e) {
153:     console.error("Could not parse steps_config_data:", e);
154: }
155: ui.setStepsConfig(stepsConfigData);
156: 
157: function initializeProcessInfoFromDOM() {
158:     const initialProcessInfo = {};
159:     document.querySelectorAll('.step').forEach(s => {
160:         const stepKey = s && s.dataset ? s.dataset.stepKey : null;
161:         if (!stepKey) return;
162:         initialProcessInfo[stepKey] = {
163:             status: 'idle',
164:             log: [],
165:             progress_current: 0,
166:             progress_total: 0,
167:             progress_text: '',
168:             is_any_sequence_running: false
169:         };
170:     });
171:     appState.setState({ processInfo: initialProcessInfo }, 'process_info_init');
172: }
173: 
174: const LOCAL_DOWNLOAD_POLLING_INTERVAL = POLLING_INTERVAL * 2;
175: 
176: const SYSTEM_MONITOR_POLLING_INTERVAL = 5000;
177: 
178: 
179: 
180: function initializeStateManagement() {
181:     if (window.location.hostname === 'localhost' || window.location.hostname === '127.0.0.1') {
182:         appState.subscribe((newState, oldState, source) => {
183:             console.debug(`[StateManagement] State changed from ${source}:`, {
184:                 changes: findStateChanges(oldState, newState),
185:                 newState: newState
186:             });
187:         });
188:     }
189: 
190:     appState.subscribeToProperty('isAnySequenceRunning', (newValue, oldValue) => {
191:         if (newValue !== oldValue) {
192:             ui.updateGlobalUIForSequenceState(newValue);
193:         }
194:     });
195: 
196:     appState.subscribeToProperty('activeStepKeyForLogsPanel', (newValue, oldValue) => {
197:         if (newValue !== oldValue && newValue) {
198:             domBatcher.scheduleUpdate('logs-panel-update', () => {
199:                 console.debug(`Active step for logs changed to: ${newValue}`);
200:             });
201:         }
202:     });
203: 
204:     setTimeout(() => {
205:         if (typeof CacheService !== 'undefined' && CacheService.warm_cache) {
206:             CacheService.warm_cache();
207:         }
208:     }, 1000);
209: 
210: }
211: 
212: function findStateChanges(oldState, newState) {
213:     const changes = {};
214: 
215:     function compareObjects(old, current, path = '') {
216:         for (const key in current) {
217:             const currentPath = path ? `${path}.${key}` : key;
218: 
219:             if (typeof current[key] === 'object' && current[key] !== null && !Array.isArray(current[key])) {
220:                 if (typeof old[key] === 'object' && old[key] !== null) {
221:                     compareObjects(old[key], current[key], currentPath);
222:                 } else {
223:                     changes[currentPath] = { from: old[key], to: current[key] };
224:                 }
225:             } else if (old[key] !== current[key]) {
226:                 changes[currentPath] = { from: old[key], to: current[key] };
227:             }
228:         }
229:     }
230: 
231:     compareObjects(oldState, newState);
232:     return changes;
233: }
234: 
235: async function pollLocalDownloadsStatus() {
236:     if (!dom.getLocalDownloadsList()) return;
237: 
238:     try {
239:         const downloads = await api.fetchLocalDownloadsStatusAPI();
240:         ui.updateLocalDownloadsListUI(downloads);
241: 
242:         appState.setState({ csvDownloads: downloads }, 'downloads_polled');
243: 
244:         updateDownloadsToggleAlert(downloads);
245: 
246:         errorHandler.clearErrors('localDownloadsStatus', {
247:             elementId: 'local-downloads-list'
248:         });
249: 
250:     } catch (error) {
251:         const delay = await errorHandler.handlePollingError('localDownloadsStatus', error, {
252:             elementId: 'local-downloads-list'
253:         });
254: 
255:         if (delay > 0) {
256:             console.debug(`Applying ${delay}ms delay for localDownloadsStatus polling`);
257:         }
258:     }
259: }
260: 
261: 
262: 
263: async function pollSystemMonitor() {
264:     const monitorWidget = document.getElementById('system-monitor-widget');
265:     if (!monitorWidget) {
266:         console.warn('[MAIN] System monitor widget not found during polling');
267:         return;
268:     }
269: 
270:     try {
271:         const response = await fetch('/api/system_monitor');
272:         if (!response.ok) {
273:             console.warn(`[MAIN] System monitor API failed: ${response.status}`);
274:             monitorWidget.style.opacity = '0.5';
275:             return;
276:         }
277:         const data = await response.json();
278:         console.debug('[MAIN] System monitor data received:', data);
279: 
280:         domBatcher.scheduleUpdate('system-monitor-update', () => {
281:             const cpuBar = document.getElementById('cpu-monitor-bar');
282:             const cpuValue = document.getElementById('cpu-monitor-value');
283:             if (cpuBar && cpuValue) {
284:                 const cpuPercent = data.cpu_percent || 0;
285:                 cpuBar.style.width = `${cpuPercent}%`;
286:                 cpuValue.textContent = `${cpuPercent.toFixed(1)} %`;
287:                 cpuBar.dataset.usageLevel = cpuPercent > 85 ? 'high' : cpuPercent > 60 ? 'medium' : 'low';
288:             }
289: 
290:             const ramBar = document.getElementById('ram-monitor-bar');
291:             const ramValue = document.getElementById('ram-monitor-value');
292:             const ramDetails = document.getElementById('ram-monitor-details');
293:             if (ramBar && ramValue && ramDetails && data.memory) {
294:                 const memPercent = data.memory.percent || 0;
295:                 ramBar.style.width = `${memPercent}%`;
296:                 ramValue.textContent = `${memPercent.toFixed(1)} %`;
297:                 ramDetails.textContent = `${data.memory.used_gb.toFixed(2)} / ${data.memory.total_gb.toFixed(2)} GB`;
298:                 ramBar.dataset.usageLevel = memPercent > 85 ? 'high' : memPercent > 70 ? 'medium' : 'low';
299:             }
300: 
301:             const gpuSection = document.getElementById('gpu-monitor-section');
302:             const gpuError = document.getElementById('gpu-monitor-error');
303:             if (gpuSection && gpuError) {
304:                 if (data.gpu && !data.gpu.error) {
305:                     gpuSection.style.display = 'block';
306:                     gpuError.style.display = 'none';
307: 
308:                     const gpuBar = document.getElementById('gpu-monitor-bar');
309:                     const gpuValue = document.getElementById('gpu-monitor-value');
310:                     const gpuDetails = document.getElementById('gpu-monitor-details');
311: 
312:                     const gpuPercent = data.gpu.utilization_percent || 0;
313:                     gpuBar.style.width = `${gpuPercent}%`;
314:                     gpuValue.textContent = `${gpuPercent.toFixed(1)} %`;
315:                     gpuBar.dataset.usageLevel = gpuPercent > 85 ? 'high' : gpuPercent > 60 ? 'medium' : 'low';
316: 
317:                     const temp = data.gpu.temperature_c || 'N/A';
318:                     const memUsed = data.gpu.memory ? data.gpu.memory.used_gb.toFixed(2) : 'N/A';
319:                     const memTotal = data.gpu.memory ? data.gpu.memory.total_gb.toFixed(2) : 'N/A';
320:                     gpuDetails.textContent = `${temp}°C | ${memUsed} / ${memTotal} GB`;
321:                 } else {
322:                     gpuSection.style.display = 'none';
323:                     if (data.gpu && data.gpu.error) {
324:                         gpuError.textContent = data.gpu.error;
325:                         gpuError.style.display = 'block';
326:                     }
327:                 }
328:             }
329: 
330:             const compactLine = document.getElementById('monitor-compact-line');
331:             if (compactLine) {
332:                 const compactCpu = document.getElementById('compact-cpu');
333:                 const compactRam = document.getElementById('compact-ram');
334:                 const compactGpu = document.getElementById('compact-gpu');
335: 
336:                 const cpuPercent = data.cpu_percent || 0;
337:                 const memPercent = (data.memory && data.memory.percent) ? data.memory.percent : 0;
338: 
339:                 if (compactCpu) compactCpu.textContent = `${cpuPercent.toFixed(1)}%`;
340:                 if (compactRam && data.memory) {
341:                     const used = (typeof data.memory.used_gb === 'number') ? data.memory.used_gb.toFixed(1) : 'N/A';
342:                     const total = (typeof data.memory.total_gb === 'number') ? data.memory.total_gb.toFixed(1) : 'N/A';
343:                     compactRam.textContent = `${memPercent.toFixed(1)}% (${used}/${total}G)`;
344:                 }
345:                 if (compactGpu) {
346:                     if (data.gpu && !data.gpu.error && typeof data.gpu.utilization_percent === 'number') {
347:                         const temp = (typeof data.gpu.temperature_c === 'number') ? data.gpu.temperature_c : 'N/A';
348:                         const gUsed = data.gpu.memory && typeof data.gpu.memory.used_gb === 'number' ? data.gpu.memory.used_gb.toFixed(1) : 'N/A';
349:                         const gTotal = data.gpu.memory && typeof data.gpu.memory.total_gb === 'number' ? data.gpu.memory.total_gb.toFixed(1) : 'N/A';
350:                         compactGpu.textContent = `${data.gpu.utilization_percent.toFixed(1)}% (${temp}C)`;
351:                     } else if (data.gpu && data.gpu.error) {
352:                         compactGpu.textContent = 'err';
353:                     } else {
354:                         compactGpu.textContent = 'N/A';
355:                     }
356:                 }
357:             }
358: 
359:             monitorWidget.style.opacity = '1';
360:         });
361: 
362:         errorHandler.clearErrors('systemMonitor', {
363:             elementId: 'system-monitor-widget'
364:         });
365: 
366:     } catch (error) {
367:         const delay = await errorHandler.handlePollingError('systemMonitor', error, {
368:             elementId: 'system-monitor-widget'
369:         });
370: 
371:         domBatcher.scheduleUpdate('system-monitor-error', () => {
372:             monitorWidget.style.opacity = '0.5';
373:         });
374: 
375:         if (delay > 0) {
376:             console.debug(`Applying ${delay}ms delay for systemMonitor polling`);
377:         }
378:     }
379: }
380: 
381: document.addEventListener('DOMContentLoaded', async () => {
382:     ui.closeLogPanelUI();
383:     if (dom.sequenceSummaryPopupOverlay) dom.sequenceSummaryPopupOverlay.style.display = 'none';
384:     
385:     if (dom.customSequenceConfirmPopupOverlay) dom.customSequenceConfirmPopupOverlay.style.display = 'none';
386: 
387:     themeManager.init();
388: 
389:     if (document.getElementById('report-overlay')) {
390:         reportViewer.init();
391:     }
392: 
393: 
394:     const initialStatusPromises = [];
395:     const allStepKeysForInitialStatus = Object.keys(stepsConfigData);
396: 
397:     initializeProcessInfoFromDOM();
398: 
399:     allStepKeysForInitialStatus.forEach(stepKey => {
400:         initialStatusPromises.push(api.fetchInitialStatusAPI(stepKey));
401:     });
402: 
403: 
404: 
405:     try {
406:         await Promise.all(initialStatusPromises);
407:     } catch (error) {
408:         console.warn("[Main.js DOMContentLoaded] Error fetching some initial statuses:", error);
409:     } finally {
410:         ui.updateGlobalUIForSequenceState(appState.getStateProperty('isAnySequenceRunning'));
411:         ui.updateCustomSequenceButtonsUI();
412:     }
413: 
414:     initializeEventHandlers();
415: 
416:     initializeSoundManager();
417: 
418:     initializeCSVDownloadMonitor();
419: 
420:     initializeStateManagement();
421: 
422:     if (dom.getLocalDownloadsList()) {
423:         pollingManager.startPolling(
424:             'localDownloadsStatus',
425:             pollLocalDownloadsStatus,
426:             LOCAL_DOWNLOAD_POLLING_INTERVAL,
427:             { immediate: true }
428:         );
429:     }
430: 
431:     setupLocalDownloadsToggle();
432: 
433: 
434: 
435: 
436:     const startSystemMonitorPolling = () => {
437:         const widget = document.getElementById('system-monitor-widget');
438:         if (widget) {
439:             pollingManager.startPolling(
440:                 'systemMonitor',
441:                 pollSystemMonitor,
442:                 SYSTEM_MONITOR_POLLING_INTERVAL,
443:                 { immediate: true }
444:             );
445:             return true;
446:         } else {
447:             console.warn('[MAIN] System monitor widget not found, retrying...');
448:             return false;
449:         }
450:     };
451: 
452:     if (!startSystemMonitorPolling()) {
453:         setTimeout(() => {
454:             if (!startSystemMonitorPolling()) {
455:                 console.error('[MAIN] System monitor widget not found after retry');
456:             }
457:         }, 1000);
458:     }
459: 
460:     setupCompactMode();
461: 
462:     setupSettingsPanel();
463: 
464:     setupSystemMonitorMinimize();
465: 
466:     setupKeyboardShortcuts();
467: 
468:     initializeStepDetailsPanel();
469: });
470: 
471: function setupCompactMode() {
472:     const wrapper = typeof dom.getWorkflowWrapper === 'function'
473:         ? dom.getWorkflowWrapper()
474:         : dom.workflowWrapper;
475:     if (!wrapper) {
476:         console.warn('[COMPACT] Wrapper not found, skipping setup');
477:         return;
478:     }
479: 
480:     // Force compact mode as the only mode
481:     appState.setState({ ui: { compactMode: true } }, 'compact_forced_default');
482:     try { localStorage.setItem('ui.compactMode', 'true'); } catch (_) {}
483: 
484:     // Apply immediately and keep in sync if state changes elsewhere
485:     applyCompactClass(wrapper, true);
486:     appState.subscribeToProperty('ui.compactMode', (newVal) => {
487:         applyCompactClass(wrapper, !!newVal);
488:     });
489: }
490: 
491: function applyCompactClass(wrapper, enabled) {
492:     domBatcher.scheduleUpdate('compact-mode-toggle', () => {
493:         if (enabled) {
494:             wrapper.classList.add('compact-mode');
495:         } else {
496:             wrapper.classList.remove('compact-mode');
497:         }
498:     });
499: }
500: 
501: 
502: function setupSystemMonitorMinimize() {
503:     const widget = document.getElementById('system-monitor-widget');
504:     const btn = document.getElementById('system-monitor-minimize');
505:     const compactLine = document.getElementById('monitor-compact-line');
506:     if (!widget || !btn) {
507:         console.warn('[SYSTEM-MONITOR] Elements not found, skipping minimize setup');
508:         return;
509:     }
510: 
511:     // Init from storage
512:     let stored = null;
513:     try { stored = localStorage.getItem('ui.systemMonitorMinimized'); } catch (_) {}
514:     const minimized = stored === 'true';
515:     appState.setState({ ui: { systemMonitorMinimized: minimized } }, 'system_monitor_init');
516:     applySystemMonitorMinimized(widget, compactLine, minimized);
517: 
518:     // Subscribe to state changes
519:     appState.subscribeToProperty('ui.systemMonitorMinimized', (newVal) => {
520:         applySystemMonitorMinimized(widget, compactLine, !!newVal);
521:         try { localStorage.setItem('ui.systemMonitorMinimized', (!!newVal).toString()); } catch (_) {}
522:     });
523: 
524:     // Click on button to minimize
525:     btn.addEventListener('click', (e) => {
526:         e.stopPropagation();
527:         const current = !!appState.getStateProperty('ui.systemMonitorMinimized');
528:         appState.setState({ ui: { systemMonitorMinimized: !current } }, 'system_monitor_toggle');
529:     });
530: 
531:     // Click on widget restores when minimized
532:     widget.addEventListener('click', () => {
533:         const isMinimized = widget.classList.contains('minimized');
534:         if (isMinimized) {
535:             appState.setState({ ui: { systemMonitorMinimized: false } }, 'system_monitor_restore_click');
536:         }
537:     });
538: }
539: 
540: function applySystemMonitorMinimized(widget, compactLine, minimized) {
541:     domBatcher.scheduleUpdate('system-monitor-minimized-toggle', () => {
542:         if (minimized) {
543:             widget.classList.add('minimized');
544:             if (compactLine) {
545:                 compactLine.style.display = 'flex';
546:                 compactLine.setAttribute('aria-hidden', 'false');
547:             }
548:         } else {
549:             widget.classList.remove('minimized');
550:             if (compactLine) {
551:                 compactLine.style.display = 'none';
552:                 compactLine.setAttribute('aria-hidden', 'true');
553:             }
554:         }
555:     });
556: }
557: 
558: function setupSettingsPanel() {
559:     const toggle = dom.getSettingsToggle();
560:     const panel = dom.getSettingsPanel();
561:     if (!toggle || !panel) {
562:         console.warn('[SETTINGS] Elements not found, skipping setup');
563:         return;
564:     }
565: 
566:     // Init from storage (optional), then AppState
567:     try {
568:         const stored = localStorage.getItem('ui.settingsOpen');
569:         if (stored !== null) {
570:             appState.setState({ ui: { settingsOpen: stored === 'true' } }, 'settings_init_storage');
571:         }
572:     } catch (e) {
573:         console.debug('[SETTINGS] localStorage not available', e);
574:     }
575: 
576:     // Apply initial state
577:     const initialOpen = !!appState.getStateProperty('ui.settingsOpen');
578:     applySettingsPanel(panel, toggle, initialOpen);
579: 
580:     // Keep DOM in sync with state changes
581:     appState.subscribeToProperty('ui.settingsOpen', (open) => {
582:         applySettingsPanel(panel, toggle, !!open);
583:     });
584: 
585:     // Toggle handler
586:     toggle.addEventListener('click', () => {
587:         const next = !appState.getStateProperty('ui.settingsOpen');
588:         appState.setState({ ui: { settingsOpen: next } }, 'settings_toggle');
589:         try { localStorage.setItem('ui.settingsOpen', String(next)); } catch (_) {}
590:     });
591: }
592: 
593: function applySettingsPanel(panel, toggle, open) {
594:     domBatcher.scheduleUpdate('settings-panel-update', () => {
595:         if (!panel || !toggle) return;
596:         if (open) {
597:             panel.classList.add('open');
598:             panel.hidden = false;
599:         } else {
600:             panel.classList.remove('open');
601:             panel.hidden = true;
602:         }
603:         toggle.setAttribute('aria-expanded', open ? 'true' : 'false');
604:         toggle.setAttribute('aria-label', open ? 'Refermer les réglages' : 'Ouvrir les réglages');
605:     });
606: }
607: 
608: function setupKeyboardShortcuts() {
609:     document.addEventListener('keydown', (e) => {
610:         // Only handle shortcuts when not typing in inputs
611:         if (e.target && (e.target.tagName === 'INPUT' || e.target.tagName === 'TEXTAREA' || e.target.contentEditable === 'true')) {
612:             return;
613:         }
614: 
615:         // Toggle Settings panel with 'S' key
616:         if (e.key === 's' || e.key === 'S') {
617:             e.preventDefault();
618:             const toggle = dom.getSettingsToggle();
619:             if (toggle) {
620:                 toggle.click();
621:             }
622:         }
623:     });
624: }
```

## File: static/uiUpdater.js
```javascript
   1: import { formatElapsedTime, showNotification } from './utils.js';
   2: import * as dom from './domElements.js';
   3: import { appState } from './state/AppState.js';
   4: import { scrollToActiveStep, isAutoScrollEnabled } from './scrollManager.js';
   5: 
   6: const lastProgressTextByStep = {};
   7: 
   8: const _lastAutoCenterTsByStep = {};
   9: const _AUTO_CENTER_THROTTLE_MS = 700;
  10: 
  11: import { soundEvents } from './soundManager.js';
  12: import { domBatcher, DOMUpdateUtils } from './utils/DOMBatcher.js';
  13: import { performanceOptimizer } from './utils/PerformanceOptimizer.js';
  14: 
  15: let _stepDetailsPanelModulePromise = null;
  16: 
  17: const STATUS_UI_MAP = {
  18:     running: { label: 'En cours', badgeClass: 'status-running', chipClass: 'state-running', icon: '⏱️' },
  19:     starting: { label: 'Préparation', badgeClass: 'status-running', chipClass: 'state-running', icon: '⚙️' },
  20:     initiated: { label: 'Initialisation', badgeClass: 'status-running', chipClass: 'state-running', icon: '⚙️' },
  21:     completed: { label: 'Terminé', badgeClass: 'status-completed', chipClass: 'state-success', icon: '✅' },
  22:     success: { label: 'Terminé', badgeClass: 'status-success', chipClass: 'state-success', icon: '✅' },
  23:     failed: { label: 'Échec', badgeClass: 'status-failed', chipClass: 'state-error', icon: '❌' },
  24:     error: { label: 'Erreur', badgeClass: 'status-error', chipClass: 'state-error', icon: '⚠️' },
  25:     cancelled: { label: 'Annulé', badgeClass: 'status-cancelled', chipClass: 'state-error', icon: '⛔' },
  26:     warning: { label: 'Attention', badgeClass: 'status-warning', chipClass: 'state-warning', icon: '⚠️' },
  27:     paused: { label: 'En pause', badgeClass: 'status-warning', chipClass: 'state-warning', icon: '⏸️' },
  28:     idle: { label: 'Prêt', badgeClass: 'status-idle', chipClass: 'state-idle', icon: '🕒' },
  29:     pending: { label: 'En attente', badgeClass: 'status-warning', chipClass: 'state-warning', icon: '⏳' }
  30: };
  31: 
  32: let STEPS_CONFIG_FROM_SERVER = {};
  33: export function setStepsConfig(config) {
  34:     STEPS_CONFIG_FROM_SERVER = config;
  35: }
  36: 
  37: function getWorkflowWrapperElement() {
  38:     return typeof dom.getWorkflowWrapper === 'function' ? dom.getWorkflowWrapper() : dom.workflowWrapper;
  39: }
  40: 
  41: function getLogsColumnElement() {
  42:     return typeof dom.getLogsColumnGlobal === 'function' ? dom.getLogsColumnGlobal() : dom.logsColumnGlobal;
  43: }
  44: 
  45: function resolveElement(getterFn, legacyValue = null) {
  46:     if (typeof getterFn === 'function') {
  47:         try {
  48:             return getterFn();
  49:         } catch (_) {
  50:             return legacyValue || null;
  51:         }
  52:     }
  53:     return legacyValue || null;
  54: }
  55: 
  56: function getIsAnySequenceRunning() {
  57:     return !!appState.getStateProperty('isAnySequenceRunning');
  58: }
  59: 
  60: function getActiveStepKeyForLogs() {
  61:     return appState.getStateProperty('activeStepKeyForLogsPanel');
  62: }
  63: 
  64: function setActiveStepKeyForLogs(stepKey) {
  65:     appState.setState({ activeStepKeyForLogsPanel: stepKey }, 'setActiveStepKeyForLogs');
  66: }
  67: 
  68: function getSelectedStepsOrder() {
  69:     return appState.getStateProperty('selectedStepsOrder') || [];
  70: }
  71: 
  72: function getProcessInfo(stepKey) {
  73:     if (!stepKey) return null;
  74:     return appState.getStateProperty(`processInfo.${stepKey}`) || null;
  75: }
  76: 
  77: function setProcessInfo(stepKey, info) {
  78:     if (!stepKey) return;
  79:     appState.setState({ processInfo: { [stepKey]: info } }, 'process_info_update');
  80: }
  81: 
  82: function getStepTimers() {
  83:     return appState.getStateProperty('stepTimers') || {};
  84: }
  85: 
  86: function getStepTimer(stepKey) {
  87:     return getStepTimers()[stepKey];
  88: }
  89: 
  90: function setStepTimer(stepKey, timerData, source = 'setStepTimer') {
  91:     const timers = getStepTimers();
  92:     appState.setState({ stepTimers: { ...timers, [stepKey]: timerData } }, source);
  93: }
  94: 
  95: function deleteStepTimer(stepKey) {
  96:     const timers = getStepTimers();
  97:     if (!timers || !Object.prototype.hasOwnProperty.call(timers, stepKey)) return;
  98:     const { [stepKey]: _removed, ...remaining } = timers;
  99:     appState.setState({ stepTimers: remaining }, 'deleteStepTimer');
 100: }
 101: 
 102: function onWrapperTransitionEndOnce(callback, fallbackMs = 500) {
 103:     const el = getWorkflowWrapperElement();
 104:     if (!el) { callback(); return; }
 105:     let called = false;
 106:     const handler = (e) => {
 107:         if (called) return;
 108:         called = true;
 109:         el.removeEventListener('transitionend', handler);
 110:         callback();
 111:     };
 112:     el.addEventListener('transitionend', handler, { once: true });
 113:     setTimeout(() => {
 114:         if (called) return;
 115:         try { el.removeEventListener('transitionend', handler); } catch (_) {}
 116:         callback();
 117:     }, fallbackMs);
 118: }
 119: 
 120: function hideNonActiveSteps(activeStepKey, hidden) {
 121:     try {
 122:         const stepDivs = dom.getAllStepDivs();
 123:         stepDivs.forEach(el => {
 124:             const isActive = activeStepKey && el.id === `step-${activeStepKey}`;
 125:             if (!isActive && hidden) {
 126:                 el.classList.add('steps-hidden');
 127:             } else if (isActive && hidden) {
 128:                 el.classList.remove('steps-hidden');
 129:             } else if (!hidden) {
 130:                 el.classList.remove('steps-hidden');
 131:             }
 132:         });
 133:     } catch (e) {
 134:         console.warn('[UI] hideNonActiveSteps error', e);
 135:     }
 136: }
 137: 
 138: let previousDownloadIds = new Set();
 139: export function getStepsConfig() {
 140:     return STEPS_CONFIG_FROM_SERVER;
 141: }
 142: 
 143: function normalizeStatus(status) {
 144:     return typeof status === 'string' ? status.toLowerCase() : 'idle';
 145: }
 146: 
 147: function getStatusMeta(status) {
 148:     const normalized = normalizeStatus(status);
 149:     return STATUS_UI_MAP[normalized] || STATUS_UI_MAP.idle;
 150: }
 151: 
 152: function getStepDisplayNameForLogPanel(stepKey) {
 153:     if (!stepKey) return '';
 154:     const config = getStepsConfig();
 155:     const stepConfig = config ? config[stepKey] : null;
 156:     if (stepConfig && stepConfig.display_name) return stepConfig.display_name;
 157: 
 158:     const stepEl = document.getElementById(`step-${stepKey}`);
 159:     const datasetName = stepEl && stepEl.dataset ? stepEl.dataset.stepName : null;
 160:     if (datasetName) return datasetName;
 161: 
 162:     return stepKey.replace(/_/g, ' ').replace(/\b\w/g, l => l.toUpperCase());
 163: }
 164: 
 165: function updateLogPanelContextUI(stepKey) {
 166:     const displayName = stepKey ? getStepDisplayNameForLogPanel(stepKey) : '';
 167: 
 168:     const statusEl = stepKey ? document.getElementById(`status-${stepKey}`) : null;
 169:     const timerEl = stepKey ? document.getElementById(`timer-${stepKey}`) : null;
 170: 
 171:     const contextStepEl = resolveElement(dom.getLogPanelContextStep, dom.logPanelContextStep);
 172:     const contextStatusEl = resolveElement(dom.getLogPanelContextStatus, dom.logPanelContextStatus);
 173:     const contextTimerEl = resolveElement(dom.getLogPanelContextTimer, dom.logPanelContextTimer);
 174: 
 175:     if (contextStepEl) {
 176:         contextStepEl.textContent = stepKey ? displayName : 'Aucune étape active';
 177:     }
 178:     if (contextStatusEl) {
 179:         contextStatusEl.textContent = statusEl ? (statusEl.textContent || '') : '';
 180:     }
 181:     if (contextTimerEl) {
 182:         contextTimerEl.textContent = timerEl ? (timerEl.textContent || '') : '';
 183:     }
 184: }
 185: 
 186: function clearLogPanelSpecificButtons() {
 187:     const container = resolveElement(dom.getLogPanelSpecificButtonsContainer, dom.logPanelSpecificButtonsContainer);
 188:     if (!container) return;
 189: 
 190:     while (container.firstChild) {
 191:         container.removeChild(container.firstChild);
 192:     }
 193: }
 194: 
 195: function positionLogsPanelNearActiveStep(stepKey) {
 196:     if (!stepKey) return;
 197:     const workflowWrapper = getWorkflowWrapperElement();
 198:     const logsColumn = getLogsColumnElement();
 199:     if (!workflowWrapper || !logsColumn) return;
 200:     if (!workflowWrapper.classList.contains('compact-mode')) return;
 201: 
 202:     const activeStepElement = document.getElementById(`step-${stepKey}`);
 203:     if (!activeStepElement || typeof activeStepElement.getBoundingClientRect !== 'function') return;
 204: 
 205:     const rect = activeStepElement.getBoundingClientRect();
 206:     const minTop = 120;
 207:     const minHeight = 280;
 208:     const bottomPadding = 20;
 209: 
 210:     const maxTop = Math.max(minTop, (window.innerHeight || 800) - minHeight - bottomPadding);
 211:     const targetTop = Math.max(minTop, Math.min(Math.round(rect.top), maxTop));
 212: 
 213:     logsColumn.style.top = `${targetTop}px`;
 214:     logsColumn.style.height = `${Math.max(minHeight, (window.innerHeight || 800) - targetTop - bottomPadding)}px`;
 215: }
 216: 
 217: function updateStepStateChip(stepKey, status) {
 218:     const chip = document.getElementById(`state-chip-${stepKey}`);
 219:     if (!chip) return;
 220:     const meta = getStatusMeta(status);
 221:     chip.className = `step-state-chip ${meta.chipClass}`;
 222:     chip.textContent = `${meta.icon} ${meta.label}`;
 223: }
 224: 
 225: export function startStepTimer(stepKey) {
 226:     const existingTimer = getStepTimer(stepKey);
 227:     if (existingTimer && existingTimer.intervalId) {
 228:         clearInterval(existingTimer.intervalId);
 229:     }
 230: 
 231:     const startTime = Date.now();
 232:     setStepTimer(stepKey, {
 233:         startTime: startTime,
 234:         startTimeDate: new Date(startTime),
 235:         intervalId: null,
 236:         elapsedTimeFormatted: "0s"
 237:     }, 'startStepTimer');
 238: 
 239:     if (stepKey !== 'clear_disk_cache') {
 240:         domBatcher.scheduleUpdate(`timer-init-${stepKey}`, () => {
 241:             const timerEl = document.getElementById(`timer-${stepKey}`);
 242:             if (timerEl) timerEl.textContent = "(0s)";
 243:         });
 244:     }
 245: 
 246:     const newIntervalId = setInterval(() => {
 247:         const currentTimer = getStepTimer(stepKey);
 248:         if (!currentTimer || (!currentTimer.startTime && !currentTimer.startTimeDate)) {
 249:             if (currentTimer && currentTimer.intervalId) clearInterval(currentTimer.intervalId);
 250:             return;
 251:         }
 252: 
 253:         const startTimeToUse = currentTimer.startTime ? new Date(currentTimer.startTime) : currentTimer.startTimeDate;
 254:         const elapsedTimeStr = formatElapsedTime(startTimeToUse);
 255:         setStepTimer(stepKey, { ...currentTimer, elapsedTimeFormatted: elapsedTimeStr }, 'timer_tick');
 256: 
 257:         if (stepKey !== 'clear_disk_cache') {
 258:             domBatcher.scheduleUpdate(`timer-update-${stepKey}`, () => {
 259:                 const timerEl = document.getElementById(`timer-${stepKey}`);
 260:                 if (timerEl) timerEl.textContent = `(${elapsedTimeStr})`;
 261:             });
 262:         }
 263:     }, 1000);
 264: 
 265:     const currentTimerData = getStepTimer(stepKey);
 266:     if (currentTimerData) {
 267:         setStepTimer(stepKey, { ...currentTimerData, intervalId: newIntervalId }, 'timer_interval_set');
 268:     }
 269: }
 270: 
 271: export function stopStepTimer(stepKey) {
 272:     const timerData = getStepTimer(stepKey);
 273:     if (timerData && timerData.intervalId) {
 274:         clearInterval(timerData.intervalId);
 275:         setStepTimer(stepKey, { ...timerData, intervalId: null }, 'timer_interval_cleared');
 276:     }
 277:     const updatedTimerData = getStepTimer(stepKey);
 278:     if (updatedTimerData && (updatedTimerData.startTime || updatedTimerData.startTimeDate)) {
 279:         const startTimeToUse = updatedTimerData.startTime ? new Date(updatedTimerData.startTime) : updatedTimerData.startTimeDate;
 280:         const elapsedTimeStr = formatElapsedTime(startTimeToUse);
 281:         setStepTimer(stepKey, { ...updatedTimerData, elapsedTimeFormatted: elapsedTimeStr }, 'timer_stopped');
 282:         if (stepKey !== 'clear_disk_cache') {
 283:             const timerEl = document.getElementById(`timer-${stepKey}`);
 284:             if (timerEl) timerEl.textContent = `(Terminé en ${elapsedTimeStr})`;
 285:         }
 286:     }
 287: }
 288: 
 289: export function resetStepTimerDisplay(stepKey) {
 290:     if (stepKey !== 'clear_disk_cache') {
 291:         const timerEl = document.getElementById(`timer-${stepKey}`);
 292:         if (timerEl) timerEl.textContent = "";
 293:     }
 294:     deleteStepTimer(stepKey);
 295: }
 296: 
 297: export function updateGlobalUIForSequenceState(isRunning) {
 298:     const runAllButton = resolveElement(dom.getRunAllButton, dom.runAllButton);
 299:     const runCustomSequenceButton = resolveElement(dom.getRunCustomSequenceButton, dom.runCustomSequenceButton);
 300:     const clearCustomSequenceButton = resolveElement(dom.getClearCustomSequenceButton, dom.clearCustomSequenceButton);
 301:     const customSequenceCheckboxes = resolveElement(dom.getCustomSequenceCheckboxes, dom.customSequenceCheckboxes) || [];
 302: 
 303:     if (runAllButton) runAllButton.disabled = isRunning;
 304:     if (runCustomSequenceButton) runCustomSequenceButton.disabled = isRunning || getSelectedStepsOrder().length === 0;
 305:     if (clearCustomSequenceButton) clearCustomSequenceButton.disabled = isRunning || getSelectedStepsOrder().length === 0;
 306: 
 307:     customSequenceCheckboxes.forEach(cb => cb.disabled = isRunning);
 308: 
 309:     Object.keys(STEPS_CONFIG_FROM_SERVER).forEach(stepKeyConfig => {
 310:         const runButton = document.querySelector(`.run-button[data-step="${stepKeyConfig}"]`);
 311:         const cancelButton = document.querySelector(`.cancel-button[data-step="${stepKeyConfig}"]`);
 312:         const stepInfo = getProcessInfo(stepKeyConfig);
 313: 
 314:         if (runButton) runButton.disabled = isRunning;
 315: 
 316:         if (cancelButton) {
 317:             if (stepInfo && ['running', 'starting', 'initiated'].includes(stepInfo.status)) {
 318:                 cancelButton.disabled = false;
 319:             } else {
 320:                 cancelButton.disabled = true;
 321:             }
 322:         }
 323:     });
 324: }
 325: 
 326: export function setActiveStepForLogPanelUI(stepKey) {
 327:     console.log(`[UI] setActiveStepForLogPanelUI, new active step for logs: ${stepKey}`);
 328:     setActiveStepKeyForLogs(stepKey);
 329: 
 330:     const allStepDivs = dom.getAllStepDivs();
 331:     allStepDivs.forEach(s => {
 332:         s.classList.remove('active-for-log-panel');
 333:     });
 334:     if (stepKey && stepKey !== 'clear_disk_cache') {
 335:         const activeStepElement = document.getElementById(`step-${stepKey}`);
 336:         if (activeStepElement) {
 337:             activeStepElement.classList.add('active-for-log-panel');
 338: 
 339:             const workflowWrapper = getWorkflowWrapperElement();
 340:             const logsOpen = workflowWrapper && workflowWrapper.classList.contains('logs-active');
 341:             if (logsOpen) {
 342:                 hideNonActiveSteps(stepKey, true);
 343:             }
 344:             if (isAutoScrollEnabled() && !logsOpen) {
 345:                 console.log(`[UI] Auto-scrolling to active step: ${stepKey}`);
 346:                 scrollToActiveStep(stepKey);
 347:             }
 348:         }
 349:     }
 350: 
 351:     clearLogPanelSpecificButtons();
 352: 
 353:     if (stepKey) {
 354:         const config = getStepsConfig();
 355:         const stepConfig = config ? config[stepKey] : null;
 356:         const displayName = stepConfig ? stepConfig.display_name : stepKey.replace(/_/g, ' ').replace(/\b\w/g, l => l.toUpperCase());
 357:         console.log(`[UI] setActiveStepForLogPanelUI, displayName for logs: ${displayName}`);
 358: 
 359:         const logPanelTitle = resolveElement(dom.getLogPanelTitle, dom.logPanelTitle);
 360:         const currentStepLogName = resolveElement(dom.getCurrentStepLogNamePanel, dom.currentStepLogNamePanel);
 361:         if(logPanelTitle) logPanelTitle.textContent = `Logs: ${displayName}`;
 362:         if(currentStepLogName) currentStepLogName.textContent = displayName;
 363:         updateLogPanelContextUI(stepKey);
 364: 
 365:         const buttonsContainer = resolveElement(dom.getLogPanelSpecificButtonsContainer, dom.logPanelSpecificButtonsContainer);
 366:         if (stepConfig && stepConfig.specific_logs && stepConfig.specific_logs.length > 0 && buttonsContainer) {
 367:             stepConfig.specific_logs.forEach((logConf, index) => {
 368:                 const button = document.createElement('button');
 369:                 button.className = 'specific-log-button';
 370:                 button.textContent = logConf.name;
 371:                 button.dataset.step = stepKey;
 372:                 button.dataset.logIndex = index;
 373:                 button.addEventListener('click', async () => {
 374:                     const apiModule = await import('./apiService.js');
 375:                     await apiModule.fetchSpecificLogAPI(stepKey, index, logConf.name);
 376:                 });
 377:                 buttonsContainer.appendChild(button);
 378:             });
 379:         }
 380:     } else {
 381:         const logPanelTitle = resolveElement(dom.getLogPanelTitle, dom.logPanelTitle);
 382:         const currentStepLogName = resolveElement(dom.getCurrentStepLogNamePanel, dom.currentStepLogNamePanel);
 383:         if(logPanelTitle) logPanelTitle.textContent = "Logs";
 384:         if(currentStepLogName) currentStepLogName.textContent = "Aucune étape active";
 385:         updateLogPanelContextUI(null);
 386:     }
 387: }
 388: 
 389: async function fetchAndDisplayLogsForPanel(stepKeyToFocus) {
 390:     console.log(`[UI] fetchAndDisplayLogsForPanel called for: ${stepKeyToFocus}. Current active log panel: ${getActiveStepKeyForLogs()}`);
 391:     if (!stepKeyToFocus) return;
 392: 
 393:     const stepConfig = getStepsConfig()[stepKeyToFocus];
 394:     const displayName = stepConfig ? (stepConfig.display_name || stepKeyToFocus) : stepKeyToFocus;
 395: 
 396:     const mainLogOutputPanel = resolveElement(dom.getMainLogOutputPanel, dom.mainLogOutputPanel);
 397:     const mainLogContainer = resolveElement(dom.getMainLogContainerPanel, dom.mainLogContainerPanel);
 398:     const specificLogContainer = resolveElement(dom.getSpecificLogContainerPanel, dom.specificLogContainerPanel);
 399: 
 400:     if (mainLogOutputPanel) {
 401:         mainLogOutputPanel.textContent = `Chargement des logs pour ${displayName}...`;
 402:     }
 403: 
 404:     if(mainLogContainer) mainLogContainer.style.display = 'flex';
 405:     if(specificLogContainer) specificLogContainer.style.display = 'none';
 406: 
 407:     try {
 408:         const response = await fetch(`/status/${stepKeyToFocus}`);
 409:         if (!response.ok) {
 410:             console.error(`[UI] fetchAndDisplayLogsForPanel - fetch failed for ${stepKeyToFocus}: ${response.status}`);
 411:             throw new Error(`Erreur ${response.status} lors de la récupération des logs pour ${displayName}`);
 412:         }
 413:         const data = await response.json();
 414:         setProcessInfo(stepKeyToFocus, { ...(getProcessInfo(stepKeyToFocus) || {}), ...data });
 415:         console.log(`[UI] fetchAndDisplayLogsForPanel - response for: ${stepKeyToFocus}, Log content length: ${data.log ? data.log.length : 'N/A'}`);
 416: 
 417:         if (getActiveStepKeyForLogs() === stepKeyToFocus && mainLogOutputPanel) {
 418:             console.log(`[UI] fetchAndDisplayLogsForPanel - Updating main log for ${stepKeyToFocus} with ${data.log ? data.log.length : 0} lines.`);
 419:             updateMainLogOutputUI(data.log.join(''));
 420:         } else {
 421:             console.log(`[UI] fetchAndDisplayLogsForPanel - Log focus changed. Current: ${getActiveStepKeyForLogs()}, Fetched for: ${stepKeyToFocus}. Not updating main log panel.`);
 422:         }
 423:     } catch (error) {
 424:         console.error(`[UI] fetchAndDisplayLogsForPanel - CATCH error for ${stepKeyToFocus}:`, error);
 425:         const logPanel = resolveElement(dom.getMainLogOutputPanel, dom.mainLogOutputPanel);
 426:         if (getActiveStepKeyForLogs() === stepKeyToFocus && logPanel) {
 427:             logPanel.textContent = `Erreur: ${error?.message || 'Erreur inconnue'}`;
 428:         }
 429:     }
 430: }
 431: 
 432: export function openLogPanelUI(stepKeyToFocus, forceOpen = false) {
 433:     const workflowWrapper = getWorkflowWrapperElement();
 434:     if (!workflowWrapper) {
 435:         console.warn('[UI] openLogPanelUI aborted: workflow wrapper missing.');
 436:         return;
 437:     }
 438: 
 439:     const currentActiveLogStep = getActiveStepKeyForLogs();
 440:     const isPanelOpen = workflowWrapper.classList.contains('logs-active');
 441:     console.log(`[UI] openLogPanelUI called for: ${stepKeyToFocus}, forceOpen: ${forceOpen}, currentActive: ${currentActiveLogStep}, isPanelOpen: ${isPanelOpen}`);
 442: 
 443:     if (forceOpen) {
 444:         console.log(`[UI] Forcing panel open/update for ${stepKeyToFocus}`);
 445:         workflowWrapper.classList.add('logs-entering');
 446:         setActiveStepForLogPanelUI(stepKeyToFocus);
 447:         hideNonActiveSteps(stepKeyToFocus, true);
 448:         requestAnimationFrame(() => {
 449:             workflowWrapper.classList.add('logs-active');
 450:             onWrapperTransitionEndOnce(() => {
 451:                 workflowWrapper.classList.remove('logs-entering');
 452:             }, 500);
 453:         });
 454:         positionLogsPanelNearActiveStep(stepKeyToFocus);
 455:         fetchAndDisplayLogsForPanel(stepKeyToFocus);
 456:         return;
 457:     }
 458: 
 459:     if (isPanelOpen && currentActiveLogStep && currentActiveLogStep !== stepKeyToFocus) {
 460:         console.log(`[UI] Log panel already open for ${currentActiveLogStep}, switching to ${stepKeyToFocus}.`);
 461:         setActiveStepForLogPanelUI(stepKeyToFocus);
 462:         hideNonActiveSteps(stepKeyToFocus, true);
 463:         positionLogsPanelNearActiveStep(stepKeyToFocus);
 464:         fetchAndDisplayLogsForPanel(stepKeyToFocus);
 465:         return;
 466:     }
 467: 
 468:     if (isPanelOpen && currentActiveLogStep === stepKeyToFocus) {
 469:         console.log(`[UI] Panel already open for ${stepKeyToFocus}. Refreshing its content.`);
 470:         fetchAndDisplayLogsForPanel(stepKeyToFocus);
 471:         return;
 472:     }
 473: 
 474:     console.log(`[UI] Opening panel for ${stepKeyToFocus} (or was closed/open for null).`);
 475:     workflowWrapper.classList.add('logs-entering');
 476:     setActiveStepForLogPanelUI(stepKeyToFocus);
 477:     hideNonActiveSteps(stepKeyToFocus, true);
 478:     requestAnimationFrame(() => {
 479:         workflowWrapper.classList.add('logs-active');
 480:         onWrapperTransitionEndOnce(() => {
 481:             workflowWrapper.classList.remove('logs-entering');
 482:         }, 500);
 483:     });
 484:     positionLogsPanelNearActiveStep(stepKeyToFocus);
 485:     fetchAndDisplayLogsForPanel(stepKeyToFocus);
 486: }
 487: 
 488: export function closeLogPanelUI() {
 489:     const workflowWrapper = getWorkflowWrapperElement();
 490:     if (!workflowWrapper) {
 491:         console.warn('[CLOSE_LOG] Workflow wrapper missing; aborting close sequence.');
 492:         setActiveStepForLogPanelUI(null);
 493:         const mainLogOutputPanel = resolveElement(dom.getMainLogOutputPanel, dom.mainLogOutputPanel);
 494:         const specificLogContainer = resolveElement(dom.getSpecificLogContainerPanel, dom.specificLogContainerPanel);
 495:         if (mainLogOutputPanel) mainLogOutputPanel.textContent = "";
 496:         if (specificLogContainer) specificLogContainer.style.display = 'none';
 497:         const logsColumn = getLogsColumnElement();
 498:         if (logsColumn) {
 499:             logsColumn.style.removeProperty('top');
 500:             logsColumn.style.removeProperty('height');
 501:         }
 502:         clearLogPanelSpecificButtons();
 503:         return;
 504:     }
 505: 
 506:     console.log('[CLOSE_LOG] Starting closeLogPanelUI, current classes:', workflowWrapper.className);
 507:     workflowWrapper.classList.add('logs-leaving');
 508:     console.log('[CLOSE_LOG] Added logs-leaving class, new classes:', workflowWrapper.className);
 509: 
 510:     requestAnimationFrame(() => {
 511:         console.log('[CLOSE_LOG] In RAF, removing logs-active class, current classes:', workflowWrapper.className);
 512:         workflowWrapper.classList.remove('logs-active');
 513: 
 514:         onWrapperTransitionEndOnce(() => {
 515:             console.log('[CLOSE_LOG] Cleanup - removing logs-leaving class, current classes:', workflowWrapper.className);
 516:             workflowWrapper.classList.remove('logs-leaving');
 517:             hideNonActiveSteps(null, false);
 518:         }, 500);
 519:     });
 520: 
 521:     setActiveStepForLogPanelUI(null);
 522:     const mainLogOutputPanel = resolveElement(dom.getMainLogOutputPanel, dom.mainLogOutputPanel);
 523:     const specificLogContainer = resolveElement(dom.getSpecificLogContainerPanel, dom.specificLogContainerPanel);
 524:     if(mainLogOutputPanel) mainLogOutputPanel.textContent = "";
 525:     if(specificLogContainer) specificLogContainer.style.display = 'none';
 526: 
 527:     const logsColumn = getLogsColumnElement();
 528:     if (logsColumn) {
 529:         logsColumn.style.removeProperty('top');
 530:         logsColumn.style.removeProperty('height');
 531:     }
 532:     clearLogPanelSpecificButtons();
 533: }
 534: 
 535: export function updateStepCardUI(stepKey, data) {
 536:     console.group(`[PROGRESS DEBUG] updateStepCardUI - ${stepKey}`);
 537:     console.log('Raw data received:', {
 538:         progress_current: data.progress_current,
 539:         progress_total: data.progress_total,
 540:         progress_current_fractional: data.progress_current_fractional,
 541:         status: data.status,
 542:         progress_text: data.progress_text,
 543:         timestamp: new Date().toISOString()
 544:     });
 545: 
 546:     performanceOptimizer.measureDomUpdate(`updateStepCard-${stepKey}`, () => {
 547:         try {
 548:             const statusEl = document.getElementById(`status-${stepKey}`);
 549:             const runButton = document.querySelector(`.run-button[data-step="${stepKey}"]`);
 550:             const cancelButton = document.querySelector(`.cancel-button[data-step="${stepKey}"]`);
 551:             const workflowWrapper = getWorkflowWrapperElement();
 552: 
 553:             const normalizedStatus = normalizeStatus(data.status || 'idle');
 554:             const statusMeta = getStatusMeta(normalizedStatus);
 555: 
 556:             if (statusEl) {
 557:                 statusEl.textContent = statusMeta.label;
 558:                 statusEl.className = `status-badge ${statusMeta.badgeClass}`;
 559:             }
 560: 
 561:             const stepCardEl = document.getElementById(`step-${stepKey}`);
 562:             if (stepCardEl) {
 563:                 stepCardEl.setAttribute('data-status', normalizedStatus);
 564:             }
 565: 
 566:             updateStepStateChip(stepKey, normalizedStatus);
 567: 
 568:             if (runButton && cancelButton) {
 569:                 const isCurrentlyRunningOrStarting = ['running', 'starting', 'initiated'].includes(normalizedStatus);
 570:                 runButton.disabled = isCurrentlyRunningOrStarting || getIsAnySequenceRunning();
 571:                 cancelButton.disabled = !isCurrentlyRunningOrStarting;
 572:             }
 573: 
 574:             const logsOpen = workflowWrapper && workflowWrapper.classList.contains('logs-active');
 575:             if (logsOpen && ['running', 'starting', 'initiated'].includes(normalizedStatus)) {
 576:                 if (getActiveStepKeyForLogs() !== stepKey) {
 577:                     setActiveStepForLogPanelUI(stepKey);
 578:                     hideNonActiveSteps(stepKey, true);
 579:                 }
 580:             }
 581: 
 582:             if (logsOpen && getActiveStepKeyForLogs() === stepKey) {
 583:                 updateLogPanelContextUI(stepKey);
 584:             }
 585: 
 586:             if (['completed', 'failed'].includes(normalizedStatus) || (normalizedStatus === 'idle' && getStepTimer(stepKey))) {
 587:                 stopStepTimer(stepKey);
 588:             } else if (normalizedStatus === 'idle' && !getStepTimer(stepKey)) {
 589:                 resetStepTimerDisplay(stepKey);
 590:             } else if (['running', 'starting', 'initiated'].includes(normalizedStatus) && !getStepTimer(stepKey)?.intervalId) {
 591:                 // TODO: Implement proper timer resumption after page reload
 592:                 // Date: 2026-01-19
 593:                 // Owner: kidpixel
 594:                 // Issue: startStepTimer doesn't resume from existing startTime
 595:                 // Solution needed: Backend should provide duration_str for running steps
 596:             }
 597: 
 598:             const progressContainer = document.getElementById(`progress-container-${stepKey}`);
 599:             const progressBar = document.getElementById(`progress-bar-${stepKey}`);
 600:             const progressTextEl = document.getElementById(`progress-text-${stepKey}`);
 601: 
 602:             let percentage = 0;
 603: 
 604:             if (progressContainer && progressBar && progressTextEl) {
 605:                 if (data.progress_total > 0) {
 606:                     let currentProgress = data.progress_current_fractional || data.progress_current;
 607: 
 608:                     if (data.progress_current_fractional === null && data.progress_text) {
 609:                         const isSpecialRunning = (['STEP3','STEP4','STEP5'].includes(stepKey)) && ['running','starting','initiated'].includes(normalizedStatus);
 610:                         if (!isSpecialRunning) {
 611:                             const percentMatch = data.progress_text.match(/(\d+)%/);
 612:                             if (percentMatch) {
 613:                                 const textPercent = parseInt(percentMatch[1]);
 614:                                 currentProgress = (textPercent / 100) * data.progress_total;
 615:                                 console.log(`[PROGRESS FALLBACK] ${stepKey}: Extracted ${textPercent}% from text, using fractional: ${currentProgress}`);
 616:                             }
 617:                         }
 618:                     }
 619: 
 620:                     percentage = Math.round((currentProgress / data.progress_total) * 100);
 621:                     percentage = Math.min(percentage, 100);
 622: 
 623:                     if ((['STEP3','STEP4','STEP5'].includes(stepKey)) && ['running', 'starting', 'initiated'].includes(normalizedStatus)) {
 624:                         if (percentage >= 100) {
 625:                             percentage = 99;
 626:                         }
 627:                         if (data.progress_total > 0 && data.progress_current === data.progress_total) {
 628:                             percentage = Math.min(percentage, 99);
 629:                         }
 630:                     }
 631: 
 632:                     console.log(`[PROGRESS CALC] ${stepKey}:`, {
 633:                         progress_current: data.progress_current,
 634:                         progress_current_fractional: data.progress_current_fractional,
 635:                         progress_total: data.progress_total,
 636:                         currentProgress: currentProgress,
 637:                         calculatedPercentage: (currentProgress / data.progress_total) * 100,
 638:                         finalPercentage: percentage,
 639:                         status: data.status,
 640:                         progress_text: data.progress_text
 641:                     });
 642: 
 643:                     let displayCurrent = data.progress_current;
 644:                     if ((!displayCurrent || displayCurrent === 0) && typeof data.progress_current_fractional === 'number' && data.progress_current_fractional > 0) {
 645:                         const frac = Math.max(0, Math.min(data.progress_total, data.progress_current_fractional));
 646:                         displayCurrent = Math.min(data.progress_total, Math.floor(frac) + 1);
 647:                     }
 648: 
 649:                     progressContainer.style.display = 'block';
 650:                     progressBar.style.backgroundColor = 'var(--blue)';
 651:                     progressBar.style.width = `${percentage}%`;
 652:                     progressBar.textContent = `${percentage}%`;
 653:                     progressBar.setAttribute('aria-valuenow', percentage);
 654: 
 655:                     if (['running','starting','initiated'].includes(normalizedStatus)) {
 656:                         progressBar.setAttribute('data-active', 'true');
 657:                     } else {
 658:                         progressBar.removeAttribute('data-active');
 659:                     }
 660: 
 661:                     const candidateText = (data.progress_text && data.progress_text.trim()) ? data.progress_text : (lastProgressTextByStep[stepKey] || '');
 662:                     if (data.progress_text && data.progress_text.trim()) {
 663:                         lastProgressTextByStep[stepKey] = data.progress_text;
 664:                     }
 665:                     const subText = candidateText ? `${candidateText} (${displayCurrent}/${data.progress_total})` : `${displayCurrent}/${data.progress_total}`;
 666:                     progressTextEl.textContent = subText;
 667: 
 668:                     const shouldAutoCenter = getIsAnySequenceRunning() && ['running', 'starting', 'initiated'].includes(normalizedStatus);
 669:                     if (shouldAutoCenter) {
 670:                         const logsOpenNow = workflowWrapper && workflowWrapper.classList.contains('logs-active');
 671:                         if (!logsOpenNow) {
 672:                             const now = performance.now();
 673:                             const lastTs = _lastAutoCenterTsByStep[stepKey] || 0;
 674:                             if ((now - lastTs) > _AUTO_CENTER_THROTTLE_MS) {
 675:                                 _lastAutoCenterTsByStep[stepKey] = now;
 676:                                 requestAnimationFrame(() => {
 677:                                     scrollToActiveStep(stepKey, { behavior: 'auto', scrollDelay: 0 });
 678:                                 });
 679:                             }
 680:                         }
 681:                     }
 682: 
 683:                     if (candidateText && ['running','starting','initiated'].includes(data.status)) {
 684:                         progressTextEl.setAttribute('data-processing', 'true');
 685:                     } else {
 686:                         progressTextEl.removeAttribute('data-processing');
 687:                     }
 688: 
 689:                     if (['STEP3','STEP4','STEP5'].includes(stepKey)) {
 690:                         const stepNames = { STEP3: 'Étape 3 — Transitions', STEP4: 'Étape 4 — Audio', STEP5: 'Étape 5 — Tracking' };
 691:                         try { updateGlobalProgressUI(`${stepNames[stepKey] || stepKey}: ${subText}`, percentage, false); } catch (_) {}
 692:                     }
 693:                 } else if (data.status === 'completed' && data.progress_total === 0) {
 694:                     percentage = 0;
 695:                     console.log(`[PROGRESS CALC] ${stepKey}: Completed with no work (0%)`);
 696:                 } else if (data.status === 'completed' && data.progress_total > 0) {
 697:                     percentage = 100;
 698:                     console.log(`[PROGRESS CALC] ${stepKey}: Completed with work (100%)`);
 699:                 } else if (['running', 'starting', 'initiated'].includes(data.status) && data.progress_total === 0) {
 700:                     percentage = 0;
 701:                     console.log(`[PROGRESS CALC] ${stepKey}: Running with no progress tracking (0%)`);
 702:                 }
 703:             } else if (['running', 'starting', 'initiated'].includes(data.status) && data.progress_total === 0) {
 704:                 progressContainer.style.display = 'block';
 705:                 progressBar.style.backgroundColor = 'var(--blue)';
 706:                 progressBar.setAttribute('data-active', 'true');
 707:                 const runningText = (data.progress_text && data.progress_text.trim()) ? data.progress_text : (lastProgressTextByStep[stepKey] || "En cours d'exécution...");
 708:                 if (data.progress_text && data.progress_text.trim()) lastProgressTextByStep[stepKey] = data.progress_text;
 709:                 progressTextEl.textContent = runningText;
 710: 
 711:                 if (['STEP3','STEP4','STEP5'].includes(stepKey)) {
 712:                     const stepNames = { STEP3: 'Étape 3 — Transitions', STEP4: 'Étape 4 — Audio', STEP5: 'Étape 5 — Tracking' };
 713:                     const globalText = `${stepNames[stepKey] || stepKey}: ${runningText || 'En cours...'}`;
 714:                     try { updateGlobalProgressUI(globalText, 0, false); } catch (_) {}
 715:                 }
 716: 
 717:                 if (runningText && runningText.trim()) {
 718:                     progressTextEl.setAttribute('data-processing', 'true');
 719:                 } else {
 720:                     progressTextEl.removeAttribute('data-processing');
 721:                 }
 722:             } else if (data.status === 'completed') {
 723:                 progressContainer.style.display = 'block';
 724:                 progressBar.style.backgroundColor = 'var(--green)';
 725:                 progressBar.removeAttribute('data-active');
 726: 
 727:                 if (data.progress_total === 0) {
 728:                     let noWorkText = "Aucun élément à traiter";
 729:                     if (data.progress_text && data.progress_text.trim() !== "") {
 730:                         noWorkText = data.progress_text;
 731:                     }
 732:                     progressTextEl.textContent = noWorkText;
 733:                     progressBar.style.width = '10%';
 734:                     progressBar.textContent = '✓';
 735:                 } else {
 736:                     let baseCompletionText = `Terminé (${data.progress_current}/${data.progress_total})`;
 737:                     if (data.progress_text && data.progress_text.toLowerCase() !== "terminé" && data.progress_text.trim() !== "") {
 738:                         baseCompletionText = `${data.progress_text} (${data.progress_current}/${data.progress_total})`;
 739:                     }
 740:                     const config = STEPS_CONFIG_FROM_SERVER[stepKey];
 741:                     if (config && config.post_completion_message_ui) {
 742:                         progressTextEl.textContent = `${baseCompletionText}\n${config.post_completion_message_ui}`;
 743:                     } else {
 744:                         progressTextEl.textContent = baseCompletionText;
 745:                     }
 746: 
 747:                     if (['STEP3','STEP4','STEP5'].includes(stepKey)) {
 748:                         const stepNames = { STEP3: 'Étape 3 — Transitions', STEP4: 'Étape 4 — Audio', STEP5: 'Étape 5 — Tracking' };
 749:                         try { updateGlobalProgressUI(`${stepNames[stepKey] || stepKey}: Terminé`, 100, false); } catch (_) {}
 750:                     }
 751:                     delete lastProgressTextByStep[stepKey];
 752:                 }
 753:             } else if (data.status === 'failed') {
 754:                 progressContainer.style.display = 'block';
 755:                 progressBar.style.backgroundColor = 'var(--red)';
 756:                 let failureText = `Échec`;
 757:                 if (data.progress_total > 0) failureText += ` à ${data.progress_current}/${data.progress_total}`;
 758:                 if (data.progress_text) failureText += `: ${data.progress_text}`;
 759:                 progressTextEl.textContent = failureText;
 760:                 progressBar.removeAttribute('data-active');
 761:                 progressTextEl.removeAttribute('data-processing');
 762: 
 763:                 if (['STEP3','STEP4','STEP5'].includes(stepKey)) {
 764:                     const stepNames = { STEP3: 'Étape 3 — Transitions', STEP4: 'Étape 4 — Audio', STEP5: 'Étape 5 — Tracking' };
 765:                     try { updateGlobalProgressUI(`${stepNames[stepKey] || stepKey}: ${failureText}`, percentage, true); } catch (_) {}
 766:                 }
 767:                 delete lastProgressTextByStep[stepKey];
 768:             } else if (data.status === 'starting' || data.status === 'initiated') {
 769:                 progressContainer.style.display = 'block';
 770:                 progressBar.style.width = `0%`;
 771:                 progressBar.textContent = `0%`;
 772:                 progressBar.style.backgroundColor = 'var(--blue)';
 773:                 progressBar.setAttribute('data-active', 'true');
 774:                 progressTextEl.textContent = "Démarrage...";
 775:             } else {
 776:                 progressContainer.style.display = 'none';
 777:                 progressBar.setAttribute('aria-valuenow', 0);
 778:             }
 779: 
 780:             const anyRunning = !!document.querySelector('.step[data-status="running"], .step[data-status="starting"], .step[data-status="initiated"]');
 781:             if (workflowWrapper) {
 782:                 if (anyRunning) {
 783:                     workflowWrapper.classList.add('any-step-running');
 784:                     if (['running','starting','initiated'].includes(data.status)) {
 785:                         workflowWrapper.setAttribute('data-active-step', stepKey);
 786:                     } else if (!document.querySelector(`.step[data-status="running"], .step[data-status="starting"], .step[data-status="initiated"]`)) {
 787:                         workflowWrapper.removeAttribute('data-active-step');
 788:                     }
 789:                 } else {
 790:                     workflowWrapper.classList.remove('any-step-running');
 791:                     workflowWrapper.removeAttribute('data-active-step');
 792:                 }
 793:             }
 794: 
 795:             try {
 796:                 if (!_stepDetailsPanelModulePromise) {
 797:                     _stepDetailsPanelModulePromise = import('./stepDetailsPanel.js');
 798:                 }
 799:                 _stepDetailsPanelModulePromise
 800:                     .then((mod) => {
 801:                         if (mod && typeof mod.refreshStepDetailsPanelIfOpen === 'function') {
 802:                             mod.refreshStepDetailsPanelIfOpen(stepKey);
 803:                         }
 804:                     })
 805:                     .catch((e) => {
 806:                         console.debug('[UI] Step details module not available:', e);
 807:                     });
 808:             } catch (_) {}
 809:         } catch (_) {}
 810: 
 811:         console.groupEnd();
 812:     });
 813: }
 814: 
 815: export function updateCustomSequenceButtonsUI() {
 816:     const hasSelection = getSelectedStepsOrder().length > 0;
 817:     if (dom.runCustomSequenceButton) dom.runCustomSequenceButton.disabled = !hasSelection || getIsAnySequenceRunning();
 818:     if (dom.clearCustomSequenceButton) dom.clearCustomSequenceButton.disabled = !hasSelection || getIsAnySequenceRunning();
 819: }
 820: 
 821: export function updateGlobalProgressUI(text, percentage, isError = false) {
 822:     if(dom.globalProgressAffix) dom.globalProgressAffix.style.display = 'flex';
 823:     if(dom.globalProgressContainer) dom.globalProgressContainer.style.display = 'block';
 824:     if(dom.globalProgressText) {
 825:         dom.globalProgressText.style.display = 'block';
 826:         dom.globalProgressText.textContent = text;
 827:         dom.globalProgressText.style.color = isError ? 'var(--red)' : 'var(--text-secondary)';
 828:     }
 829:     if(dom.globalProgressBar) {
 830:         dom.globalProgressBar.style.width = `${percentage}%`;
 831:         dom.globalProgressBar.textContent = `${percentage}%`;
 832:         dom.globalProgressBar.setAttribute('aria-valuenow', percentage);
 833:         dom.globalProgressBar.style.backgroundColor = isError ? 'var(--red)' : 'var(--green)';
 834:     }
 835: }
 836: 
 837: export function updateSpecificLogUI(logName, path, content, isError = false, errorMessage = '') {
 838:     domBatcher.scheduleUpdate('specific-log-ui', () => {
 839:         const headerText = resolveElement(dom.getSpecificLogHeaderTextPanel, dom.specificLogHeaderTextPanel);
 840:         const pathInfo = resolveElement(dom.getSpecificLogPathInfoPanel, dom.specificLogPathInfoPanel);
 841:         const outputContent = resolveElement(dom.getSpecificLogOutputContentPanel, dom.specificLogOutputContentPanel);
 842:         const specificLogContainer = resolveElement(dom.getSpecificLogContainerPanel, dom.specificLogContainerPanel);
 843:         const mainLogContainer = resolveElement(dom.getMainLogContainerPanel, dom.mainLogContainerPanel);
 844: 
 845:         if(headerText) headerText.textContent = isError ? `Erreur chargement "${logName}"` : `Log Spécifique: "${logName}"`;
 846:         if(pathInfo) pathInfo.textContent = path ? `(Source: ${path})` : "";
 847:         if (isError) {
 848:             if(outputContent) {
 849:                 const escapedErrorMessage = DOMUpdateUtils.escapeHtml(errorMessage);
 850:                 outputContent.innerHTML = `<span class="log-line log-error">${escapedErrorMessage}</span>`;
 851:             }
 852:         } else {
 853:             const styledContent = parseAndStyleLogContent(content);
 854:             if(outputContent) outputContent.innerHTML = styledContent;
 855:         }
 856:         if(specificLogContainer) specificLogContainer.style.display = 'flex';
 857:         if(mainLogContainer) mainLogContainer.style.display = 'none';
 858:         if(outputContent) outputContent.scrollTop = 0;
 859:     });
 860: }
 861: 
 862: const _LOG_LINE_EMPTY_OR_WHITESPACE_PATTERN = /^\s*$/;
 863: 
 864: const _LOG_TIMESTAMP_PATTERN = /^(?:\d{4}-\d{2}-\d{2}|\d{2}:\d{2}:\d{2})/;
 865: const _LOG_ERROR_PATTERN = /(?:erreur|error|échec|failed|exception|critical|fatal|crash)/i;
 866: const _LOG_WARNING_PATTERN = /(?:warning|attention|avertissement|warn|caution|deprecated)/i;
 867: const _LOG_SUCCESS_PATTERN = /(?:success|réussi|terminé|completed|finished|done|✓|✔|ok\b)/i;
 868: const _LOG_INFO_PATTERN = /(?:info|information|démarrage|starting|lancement|initiated|status)/i;
 869: const _LOG_DEBUG_PATTERN = /(?:debug|trace|verbose|détail)/i;
 870: const _LOG_COMMAND_PATTERN = /^(?:commande:|command:|executing:|exécution:|\$|>)/i;
 871: const _LOG_PROGRESS_PATTERN = /(?:\d+%|\d+\/\d+|progress|progression|chargement|loading|téléchargement|downloading)/i;
 872: 
 873: const _LOG_PATTERNS = [
 874:     {
 875:         regex: _LOG_ERROR_PATTERN,
 876:         type: 'error'
 877:     },
 878:     {
 879:         regex: _LOG_WARNING_PATTERN,
 880:         type: 'warning'
 881:     },
 882:     {
 883:         regex: _LOG_SUCCESS_PATTERN,
 884:         type: 'success'
 885:     },
 886:     {
 887:         regex: _LOG_PROGRESS_PATTERN,
 888:         type: 'progress'
 889:     },
 890:     {
 891:         regex: _LOG_COMMAND_PATTERN,
 892:         type: 'command'
 893:     },
 894:     {
 895:         regex: _LOG_INFO_PATTERN,
 896:         type: 'info'
 897:     },
 898:     {
 899:         regex: _LOG_TIMESTAMP_PATTERN,
 900:         type: 'info'
 901:     },
 902:     {
 903:         regex: _LOG_DEBUG_PATTERN,
 904:         type: 'debug'
 905:     }
 906: ];
 907: 
 908: const _COMPILED_LOG_PATTERNS = _LOG_PATTERNS.map(p => ({
 909:     ...p,
 910:     regex: new RegExp(p.regex.source, p.regex.flags)
 911: }));
 912: 
 913: /**
 914:  * Parse and style log content with CSS classes for different log types.
 915:  * Escapes all HTML to prevent XSS.
 916:  * 
 917:  * @param {string} rawContent - Raw log content
 918:  * @returns {string} - Styled HTML content
 919:  */
 920: export function parseAndStyleLogContent(rawContent) {
 921:     if (!rawContent || typeof rawContent !== 'string') {
 922:         return rawContent || '';
 923:     }
 924: 
 925: const lines = rawContent.split('\n');
 926:     const styledLines = new Array(lines.length);
 927: 
 928:     for (let i = 0; i < lines.length; i++) {
 929:         const line = lines[i];
 930:         if (line === '' || _LOG_LINE_EMPTY_OR_WHITESPACE_PATTERN.test(line)) {
 931:             styledLines[i] = line;
 932:             continue;
 933:         }
 934: 
 935:         const escapedLine = DOMUpdateUtils.escapeHtml(line);
 936: 
 937:         let logType = 'default';
 938:         for (let j = 0; j < _COMPILED_LOG_PATTERNS.length; j++) {
 939:             const pattern = _COMPILED_LOG_PATTERNS[j];
 940:             if (pattern.regex.test(line)) {
 941:                 logType = pattern.type;
 942:                 break;
 943:             }
 944:         }
 945: 
 946:         styledLines[i] = logType !== 'default'
 947:             ? `<span class="log-line log-${logType}">${escapedLine}</span>`
 948:             : escapedLine;
 949:     }
 950: 
 951:     return styledLines.join('\n');
 952: }
 953: 
 954: export function updateMainLogOutputUI(htmlContent) {
 955:     if(dom.mainLogOutputPanel) {
 956: const styledContent = parseAndStyleLogContent(htmlContent);
 957:         dom.mainLogOutputPanel.innerHTML = styledContent;
 958:     }
 959:     if(dom.mainLogOutputPanel) dom.mainLogOutputPanel.scrollTop = dom.mainLogOutputPanel.scrollHeight;
 960: 
 961:     if(dom.mainLogContainerPanel) dom.mainLogContainerPanel.style.display = 'flex';
 962:     if(dom.specificLogContainerPanel) dom.specificLogContainerPanel.style.display = 'none';
 963: }
 964: 
 965: export function updateLocalDownloadsListUI(downloadsData) {
 966:     if (!dom.getLocalDownloadsList()) return;
 967:     dom.getLocalDownloadsList().innerHTML = '';
 968:     if (!downloadsData || downloadsData.length === 0) {
 969:         const li = document.createElement('li');
 970:         li.textContent = 'Aucune activité de téléchargement locale récente.';
 971:         li.classList.add('placeholder');
 972:         dom.getLocalDownloadsList().appendChild(li);
 973:         return;
 974:     }
 975: 
 976: const currentDownloadIds = new Set();
 977:     downloadsData.forEach(download => {
 978:         if (download.id) {
 979:             currentDownloadIds.add(download.id);
 980: if (!previousDownloadIds.has(download.id) &&
 981:                 (download.status === 'pending' || download.status === 'downloading')) {
 982:                 console.log(`[SOUND] New CSV download detected: ${download.filename}`);
 983:                 soundEvents.csvDownloadInitiation();
 984: 
 985: const filename = download.filename && download.filename !== 'Détermination en cours...'
 986:                     ? download.filename.substring(0, 30) + (download.filename.length > 30 ? '...' : '')
 987:                     : 'nouveau fichier';
 988:                 showNotification(`Mode Auto: Téléchargement démarré - ${filename}`, "info", 5000);
 989:             }
 990:         }
 991:     });
 992: 
 993: previousDownloadIds = currentDownloadIds;
 994: 
 995:     downloadsData.forEach(download => {
 996:         const li = document.createElement('li');
 997:         li.classList.add(`download-status-${download.status}`);
 998: 
 999:         const escapedOriginalUrl = DOMUpdateUtils.escapeHtml(download.original_url || '');
1000:         const escapedFilename = DOMUpdateUtils.escapeHtml(download.filename || 'Nom inconnu');
1001:         const escapedStatus = DOMUpdateUtils.escapeHtml(download.status || '');
1002:         const escapedDisplayTimestamp = DOMUpdateUtils.escapeHtml(download.display_timestamp || 'N/A');
1003: 
1004:         const timestampSpan = `<span class="timestamp">${escapedDisplayTimestamp}</span>`;
1005:         const filenameSpan = `<span class="filename" title="${escapedOriginalUrl}">${escapedFilename}</span>`;
1006:         let statusText = `Statut: <span class="status-text">${escapedStatus}</span>`;
1007:         let progressText = '';
1008:         if (download.status === 'downloading' && typeof download.progress === 'number') {
1009:             progressText = ` <span class="progress-percentage">(${download.progress}%)</span>`;
1010:         }
1011:         if (download.message) {
1012:             const escapedMessage = DOMUpdateUtils.escapeHtml(download.message);
1013:             const messagePreview = escapedMessage.substring(0, 50) + (escapedMessage.length > 50 ? '...' : '');
1014:             statusText += ` <span class="message" title="${escapedMessage}">${messagePreview}</span>`;
1015:         }
1016:         li.innerHTML = `${timestampSpan} - ${filenameSpan} - ${statusText}${progressText}`;
1017:         dom.getLocalDownloadsList().appendChild(li);
1018:     });
1019: }
1020: 
1021: export function updateClearCacheGlobalButtonState(status, message = '') {
1022:     if (!dom.clearCacheGlobalButton) return;
1023: 
1024:     dom.clearCacheGlobalButton.classList.remove('idle', 'running', 'completed', 'failed');
1025:     const textSpan = dom.clearCacheGlobalButton.querySelector('.button-text');
1026:     const currentStepInfo = getProcessInfo('clear_disk_cache');
1027: 
1028: const isOtherSequenceRunning = getIsAnySequenceRunning() && currentStepInfo?.status !== 'running';
1029: 
1030: 
1031:     switch (status) {
1032:         case 'idle':
1033:             dom.clearCacheGlobalButton.disabled = isOtherSequenceRunning;
1034:             if (textSpan) textSpan.textContent = "Vider le Cache";
1035:             dom.clearCacheGlobalButton.classList.add('idle');
1036:             break;
1037:         case 'starting':
1038:         case 'initiated':
1039:             dom.clearCacheGlobalButton.disabled = true;
1040:             if (textSpan) textSpan.textContent = "Lancement...";
1041:             dom.clearCacheGlobalButton.classList.add('running');
1042:             break;
1043:         case 'running':
1044:             dom.clearCacheGlobalButton.disabled = true;
1045:             if (textSpan) textSpan.textContent = "Nettoyage...";
1046:             dom.clearCacheGlobalButton.classList.add('running');
1047:             break;
1048:         case 'completed':
1049:             dom.clearCacheGlobalButton.disabled = isOtherSequenceRunning;
1050:             if (textSpan) textSpan.textContent = "Cache Vidé";
1051:             dom.clearCacheGlobalButton.classList.add('completed');
1052:             showNotification("Nettoyage du cache disque terminé avec succès.", "success");
1053:             setTimeout(() => updateClearCacheGlobalButtonState('idle'), 5000);
1054:             break;
1055:         case 'failed':
1056:             dom.clearCacheGlobalButton.disabled = isOtherSequenceRunning;
1057:             if (textSpan) textSpan.textContent = "Échec Nettoyage";
1058:             dom.clearCacheGlobalButton.classList.add('failed');
1059:             let notifMessage = "Échec du nettoyage du cache disque.";
1060:             if (message && typeof message === 'string' && message.trim() !== '' && !message.startsWith('<')) {
1061:                 notifMessage += ` Détail: ${message.substring(0,100)}`;
1062:             }
1063:             showNotification(notifMessage, "error");
1064:             setTimeout(() => updateClearCacheGlobalButtonState('idle'), 8000);
1065:             break;
1066:         default:
1067:             dom.clearCacheGlobalButton.disabled = isOtherSequenceRunning;
1068:             if (textSpan) textSpan.textContent = "Vider le Cache";
1069:             dom.clearCacheGlobalButton.classList.add('idle');
1070:     }
1071: }
```
